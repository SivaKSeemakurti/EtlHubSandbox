{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "asa-kyn-001494-dev-eus-001"
		},
		"AzureBlobStorage1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureBlobStorage1'"
		},
		"AzureDataLakeStorage1_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage1'"
		},
		"AzureDataLakeStorage2_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage2'"
		},
		"AzureSqlDatabase1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureSqlDatabase1'"
		},
		"Sftp1_privateKeyContent": {
			"type": "secureString",
			"metadata": "Secure string for 'privateKeyContent' of 'Sftp1'"
		},
		"Sftp1_passPhrase": {
			"type": "secureString",
			"metadata": "Secure string for 'passPhrase' of 'Sftp1'"
		},
		"SftpTririga_privateKeyContent": {
			"type": "secureString",
			"metadata": "Secure string for 'privateKeyContent' of 'SftpTririga'"
		},
		"SftpTririga_passPhrase": {
			"type": "secureString",
			"metadata": "Secure string for 'passPhrase' of 'SftpTririga'"
		},
		"asa-kyn-001494-dev-eus-001-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'asa-kyn-001494-dev-eus-001-WorkspaceDefaultSqlServer'"
		},
		"asdb_etlhub_confirmed_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'asdb_etlhub_confirmed'"
		},
		"linkedService1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'linkedService1'"
		},
		"ln_rfs_pgmp_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'ln_rfs_pgmp'"
		},
		"ls_adls_project_dimension_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'ls_adls_project_dimension'"
		},
		"ls_db2_esa_kyndryl_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'ls_db2_esa_kyndryl'"
		},
		"ls_pgmp_rfs_db_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'ls_pgmp_rfs_db'"
		},
		"tgt_geo_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'tgt_geo'"
		},
		"tririga-prod-sftp_privateKeyContent": {
			"type": "secureString",
			"metadata": "Secure string for 'privateKeyContent' of 'tririga-prod-sftp'"
		},
		"tririga-prod-sftp_passPhrase": {
			"type": "secureString",
			"metadata": "Secure string for 'passPhrase' of 'tririga-prod-sftp'"
		},
		"AzureDataLakeStorage1_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://adlskyn001494deveus001.dfs.core.windows.net"
		},
		"AzureDataLakeStorage2_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://adls4fsoetlhubdevuseast.dfs.core.windows.net"
		},
		"Sftp1_properties_typeProperties_host": {
			"type": "string",
			"defaultValue": "52.116.64.97"
		},
		"Sftp1_properties_typeProperties_userName": {
			"type": "string",
			"defaultValue": "sftpNewCoProd"
		},
		"SftpTririga_properties_typeProperties_host": {
			"type": "string",
			"defaultValue": "52.116.64.97"
		},
		"SftpTririga_properties_typeProperties_userName": {
			"type": "string",
			"defaultValue": "sftpNewCoProd"
		},
		"asa-kyn-001494-dev-eus-001-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://adls4fsoetlhubdevuseast.dfs.core.windows.net"
		},
		"ls_adls_project_dimension_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://adls4fsoetlhubdevuseast.dfs.core.windows.net"
		},
		"tgt_geo_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://adls4fsoetlhubdevuseast.dfs.core.windows.net"
		},
		"tririga-prod-sftp_properties_typeProperties_host": {
			"type": "string",
			"defaultValue": "52.116.64.97"
		},
		"tririga-prod-sftp_properties_typeProperties_userName": {
			"type": "string",
			"defaultValue": "sftpNewCoProd"
		},
		"Trigger_BALIN_MAster_Seq_properties_BALSEQ0000_MASTER_EN_SEQ_parameters_BLOCK_SIZE": {
			"type": "int",
			"defaultValue": 2500
		},
		"Trigger_BALIN_MAster_Seq_properties_BALSEQ0000_MASTER_EN_SEQ_parameters_PROCESS_CTR_IND": {
			"type": "string",
			"defaultValue": "'Y'"
		},
		"Trigger_BALIN_MAster_Seq_properties_BALSEQ0000_MASTER_EN_SEQ_parameters_PARAM_START_TS": {
			"type": "string",
			"defaultValue": "''"
		},
		"Trigger_BALIN_MAster_Seq_properties_BALSEQ0000_MASTER_EN_SEQ_parameters_PARAM_END_TS": {
			"type": "string",
			"defaultValue": "''"
		},
		"Trigger_BALIN_MAster_Seq_properties_BALSEQ0000_MASTER_EN_SEQ_parameters_REPLICATION_OFFSET": {
			"type": "int",
			"defaultValue": 50
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/BALSEQ0000_MASTER_EN_SEQ')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job creation:28-06-2022\nJob name: BALSEQ0000_MASTER_EN_SEQ\nCreatedBy: Varaprasad",
				"activities": [
					{
						"name": "MASTER_SEQ_CLEAN_UP",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "BALSEQ0100_MASTER_SEQ_CLEAN_UP",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"IdentifyCleanUpObjects": {},
									"CleanUpRecords": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "MASTER_SEQ_START",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "MASTER_SEQ_CLEAN_UP",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "BALSEQ0110_MASTER_SEQ_START",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"getExecutionSignatureData": {},
									"storeExecution": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "MASTER_SET_LIMITS",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "MASTER_SEQ_START",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "BALSEQ0120_MASTER_SET_LIMITS",
								"type": "DataFlowReference",
								"parameters": {
									"BLOCK_SIZE": {
										"value": "@pipeline().parameters.BLOCK_SIZE",
										"type": "Expression"
									},
									"PROCESS_CTR_IND": {
										"value": "'@{pipeline().parameters.PROCESS_CTR_IND}'",
										"type": "Expression"
									},
									"PARAM_START_TS": {
										"value": "'@{pipeline().parameters.PARAM_START_TS}'",
										"type": "Expression"
									},
									"PARAM_END_TS": {
										"value": "'@{pipeline().parameters.PARAM_END_TS}'",
										"type": "Expression"
									},
									"REPLICATION_OFFSET": {
										"value": "@pipeline().parameters.REPLICATION_OFFSET",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getParamTS": {},
									"updateTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "PgMP_Dimension_Pipeline",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "MASTER_SET_LIMITS",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "PgMP_Dimension_Pipeline",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"BLOCK_SIZE": {
						"type": "int",
						"defaultValue": 1000
					},
					"PROCESS_CTR_IND": {
						"type": "string"
					},
					"PARAM_START_TS": {
						"type": "string"
					},
					"PARAM_END_TS": {
						"type": "string"
					},
					"REPLICATION_OFFSET": {
						"type": "int",
						"defaultValue": 15
					}
				},
				"folder": {
					"name": "PGMP/Sequence"
				},
				"annotations": [],
				"lastPublishTime": "2022-07-18T10:50:52Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/BALSEQ0100_MASTER_SEQ_CLEAN_UP')]",
				"[concat(variables('workspaceId'), '/dataflows/BALSEQ0110_MASTER_SEQ_START')]",
				"[concat(variables('workspaceId'), '/dataflows/BALSEQ0120_MASTER_SET_LIMITS')]",
				"[concat(variables('workspaceId'), '/pipelines/PgMP_Dimension_Pipeline')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/BAL_GEO_DIM001')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "TMF_GEO",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"sqlReaderQuery": "with\n\t-- This sub query gets the limits of the data that will be processed\n\tFIL as (\n\t\tselect\n\t\t\tETL_EXCTN_ID\n\t\tfrom\n\t\t\tPGMPDM.[ZAUX_ETL_EXCTN]\n\t\twhere\n\t\t\tIS_CURR_IND = 'Y'\n\t),\n\t\n\t-- This subquery gets the JOB ID. Returns -1 is the value is unknown\n\tJOB as (\n\t\tselect\n\t\t\tcoalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\n\t\tfrom\n\t\t\tPGMPDM.ZAUX_ETL_JOBS\n\t\twhere\n\t\t\tETL_JOB_NM = '#DSJobName#'\t\t\n\t),\n\t\n\t-- This subquery gets the SRC_SYS_ID. Returns -1 is the value is unknown\n\tSYS as (\n\t\tselect\n\t\t\tcoalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\n\t\tfrom\n\t\t\tPGMPDM.SRC_SYS_DIM\n\t\twhere\n\t\t\tSRC_SYS_CD = 'TMF'\t\t\t\n\t)\n\t\n\t\n\n\nselect\n\tG.GEO_DIM_UID,\n\tG.IOT_RGN_CD,\n\tG.IOT_RGN_NM,\n\tG.IMT_RGN_CD,\n\tG.IMT_SRGN_NM,\n\tG.SRGN_CD,\n\tG.SRGNCTRY_CD,\n\tG.SRGNCTRY_NM,\n\tG.IOTSORT_NUM,\n\tG.GEO_CD,\n\tG.GEO_TYP_CD,\n\tG.GMR_RGN_CD,\n\tG.GMR_RGN_NM,\n\tG.IOT_SHORT_NM,\n\tG.IMT_SHORT_NM,\n\tJOB.ETL_JOB_ID,\n\tFIL.ETL_EXCTN_ID,\n\tSYS.SRC_SYS_DIM_UID\nfrom\n\tPGMPDM.GEO_DIM_TMF G\n\tleft join\n\tJOB\n\ton 1 = 1\n\tleft join\n\tFIL\n\ton 1 = 1\n\tleft join\n\tSYS\n\ton 1 = 1",
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".txt"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "AzureSqlTable3",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "dl_tgt_geo",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-07-18T10:51:48Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/AzureSqlTable3')]",
				"[concat(variables('workspaceId'), '/datasets/dl_tgt_geo')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CP_SUMMRY_SEQ')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "CP_CNTRCT_CHNG_RPT",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0001_CP_CNTRCT_CHNG_RPT",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"Srccntrchnagrpt": {},
									"tgtcntrctchngrpt": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "Cp_PCR_RPT",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "CP_CNTRCT_CHNG_RPT",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0002_CP_PCR_RPT",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"getSourceDataforPCR": {},
									"upsertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "CP_RISK_RPT",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "Cp_PCR_RPT",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0006_CP_RISK_RPT",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"getSourceData": {},
									"upsertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "PGMP"
				},
				"annotations": [],
				"lastPublishTime": "2022-07-18T10:48:32Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0001_CP_CNTRCT_CHNG_RPT')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0002_CP_PCR_RPT')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0006_CP_RISK_RPT')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Kitty-Pipeline1')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "kitty",
				"activities": [
					{
						"name": "Kitty-Test",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Test",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "sPoolKyn001494",
								"type": "BigDataPoolReference"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-07-18T10:48:37Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/Test')]",
				"[concat(variables('workspaceId'), '/bigDataPools/sPoolKyn001494')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PL_EMAIL_ATTACHMENT_TEST')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Web1",
						"type": "WebActivity",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": "https://prod-37.westus.logic.azure.com:443/workflows/8d8c446f15cc420fb463e0b9770fe094/triggers/manual/paths/invoke?api-version=2016-06-01&sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0&sig=M-j7XM7xGl5O0cddQucoPzYyvzC40Dx6-Qr0_Dr1VaE",
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "POST",
							"headers": {
								"Content-Type": "application/json"
							},
							"body": {
								"ErrorMessage": "",
								"JobStatus": "Success",
								"dataFactoryName": "",
								"pipelineName": "pl_Country_Load",
								"message": "",
								"receiver": "siva.seemakurti@kyndryl.com",
								"filepath": "/etlhubfilestorage/reject/Country(kyndryl).csv"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "ETLHub"
				},
				"annotations": [],
				"lastPublishTime": "2022-08-03T13:53:55Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PL_EMAIL_WITH_ATTACHMENT')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Get Metadata1",
						"type": "GetMetadata",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "Attachment2",
								"type": "DatasetReference",
								"parameters": {}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "DelimitedTextReadSettings"
							}
						}
					},
					{
						"name": "ForEach1",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Get Metadata1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Get Metadata1').output.childItems",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "Set variable1",
									"type": "SetVariable",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"variableName": "filefullpath",
										"value": {
											"value": "@concat('/etlhubfilestorage/reject/',item().name)",
											"type": "Expression"
										}
									}
								},
								{
									"name": "Web1",
									"type": "WebActivity",
									"dependsOn": [
										{
											"activity": "Set variable1",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"url": "https://prod-37.westus.logic.azure.com:443/workflows/8d8c446f15cc420fb463e0b9770fe094/triggers/manual/paths/invoke?api-version=2016-06-01&sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0&sig=M-j7XM7xGl5O0cddQucoPzYyvzC40Dx6-Qr0_Dr1VaE",
										"connectVia": {
											"referenceName": "AutoResolveIntegrationRuntime",
											"type": "IntegrationRuntimeReference"
										},
										"method": "POST",
										"headers": {
											"Content-Type": "application/json"
										},
										"body": {
											"value": "{'ErrorMessage':'',\n'JobStatus':'Success',\n'dataFactoryName':'',\n'pipelineName':@{pipeline().Pipeline},\n'message':'',\n'receiver':'siva.seemakurtikyndryl.com',\n'filepath':@{variables('filefullpath')}}",
											"type": "Expression"
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"variables": {
					"filefullpath": {
						"type": "String"
					}
				},
				"folder": {
					"name": "ETLHub"
				},
				"annotations": [],
				"lastPublishTime": "2022-08-03T13:57:35Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Attachment2')]",
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PL_ETLHUB_DIMENSION_LOAD')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "nb_Employee_Load",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "nb_dht_employee_load",
								"type": "NotebookReference"
							},
							"snapshot": true
						}
					},
					{
						"name": "nb_Market_Load",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "nb_dht_market_load",
								"type": "NotebookReference"
							},
							"snapshot": true
						}
					},
					{
						"name": "Employee_Load_Success_Notification",
						"type": "WebActivity",
						"dependsOn": [
							{
								"activity": "nb_Employee_Load",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": "https://lpemailalert.azurewebsites.net:443/api/email_alerts_job_status_new/triggers/manual/invoke?api-version=2022-05-01&sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0&sig=H6UgWY_Zx65ADu1blVhcNMBt3KLkG6X9lNIHj7QRiEo",
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "POST",
							"headers": {
								"Content-Type": "Application/Json"
							},
							"body": {
								"value": "{\n        \"dataFactoryName\": \"@{pipeline().DataFactory}\",\n        \"message\": \"This is an automated mail notification from Azure\",\n        \"pipelineName\": \"@{pipeline().Pipeline}\",\n        \"receiver\": \"siva.seemakurti@kyndryl.com\",\n        \"ErrorMessage\":@{activity('nb_Employee_Load').output},\n        \"JobStatus\" : \"Success\"\n}",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Employee_Load_Failure_Notification",
						"type": "WebActivity",
						"dependsOn": [
							{
								"activity": "nb_Employee_Load",
								"dependencyConditions": [
									"Failed"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": "https://lpemailalert.azurewebsites.net:443/api/email_alerts_job_status_new/triggers/manual/invoke?api-version=2022-05-01&sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0&sig=H6UgWY_Zx65ADu1blVhcNMBt3KLkG6X9lNIHj7QRiEo",
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "POST",
							"headers": {
								"Content-Type": "Application/Json"
							},
							"body": {
								"value": "{\n        \"dataFactoryName\": \"@{pipeline().DataFactory}\",\n        \"message\": \"This is an automated mail notification from Azure\",\n        \"pipelineName\": \"@{pipeline().Pipeline}\",\n        \"receiver\": \"siva.seemakurti@kyndryl.com\",\n        \"ErrorMessage\":@{activity('nb_Employee_Load').output}\n}",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Market_Load_Success_Notification",
						"type": "WebActivity",
						"dependsOn": [
							{
								"activity": "nb_Market_Load",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": "https://lpemailalert.azurewebsites.net:443/api/email_alerts_job_status_new/triggers/manual/invoke?api-version=2022-05-01&sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0&sig=H6UgWY_Zx65ADu1blVhcNMBt3KLkG6X9lNIHj7QRiEo",
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "POST",
							"headers": {
								"Content-Type": "Application/Json"
							},
							"body": {
								"value": "{\n        \"dataFactoryName\": \"@{pipeline().DataFactory}\",\n        \"message\": \"This is an automated mail notification from Azure\",\n        \"pipelineName\": \"@{pipeline().Pipeline}\",\n        \"receiver\": \"siva.seemakurti@kyndryl.com\",\n        \"ErrorMessage\":@{activity('nb_Market_Load').output},\n        \"JobStatus\" : \"Success\"\n}",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Market_Load_Failure_Notification",
						"type": "WebActivity",
						"dependsOn": [
							{
								"activity": "nb_Market_Load",
								"dependencyConditions": [
									"Failed"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": "https://lpemailalert.azurewebsites.net:443/api/email_alerts_job_status_new/triggers/manual/invoke?api-version=2022-05-01&sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0&sig=H6UgWY_Zx65ADu1blVhcNMBt3KLkG6X9lNIHj7QRiEo",
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "POST",
							"headers": {
								"Content-Type": "Application/Json"
							},
							"body": {
								"value": "{\n        \"dataFactoryName\": \"@{pipeline().DataFactory}\",\n        \"message\": \"This is an automated mail notification from Azure\",\n        \"pipelineName\": \"@{pipeline().Pipeline}\",\n        \"receiver\": \"siva.seemakurti@kyndryl.com\",\n        \"ErrorMessage\":@{activity('nb_Market_Load').output}\n}",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "ETLHub"
				},
				"annotations": [],
				"lastPublishTime": "2022-07-18T10:48:40Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/nb_dht_employee_load')]",
				"[concat(variables('workspaceId'), '/notebooks/nb_dht_market_load')]",
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PL_TRIRIGA_CAMPUS_SFTP_PULL')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_TRIRIGA_CAMPUS_SFTP_PULL",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"TririgaRemoteServerSite": {},
									"AzureDataLakeStorage": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "Notebook1",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Data flow1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "nb_dht_campus",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "sPool001494New",
								"type": "BigDataPoolReference"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "ETLHub"
				},
				"annotations": [],
				"lastPublishTime": "2022-08-03T12:03:02Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/DF_TRIRIGA_CAMPUS_SFTP_PULL')]",
				"[concat(variables('workspaceId'), '/notebooks/nb_dht_campus')]",
				"[concat(variables('workspaceId'), '/bigDataPools/sPool001494New')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PL_TRIRIGA_LOCATION_SFTP_PULL')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_TRIRIGA_LOCATION_SFTP_PULL",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"TririgaRemoteServer": {
										"IWTRIRIGABuildingQuery": "IWTRIRIGABuildingQuery.csv"
									},
									"AzureDataLakeStorage": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "ETLHub"
				},
				"annotations": [],
				"lastPublishTime": "2022-07-29T06:53:34Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/DF_TRIRIGA_LOCATION_SFTP_PULL')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PL_TRIRIGA_OFFICE_SPACE_SFTP_PULL')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_TRIRIGA_OfficeSpace_SFTP_PULL",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"TririgaRemoteServerSite": {},
									"AzureDataLakeStorage": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "ETLHub"
				},
				"annotations": [],
				"lastPublishTime": "2022-08-12T10:35:03Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/DF_TRIRIGA_OfficeSpace_SFTP_PULL')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PL_TRIRIGA_SITE_SFTP_PULL')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_TRIRIGA_SITE_SFTP_PULL",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"TririgaRemoteServerSite": {},
									"AzureDataLakeStorage": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "ETLHub"
				},
				"annotations": [],
				"lastPublishTime": "2022-08-03T12:02:54Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/DF_TRIRIGA_SITE_SFTP_PULL')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PgMP_CPSummary_and_Field_Audit_Pipeline')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "CNTRCT CHNG RPT",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0001_CP_CNTRCT_CHNG_RPT",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"Srccntrchnagrpt": {},
									"tgtcntrctchngrpt": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "RISK RPT",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "PCR RPT",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0006_CP_RISK_RPT",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"getSourceData": {},
									"upsertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "CSTM KYNDRYL ONLY AUDIT RPT",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "CSTM PUBLIC AUDIT RPT",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0008_CSTM_DATA_KYNDRTL_ONLY_AUDIT_RPT",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"getSourceData": {},
									"upsertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "CNTRY CSTM KYNDRYL ONLY AUDIT RPT",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "CNTRY CSTM PUBLIC AUDIT RPT",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0010_CNTRY_CSTM_DATA_KYNDRTL_ONLY_AUDIT_RPT",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"getSourceData": {},
									"upsertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "CSTM PUBLIC AUDIT RPT",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0007_CSTM_DATA_PUBLIC_AUDIT_RPT",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"getSourceData": {},
									"upsertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "CNTRY CSTM PUBLIC AUDIT RPT",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "CSTM KYNDRYL ONLY AUDIT RPT",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0009_CNTRY_CSTM_DATA_PUBLIC_AUDIT_RPT",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"getSourceData": {},
									"upsertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "PCR RPT",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "CNTRCT CHNG RPT",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0002_CP_PCR_RPT",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"getSourceDataforPCR": {},
									"upsertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "PGMP"
				},
				"annotations": [],
				"lastPublishTime": "2022-07-18T10:48:47Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0001_CP_CNTRCT_CHNG_RPT')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0006_CP_RISK_RPT')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0008_CSTM_DATA_KYNDRTL_ONLY_AUDIT_RPT')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0010_CNTRY_CSTM_DATA_KYNDRTL_ONLY_AUDIT_RPT')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0007_CSTM_DATA_PUBLIC_AUDIT_RPT')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0009_CNTRY_CSTM_DATA_PUBLIC_AUDIT_RPT')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0002_CP_PCR_RPT')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PgMP_Dimension_Pipeline')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "GEO DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0260_GEO_DIM",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"srcgeo": {},
									"srtdtgtgeolkp": {},
									"Tgtupdgeodimtable": {},
									"InsTgtGeoDim": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "ACCTRPTS DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0010_ACCTRPTS_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "ACCTSPFDIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0020_ACCTSPFC_DIM",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"appfunacctspcf": {},
									"LkpTgtAcctspcfDim": {},
									"TgtAcctspfDimupd": {},
									"TgtpgmdmacctspfDim": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "ACTN_DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0030_ACTN_DIM",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"appfunacct": {},
									"LkpTgtAcctDim": {},
									"TgtAcctspfDimupd": {},
									"TgtpgmdmacctspfDim": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "BASELN DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0060_BASELN_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "BOTE DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0070_BOTE_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "BIZDEVMNT DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0600_BIZDEVMT_DIM",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "Df_BALD280_IBMCOMMS_DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0280_IBMCOMMS_DIM",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"srcappfunibmcomms": {},
									"srtdtgtibmcommlkp": {},
									"Tgtupdibmcommdimtable": {},
									"InsTgtibmcommsDim": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "Issue_dim",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0290_ISSUE_DIM",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"srcissuedim": {},
									"srtgtissuedimlkp": {},
									"srcissuesmpl": {},
									"Tgtupdibmcommdimtable": {},
									"InsTgtibmcommsDim": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "PROC_CSTM_DATA_IBM_CLIENT",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0292_PROC_CSTM_DATA_PUBLIC_DIM",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"srcproccstmpublicdata": {},
									"tgtproccstmpublicdatalkp": {},
									"Tgtupdibmcommdimtable": {},
									"InsTgtibmcommsDim": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "DF_BALD0291_PROC_CSTM_IBM_DATA_DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0291_PROC_CSTM_IBM_DATA_DIM",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"srcproccstmpublicdata": {},
									"tgtproccstmibmdatalkp": {},
									"Tgtupdibmcommdimtable": {},
									"InsTgtibmcommsDim": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "CLNT DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0130_CLNT_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "CNTRCT CHG DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0150_CNTRCT_CHG_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "CNTRCT DLVRBL DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0160_CNTRCT_DLVRBL_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "CNTRCT TYPE DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0170_CNTRCT_TYPE_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "CRNCY DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0180_CRNCY_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "CUSTSTSFCN DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0200_CUSTSTSFCN_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "DATE CHG RSN DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0205_DATE_CHG_RSN_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceRqstData": {},
									"getSourcePCRData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "DELD DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0210_DELD_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "DELIVOPS DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0220_DELIVOPS_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "DOCMTCTL DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0230_DOCMTCTL_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "FINNC DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0240_FINNC_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "FMLCRSPN DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0250_FMLCRSPN_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "GOVRNNC DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0270_GOVRNNC_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "ADTNL INFRMTN DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0040_ADTNL_INFRMTN_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "CNTRCT DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD1090_CNTRCT_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "APPRVL STATE DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0050_APPRVL_STATE_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "PRICE TYPE CD DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0345_PRICE_TYPE_CD_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "PRFRMNC DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0340_PRFRMNC_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "CATLG BRAND DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0080_CATLG_BRAND_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "CSTM FIELDS DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0190_CSTM_FIELDS_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"current_user": {
						"type": "string",
						"defaultValue": "pgmpetl"
					}
				},
				"folder": {
					"name": "PGMP"
				},
				"annotations": [],
				"lastPublishTime": "2022-07-18T10:48:50Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0260_GEO_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0010_ACCTRPTS_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0020_ACCTSPFC_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0030_ACTN_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0060_BASELN_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0070_BOTE_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0600_BIZDEVMT_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0280_IBMCOMMS_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0290_ISSUE_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0292_PROC_CSTM_DATA_PUBLIC_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0291_PROC_CSTM_IBM_DATA_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0130_CLNT_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0150_CNTRCT_CHG_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0160_CNTRCT_DLVRBL_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0170_CNTRCT_TYPE_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0180_CRNCY_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0200_CUSTSTSFCN_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0205_DATE_CHG_RSN_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0210_DELD_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0220_DELIVOPS_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0230_DOCMTCTL_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0240_FINNC_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0250_FMLCRSPN_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0270_GOVRNNC_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0040_ADTNL_INFRMTN_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD1090_CNTRCT_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0050_APPRVL_STATE_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0345_PRICE_TYPE_CD_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0340_PRFRMNC_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0080_CATLG_BRAND_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0190_CSTM_FIELDS_DIM')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PgMP_Test_Pipeline')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Job1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "00:30:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0080_CATLG_BRAND_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0190_CSTM_FIELDS_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "Data flow2",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0330_PCR_CRC_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									},
									"hash_columns": "'PCR_CRC_DIM_UID, PRPSL_PRICE_TERM_DIM_UID, CMPLXTY_DIM_UID, RQST_STAT_DIM_UID, RQST_TYPE_DIM_UID, RQST_RCVD_DT, PRPSL_RQST_DT, CLNT_RVSD_PRPSL_RQSTD_DT, PRPSL_SENT_TO_CLNT_DT, PRPSL_SENT_TO_CLNT_DT_WD, PRPSL_ACCPTD_DT, PRPSL_ACCPTD_DT_WD, PRPSL_REJCT_DT, RQST_IMPLMNT_CMPLTD_DT, CLNT_RVSD_RQST_IMPL_CMPL_DT, IMPLMNT_READY_CUST_ACCEPT_DT, IMPLMNT_READY_CUST_ACCEPT_DT_WD, PRPSL_EXP_DT, PRPSL_DSPSN_DT_WD, ON_HOLD_DT, WITHDRWN_DT, DELD_DT, LATEST_PRPSL_RQSTD_DT, LATEST_IMPLMNT_RQSTD_DT, PRPSL_OR_REJCT_DT, MERGED_DT, CLNT_ACCNTG_CD, CLNT_CNTCT_NM, CLNT_AFFCT_AREA_TXT, AGRMT_REF_DTL_TXT, SRVC_TYPE_TXT, CLNT_APPRVL_NUM, RFS_GROUP_NM, SRVC_PRVDR_ORGNZN_TXT, PREDEFND_SRVCS_DESC, SIEBEL_OPPRTNTY_NUM, BTT_NUM, SLTN_OWNR_TXT, DLVR_CRDNTR_TXT, SLTN_DSGNR_TXT, CLAIM_ACCNT_CD, CLAIM_WORK_ITEM_CD, RSRC_SUMRY_PRSNL_INVOLV_IND, RSRC_SUMRY_SERVER_COM_IND, RSRC_SUMRY_STRG_COM_IND, RSRC_SUMRY_HW_COM_IND, RSRC_SUMRY_SOFTWR_COM_IND, RJCT_RSN_CD, RJCT_SRC_CD, RJCT_EXPLNN_TXT, PRICE_TYPE_CD, RSN_CD, PURCHS_ORDR_NM, UNSLCTD_RQST_IND, CNTRCT_CHNG_RQRD_IND, OATS_RCD_RRQRD_IND, NON_STD_T_AND_C_IND, TRVL_INVC_IND, TERMNTN_CHRGS_IND, RNWL_MNTH_CD, BYPSS_SLTN_DSGN_IND, PRPSL_OWNR_TXT, ETL_JOB_ID, SRC_SYS_DIM_UID, HAS_PRNT_RFS_IND, PRNT_PROC_DIM_UID, PRJCT_DIM_UID, L30_OFFRNG_ID_1,  L30_OFFRNG_ID_2,  L30_OFFRNG_ID_3, CLNT_PRPSL_RQSTD_FIRST_TM_DT,  CLNT_RVSD_PRPSL_RQSTD_FIRST_TM_DT,  PRPSL_SENT_TO_CLNT_FIRST_TM_DT,  CLNT_RQSTD_IMPLMTN_CMPLT_FIRST_TM_DT,  CLNT_RVSD_RQSTD_IMPLMTN_CMPLT_FIRST_TM_DT,  IMPLMTN_READY_FOR_CUST_ACCPTNC_FIRST_TM_DT, GEO_DIM_UID, ORGNTG_ORG, PRICER_NM, SOLTN_TEAM, ERO_CHCKD_DT,  EA_CHCKD_DT, CE_CHCKD_DT, EXPORT_REG, ENVMNTL_AFF, CMPLNCE_ENGG, GROSS_TCP_CTRY_CD, GROSS_TCP_GEO_DIM_UID'"
								},
								"datasetParameters": {
									"getSourcePCRData": {},
									"getSourceCRCData": {},
									"clientProposalRqstDate": {},
									"clientRevisedProposalRqstDate": {},
									"proposalSentToClientDate": {},
									"clientRqstImplementCompletionDate": {},
									"clientRevisedRqstImplementCompletionDate": {},
									"implementationReadyForCADate": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "Data flow3",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0340_PRFRMNC_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "Data flow4",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0345_PRICE_TYPE_CD_DIM",
								"type": "DataFlowReference",
								"parameters": {
									"current_user": {
										"value": "'@{pipeline().parameters.current_user}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"getSourceData": {},
									"getLookupData": {},
									"updateTable": {},
									"insertTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"current_user": {
						"type": "string"
					}
				},
				"folder": {
					"name": "PGMP"
				},
				"annotations": [],
				"lastPublishTime": "2022-07-18T11:33:53Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0080_CATLG_BRAND_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0190_CSTM_FIELDS_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0330_PCR_CRC_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0340_PRFRMNC_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0345_PRICE_TYPE_CD_DIM')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 1')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Notebook_R",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Notebook_R",
								"type": "NotebookReference"
							},
							"snapshot": true
						}
					},
					{
						"name": "Notebook_R_copy1",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Notebook_R",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Notebook_R",
								"type": "NotebookReference"
							},
							"snapshot": true
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-07-18T10:49:03Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/Notebook_R')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline _Df_GEO_DIM')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "pipeline_Geo_dim",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0260_GEO_DIM",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"srcgeo": {},
									"srtdtgtgeolkp": {},
									"Tgtupdgeodimtable": {},
									"InsTgtGeoDim": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "PGMP"
				},
				"annotations": [],
				"lastPublishTime": "2022-07-18T10:49:07Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0260_GEO_DIM')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PipelineTest')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Copy data1",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"sqlReaderQuery": "select * from DBXDH.DHT_CUSTOMER\nwhere CUSTOMER_NO='9549530'",
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"sink": {
								"type": "SqlPoolSink",
								"preCopyScript": "DELETE FROM DBXDH.DHT_CUSTOMER WHERE CUSTOMER_NO='9549530'",
								"allowCopyCommand": true,
								"copyCommandSettings": {}
							},
							"enableStaging": true,
							"stagingSettings": {
								"linkedServiceName": {
									"referenceName": "linkedService1",
									"type": "LinkedServiceReference"
								}
							}
						},
						"inputs": [
							{
								"referenceName": "AzureSqlTable2",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "SqlPoolTable1",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Web1",
						"type": "WebActivity",
						"dependsOn": [
							{
								"activity": "Copy data1",
								"dependencyConditions": [
									"Failed"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": "https://lpemailalert.azurewebsites.net:443/api/email_alerts_job_status_new/triggers/manual/invoke?api-version=2022-05-01&sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0&sig=H6UgWY_Zx65ADu1blVhcNMBt3KLkG6X9lNIHj7QRiEo",
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "POST",
							"headers": {
								"Content-Type": "Application/Json"
							},
							"body": {
								"value": "{\n        \"dataFactoryName\": \"@{pipeline().DataFactory}\",\n        \"message\": \"This is an automated mail notification from Azure\",\n        \"pipelineName\": \"@{pipeline().Pipeline}\",\n        \"receiver\": \"siva.seemakurti@kyndryl.com\",\n        \"ErrorMessage\":@{activity('Copy data1').output}\n}",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Web1_copy1",
						"type": "WebActivity",
						"dependsOn": [
							{
								"activity": "Copy data1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": "https://lpemailalert.azurewebsites.net:443/api/email_alerts_job_status_new/triggers/manual/invoke?api-version=2022-05-01&sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0&sig=H6UgWY_Zx65ADu1blVhcNMBt3KLkG6X9lNIHj7QRiEo",
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "POST",
							"headers": {
								"Content-Type": "Application/Json"
							},
							"body": {
								"value": "{\n        \"dataFactoryName\": \"@{pipeline().DataFactory}\",\n        \"message\": \"This is an automated mail notification from Azure\",\n        \"pipelineName\": \"@{pipeline().Pipeline}\",\n        \"receiver\": \"siva.seemakurti@kyndryl.com\",\n        \"ErrorMessage\":@{activity('Copy data1').output},\n        \"JobStatus\" : \"Success\"\n}",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-07-18T10:49:14Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/AzureSqlTable2')]",
				"[concat(variables('workspaceId'), '/datasets/SqlPoolTable1')]",
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/linkedService1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Sample_pl')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Dataflow_copy1",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-07-18T10:58:51Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/Dataflow_copy1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Sample_pl_sell_cycle')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "df_SELL_CYCLE",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-07-18T10:58:57Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/df_SELL_CYCLE')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/customer_dimension_load')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "scdType2_Generict_Dimension_Load",
								"type": "DataFlowReference",
								"parameters": {
									"NaturalKey": {
										"value": "'@{pipeline().parameters.NaturalKey}'",
										"type": "Expression"
									},
									"NonkeyColumns": {
										"value": "'@{pipeline().parameters.NonKeyColumns}'",
										"type": "Expression"
									},
									"EXTRACT_DT": "currentTimestamp()",
									"REC_START_DT": "currentTimestamp()",
									"SurrogateKey": {
										"value": "'@{pipeline().parameters.Key}'",
										"type": "Expression"
									},
									"DimTableName": "'DHT_CUSTOMER'"
								},
								"datasetParameters": {
									"genericInput": {
										"FolderName": {
											"value": "@pipeline().parameters.folder_name",
											"type": "Expression"
										},
										"fileName": {
											"value": "@pipeline().parameters.file_name",
											"type": "Expression"
										}
									},
									"genericDimensionTable": {
										"DimTableName1": {
											"value": "@pipeline().parameters.DimTableName",
											"type": "Expression"
										},
										"Key1": "'key'"
									},
									"Update4ChangedRecords": {
										"DimTableName1": {
											"value": "@pipeline().parameters.DimTableName",
											"type": "Expression"
										},
										"Key1": "'key'"
									},
									"Insert4ChangedRows": {
										"DimTableName1": {
											"value": "@pipeline().parameters.DimTableName",
											"type": "Expression"
										},
										"Key1": "'key'"
									},
									"SinkInsert4NewRows": {
										"DimTableName1": {
											"value": "@pipeline().parameters.DimTableName",
											"type": "Expression"
										},
										"Key1": "'key'"
									},
									"Update4SoftDeletedRows": {
										"DimTableName1": {
											"value": "@pipeline().parameters.DimTableName",
											"type": "Expression"
										},
										"Key1": "'key'"
									}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"NaturalKey": {
						"type": "string",
						"defaultValue": "CUSTOMER_NO"
					},
					"NonKeyColumns": {
						"type": "string",
						"defaultValue": "FINANCIAL_COUNTRY_CD,CUSTOMER_DESC,GBG_ID"
					},
					"Key": {
						"type": "string",
						"defaultValue": "CUSTOMER_KEY"
					},
					"DimTableName": {
						"type": "string",
						"defaultValue": "DHT_CUSTOMER"
					},
					"folder_name": {
						"type": "string",
						"defaultValue": "customer"
					},
					"file_name": {
						"type": "string",
						"defaultValue": "customer_data.csv"
					}
				},
				"annotations": [],
				"lastPublishTime": "2022-07-18T10:59:04Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/scdType2_Generict_Dimension_Load')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/customer_load_woversion')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "scdType2_Generict_Dimension_Load_WOVersion",
								"type": "DataFlowReference",
								"parameters": {
									"NaturalKey": "'CUSTOMER_NO'",
									"NonkeyColumns": "'FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME'",
									"EXTRACT_DT": "currentTimestamp()",
									"REC_START_DT": "currentTimestamp()",
									"DimTableName": "'DHT_CUSTOMER1'",
									"UpdateKey": {
										"value": "'@{pipeline().parameters.Key1}'",
										"type": "Expression"
									},
									"SurrogateKey": "'CUSTOMER_KEY'"
								},
								"datasetParameters": {
									"genericInput": {
										"FolderName": {
											"value": "@pipeline().parameters.Folder_Name",
											"type": "Expression"
										},
										"fileName": {
											"value": "@pipeline().parameters.File_Name",
											"type": "Expression"
										}
									},
									"genericDimensionTable": {
										"DimTableName1": {
											"value": "@pipeline().parameters.Dimension_Table_Name",
											"type": "Expression"
										},
										"Key1": {
											"value": "@pipeline().parameters.Key1",
											"type": "Expression"
										}
									},
									"Update4ChangedRecords": {
										"DimTableName1": {
											"value": "@pipeline().parameters.Dimension_Table_Name",
											"type": "Expression"
										},
										"Key1": {
											"value": "@pipeline().parameters.Key1",
											"type": "Expression"
										}
									},
									"Insert4ChangedRows": {
										"DimTableName1": {
											"value": "@pipeline().parameters.Dimension_Table_Name",
											"type": "Expression"
										},
										"Key1": "'key'"
									},
									"SinkInsert4NewRows": {
										"DimTableName1": {
											"value": "@pipeline().parameters.Dimension_Table_Name",
											"type": "Expression"
										},
										"Key1": "'key'"
									},
									"Update4SoftDeletedRows": {
										"DimTableName1": {
											"value": "@pipeline().parameters.Dimension_Table_Name",
											"type": "Expression"
										},
										"Key1": "'key'"
									}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"Key1": {
						"type": "string",
						"defaultValue": "CUSTOMER_KEY"
					},
					"Folder_Name": {
						"type": "string",
						"defaultValue": "customer"
					},
					"Dimension_Table_Name": {
						"type": "string",
						"defaultValue": "DHT_CUSTOMER1"
					},
					"File_Name": {
						"type": "string",
						"defaultValue": "customer_data.csv"
					}
				},
				"annotations": [],
				"lastPublishTime": "2022-07-18T10:59:19Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/scdType2_Generict_Dimension_Load_WOVersion')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pgmp_pipeline')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Test Job",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "00:30:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0020_ACCTSPFC_DIM",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"appfunacctspcf": {},
									"LkpTgtAcctspcfDim": {},
									"TgtAcctspfDimupd": {},
									"TgtpgmdmacctspfDim": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-07-18T10:49:19Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0020_ACCTSPFC_DIM')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pgmpcdctestpipeline')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "DF_BALD0297_BTT_DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0297_BTT_DIM",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"srcBttdim": {},
									"tgtBttdimlkp": {},
									"Tgtupdibmcommdimtable": {},
									"InsTgtibmcommsDim": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "DF_BALD0296_EVALN_ANSWR_DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0296_EVALN_ANSWR_DIM",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"srcEvalnansdim": {},
									"tgtEvalnansdimlkp": {},
									"Tgtupdibmcommdimtable": {},
									"InsTgtibmcommsDim": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "DF_BALD0298_SRVC_PRVDR_DIM",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_BALD0298_SRVC_PRVDR_DIM",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"srcBttdim": {},
									"tgtBttdimlkp": {},
									"Tgtupdibmcommdimtable": {},
									"InsTgtibmcommsDim": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "PGMP"
				},
				"annotations": [],
				"lastPublishTime": "2022-07-18T10:49:25Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0297_BTT_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0296_EVALN_ANSWR_DIM')]",
				"[concat(variables('workspaceId'), '/dataflows/DF_BALD0298_SRVC_PRVDR_DIM')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pl_deltalake_customer_dimension')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Notebook1",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "xfrm_deltalake_Customer_Dimension",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "sPoolKyn001494",
								"type": "BigDataPoolReference"
							}
						}
					},
					{
						"name": "Notebook2",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Notebook1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Notebook 9",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "sPoolKyn001494",
								"type": "BigDataPoolReference"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-07-18T10:49:29Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/xfrm_deltalake_Customer_Dimension')]",
				"[concat(variables('workspaceId'), '/bigDataPools/sPoolKyn001494')]",
				"[concat(variables('workspaceId'), '/notebooks/Notebook 9')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/project_dim_no_parameters')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "scdType2_Project_Dimension_Latest",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "bkp_scdType2_Project_Dimension_Latest",
								"type": "DataFlowReference",
								"parameters": {
									"NaturalKey": "'PROJECT_ID'",
									"NonkeyColumns": "'FINANCIAL_COUNTRY_CD,LEDGER_CD,OFFERING_COMPONENT_CD,OPPORTUNITY_NUM,PROJECT_DESC,SIGNINGS_CD,SIGNINGS_DESC,BUSINESS_TYPE_CD,BUSINESS_TYPE_DESC,PROJECT_STATUS_CD,PROJECT_STATUS_DESC,PROJECT_CUSTOMER_NO,PROJECT_CREATION_DATE,ACCOUTNING_DIVISION,RESPONSIBLE_SERV_OFFICE'",
									"EXTRACT_DT": "currentTimestamp()",
									"REC_START_DT": "currentTimestamp()",
									"SurrogateKey": "'PROJECT_KEY'",
									"DimTableName": "'DHT_PROJECT_SIV'"
								},
								"datasetParameters": {
									"genericInput": {
										"FolderName": "project"
									},
									"genericDimensionTable": {
										"DimTableName1": "DHT_PROJECT_SIV"
									},
									"Update4ChangedRecords": {
										"DimTableName1": "DHT_PROJECT_SIV"
									},
									"Insert4ChangedRows": {
										"DimTableName1": "DHT_PROJECT_SIV"
									},
									"SinkInsert4NewRows": {
										"DimTableName1": "DHT_PROJECT_SIV"
									},
									"sink1": {
										"DimTableName1": "DHT_PROJECT_SIV"
									}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-07-18T10:59:43Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/bkp_scdType2_Project_Dimension_Latest')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/project_dimension_load')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "scdType2_Generict_Dimension_Load",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "scdType2_Generict_Dimension_Load",
								"type": "DataFlowReference",
								"parameters": {
									"NaturalKey": {
										"value": "'@{pipeline().parameters.NaturalKey}'",
										"type": "Expression"
									},
									"NonkeyColumns": {
										"value": "'@{pipeline().parameters.NonkeyColumns}'",
										"type": "Expression"
									},
									"EXTRACT_DT": "currentTimestamp()",
									"REC_START_DT": "currentTimestamp()",
									"SurrogateKey": {
										"value": "'@{pipeline().parameters.SurrogateKey}'",
										"type": "Expression"
									},
									"DimTableName": {
										"value": "'@{pipeline().parameters.Table_Name1}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"genericInput": {
										"FolderName": {
											"value": "@pipeline().parameters.FolderName",
											"type": "Expression"
										},
										"fileName": "EnterDimensionFileName"
									},
									"genericDimensionTable": {
										"DimTableName1": {
											"value": "@pipeline().parameters.Table_Name1",
											"type": "Expression"
										},
										"Key1": "'key'"
									},
									"Update4ChangedRecords": {
										"DimTableName1": {
											"value": "@pipeline().parameters.Table_Name1",
											"type": "Expression"
										},
										"Key1": "'key'"
									},
									"Insert4ChangedRows": {
										"DimTableName1": {
											"value": "@pipeline().parameters.Table_Name1",
											"type": "Expression"
										},
										"Key1": "'key'"
									},
									"SinkInsert4NewRows": {
										"DimTableName1": {
											"value": "@pipeline().parameters.Table_Name1",
											"type": "Expression"
										},
										"Key1": "'key'"
									},
									"Update4SoftDeletedRows": {
										"DimTableName1": "EnterDimensionTableName",
										"Key1": "'key'"
									}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"SurrogateKey": {
						"type": "string",
						"defaultValue": "PROJECT_KEY"
					},
					"NaturalKey": {
						"type": "string",
						"defaultValue": "PROJECT_ID"
					},
					"NonkeyColumns": {
						"type": "string",
						"defaultValue": "FINANCIAL_COUNTRY_CD,LEDGER_CD,OFFERING_COMPONENT_CD,OPPORTUNITY_NUM,PROJECT_DESC,SIGNINGS_CD,SIGNINGS_DESC,BUSINESS_TYPE_CD,BUSINESS_TYPE_DESC,PROJECT_STATUS_CD,PROJECT_STATUS_DESC,PROJECT_CUSTOMER_NO,PROJECT_CREATION_DATE,ACCOUTNING_DIVISION,RESPONSIBLE_SERV_OFFICE"
					},
					"FolderName": {
						"type": "string",
						"defaultValue": "project"
					},
					"Table_Name1": {
						"type": "string",
						"defaultValue": "DHT_PROJECT_SIV"
					}
				},
				"annotations": [],
				"lastPublishTime": "2022-07-18T11:00:05Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/scdType2_Generict_Dimension_Load')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sanple_proj_pl')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Dataflow1",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-07-18T11:00:24Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/Dataflow1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Attachment')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "linkedService1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "Country(kyndryl).csv",
						"folderPath": "reject",
						"container": "etlhubfilestorage"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": [
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/linkedService1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Attachment2')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_adls_project_dimension",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "reject",
						"fileSystem": "etlhubfilestorage"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": [
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_adls_project_dimension')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlTable1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "asdb_etlhub_confirmed",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": "DBXDH",
					"table": "DHT_SELL_CYCLE"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asdb_etlhub_confirmed')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlTable2')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "asdb_etlhub_confirmed",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [
					{
						"name": "CUSTOMER_KEY",
						"type": "int",
						"precision": 10
					},
					{
						"name": "VERSION",
						"type": "int",
						"precision": 10
					},
					{
						"name": "CUSTOMER_NO",
						"type": "varchar"
					},
					{
						"name": "FINANCIAL_COUNTRY_CD",
						"type": "varchar"
					},
					{
						"name": "GBG_ID",
						"type": "varchar"
					},
					{
						"name": "CUSTOMER_NAME",
						"type": "varchar"
					},
					{
						"name": "CURRENT_IND",
						"type": "varchar"
					},
					{
						"name": "EXTRACT_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "REC_START_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "REC_END_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "SOURCE_SYSTEM",
						"type": "varchar"
					},
					{
						"name": "REC_CHECKSUM",
						"type": "varchar"
					},
					{
						"name": "REC_STATUS",
						"type": "varchar"
					},
					{
						"name": "IMG_LST_UPD_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "IMG_CREATED_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "DATA_IND",
						"type": "varchar"
					},
					{
						"name": "ACTIVE_IN_SOURCE_IND",
						"type": "char"
					}
				],
				"typeProperties": {
					"schema": "DBXDH",
					"table": "DHT_CUSTOMER"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asdb_etlhub_confirmed')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlTable3')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_pgmp_rfs_db",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DBXDH_DHTS_SELL_CYCLE')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "asdb_etlhub_confirmed",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": "DBXDH",
					"table": "DHTS_SELL_CYCLE"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asdb_etlhub_confirmed')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DBXDH_DHT_PROJECT_SIV')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "asdb_etlhub_confirmed",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [
					{
						"name": "PROJECT_KEY",
						"type": "int",
						"precision": 10
					},
					{
						"name": "PROJECT_VERSION",
						"type": "int",
						"precision": 10
					},
					{
						"name": "PROJECT_ID",
						"type": "varchar"
					},
					{
						"name": "FINANCIAL_COUNTRY_CD",
						"type": "varchar"
					},
					{
						"name": "LEDGER_CD",
						"type": "varchar"
					},
					{
						"name": "OFFERING_COMPONENT_CD",
						"type": "varchar"
					},
					{
						"name": "OPPORTUNITY_NUM",
						"type": "varchar"
					},
					{
						"name": "PROJECT_DESC",
						"type": "varchar"
					},
					{
						"name": "SIGNINGS_CD",
						"type": "varchar"
					},
					{
						"name": "SIGNINGS_DESC",
						"type": "varchar"
					},
					{
						"name": "BUSINESS_TYPE_CD",
						"type": "varchar"
					},
					{
						"name": "BUSINESS_TYPE_DESC",
						"type": "varchar"
					},
					{
						"name": "PROJECT_STATUS_CD",
						"type": "varchar"
					},
					{
						"name": "PROJECT_STATUS_DESC",
						"type": "varchar"
					},
					{
						"name": "PROJECT_CUSTOMER_NO",
						"type": "varchar"
					},
					{
						"name": "PROJECT_CREATION_DATE",
						"type": "date"
					},
					{
						"name": "ACCOUTNING_DIVISION",
						"type": "varchar"
					},
					{
						"name": "RESPONSIBLE_SERV_OFFICE",
						"type": "varchar"
					},
					{
						"name": "CURRENT_IND",
						"type": "varchar"
					},
					{
						"name": "EXTRACT_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "REC_START_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "REC_END_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "SOURCE_SYSTEM",
						"type": "varchar"
					},
					{
						"name": "REC_CHECKSUM",
						"type": "varchar"
					},
					{
						"name": "REC_STATUS",
						"type": "varchar"
					},
					{
						"name": "IMG_LST_UPD_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "IMG_CREATED_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "DATA_IND",
						"type": "varchar"
					},
					{
						"name": "ACTIVE_IN_SOURCE_IND",
						"type": "char"
					}
				],
				"typeProperties": {
					"schema": "DBXDH",
					"table": "DHT_PROJECT_SIV"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asdb_etlhub_confirmed')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DBXDH_DHT_SELL_CYCLE')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "asdb_etlhub_confirmed",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": "DBXDH",
					"table": "DHTS_SELL_CYCLE"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asdb_etlhub_confirmed')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dataset')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_adls_project_dimension",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "sell_cycle.csv",
						"fileSystem": "project"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "SIEBEL_SALES_STAGE_CODE",
						"type": "String"
					},
					{
						"name": "SIEBEL_SALES_STAGE_NAME",
						"type": "String"
					},
					{
						"name": "SSM_STEP_NO",
						"type": "String"
					},
					{
						"name": "SSM_STEP_NAME",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_adls_project_dimension')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DelimitedText1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_adls_project_dimension",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "sell_cycle.csv",
						"fileSystem": "project"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "SIEBEL_SALES_STAGE_CODE",
						"type": "String"
					},
					{
						"name": "SIEBEL_SALES_STAGE_NAME",
						"type": "String"
					},
					{
						"name": "SSM_STEP_NO",
						"type": "String"
					},
					{
						"name": "SSM_STEP_NAME",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_adls_project_dimension')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DelimitedText2')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_adls_project_dimension",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "sell_cycle_tgt.csv",
						"fileSystem": "project"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "SIEBEL_SALES_STAGE_CODE",
						"type": "String"
					},
					{
						"name": "SIEBEL_SALES_STAGE_NAME",
						"type": "String"
					},
					{
						"name": "SSM_STEP_NO",
						"type": "String"
					},
					{
						"name": "SSM_STEP_NAME",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_adls_project_dimension')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DelimitedText3')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "tgt_geo",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/tgt_geo')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DelimitedText4')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "asa-kyn-001494-dev-eus-001-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asa-kyn-001494-dev-eus-001-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DelimitedText5')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "SftpTririga",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "SftpLocation"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/SftpTririga')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IWTRIRIGABuildingQuery')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "SftpTririga",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"IWTRIRIGABuildingQuery": {
						"type": "String",
						"defaultValue": "IWTRIRIGABuildingQuery.csv"
					}
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "SftpLocation",
						"fileName": {
							"value": "@dataset().IWTRIRIGABuildingQuery",
							"type": "Expression"
						},
						"folderPath": "/GlobalDir/CustomerFiles/tririga"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "CAMPUS_ID",
						"type": "String"
					},
					{
						"name": "BUILDING_ID",
						"type": "String"
					},
					{
						"name": "BUILDING_NAME",
						"type": "String"
					},
					{
						"name": "BUILDING_STATUS",
						"type": "String"
					},
					{
						"name": "BUILDING_OWNERSHIP_ID",
						"type": "String"
					},
					{
						"name": "BUILDING_OWNERSHIP_NAME",
						"type": "String"
					},
					{
						"name": "BUILDING_OWNERSHIP_DESCRIPTION",
						"type": "String"
					},
					{
						"name": "BUILDING_PRIMARY_USE_ID",
						"type": "String"
					},
					{
						"name": "BUILDING_PRIMARY_USE_NAME",
						"type": "String"
					},
					{
						"name": "BUILDING_PRIMARY_USE_DESCRIPTION",
						"type": "String"
					},
					{
						"name": "BUILDING_SECONDARY_USE_ID",
						"type": "String"
					},
					{
						"name": "BUILDING_SECONDARY_USE_NAME",
						"type": "String"
					},
					{
						"name": "BUILDING_SECONDARY_USE_DESCRIPTION",
						"type": "String"
					},
					{
						"name": "RENTABLE_AREA",
						"type": "String"
					},
					{
						"name": "ACTIVE_YEAR",
						"type": "String"
					},
					{
						"name": "ACTUAL_RETIREMENT_INACTIVATION",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/SftpTririga')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IWTRIRIGABuildingQueryCSV')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage2",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "extract/Tririga/building",
						"fileSystem": "etlhubfilestorage"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IWTRIRIGACampusQuery')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "SftpTririga",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "SftpLocation",
						"fileName": "IWTRIRIGACampusQuery.csv",
						"folderPath": "/GlobalDir/CustomerFiles/tririga"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "CAMPUS_ID",
						"type": "String"
					},
					{
						"name": "CAMPUS_NAME",
						"type": "String"
					},
					{
						"name": "CAMPUS_STATUS",
						"type": "String"
					},
					{
						"name": "CAMPUS_GROUP",
						"type": "String"
					},
					{
						"name": "SITE_ID",
						"type": "String"
					},
					{
						"name": "PHYSICAL_GEO",
						"type": "String"
					},
					{
						"name": "WORLD_REGION_CODE",
						"type": "String"
					},
					{
						"name": "WORLD_REGION_NAME",
						"type": "String"
					},
					{
						"name": "MARKET_TEAM_REGION_CODE",
						"type": "String"
					},
					{
						"name": "MARKET_TEAM_REGION_NAME",
						"type": "String"
					},
					{
						"name": "WORK_LOCATION_CODE",
						"type": "String"
					},
					{
						"name": "ADDRESS",
						"type": "String"
					},
					{
						"name": "CITY",
						"type": "String"
					},
					{
						"name": "STATE_PROVINCE_ID",
						"type": "String"
					},
					{
						"name": "POSTAL_CODE",
						"type": "String"
					},
					{
						"name": "COUNTRY_CODE",
						"type": "String"
					},
					{
						"name": "LATITUDE",
						"type": "String"
					},
					{
						"name": "LONGITUDE",
						"type": "String"
					},
					{
						"name": "UTC_OFFSET",
						"type": "String"
					},
					{
						"name": "ICU_TIME_ZONE",
						"type": "String"
					},
					{
						"name": "PEOPLE_HOUSED_FLAG",
						"type": "String"
					},
					{
						"name": "REMOTE_SUPPORT_FLAG",
						"type": "String"
					},
					{
						"name": "PRIMARY_CAMPUS_USE_ID",
						"type": "String"
					},
					{
						"name": "PRIMARY_CAMPUS_USE_NAME",
						"type": "String"
					},
					{
						"name": "PRIMARY_CAMPUS_USE_DESCRIPTION",
						"type": "String"
					},
					{
						"name": "CAMPUS_OWNERSHIP",
						"type": "String"
					},
					{
						"name": "CAMPUS_ACTIVATION_YEAR",
						"type": "String"
					},
					{
						"name": "CAMPUS_INACTIVATION_YEAR",
						"type": "String"
					},
					{
						"name": "CAMPUS_MODIFIED_DATE_AND_TIME",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/SftpTririga')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IWTRIRIGACampusQueryCSV')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage2",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "extract/Tririga/campus",
						"fileSystem": "etlhubfilestorage"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IWTRIRIGASiteQuery')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "SftpTririga",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "SftpLocation",
						"fileName": "IWTRIRIGASiteQuery.csv",
						"folderPath": "/GlobalDir/CustomerFiles/tririga"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "SITE_ID",
						"type": "String"
					},
					{
						"name": "SITE_NAME",
						"type": "String"
					},
					{
						"name": "SITE_STATUS",
						"type": "String"
					},
					{
						"name": "GEOGRAPHY_ID",
						"type": "String"
					},
					{
						"name": "GEOGRAPHY_NAME",
						"type": "String"
					},
					{
						"name": "REGION_ID",
						"type": "String"
					},
					{
						"name": "REGION_NAME",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/SftpTririga')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IWTRIRIGASiteQueryCSV')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage2",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "IWTRIRIGASiteQueryCSV",
						"folderPath": "extract/Tririga/Site",
						"fileSystem": "etlhubfilestorage"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/P_ACCTRPTS_DIM')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_pgmp_rfs_db",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [
					{
						"name": "ACCTRPTS_DIM_UID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "PRJCT_ID",
						"type": "varchar"
					},
					{
						"name": "ACCTRPTS_REM_TXT",
						"type": "varchar"
					},
					{
						"name": "SRC_CRETD_TMS",
						"type": "datetime2",
						"scale": 6
					},
					{
						"name": "SRC_CRETD_USER_ID",
						"type": "varchar"
					},
					{
						"name": "SRC_UPDTD_TMS",
						"type": "datetime2",
						"scale": 6
					},
					{
						"name": "SRC_UPDTD_USER_ID",
						"type": "varchar"
					},
					{
						"name": "ROW_STAT_CD",
						"type": "char"
					},
					{
						"name": "SRC_SYS_DIM_UID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "ETL_JOB_ID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "ETL_EXCTN_ID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "DM_CRETD_TMS",
						"type": "datetime2",
						"scale": 6
					},
					{
						"name": "DM_CRETD_USER_ID",
						"type": "varchar"
					},
					{
						"name": "DM_UPDTD_TMS",
						"type": "datetime2",
						"scale": 6
					},
					{
						"name": "DM_UPDTD_USER_ID",
						"type": "varchar"
					},
					{
						"name": "ORIG_ORG",
						"type": "varchar"
					}
				],
				"typeProperties": {
					"schema": "PGMPDM",
					"table": "ACCTRPTS_DIM"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/P_ZAUX_DATE_TRIGGERS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_pgmp_rfs_db",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [
					{
						"name": "PROC_ID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "CRETD_DT",
						"type": "date"
					},
					{
						"name": "RCVD_DT",
						"type": "date"
					},
					{
						"name": "PRPSL_SENT_TO_CLNT_DT",
						"type": "date"
					},
					{
						"name": "PRPSL_DSPSN_DT",
						"type": "date"
					},
					{
						"name": "PRPSL_ACCPTD_DT",
						"type": "date"
					},
					{
						"name": "IMPLMTN_READY_DT",
						"type": "date"
					},
					{
						"name": "IMPLMTN_CLOSE_OUT_DT",
						"type": "date"
					}
				],
				"typeProperties": {
					"schema": "PGMPDM",
					"table": "ZAUX_DATE_TRIGGERS"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RISK_RPT')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_pgmp_rfs_db",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [
					{
						"name": "CNTRCT_DIM_UID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "CONTRACT",
						"type": "varchar"
					},
					{
						"name": "UNIQUE_ID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "CLIENT_REFERENCE",
						"type": "varchar"
					},
					{
						"name": "DISPLAY_ID",
						"type": "varchar"
					},
					{
						"name": "GLBL_BUY_GRP_ID",
						"type": "varchar"
					},
					{
						"name": "PARENT_UNIQUE_ID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "PARENT_DISPLAY_ID",
						"type": "varchar"
					},
					{
						"name": "PARENT_TITLE",
						"type": "varchar"
					},
					{
						"name": "PROC_TITLE",
						"type": "varchar"
					},
					{
						"name": "WKFLW_DEF_ID",
						"type": "varchar"
					},
					{
						"name": "INIT_WKFLW_DEF_ID_DESCR",
						"type": "varchar"
					},
					{
						"name": "PROC_DESCRIPTION",
						"type": "varchar"
					},
					{
						"name": "CURRENT_WORKFLOW",
						"type": "varchar"
					},
					{
						"name": "CURRENT_WKFLW_STEP",
						"type": "varchar"
					},
					{
						"name": "CURRENT_STEP_SEQ",
						"type": "int",
						"precision": 10
					},
					{
						"name": "STEP_DESCRIPTION",
						"type": "varchar"
					},
					{
						"name": "STATE_DESC",
						"type": "varchar"
					},
					{
						"name": "STATUS_DESC",
						"type": "varchar"
					},
					{
						"name": "ASSIGNED_TO",
						"type": "varchar"
					},
					{
						"name": "AUDIENCE",
						"type": "varchar"
					},
					{
						"name": "PRIORITY_TXT",
						"type": "varchar"
					},
					{
						"name": "CONDITION",
						"type": "varchar"
					},
					{
						"name": "REGION",
						"type": "varchar"
					},
					{
						"name": "REQUESTOR",
						"type": "varchar"
					},
					{
						"name": "CREATED_BY",
						"type": "varchar"
					},
					{
						"name": "CREATED_DATE",
						"type": "datetime2",
						"scale": 6
					},
					{
						"name": "LAST_UPDATED",
						"type": "datetime2",
						"scale": 6
					},
					{
						"name": "RISK_RESPONSE_DUE_DATE",
						"type": "date"
					},
					{
						"name": "REVISED_RISK_RESPONSE_DUE_DATE",
						"type": "date"
					},
					{
						"name": "DUE_IN_DAY_CNT",
						"type": "int",
						"precision": 10
					},
					{
						"name": "OVERDUE",
						"type": "varchar"
					},
					{
						"name": "COMPLETION_DATE",
						"type": "date"
					},
					{
						"name": "COMPLETION_REASON",
						"type": "varchar"
					},
					{
						"name": "COUNTRY",
						"type": "varchar"
					},
					{
						"name": "RISK_SOURCE",
						"type": "varchar"
					},
					{
						"name": "REMARKS",
						"type": "varchar"
					},
					{
						"name": "ORIGINATING_ORG",
						"type": "varchar"
					},
					{
						"name": "RISK_OWNER",
						"type": "varchar"
					},
					{
						"name": "PROBABILITY",
						"type": "int",
						"precision": 10
					},
					{
						"name": "PROBABILITY_SMPL",
						"type": "varchar"
					},
					{
						"name": "IMPACT",
						"type": "char"
					},
					{
						"name": "RESPONSE_PLAN",
						"type": "varchar"
					},
					{
						"name": "LOCAL_CURRENCY",
						"type": "varchar"
					},
					{
						"name": "IMPACT_AMOUNT_LOCAL_CURRENCY",
						"type": "decimal",
						"precision": 15,
						"scale": 2
					},
					{
						"name": "GS_RISK_ID",
						"type": "varchar"
					},
					{
						"name": "BUSINESS_CONTROLS_RISK",
						"type": "varchar"
					},
					{
						"name": "WWBCIT_REFERENCE",
						"type": "varchar"
					},
					{
						"name": "RISK_ANLYSS_DUE_DT",
						"type": "date"
					},
					{
						"name": "RVSD_RISK_ANLYSS_DUE_DT",
						"type": "date"
					},
					{
						"name": "RISK_RSPNS_TYPE_CD",
						"type": "varchar"
					},
					{
						"name": "RISK_OCCURRED_CD",
						"type": "char"
					},
					{
						"name": "RISK_CLOSE_RSN_TXT",
						"type": "varchar"
					},
					{
						"name": "SRC_SYS_CD",
						"type": "varchar"
					},
					{
						"name": "LEGACY_UNIQUE_ID",
						"type": "varchar"
					},
					{
						"name": "LEGACY_DISPLAYED_ID",
						"type": "varchar"
					},
					{
						"name": "LATEST_IBM_ONLY_NOTE",
						"type": "varchar"
					},
					{
						"name": "LATEST_IBM_AND_CLIENT_NOTE",
						"type": "varchar"
					},
					{
						"name": "CNTRCT_TYPE_DESC",
						"type": "varchar"
					},
					{
						"name": "PROC_TYPE_ID",
						"type": "varchar"
					},
					{
						"name": "PROC_TYPE_DESC",
						"type": "varchar"
					},
					{
						"name": "ON_HOLD_CD",
						"type": "varchar"
					},
					{
						"name": "WITHDRWN_DIM_UID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "WITHDRWN_DT",
						"type": "date"
					},
					{
						"name": "ORGNZN_DIM_UID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "IBM_ONLY_TEXT_1",
						"type": "varchar"
					},
					{
						"name": "IBM_ONLY_TEXT_2",
						"type": "varchar"
					},
					{
						"name": "IBM_ONLY_TEXT_3",
						"type": "varchar"
					},
					{
						"name": "IBM_ONLY_TEXT_4",
						"type": "varchar"
					},
					{
						"name": "IBM_ONLY_TEXT_5",
						"type": "varchar"
					},
					{
						"name": "IBM_ONLY_TEXT_6",
						"type": "varchar"
					},
					{
						"name": "IBM_ONLY_DATE_1",
						"type": "date"
					},
					{
						"name": "IBM_ONLY_DATE_2",
						"type": "date"
					},
					{
						"name": "IBM_ONLY_DATE_3",
						"type": "date"
					},
					{
						"name": "IBM_ONLY_DATE_4",
						"type": "date"
					},
					{
						"name": "IBM_ONLY_DATE_5",
						"type": "date"
					},
					{
						"name": "IBM_ONLY_DATE_6",
						"type": "date"
					},
					{
						"name": "PUBLIC_TEXT_1",
						"type": "varchar"
					},
					{
						"name": "PUBLIC_TEXT_2",
						"type": "varchar"
					},
					{
						"name": "PUBLIC_TEXT_3",
						"type": "varchar"
					},
					{
						"name": "PUBLIC_TEXT_4",
						"type": "varchar"
					},
					{
						"name": "PUBLIC_TEXT_5",
						"type": "varchar"
					},
					{
						"name": "PUBLIC_TEXT_6",
						"type": "varchar"
					},
					{
						"name": "PUBLIC_DATE_1",
						"type": "date"
					},
					{
						"name": "PUBLIC_DATE_2",
						"type": "date"
					},
					{
						"name": "PUBLIC_DATE_3",
						"type": "date"
					},
					{
						"name": "PUBLIC_DATE_4",
						"type": "date"
					},
					{
						"name": "PUBLIC_DATE_5",
						"type": "date"
					},
					{
						"name": "PUBLIC_DATE_6",
						"type": "date"
					},
					{
						"name": "STATE_OF_ORGANIZATION",
						"type": "varchar"
					}
				],
				"typeProperties": {
					"schema": "PGMPDM",
					"table": "RISK_RPT"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ResodbOfficeSpaceTririgaStaging')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage2",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "extract/Tririga/OfficeSpace",
						"fileSystem": "etlhubfilestorage"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "CAMPUS_ID",
						"type": "String"
					},
					{
						"name": "BLDG_ID",
						"type": "String"
					},
					{
						"name": "FLOOR_ID",
						"type": "String"
					},
					{
						"name": "SPACE_ID",
						"type": "String"
					},
					{
						"name": "SPACE_DESCR",
						"type": "String"
					},
					{
						"name": "AREA",
						"type": "String"
					},
					{
						"name": "CAPACITY",
						"type": "String"
					},
					{
						"name": "SPACE_CLASS_CODE1",
						"type": "String"
					},
					{
						"name": "SPACE_CLASS_CODE2",
						"type": "String"
					},
					{
						"name": "OSCRE_CODE",
						"type": "String"
					},
					{
						"name": "WORKPOINT",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SqlPoolTable1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlPoolTable",
				"schema": [
					{
						"name": "CUSTOMER_KEY",
						"type": "int",
						"precision": 10
					},
					{
						"name": "VERSION",
						"type": "int",
						"precision": 10
					},
					{
						"name": "CUSTOMER_NO",
						"type": "varchar"
					},
					{
						"name": "FINANCIAL_COUNTRY_CD",
						"type": "varchar"
					},
					{
						"name": "GBG_ID",
						"type": "varchar"
					},
					{
						"name": "CUSTOMER_NAME",
						"type": "varchar"
					},
					{
						"name": "CURRENT_IND",
						"type": "varchar"
					},
					{
						"name": "EXTRACT_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "REC_START_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "REC_END_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "SOURCE_SYSTEM",
						"type": "varchar"
					},
					{
						"name": "REC_CHECKSUM",
						"type": "varchar"
					},
					{
						"name": "REC_STATUS",
						"type": "varchar"
					},
					{
						"name": "IMG_LST_UPD_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "IMG_CREATED_DT",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "DATA_IND",
						"type": "varchar"
					},
					{
						"name": "ACTIVE_IN_SOURCE_IND",
						"type": "char"
					}
				],
				"typeProperties": {
					"schema": "DBXDH",
					"table": "DHT_CUSTOMER"
				},
				"sqlPool": {
					"referenceName": "dsqlpoolKyn001494DevEtlHubEUS001",
					"type": "SqlPoolReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/dsqlpoolKyn001494DevEtlHubEUS001')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/account_reports_lookup')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "asa-kyn-001494-dev-eus-001-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "output-lookup",
						"fileSystem": "pgmp-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asa-kyn-001494-dev-eus-001-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/account_reports_source')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "asa-kyn-001494-dev-eus-001-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "output-source",
						"fileSystem": "pgmp-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asa-kyn-001494-dev-eus-001-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/appfun_acctrpts')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "linkedService1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "ACCTRPTS_202205311910.csv",
						"container": "pgmp-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "PROC_ID",
						"type": "String"
					},
					{
						"name": "CNTRY",
						"type": "String"
					},
					{
						"name": "PROJECT_ID",
						"type": "String"
					},
					{
						"name": "DUE_DATE",
						"type": "String"
					},
					{
						"name": "REVISD_DUE_DATE",
						"type": "String"
					},
					{
						"name": "REMARKS",
						"type": "String"
					},
					{
						"name": "ORIG_ORG",
						"type": "String"
					},
					{
						"name": "CREATED_TS",
						"type": "String"
					},
					{
						"name": "CREATED_USERID",
						"type": "String"
					},
					{
						"name": "UPDATED_TS",
						"type": "String"
					},
					{
						"name": "UPDATED_USERID",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/linkedService1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dl_tgt_geo')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "tgt_geo",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "GEO1.csv",
						"folderPath": "Output",
						"fileSystem": "prasad"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/tgt_geo')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsAzureSqlDBEtlhubGenericDimension')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "asdb_etlhub_confirmed",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"DimTableName1": {
						"type": "String",
						"defaultValue": "EnterDimensionTableName"
					},
					"Key1": {
						"type": "string",
						"defaultValue": "'key'"
					}
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": "DBXDH",
					"table": {
						"value": "@dataset().DimTableName1",
						"type": "Expression"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asdb_etlhub_confirmed')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsProjectDimensionRaw')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ls_adls_project_dimension",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"FolderName": {
						"type": "string",
						"defaultValue": "EnterDimensionFolderName"
					},
					"fileName": {
						"type": "String",
						"defaultValue": "EnterDimensionFileName"
					}
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().fileName",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().FolderName",
							"type": "Expression"
						}
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_adls_project_dimension')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/output_ar')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "asa-kyn-001494-dev-eus-001-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "new-ar",
						"fileSystem": "pgmp-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asa-kyn-001494-dev-eus-001-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pgmpdm_acctrpts_dim')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "linkedService1",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "pgmp-account_reports"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "ACCTRPTS_DIM_202205301234.csv",
						"container": "pgmp-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "ACCTRPTS_DIM_UID",
						"type": "String"
					},
					{
						"name": "PRJCT_ID",
						"type": "String"
					},
					{
						"name": "ACCTRPTS_REM_TXT",
						"type": "String"
					},
					{
						"name": "SRC_CRETD_TMS",
						"type": "String"
					},
					{
						"name": "SRC_CRETD_USER_ID",
						"type": "String"
					},
					{
						"name": "SRC_UPDTD_TMS",
						"type": "String"
					},
					{
						"name": "SRC_UPDTD_USER_ID",
						"type": "String"
					},
					{
						"name": "ROW_STAT_CD",
						"type": "String"
					},
					{
						"name": "SRC_SYS_DIM_UID",
						"type": "String"
					},
					{
						"name": "ETL_JOB_ID",
						"type": "String"
					},
					{
						"name": "ETL_EXCTN_ID",
						"type": "String"
					},
					{
						"name": "DM_CRETD_TMS",
						"type": "String"
					},
					{
						"name": "DM_CRETD_USER_ID",
						"type": "String"
					},
					{
						"name": "DM_UPDTD_TMS",
						"type": "String"
					},
					{
						"name": "DM_UPDTD_USER_ID",
						"type": "String"
					},
					{
						"name": "ORIG_ORG",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/linkedService1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pgmpdm_src_sys_dim')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "linkedService1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "SRC_SYS_DIM_202205311909.csv",
						"container": "pgmp-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "SRC_SYS_DIM_UID",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/linkedService1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pgmpdm_zaux_date_triggers')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "linkedService1",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "pgmp-account_reports"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "ZAUX_DATE_TRIGGERS_202205301233.csv",
						"container": "pgmp-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "PROC_ID",
						"type": "String"
					},
					{
						"name": "CRETD_DT",
						"type": "String"
					},
					{
						"name": "RCVD_DT",
						"type": "String"
					},
					{
						"name": "PRPSL_SENT_TO_CLNT_DT",
						"type": "String"
					},
					{
						"name": "PRPSL_DSPSN_DT",
						"type": "String"
					},
					{
						"name": "PRPSL_ACCPTD_DT",
						"type": "String"
					},
					{
						"name": "IMPLMTN_READY_DT",
						"type": "String"
					},
					{
						"name": "IMPLMTN_CLOSE_OUT_DT",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/linkedService1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pgmpdm_zaux_date_triggers_src')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "linkedService1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "ZAUX_DATE_TRIGGERS_202205311911.csv",
						"container": "pgmp-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "PROC_ID",
						"type": "String"
					},
					{
						"name": "CRETD_DT",
						"type": "String"
					},
					{
						"name": "RCVD_DT",
						"type": "String"
					},
					{
						"name": "PRPSL_SENT_TO_CLNT_DT",
						"type": "String"
					},
					{
						"name": "PRPSL_DSPSN_DT",
						"type": "String"
					},
					{
						"name": "PRPSL_ACCPTD_DT",
						"type": "String"
					},
					{
						"name": "IMPLMTN_READY_DT",
						"type": "String"
					},
					{
						"name": "IMPLMTN_CLOSE_OUT_DT",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/linkedService1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pgmpdm_zaux_deld_proc')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "linkedService1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "ZAUX_DELD_PROC_ID_202205311912.csv",
						"container": "pgmp-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "PROC_ID",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/linkedService1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pgmpdm_zaux_etl_exctn')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "linkedService1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "ZAUX_ETL_EXCTN_202205311906.csv",
						"container": "pgmp-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "ETL_EXCTN_ID",
						"type": "String"
					},
					{
						"name": "ETL_PARAM_START_TMS",
						"type": "String"
					},
					{
						"name": "ETL_PARAM_END_TMS",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/linkedService1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pgmpdm_zaux_etl_jobs')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "linkedService1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "ZAUX_ETL_JOBS_202205311908.csv",
						"container": "pgmp-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "ETL_JOB_ID",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/linkedService1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/resodbSpaceTririgaStagingCSV')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage2",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "extract/Tririga/OfficeSpace",
						"fileSystem": "etlhubfilestorage"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "CAMPUS_ID",
						"type": "String"
					},
					{
						"name": "CAMPUS_NAME",
						"type": "String"
					},
					{
						"name": "CAMPUS_STATUS",
						"type": "String"
					},
					{
						"name": "CAMPUS_GROUP",
						"type": "String"
					},
					{
						"name": "SITE_ID",
						"type": "String"
					},
					{
						"name": "PHYSICAL_GEO",
						"type": "String"
					},
					{
						"name": "WORLD_REGION_CODE",
						"type": "String"
					},
					{
						"name": "WORLD_REGION_NAME",
						"type": "String"
					},
					{
						"name": "MARKET_TEAM_REGION_CODE",
						"type": "String"
					},
					{
						"name": "MARKET_TEAM_REGION_NAME",
						"type": "String"
					},
					{
						"name": "WORK_LOCATION_CODE",
						"type": "String"
					},
					{
						"name": "ADDRESS",
						"type": "String"
					},
					{
						"name": "CITY",
						"type": "String"
					},
					{
						"name": "STATE_PROVINCE_ID",
						"type": "String"
					},
					{
						"name": "POSTAL_CODE",
						"type": "String"
					},
					{
						"name": "COUNTRY_CODE",
						"type": "String"
					},
					{
						"name": "LATITUDE",
						"type": "String"
					},
					{
						"name": "LONGITUDE",
						"type": "String"
					},
					{
						"name": "UTC_OFFSET",
						"type": "String"
					},
					{
						"name": "ICU_TIME_ZONE",
						"type": "String"
					},
					{
						"name": "PEOPLE_HOUSED_FLAG",
						"type": "String"
					},
					{
						"name": "REMOTE_SUPPORT_FLAG",
						"type": "String"
					},
					{
						"name": "PRIMARY_CAMPUS_USE_ID",
						"type": "String"
					},
					{
						"name": "PRIMARY_CAMPUS_USE_NAME",
						"type": "String"
					},
					{
						"name": "PRIMARY_CAMPUS_USE_DESCRIPTION",
						"type": "String"
					},
					{
						"name": "CAMPUS_OWNERSHIP",
						"type": "String"
					},
					{
						"name": "CAMPUS_ACTIVATION_YEAR",
						"type": "String"
					},
					{
						"name": "CAMPUS_INACTIVATION_YEAR",
						"type": "String"
					},
					{
						"name": "CAMPUS_MODIFIED_DATE_AND_TIME",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/resodb_space_tririga_staging')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "SftpTririga",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "SftpLocation",
						"fileName": "resodb_space_tririga_staging.csv",
						"folderPath": "/GlobalDir/CustomerFiles/tririga"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": false,
					"quoteChar": "\""
				},
				"schema": [
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/SftpTririga')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/resodb_space_tririga_stagingCsv')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage2",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "resodb_space_tririga_staging.csv",
						"folderPath": "extract/Tririga/office_space",
						"fileSystem": "etlhubfilestorage"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/resodbspacetririgastaging')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "SftpTririga",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "SftpLocation",
						"fileName": "resodb_space_tririga_staging.csv",
						"folderPath": "/GlobalDir/CustomerFiles/tririga"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": false,
					"quoteChar": "\""
				},
				"schema": [
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/SftpTririga')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/risk_op')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "asa-kyn-001494-dev-eus-001-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileSystem": "pgmp-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "PROC_ID",
						"type": "String"
					},
					{
						"name": "CNTRY",
						"type": "String"
					},
					{
						"name": "PROJECT_ID",
						"type": "String"
					},
					{
						"name": "DUE_DATE",
						"type": "String"
					},
					{
						"name": "REVISD_DUE_DATE",
						"type": "String"
					},
					{
						"name": "REMARKS",
						"type": "String"
					},
					{
						"name": "ORIG_ORG",
						"type": "String"
					},
					{
						"name": "CREATED_TS",
						"type": "String"
					},
					{
						"name": "CREATED_USERID",
						"type": "String"
					},
					{
						"name": "UPDATED_TS",
						"type": "String"
					},
					{
						"name": "UPDATED_USERID",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asa-kyn-001494-dev-eus-001-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('AzureBlobStorage1_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage2')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage2_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage2_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlDatabase1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('AzureSqlDatabase1_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Sftp1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "test",
				"annotations": [],
				"type": "Sftp",
				"typeProperties": {
					"host": "[parameters('Sftp1_properties_typeProperties_host')]",
					"port": "22222",
					"skipHostKeyValidation": true,
					"authenticationType": "SshPublicKey",
					"userName": "[parameters('Sftp1_properties_typeProperties_userName')]",
					"privateKeyContent": {
						"type": "SecureString",
						"value": "[parameters('Sftp1_privateKeyContent')]"
					},
					"passPhrase": {
						"type": "SecureString",
						"value": "[parameters('Sftp1_passPhrase')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SftpTririga')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "Sftp",
				"typeProperties": {
					"host": "[parameters('SftpTririga_properties_typeProperties_host')]",
					"port": "22222",
					"skipHostKeyValidation": true,
					"authenticationType": "SshPublicKey",
					"userName": "[parameters('SftpTririga_properties_typeProperties_userName')]",
					"privateKeyContent": {
						"type": "SecureString",
						"value": "[parameters('SftpTririga_privateKeyContent')]"
					},
					"passPhrase": {
						"type": "SecureString",
						"value": "[parameters('SftpTririga_passPhrase')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asa-kyn-001494-dev-eus-001-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('asa-kyn-001494-dev-eus-001-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asa-kyn-001494-dev-eus-001-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('asa-kyn-001494-dev-eus-001-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asdb_etlhub_confirmed')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('asdb_etlhub_confirmed_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/linkedService1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('linkedService1_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ln_rfs_pgmp')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('ln_rfs_pgmp_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ls_adls_project_dimension')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('ls_adls_project_dimension_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('ls_adls_project_dimension_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ls_db2_esa_kyndryl')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Linked Service for ESA Kyndryl DB2 database",
				"annotations": [],
				"type": "Db2",
				"typeProperties": {
					"connectionString": "[parameters('ls_db2_esa_kyndryl_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ls_pgmp_rfs_db')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('ls_pgmp_rfs_db_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tgt_geo')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('tgt_geo_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('tgt_geo_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tririga-prod-sftp')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "test",
				"annotations": [],
				"type": "Sftp",
				"typeProperties": {
					"host": "[parameters('tririga-prod-sftp_properties_typeProperties_host')]",
					"port": "22222",
					"skipHostKeyValidation": true,
					"authenticationType": "SshPublicKey",
					"userName": "[parameters('tririga-prod-sftp_properties_typeProperties_userName')]",
					"privateKeyContent": {
						"type": "SecureString",
						"value": "[parameters('tririga-prod-sftp_privateKeyContent')]"
					},
					"passPhrase": {
						"type": "SecureString",
						"value": "[parameters('tririga-prod-sftp_passPhrase')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Kitty-trig')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "Kitty-Pipeline1",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Day",
						"interval": 1,
						"startTime": "2022-07-08T18:55:00",
						"endTime": "2022-07-12T00:55:00",
						"timeZone": "India Standard Time",
						"schedule": {
							"minutes": [
								30
							],
							"hours": [
								0
							]
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Kitty-Pipeline1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Trigger_BALIN_MAster_Seq')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "BALSEQ0000_MASTER_EN_SEQ",
							"type": "PipelineReference"
						},
						"parameters": {
							"BLOCK_SIZE": "[parameters('Trigger_BALIN_MAster_Seq_properties_BALSEQ0000_MASTER_EN_SEQ_parameters_BLOCK_SIZE')]",
							"PROCESS_CTR_IND": "[parameters('Trigger_BALIN_MAster_Seq_properties_BALSEQ0000_MASTER_EN_SEQ_parameters_PROCESS_CTR_IND')]",
							"PARAM_START_TS": "[parameters('Trigger_BALIN_MAster_Seq_properties_BALSEQ0000_MASTER_EN_SEQ_parameters_PARAM_START_TS')]",
							"PARAM_END_TS": "[parameters('Trigger_BALIN_MAster_Seq_properties_BALSEQ0000_MASTER_EN_SEQ_parameters_PARAM_END_TS')]",
							"REPLICATION_OFFSET": "[parameters('Trigger_BALIN_MAster_Seq_properties_BALSEQ0000_MASTER_EN_SEQ_parameters_REPLICATION_OFFSET')]"
						}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Day",
						"interval": 1,
						"startTime": "2022-07-02T13:54:00Z",
						"timeZone": "UTC",
						"schedule": {
							"minutes": [
								58
							],
							"hours": [
								13
							]
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/BALSEQ0000_MASTER_EN_SEQ')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Trigger_EtlHub_Dimension_Load')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "PL_ETLHUB_DIMENSION_LOAD",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Week",
						"interval": 1,
						"startTime": "2022-07-14T00:24:00",
						"timeZone": "Central Standard Time",
						"schedule": {
							"minutes": [
								0
							],
							"hours": [
								3
							],
							"weekDays": [
								"Monday",
								"Wednesday",
								"Friday"
							]
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/PL_ETLHUB_DIMENSION_LOAD')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Trigger_test')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "PipelineTest",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Hour",
						"interval": 1,
						"startTime": "2022-07-08T11:17:00Z",
						"timeZone": "UTC"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/PipelineTest')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tr_customer_dimension_load')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "pl_deltalake_customer_dimension",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Minute",
						"interval": 2,
						"startTime": "2022-05-31T16:58:00",
						"endTime": "2022-05-31T17:03:00",
						"timeZone": "India Standard Time"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/pl_deltalake_customer_dimension')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tr_pgmp_cpsummary_and_field_audit_pipeline')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "PgMP_CPSummary_and_Field_Audit_Pipeline",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Hour",
						"interval": 2,
						"startTime": "2022-06-22T15:00:00Z",
						"timeZone": "UTC"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/PgMP_CPSummary_and_Field_Audit_Pipeline')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tr_pgmp_dimensions_pipeline')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "PgMP_Dimension_Pipeline",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Hour",
						"interval": 4,
						"startTime": "2022-06-14T21:00:00",
						"timeZone": "India Standard Time"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/PgMP_Dimension_Pipeline')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/BALSEQ0100_MASTER_SEQ_CLEAN_UP')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job creation:28-06-2022\nJob name: BALSEQ0100_MASTER_SEQ_CLEAN_UP\nCreatedBy: Varaprasad",
				"folder": {
					"name": "PGMP/Sequence"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "IdentifyCleanUpObjects"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "CleanUpRecords"
						}
					],
					"transformations": [
						{
							"name": "alterRow1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          ETL_EXCTN_ID as integer,",
						"          IS_CURR_IND as string,",
						"          UPDTD_USER_ID as string,",
						"          WAS_SUCCESS_IND as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n     ETL_EXCTN_ID,\\n     cast(\\'N\\' as char(1)) as IS_CURR_IND,\\n     CURRENT_USER as UPDTD_USER_ID,\\n     cast(\\'N\\' as char(1)) as WAS_SUCCESS_IND\\nfrom\\n     PGMPDM.ZAUX_ETL_EXCTN\\nwhere\\n     IS_CURR_IND = \\'Y\\'',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> IdentifyCleanUpObjects",
						"IdentifyCleanUpObjects alterRow(updateIf(true())) ~> alterRow1",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'ZAUX_ETL_EXCTN',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['ETL_EXCTN_ID'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> CleanUpRecords"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/BALSEQ0110_MASTER_SEQ_START')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 28-06-2022\nJob Name: BALSEQ0110_MASTER_SEQ_START\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Sequence"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getExecutionSignatureData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "storeExecution"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(output(",
						"          IS_CURR_IND as string,",
						"          CRETD_USER_ID as string,",
						"          UPDTD_USER_ID as string,",
						"          ETL_EXCTN_START_TMS as timestamp",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select \\n     cast(\\'Y\\' as char(1)) as IS_CURR_IND,\\n     CURRENT_USER as CRETD_USER_ID,\\n     CURRENT_USER as UPDTD_USER_ID,\\n     CURRENT_TIMESTAMP as ETL_EXCTN_START_TMS',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getExecutionSignatureData",
						"getExecutionSignatureData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'ZAUX_ETL_EXCTN',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> storeExecution"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/BALSEQ0120_MASTER_SET_LIMITS')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 29-06-2022\nJob Name: BALSEQ0120_MASTER_SET_LIMITS\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Sequence"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getParamTS"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						}
					],
					"transformations": [
						{
							"name": "addParamStartEndTS"
						},
						{
							"name": "addDefaultStartEndTS"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "setLimitsInput"
						},
						{
							"name": "updateRows"
						}
					],
					"scriptLines": [
						"parameters{",
						"     BLOCK_SIZE as integer,",
						"     PROCESS_CTR_IND as string,",
						"     PARAM_START_TS as string,",
						"     PARAM_END_TS as string,",
						"     REPLICATION_OFFSET as integer",
						"}",
						"source(output(",
						"          ETL_PARAM_END_TMS as timestamp",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     ETL_PARAM_END_TMS\\nFrom\\n     PGMPDM.ZAUX_ETL_EXCTN PE\\n     inner join (\\n          Select\\n               max(ETL_EXCTN_ID) as LAST_ETL_EXECUTION_ID\\n          From\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'N\\'\\n               and WAS_SUCCESS_IND = \\'Y\\'\\n     ) LE\\n     on PE.ETL_EXCTN_ID = LE.LAST_ETL_EXECUTION_ID\\n',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getParamTS",
						"getParamTS derive(ETL_PARAM_START_TMS = toTimestamp(case(ltrim(rtrim($PARAM_START_TS))=='', toString(ETL_PARAM_END_TMS), $PARAM_START_TS)),",
						"          ETL_PARAM_END_TMS = toTimestamp(case(ltrim(rtrim($PARAM_END_TS))=='', '', $PARAM_END_TS))) ~> addParamStartEndTS",
						"getParamTS derive(ETL_PARAM_START_TMS = toTimestamp(coalesce(case(ltrim(rtrim($PARAM_START_TS))=='', toString(null()), $PARAM_START_TS), '2010-01-01 00:00:00.000001')),",
						"          ETL_PARAM_END_TMS = toTimestamp(coalesce(case(ltrim(rtrim($PARAM_END_TS))=='', toString(null()), $PARAM_END_TS), toString(currentTimestamp())))) ~> addDefaultStartEndTS",
						"addDefaultStartEndTS, addParamStartEndTS join(1 == 1,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'right')~> mergedData",
						"mergedData derive(ETL_PARAM_START_TMS = coalesce(addParamStartEndTS@ETL_PARAM_START_TMS, addDefaultStartEndTS@ETL_PARAM_START_TMS) - minutes($REPLICATION_OFFSET),",
						"          ETL_PARAM_END_TMS = coalesce(addParamStartEndTS@ETL_PARAM_END_TMS, addDefaultStartEndTS@ETL_PARAM_END_TMS),",
						"          ETL_PARAM_CTR_BLOCK_SIZE = $BLOCK_SIZE,",
						"          ETL_PARAM_CTR_IND = coalesce($PROCESS_CTR_IND, 'N'),",
						"          IS_CURR_IND = 'Y') ~> setLimitsInput",
						"setLimitsInput alterRow(updateIf(true())) ~> updateRows",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'ZAUX_ETL_EXCTN',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['IS_CURR_IND'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> updateTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/BALSEQ0130_MASTER_SEQ_END')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 30-06-2022\nJob Name: BALSEQ0130_MASTER_SEQ_END\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Sequence"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getExecutionSignatureData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateExecution"
						}
					],
					"transformations": [
						{
							"name": "updateRow"
						}
					],
					"scriptLines": [
						"source(output(",
						"          ETL_EXCTN_ID as integer,",
						"          ETL_EXCTN_END_TMS as timestamp,",
						"          IS_CURR_IND as string,",
						"          WAS_SUCCESS_IND as string,",
						"          PROC_ID_QTY as integer,",
						"          UPDTD_TMS as timestamp,",
						"          UPDTD_USER_ID as timestamp",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     E.ETL_EXCTN_ID,\\n     CURRENT_TIMESTAMP as ETL_EXCTN_END_TMS,\\n     cast(\\'N\\' as char(1)) as IS_CURR_IND,\\n     cast(\\'Y\\' as char(1)) as WAS_SUCCESS_IND,\\n     PID.QTY as PROC_ID_QTY,\\n     CURRENT_TIMESTAMP as UPDTD_TMS,\\n     CURRENT_TIMESTAMP as UPDTD_USER_ID\\nFrom\\n     PGMPDM.ZAUX_ETL_EXCTN E\\n     cross join\\n     (\\n          Select\\n               count(*) as QTY\\n          From\\n               PGMPDM.ZAUX_DATE_TRIGGERS\\n     ) PID\\nWhere\\n     E.IS_CURR_IND = \\'Y\\'',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getExecutionSignatureData",
						"getExecutionSignatureData alterRow(updateIf(true())) ~> updateRow",
						"updateRow sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'ZAUX_ETL_EXCTN',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['ETL_EXCTN_ID'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> updateExecution"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0001_CP_CNTRCT_CHNG_RPT')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job creation:05-06-2022\nJob name: DF_BALD0001_CP_CNTRCT_CHNG_RPT\nCreatedBy: Varaprasad",
				"folder": {
					"name": "PGMP/CP Summary & Custom Field audit"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "Srccntrchnagrpt"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "tgtcntrctchngrpt"
						}
					],
					"transformations": [
						{
							"name": "AlterRow1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          CNTRCT_DIM_UID as integer,",
						"          CONTRACT as string,",
						"          UNIQUE_ID as integer,",
						"          DISPLAY_ID as string,",
						"          GLBL_BUY_GRP_ID as string,",
						"          PARENT_UNIQUE_ID as integer,",
						"          PARENT_DISPLAY_ID as string,",
						"          PARENT_TITLE as string,",
						"          CLIENT_REFERENCE as string,",
						"          PROC_TITLE as string,",
						"          PROC_DESCRIPTION as string,",
						"          CURRENT_WORKFLOW as string,",
						"          CURRENT_WKFLW_STEP as string,",
						"          CURRENT_STEP_SEQ as integer,",
						"          STEP_DESCRIPTION as string,",
						"          STATE_DESC as string,",
						"          STATUS_DESC as string,",
						"          ON_HOLD_DESC as string,",
						"          ON_HOLD_DT as date,",
						"          ON_HOLD_RSN_TXT as string,",
						"          ADTNL_DTLS_FOR_WITHDRW_HOLD_TXT as string,",
						"          AUDIENCE as string,",
						"          PRIORITY_TXT as string,",
						"          CONDITION as string,",
						"          REGION as string,",
						"          REQUESTOR as string,",
						"          COUNTRY as string,",
						"          AGREEMENT_REFERENCE_DETAILS as string,",
						"          ORIGINATING_ORG as string,",
						"          CREATED_DATE as timestamp,",
						"          LAST_UPDATED as timestamp,",
						"          DUE_DT as date,",
						"          RVSD_DUE_DT as date,",
						"          IBM_APPRVL_SIGNED_DT as date,",
						"          CLNT_APPRVL_SIGNED_DT as date,",
						"          CNTRCT_EX_DT as date,",
						"          COMPLETION_DATE as date,",
						"          CMPLTD_RSN_TXT as string,",
						"          CNTRCT_DELVRBL_CNFRMN_IND as string,",
						"          FRML_AMNDMNT_NBR_TXT as string,",
						"          CNTRCT_BSLN_CNFRMN_IND as string,",
						"          STAFFG_SKILL_CNFRMN_IND as string,",
						"          FIN_CNFRMN_IND as string,",
						"          RJCT_RSN_ID as integer,",
						"          RJCT_EXPLNN_TXT as string,",
						"          CNTRCT_REM_TXT as string,",
						"          RJCT_RSN_TXT as string,",
						"          SRC_SYS_CD as string,",
						"          LEGACY_UNIQUE_ID as string,",
						"          LEGACY_DISPLAYED_ID as string,",
						"          LATEST_IBM_ONLY_NOTE as string,",
						"          LATEST_IBM_AND_CLIENT_NOTE as string,",
						"          CNTRCT_TYPE_DESC as string,",
						"          PROC_TYPE_ID as string,",
						"          PROC_TYPE_DESC as string,",
						"          ON_HOLD_CD as string,",
						"          WITHDRWN_DIM_UID as integer,",
						"          WITHDRWN_DT as date,",
						"          ORGNZN_DIM_UID as integer,",
						"          IBM_ONLY_TEXT_1 as string,",
						"          IBM_ONLY_TEXT_2 as string,",
						"          IBM_ONLY_TEXT_3 as string,",
						"          IBM_ONLY_TEXT_4 as string,",
						"          IBM_ONLY_TEXT_5 as string,",
						"          IBM_ONLY_TEXT_6 as string,",
						"          IBM_ONLY_DATE_1 as date,",
						"          IBM_ONLY_DATE_2 as date,",
						"          IBM_ONLY_DATE_3 as date,",
						"          IBM_ONLY_DATE_4 as date,",
						"          IBM_ONLY_DATE_5 as date,",
						"          IBM_ONLY_DATE_6 as date,",
						"          PUBLIC_TEXT_1 as string,",
						"          PUBLIC_TEXT_2 as string,",
						"          PUBLIC_TEXT_3 as string,",
						"          PUBLIC_TEXT_4 as string,",
						"          PUBLIC_TEXT_5 as string,",
						"          PUBLIC_TEXT_6 as string,",
						"          PUBLIC_DATE_1 as date,",
						"          PUBLIC_DATE_2 as date,",
						"          PUBLIC_DATE_3 as date,",
						"          PUBLIC_DATE_4 as date,",
						"          PUBLIC_DATE_5 as date,",
						"          PUBLIC_DATE_6 as date,",
						"          STATE_OF_ORGANIZATION as string,",
						"          SCTR_NM as string,",
						"          CLIENT_UNIT as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'SELECT \\nC.CNTRCT_DIM_UID,\\nC.CNTRCT_NM AS CONTRACT,\\nH.PROC_DIM_UID AS UNIQUE_ID,\\nP.PROC_DSPLY_ID AS DISPLAY_ID,\\nZ.GLBL_BUY_GRP_ID,\\nPC.PROC_DIM_UID AS PARENT_UNIQUE_ID,  \\nPC.PROC_DSPLY_ID AS PARENT_DISPLAY_ID,\\nPC.TITLE_TXT AS PARENT_TITLE,\\nP.CLNT_REF_NUM AS CLIENT_REFERENCE,\\n--I.WKFLW_DEF_ID,\\n--I.\"DESC\"  as  INIT_WKFLW_DEF_ID_DESCR,\\nP.TITLE_TXT AS PROC_TITLE,\\nP.PROC_DESC AS PROC_DESCRIPTION,\\nW.WKFLW_DEF_ID AS CURRENT_WORKFLOW,\\nWS.WKFLW_STEP_DEF_ID AS CURRENT_WKFLW_STEP,\\nWS.WKFLW_STEP_SEQ_NUM AS CURRENT_STEP_SEQ,\\nWS.WKFLW_STEP_DESC AS STEP_DESCRIPTION,\\nST.STATE_TITLE_TXT STATE_DESC,\\nS.STAT_IBM_DESC AS STATUS_DESC,\\nO.ON_HOLD_DESC, \\nH.ON_HOLD_DT,\\nHR.ON_HOLD_RSN_TXT,\\nP.ADTNL_DTLS_FOR_WITHDRW_HOLD_TXT, \\n--decrypt_char(UD.CONCAT_NM_LOGIN,MR.MIS_REP_REF_CD) AS ASSIGNED_TO,\\nAU.ADNC_TXT AS AUDIENCE,\\nPD.PRIORITY_TXT as PRIORITY_TXT,\\nCN.COND_TXT AS CONDITION,\\nCOALESCE(RGN.REGION,GEO.KYNDRYL_RGN) AS REGION,\\n--decrypt_char(CR.CONCAT_NM_LOGIN,MR.MIS_REP_REF_CD) AS CREATED_BY,\\n--decrypt_char(P.RQSTR_TXT,\\nMR.MIS_REP_REF_CD AS REQUESTOR,\\nGEO.SRGNCTRY_NM AS COUNTRY,                                                                                                                                  \\nCC.CNTRCT_REF_TXT AS AGREEMENT_REFERENCE_DETAILS,\\nCASE CC.ORIG_ORG WHEN \\'C\\' THEN \\'Client\\' WHEN \\'I\\' THEN \\'IBM\\' WHEN \\'R\\' THEN \\'RFS\\' ELSE \\'\\' END AS ORIGINATING_ORG,\\n---decrypt_char(CC.IBM_APPRVR_TXT,MR.MIS_REP_REF_CD)  AS IBM_APPRVR_TXT,\\n--decrypt_char(CC.CLNT_APPRVR_TXT,MR.MIS_REP_REF_CD) AS CLNT_APPRVR_TXT ,     \\nH.SRC_CRETD_TMS AS CREATED_DATE,\\nH.SRC_UPDTD_TMS AS LAST_UPDATED,\\nF.DUE_DT,\\nF.RVSD_DUE_DT,\\n/*DAYS(NVL(F.DUE_DT,F.RVSD_DUE_DT)) - DAYS((CURRENT_TIMESTAMP - CURRENT_TIMEZONE) + AF.TZ HOURS) AS DUE_IN_DAY_CNT,\\nCASE WHEN DAYS(NVL(F.RVSD_DUE_DT,F.DUE_DT)) - DAYS((CURRENT_TIMESTAMP - CURRENT_TIMEZONE) + AF.TZ HOURS)  < 0 THEN \\'Y\\' ELSE \\'N\\' END AS OVERDUE,*/      \\nF.IBM_APPRVL_SIGNED_DT,\\nF.CLNT_APPRVL_SIGNED_DT,\\nF.CNTRCT_EX_DT,\\nH.CMPLTD_DT AS COMPLETION_DATE,\\nP.CMPLTD_RSN_TXT,\\nCC.CNTRCT_DELVRBL_CNFRMN_IND,\\nCC.FRML_AMNDMNT_NBR_TXT,\\nCC.CNTRCT_BSLN_CNFRMN_IND,\\nCC.STAFFG_SKILL_CNFRMN_IND,\\nCC.FIN_CNFRMN_IND,\\nCC.RJCT_RSN_ID,\\nCC.RJCT_EXPLNN_TXT,\\nCC.CNTRCT_REM_TXT,\\nCC.RJCT_RSN_TXT, \\nSR.SRC_SYS_CD,\\n\\' \\' AS LEGACY_UNIQUE_ID,\\n\\' \\' AS LEGACY_DISPLAYED_ID,\\nTRIM(SUBSTRING(ND.NOTES_DESC,1,50)) AS LATEST_IBM_ONLY_NOTE,\\nTRIM(SUBSTRING(PN.NOTES_DESC,1,50)) AS LATEST_IBM_AND_CLIENT_NOTE,\\nCT.CNTRCT_TYPE_DESC,\\nPT.PROC_TYPE_ID,\\nPT.PROC_TYPE_DESC,\\nO.ON_HOLD_CD,\\nH.WITHDRWN_DIM_UID,\\nH.WITHDRWN_DT,\\nH.ORGNZN_DIM_UID,\\nDI.IBM_ONLY_TEXT_1,\\nDI.IBM_ONLY_TEXT_2,\\nDI.IBM_ONLY_TEXT_3,\\nDI.IBM_ONLY_TEXT_4,\\nDI.IBM_ONLY_TEXT_5,\\nDI.IBM_ONLY_TEXT_6,\\nDI.IBM_ONLY_DATE_1,\\nDI.IBM_ONLY_DATE_2,\\nDI.IBM_ONLY_DATE_3,\\nDI.IBM_ONLY_DATE_4,\\nDI.IBM_ONLY_DATE_5,\\nDI.IBM_ONLY_DATE_6,\\nDP.PUBLIC_TEXT_1,\\nDP.PUBLIC_TEXT_2,\\nDP.PUBLIC_TEXT_3,\\nDP.PUBLIC_TEXT_4,\\nDP.PUBLIC_TEXT_5,\\nDP.PUBLIC_TEXT_6,\\nDP.PUBLIC_DATE_1,\\nDP.PUBLIC_DATE_2,\\nDP.PUBLIC_DATE_3,\\nDP.PUBLIC_DATE_4,\\nDP.PUBLIC_DATE_5,\\nDP.PUBLIC_DATE_6,\\nSO.STAT_OF_ORGNZN_DESC AS STATE_OF_ORGANIZATION,\\nSD.SCTR_NM,\\nZ.CLIENT_UNIT_NM AS CLIENT_UNIT\\nFROM PGMPDM.PROC_HEADR_FCT H\\n\\nINNER JOIN PGMPDM.PROC_TYPE_DIM PT ON PT.PROC_TYPE_DIM_UID = H.PROC_TYPE_DIM_UID AND PT.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.PROC_DIM P ON P.PROC_DIM_UID = H.PROC_DIM_UID AND P.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.CNTRCT_DIM C ON C.CNTRCT_DIM_UID = P.CNTRCT_DIM_UID AND C.ROW_STAT_CD <> \\'D\\'\\n\\nINNER JOIN PGMPDM.CNTRCT_TYPE_DIM CT ON CT.CNTRCT_TYPE_DIM_UID = C.CNTRCT_TYPE_DIM_UID AND CT.ROW_STAT_CD <> \\'D\\'\\n--INNER JOIN PGMPDM.WKFLW_DEF_DIM I ON I.WKFLW_DEF_DIM_UID = H.INIT_WKFLW_DEF_DIM_UID AND I.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.WKFLW_DEF_DIM W ON W.WKFLW_DEF_DIM_UID = H.WKFLW_DEF_DIM_UID AND W.ROW_STAT_CD <> \\'D\\'  \\n\\nINNER JOIN PGMPDM.WKFLW_STEP_DEF_DIM WS ON WS.WKFLW_STEP_DEF_DIM_UID = H.WKFLW_STEP_DEF_DIM_UID AND WS.ROW_STAT_CD <> \\'D\\' \\nINNER JOIN PGMPDM.STATE_DIM ST ON ST.STATE_DIM_UID = H.STATE_DIM_UID AND ST.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.PROC_STEP_DAT_DIM PS ON P.PROC_DIM_UID = PS.PROC_DIM_UID AND P.CURR_PROC_STEP_DAT_DIM_UID = PS.PROC_STEP_DAT_DIM_UID AND PS.ROW_STAT_CD <> \\'D\\'\\n\\nINNER JOIN PGMPDM.GEO_DIM GEO ON 1=1\\n-- GEO.GEO_DIM_UID = H.GEO_DIM_UID AND GEO.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.MISC_FCT F ON F.PROC_HEADR_FCT_UID = H.PROC_HEADR_FCT_UID AND F.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.CNTRCT_CHG_DIM CC ON CC.CNTRCT_CHG_DIM_UID = P.PROC_DIM_UID AND CC.ROW_STAT_CD <> \\'D\\'\\n\\nINNER JOIN PGMPDM.SRC_SYS_DIM SR ON SR.SRC_SYS_DIM_UID = H.SRC_SYS_DIM_UID AND SR.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.SCTR_DIM SD ON SD.SCTR_DIM_UID = C.SCTR_DIM_UID AND SD.ROW_STAT_CD <> \\'D\\'\\n\\nLEFT OUTER JOIN PGMPDM.STAT_DIM S ON S.STAT_DIM_UID = H.STAT_DIM_UID AND S.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.MISC_REP_REF MR ON MIS_REP_REF_UID = 3\\n\\nLEFT OUTER JOIN PGMPDM.USER_DIM UD ON UD.USER_ID = PS.ASSGN_TO_NUM AND UD.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.ADNC_DIM AU ON AU.ADNC_CD = P.ADNC_CD\\nLEFT OUTER JOIN PGMPDM.PRIORITY_DIM PD ON PD.PRIORITY_NUM = P.PRIORTY_NUM\\n\\nLEFT OUTER JOIN PGMPDM.COND_DIM CN ON CN.COND_CD = P.CNDTN_CD\\nLEFT OUTER JOIN APPFUN.PROC_REGION_V RGN ON RGN.PROC_ID = P.PROC_DIM_UID AND RGN.INTERNAL_VAL <> \\'NONE\\'\\nLEFT OUTER JOIN PGMPDM.USER_DIM CR ON CR.USER_DIM_UID = H.CRETD_BY_USER_DIM_UID AND CR.ROW_STAT_CD <> \\'D\\'\\n\\nLEFT OUTER JOIN PGMPDM.NOTES_DIM ND ON ND.PROC_DIM_UID = P.PROC_DIM_UID AND ND.ROW_STAT_CD <> \\'D\\' AND ND.NOTE_TYPE_CD = \\'I\\' AND ND.LATEST_NOTE_IND = \\'Y\\'\\nLEFT OUTER JOIN PGMPDM.NOTES_DIM PN ON PN.PROC_DIM_UID = P.PROC_DIM_UID AND PN.ROW_STAT_CD <> \\'D\\' AND PN.NOTE_TYPE_CD = \\'P\\' AND PN.LATEST_NOTE_IND = \\'Y\\'\\nLEFT OUTER JOIN PGMPDM.ON_HOLD_DIM O ON O.ON_HOLD_DIM_UID = H.ON_HOLD_DIM_UID AND O.ROW_STAT_CD <> \\'D\\'\\n\\nLEFT OUTER JOIN PGMPDM.ON_HOLD_RSN_DIM HR ON HR.ON_HOLD_RSN_ID = P.ON_HOLD_RSN_ID AND HR.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.PROC_DIM PC ON PC.PROC_DIM_UID = P.PARNT_PROC_ID AND PC.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP ON DP.PROC_DIM_UID = P.PROC_DIM_UID\\n\\nLEFT OUTER JOIN PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DI ON DI.PROC_DIM_UID = P.PROC_DIM_UID\\nLEFT OUTER JOIN PGMPDM.ORG_CNTRCT_MAP_DIM OM ON OM.CNTRCT_DIM_UID = C.CNTRCT_DIM_UID\\nLEFT OUTER JOIN PGMPDM.ORGNZN_DIM OD ON OD.ORGNZN_DIM_UID = OM.ORGNZN_DIM_UID\\n\\nLEFT OUTER JOIN PGMPDM.STAT_OF_ORGNZN_DIM SO ON SO.STAT_OF_ORGNZN_DIM_UID = OD.STAT_OF_ORGNZN_DIM_UID\\n--LEFT OUTER JOIN PGMPDM.CNTRCT_TZ_V AF ON C.CNTRCT_DIM_UID = AF.CNTRCT_DIM_UID\\nLEFT OUTER JOIN PGMPDM.ZAUX_CNTRCT_GLBL_BUY_GRP_MAP Z ON Z.CNTRCT_DIM_UID = C.CNTRCT_DIM_UID \\n\\nWHERE H.ROW_STAT_CD <> \\'D\\' AND PT.PROC_TYPE_ID = \\'CONTRCHG\\' AND P.ADNC_ACCSS_CD <> \\'C\\' AND H.DELD_DT IS NULL AND SR.SRC_SYS_CD = \\'PGMP\\'\\n\\n\\n\\n',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> Srccntrchnagrpt",
						"Srccntrchnagrpt alterRow(upsertIf(true())) ~> AlterRow1",
						"AlterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'PGMPDM.CONTRACT_CHANGE_RPT',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     truncate: true,",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> tgtcntrctchngrpt"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0002_CP_PCR_RPT')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job creation:05-06-2022\nJob name: DF_BALD0002_CP_PCR_RPT\nCreated By: Varaprasad",
				"folder": {
					"name": "PGMP/CP Summary & Custom Field audit"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceDataforPCR"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "upsertTable"
						}
					],
					"transformations": [
						{
							"name": "upsertRows"
						}
					],
					"scriptLines": [
						"source(output(",
						"          CNTRCT_DIM_UID as integer,",
						"          CNTRCT_NM as string,",
						"          GLBL_BUY_GRP_ID as string,",
						"          UNIQUE_ID as integer,",
						"          DISPLAYED_ID as string,",
						"          PARENT_UNIQUE_ID as integer,",
						"          PARENT_VISIBLE_ID as string,",
						"          PARENT_TITLE as string,",
						"          CLIENT_REFERENCE_NUM as string,",
						"          PURCHASE_ORDER as string,",
						"          WKFLW_DEF_ID as string,",
						"          INIT_WKFLW_DEF_ID_DESCR as string,",
						"          PROC_TITLE as string,",
						"          PROC_DESC as string,",
						"          CURR_WRKFLW_NM as string,",
						"          WKFLW_STEP_DEF_ID as string,",
						"          WKFLW_STEP_SEQ_NUM as integer,",
						"          WKFLW_STEP_DESC as string,",
						"          STATE_DESC as string,",
						"          STATUS_DESC as string,",
						"          AUDIENCE as string,",
						"          CONDITION as string,",
						"          REGION as string,",
						"          HOLD_STATUS as string,",
						"          ON_HOLD_DT as date,",
						"          HOLD_REASON as string,",
						"          HOLD_OR_WITHDRAW_EXPLANATION as string,",
						"          CREATED_DATE as timestamp,",
						"          LAST_UPDATED as timestamp,",
						"          RQST_RCVD_BY_IBM_DATE as date,",
						"          ACKNOWLEDGED_DATE as date,",
						"          RQST_QUALIFIED_DATE as date,",
						"          LETTER_AUTH_SIGNATURE_DT as date,",
						"          PLANNED_RQMTS_APPROVAL_DATE as date,",
						"          RQMTS_REWORK_RQSTD_DATE as date,",
						"          RQMTS_DEFINITION_COMPLETE_DT as date,",
						"          RQMTS_AGREED_DATE as date,",
						"          CLIENT_REJECT_RQMTS_DATE as date,",
						"          PLAN_SLTN_CMPLTD_DT as date,",
						"          RVSD_PLAN_SLTN_CMPLTD_DT as date,",
						"          TCHNLGY_DLVR_ASSESSMT_DT as date,",
						"          SOLUTION_COMPLETED_DATE as date,",
						"          SLTN_APPRVL_RQRD_DT as date,",
						"          CLNT_ACCPTD_SLTN_DT as date,",
						"          CLNT_REJCT_SLTN_DT as date,",
						"          SLTN_PKGNG_DT as date,",
						"          SOLTN_PACKAGE_VALIDATE_DT as date,",
						"          PRICING_COMPLETED_DT as date,",
						"          PRPSL_BSLN_ASSESSMT_DT as date,",
						"          CNTRCT_BSLN_RVW_DT as date,",
						"          CLIENT_PRPSL_REQUESTED_DATE as date,",
						"          CLIENT_REVISED_PRPSL_REQUESTED_DATE as date,",
						"          PLANNED_PRPSL_DATE as date,",
						"          REVISED_PLANNED_PRPSL_DATE as date,",
						"          CLIENT_RQSTD_IMPL_START_DT as date,",
						"          DUE_DT as date,",
						"          PROPOSAL_RESPONSE_REQUESTED_DATE as date,",
						"          PROPOSAL_EXPIRE_DATE as date,",
						"          PRPSL_SENT_TO_CLNT_DT as date,",
						"          PROPOSAL_ACCEPTED_DATE as date,",
						"          PROPOSAL_REJECTED_DATE as date,",
						"          COMPLETION_DATE as date,",
						"          CLNT_SIGNTUR_RCVD_DT as date,",
						"          IBM_SIGNTUR_RCVD_DT as date,",
						"          CLIENT_REQ_IMPLMNT_COMPLETE_DATE as date,",
						"          CLIENT_REQ_IMPLMNT_START_DATE as date,",
						"          ENGAGEMENT_HANDOFF_DATE as date,",
						"          PLANNED_IMPLMNT_COMPLETED_DATE as date,",
						"          REVISED_PLANNED_IMPLMNT_COMPLETED_DATE as date,",
						"          WITHDRWN_DT as date,",
						"          CMPLTD_RSN_TXT as string,",
						"          REJECT_REWORK_REASON as string,",
						"          REJECT_REWORK_SOURCE as string,",
						"          REJECT_REWORK_EXPLANATION as string,",
						"          LAG_DAYS_NUM as integer,",
						"          CLIENT_AFFECTED_AREA as string,",
						"          CLIENT_ACCNTG_CD as string,",
						"          AGREEMENT_REF_DETAILS as string,",
						"          ORIGINATING_ORG as string,",
						"          UNSOLICITED_REQUEST as string,",
						"          CONTRACT_CHANGE_REQUIRED as string,",
						"          OATS_RECORD_NEEDED as string,",
						"          NON_STD_TERMS_CONDITIONS as string,",
						"          TRAVEL_EXPENSES_INVOICED as string,",
						"          TERMINATION_CHARGES as string,",
						"          ANNUAL_RENEWAL_MONTH as string,",
						"          SERVICE_PROVIDER_ORG as string,",
						"          PREDEFINED_SRVCS as string,",
						"          COUNTRY as string,",
						"          SIEBEL_OPP_NUM as string,",
						"          BTT_NUM as integer,",
						"          COMPLEXITY as string,",
						"          ESTIMATED_SOLUTION_HOURS as integer,",
						"          CLAIM_ACCOUNT_CODE as string,",
						"          EST_IMPLEMENTATION_HOURS as integer,",
						"          PERSONNEL_CHANGES as string,",
						"          SERVER_COMPONENT as string,",
						"          STORAGE_COMPONENT as string,",
						"          HARDWARE_COMPONENT as string,",
						"          SOFTWARE_COMPONENT as string,",
						"          LOCAL_CURRENCY as string,",
						"          ALT_CURRENCY as string,",
						"          ESTIMATED_OTC_LOCAL_CURR as decimal(38,2),",
						"          ESTIMATED_TCV_LOCAL_CURR as decimal(38,2),",
						"          ESTIMATED_PRPSL_PROD_COST_LOCAL_CURR as decimal(38,2),",
						"          ACTUAL_PRPSL_PROD_COST_LOCAL_CURR as decimal(38,2),",
						"          NON_RECURRING_COST_LOCAL_CURR as decimal(38,2),",
						"          NON_RECURRING_COST_ALT_CURR as decimal(38,2),",
						"          MNTHLY_RECURRING_COST_LOCAL_CURR as decimal(38,2),",
						"          MNTHLY_RECURRING_COST_ALT_CURR as decimal(38,2),",
						"          TOTAL_CNTRCT_COST_LOCAL_CURR as decimal(38,2),",
						"          TOTAL_CNTRCT_COST_ALT_CURR as decimal(38,2),",
						"          NON_RECURRING_PRICE_LOCAL_CURR as decimal(38,2),",
						"          NON_RECURRING_PRICE_ALT_CURR as decimal(38,2),",
						"          MNTHLY_RECURRING_CHARGES_LOCAL_CURR as decimal(38,2),",
						"          MNTHLY_RECURRING_CHARGES_ALT_CURR as decimal(38,2),",
						"          TOTAL_CNTRCT_PRICE_LOCAL_CURR as decimal(38,2),",
						"          TOTAL_CNTRCT_PRICE_ALT_CURR as decimal(38,2),",
						"          CLIENT_FOCAL_POINT as string,",
						"          IBM_RFS_MANAGER as string,",
						"          REQUIREMENTS_TEAM_LEAD as string,",
						"          CLIENT_CATALOG_ONLY_REQUESTOR as string,",
						"          CLIENT_CATALOG_ONLY_APPROVER as string,",
						"          CLIENT_CTLG_ONLY_BUDGET_APPROVER as string,",
						"          CLIENT_CTLG_ONLY_FOCAL_POINT as string,",
						"          SRC_SYS_CD as string,",
						"          LEGACY_UNIQUE_ID as string,",
						"          LEGACY_DISPLAYED_ID as string,",
						"          LATEST_IBM_ONLY_NOTE as string,",
						"          LATEST_IBM_AND_CLIENT_NOTE as string,",
						"          L30_OFFERING_1 as string,",
						"          L30_OFFERING_2 as string,",
						"          L30_OFFERING_3 as string,",
						"          PRIOR_TOTAL_CNTRCT_PRICE_AMT as decimal(38,2),",
						"          PRPSL_CHG_RSN_TXT as string,",
						"          PRPSL_CHG_EXPLNN_TXT as string,",
						"          IMPLMTN_CHG_RSN_TXT as string,",
						"          IMPLMTN_CHG_EXPLNN_TXT as string,",
						"          PRPSL_LATE_RSN_TXT as string,",
						"          PRPSL_LATE_EXPLNN_TXT as string,",
						"          IMPLMTN_LATE_RSN_TXT as string,",
						"          IMPLMTN_LATE_EXPLNN_TXT as string,",
						"          CLIENT_APPROVER as string,",
						"          CLIENT_BUDGET_APPROVER as string,",
						"          IBM_ONLY_TEXT_1 as string,",
						"          IBM_ONLY_TEXT_2 as string,",
						"          IBM_ONLY_TEXT_3 as string,",
						"          IBM_ONLY_TEXT_4 as string,",
						"          IBM_ONLY_TEXT_5 as string,",
						"          IBM_ONLY_TEXT_6 as string,",
						"          IBM_ONLY_TEXT_7 as string,",
						"          IBM_ONLY_TEXT_8 as string,",
						"          IBM_ONLY_TEXT_9 as string,",
						"          IBM_ONLY_TEXT_10 as string,",
						"          IBM_ONLY_TEXT_11 as string,",
						"          IBM_ONLY_TEXT_12 as string,",
						"          IBM_ONLY_TEXT_13 as string,",
						"          IBM_ONLY_TEXT_14 as string,",
						"          IBM_ONLY_TEXT_15 as string,",
						"          IBM_ONLY_TEXT_16 as string,",
						"          IBM_ONLY_TEXT_17 as string,",
						"          IBM_ONLY_TEXT_18 as string,",
						"          IBM_ONLY_TEXT_19 as string,",
						"          IBM_ONLY_TEXT_20 as string,",
						"          IBM_ONLY_TEXT_21 as string,",
						"          IBM_ONLY_TEXT_22 as string,",
						"          IBM_ONLY_TEXT_23 as string,",
						"          IBM_ONLY_TEXT_24 as string,",
						"          IBM_ONLY_TEXT_25 as string,",
						"          IBM_ONLY_DATE_1 as date,",
						"          IBM_ONLY_DATE_2 as date,",
						"          IBM_ONLY_DATE_3 as date,",
						"          IBM_ONLY_DATE_4 as date,",
						"          IBM_ONLY_DATE_5 as date,",
						"          IBM_ONLY_DATE_6 as date,",
						"          IBM_ONLY_DATE_7 as date,",
						"          IBM_ONLY_DATE_8 as date,",
						"          IBM_ONLY_DATE_9 as date,",
						"          IBM_ONLY_DATE_10 as date,",
						"          IBM_ONLY_DATE_11 as date,",
						"          IBM_ONLY_DATE_12 as date,",
						"          IBM_ONLY_DATE_13 as date,",
						"          IBM_ONLY_DATE_14 as date,",
						"          IBM_ONLY_DATE_15 as date,",
						"          PUBLIC_TEXT_1 as string,",
						"          PUBLIC_TEXT_2 as string,",
						"          PUBLIC_TEXT_3 as string,",
						"          PUBLIC_TEXT_4 as string,",
						"          PUBLIC_TEXT_5 as string,",
						"          PUBLIC_TEXT_6 as string,",
						"          PUBLIC_TEXT_7 as string,",
						"          PUBLIC_TEXT_8 as string,",
						"          PUBLIC_TEXT_9 as string,",
						"          PUBLIC_TEXT_10 as string,",
						"          PUBLIC_TEXT_11 as string,",
						"          PUBLIC_TEXT_12 as string,",
						"          PUBLIC_TEXT_13 as string,",
						"          PUBLIC_TEXT_14 as string,",
						"          PUBLIC_TEXT_15 as string,",
						"          PUBLIC_TEXT_16 as string,",
						"          PUBLIC_TEXT_17 as string,",
						"          PUBLIC_TEXT_18 as string,",
						"          PUBLIC_TEXT_19 as string,",
						"          PUBLIC_TEXT_20 as string,",
						"          PUBLIC_TEXT_21 as string,",
						"          PUBLIC_TEXT_22 as string,",
						"          PUBLIC_TEXT_23 as string,",
						"          PUBLIC_TEXT_24 as string,",
						"          PUBLIC_TEXT_25 as string,",
						"          PUBLIC_TEXT_26 as string,",
						"          PUBLIC_TEXT_27 as string,",
						"          PUBLIC_TEXT_28 as string,",
						"          PUBLIC_TEXT_29 as string,",
						"          PUBLIC_TEXT_30 as string,",
						"          PUBLIC_TEXT_31 as string,",
						"          PUBLIC_TEXT_32 as string,",
						"          PUBLIC_TEXT_33 as string,",
						"          PUBLIC_TEXT_34 as string,",
						"          PUBLIC_TEXT_35 as string,",
						"          PUBLIC_DATE_1 as date,",
						"          PUBLIC_DATE_2 as date,",
						"          PUBLIC_DATE_3 as date,",
						"          PUBLIC_DATE_4 as date,",
						"          PUBLIC_DATE_5 as date,",
						"          PUBLIC_DATE_6 as date,",
						"          PUBLIC_DATE_7 as date,",
						"          PUBLIC_DATE_8 as date,",
						"          PUBLIC_DATE_9 as date,",
						"          PUBLIC_DATE_10 as date,",
						"          PUBLIC_DATE_11 as date,",
						"          PUBLIC_DATE_12 as date,",
						"          PUBLIC_DATE_13 as date,",
						"          PUBLIC_DATE_14 as date,",
						"          PUBLIC_DATE_15 as date,",
						"          STATE_OF_ORGANIZATION as string,",
						"          SCTR_NM as string,",
						"          ERO_CHCKD_DT as date,",
						"          EA_CHCKD_DT as date,",
						"          CE_CHCKD_DT as date,",
						"          EXPORT_REG as string,",
						"          ENVMNTL_AFF as string,",
						"          CMPLNCE_ENGG as string,",
						"          CLIENT_UNIT as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'SELECT \\nC.CNTRCT_DIM_UID, C.CNTRCT_NM, Z.GLBL_BUY_GRP_ID,\\n P.PROC_DIM_UID UNIQUE_ID, \\nP.PROC_DSPLY_ID AS DISPLAYED_ID, \\nPR.PROC_DIM_UID AS PARENT_UNIQUE_ID,\\n PR.PROC_DSPLY_ID AS PARENT_VISIBLE_ID, PR.TITLE_TXT AS PARENT_TITLE, COALESCE(P.CLNT_REF_NUM, \\'\\') AS CLIENT_REFERENCE_NUM,\\n COALESCE(R.PURCHS_ORDR_NM, \\'\\') AS PURCHASE_ORDER,\\n -- PCR_REASONS.PCR_REASON_LIST as PCR_REASON,\\n   INITWDD.WKFLW_DEF_ID, INITWDD.\"DESC\" AS INIT_WKFLW_DEF_ID_DESCR, \\n   P.TITLE_TXT PROC_TITLE, P.PROC_DESC, WKFLW.WKFLW_DEF_ID CURR_WRKFLW_NM,\\n    WKFLW_STEP.WKFLW_STEP_DEF_ID, WKFLW_STEP.WKFLW_STEP_SEQ_NUM, \\n    WKFLW_STEP.WKFLW_STEP_DESC, ST.STATE_TITLE_TXT STATE_DESC, \\n    STATUS.STAT_IBM_DESC AS STATUS_DESC, \\n    --decrypt_char(ASSGNTO.CONCAT_NM_LOGIN,MIS_REP_REF.MIS_REP_REF_CD) AS ASSIGNED_TO,\\n     AUDIENCE.ADNC_TXT AS AUDIENCE,\\n     -- P.PRIORITY_TXT, \\n     COND.COND_TXT AS CONDITION, COALESCE(RGN.REGION,GEO.KYNDRYL_RGN) REGION,\\n      ONHOLD.ON_HOLD_DESC AS HOLD_STATUS, P.ON_HOLD_DT, HOLD_RSN.ON_HOLD_RSN_TXT AS HOLD_REASON, \\n      CAST(P.ADTNL_DTLS_FOR_WITHDRW_HOLD_TXT AS VARCHAR(500)) HOLD_OR_WITHDRAW_EXPLANATION, \\n      --decrypt_char(P.RQSTR_TXT, MIS_REP_REF.MIS_REP_REF_CD) AS REQUESTOR, \\n      --decrypt_char(CREATOR.CONCAT_NM_LOGIN, MIS_REP_REF.MIS_REP_REF_CD) AS CREATED_BY,\\n      H.SRC_CRETD_TMS AS CREATED_DATE,H.SRC_UPDTD_TMS AS LAST_UPDATED, \\n      F.RQST_RCVD_DT AS RQST_RCVD_BY_IBM_DATE, \\n      F.ACKNWLDG_DT AS ACKNOWLEDGED_DATE,\\n       F.RQST_QLFYCN_DT AS RQST_QUALIFIED_DATE,\\n       F.LTTR_OF_AUTHZN_SIGNTUR_DT AS LETTER_AUTH_SIGNATURE_DT, \\n       F.PLAN_RQRD_APPRVL_DT AS PLANNED_RQMTS_APPROVAL_DATE,\\n        F.RQRD_REWORK_RQST_DT AS RQMTS_REWORK_RQSTD_DATE, \\n        F.RQRD_DEF_CMPLTD_DT AS RQMTS_DEFINITION_COMPLETE_DT, F.RQRD_AGRMT_DT AS RQMTS_AGREED_DATE,\\n         F.RQRD_REJCT_DT AS CLIENT_REJECT_RQMTS_DATE, F.PLAN_SLTN_CMPLTD_DT, F.RVSD_PLAN_SLTN_CMPLTD_DT, \\n         F.TCHNLGY_DLVR_ASSESSMT_DT, F.SLTN_CMPLTD_DT AS SOLUTION_COMPLETED_DATE, \\n         F.SLTN_APPRVL_RQRD_DT, \\n         F.CLNT_ACCPTD_SLTN_DT, \\n         F.CLNT_REJCT_SLTN_DT, F.SLTN_PKGNG_DT,\\n          F.SLTN_PKG_VALIDTD_DT AS SOLTN_PACKAGE_VALIDATE_DT, F.PRICE_CMPLTD_DT AS PRICING_COMPLETED_DT,  --  PRICING COMPLETED DATE\\n F.PRPSL_BSLN_ASSESSMT_DT,  --   PROPOSAL BASELINE ASSESSMENT \\n F.CNTRCT_BSLN_RVW_DT,  --  CONTRACT BASELINE REVIEW DATE\\n F.PRPSL_RQST_DT AS CLIENT_PRPSL_REQUESTED_DATE,  --  CLIENT PROPOSAL REQUESTED DATE\\n F.CLNT_RVSD_PRPSL_RQSTD_DT AS CLIENT_REVISED_PRPSL_REQUESTED_DATE,  --  CLIENT REVISED PROPOSAL REQUESTED DATE\\n F.PLAN_PRPSL_DT AS PLANNED_PRPSL_DATE,  --  PLANNED PROPOSAL DATE\\n F.RVSD_PLAN_PRPSL_DT AS REVISED_PLANNED_PRPSL_DATE,  --  REVISED PLANNED PROPOSAL DATE\\n F.RQST_IMPLMNT_START_DT AS  CLIENT_RQSTD_IMPL_START_DT,\\n H.DUE_DT, \\n /*DATEDIFF(day, \\'2000/01/01\\', H.DUE_DT) - DATEDIFF(day, \\'2000/01/01\\', DATEADD(hh, AF.TZ, CURRENT_TIMESTAMP)) AS DUE_IN_DAY_CNT,\\n F.PRPSL_CMPLTD_DT AS PROPOSAL_COMPLETED_DATE,  --  PROPOSAL COMPLETED DATE\\n  CASE \\n          WHEN DATEDIFF(day, \\'2000/01/01\\', ISNULL(F.PLAN_PRPSL_DT,F.RVSD_PLAN_PRPSL_DT )) - DATEDIFF(day, \\'2000/01/01\\', DATEADD(hh, AF.TZ, CURRENT_TIMESTAMP)) < 0 THEN \\'Y\\' \\n          ELSE \\'N\\' \\n     END AS OVERDUE,*/\\n F.PRPSL_APPRVL_RQRD_DT AS PROPOSAL_RESPONSE_REQUESTED_DATE,  --  PROPOSAL RESPONSE REQUESTED DATE\\n F.PRPSL_EXP_DT AS PROPOSAL_EXPIRE_DATE,  --  PROPOSAL EXPIRATION DATE\\n F.PRPSL_SENT_TO_CLNT_DT,  --  PROPOSAL SENT TO CLIENT DATE\\n F.PRPSL_ACCPTD_DT AS PROPOSAL_ACCEPTED_DATE,  --  PROPOSAL ACCEPTED DATE\\n F.PRPSL_REJCT_DT AS PROPOSAL_REJECTED_DATE,  --  PROPOSAL REJECTED DATE\\n H.CMPLTD_DT AS COMPLETION_DATE,  --  COMPLETION DATE\\n F.CLNT_SIGNTUR_RCVD_DT,  --  CLIENT SIGNATURE RECEIVED DATE\\n F.IBM_SIGNTUR_RCVD_DT,  --  IBM SIGNATURE RECEIVED DATE\\n F.RQST_IMPLMNT_CMPLTD_DT AS CLIENT_REQ_IMPLMNT_COMPLETE_DATE,  --  CLIENT REQUESTED IMPLEMENTATION COMPLETE DATE\\n F.RQST_IMPLMNT_START_DT AS CLIENT_REQ_IMPLMNT_START_DATE,  --  CLIENT REQUESTED IMPLEMENTATION START DATE\\n --decrypt_char(R.PRICER_NM, MIS_REP_REF.MIS_REP_REF_CD) AS PRICER_NM,----PRICER\\n --decrypt_char(R.PRPSL_OWNR_TXT,MIS_REP_REF.MIS_REP_REF_CD) AS PROPOSAL_OWNER, F.CLNT_RVSD_RQST_IMPL_CMPL_DT AS REVISED_CLIENT_REQ_IMPLMNT_COMPLETE_DATE,  --  CLIENT REVISED REQUESTED IMPLEMENTATION COMPLETE DATE\\n F.ENGGMT_HANDOFF_DT AS ENGAGEMENT_HANDOFF_DATE,  --  ENGAGEMENT HANDOFF DATE\\n F.PLAN_IMPLMNT_CMPLTD_DT AS PLANNED_IMPLMNT_COMPLETED_DATE,  --  PLANNED IMPLEMENTATION COMPLETED DATE\\n F.RVSD_PLN_IMPLMNT_CMPLTD_DT AS REVISED_PLANNED_IMPLMNT_COMPLETED_DATE,  --  REVISED PLANNED IMPLEMENTATION COMPLETED DATE\\n F.WITHDRWN_DT,  --  WITHDRAWN DATE\\n P.CMPLTD_RSN_TXT, R.RJCT_RSN_CD AS REJECT_REWORK_REASON,\\n  R.RJCT_SRC_CD AS REJECT_REWORK_SOURCE, R.RJCT_EXPLNN_TXT AS REJECT_REWORK_EXPLANATION, \\n  SF.LAG_DAYS_NUM, COALESCE(R.CLNT_AFFCT_AREA_TXT, \\'\\') AS CLIENT_AFFECTED_AREA, COALESCE(R.CLNT_ACCNTG_CD, \\'\\') AS CLIENT_ACCNTG_CD,\\n  -- decrypt_char(R.CLNT_CNTCT_NM, MIS_REP_REF.MIS_REP_REF_CD) AS CLIENT_CONTACT_NAME, \\n  COALESCE(R.AGRMT_REF_DTL_TXT, \\'\\') AS AGREEMENT_REF_DETAILS, COALESCE(R.ORGNTG_ORG, \\'\\') AS ORIGINATING_ORG,\\n   COALESCE(R.UNSLCTD_RQST_IND, \\'\\') AS UNSOLICITED_REQUEST, COALESCE(R.CNTRCT_CHNG_RQRD_IND, \\'\\') AS CONTRACT_CHANGE_REQUIRED,\\n    COALESCE(R.OATS_RCD_RRQRD_IND, \\'\\') AS OATS_RECORD_NEEDED, COALESCE(R.NON_STD_T_AND_C_IND, \\'\\') AS NON_STD_TERMS_CONDITIONS,\\n     COALESCE(R.TRVL_INVC_IND, \\'\\') TRAVEL_EXPENSES_INVOICED, COALESCE(R.TERMNTN_CHRGS_IND, \\'\\') AS TERMINATION_CHARGES, \\n     R.RNWL_MNTH_CD AS ANNUAL_RENEWAL_MONTH, COALESCE(R.SRVC_PRVDR_ORGNZN_TXT, \\'\\') AS SERVICE_PROVIDER_ORG,\\n      --TRIM(SUBSTRING(STYPES.SRVC_TYPE_LIST,1,300)) AS SERVICE_TYPES, COALESCE(R.RFS_GROUP_NM, \\'\\') AS REQUEST_GROUP_NAME, \\n      TRIM(SUBSTRING(R.PREDEFND_SRVCS_DESC,1,200)) AS PREDEFINED_SRVCS, GEO.SRGNCTRY_NM AS COUNTRY, \\n      COALESCE(R.SIEBEL_OPPRTNTY_NUM, \\'\\') AS SIEBEL_OPP_NUM, R.BTT_NUM, CMPLXTY.CMPLXTY_DESC AS COMPLEXITY,\\n      -- decrypt_char(R.SLTN_OWNR_TXT, MIS_REP_REF.MIS_REP_REF_CD) AS SOLUTION_OWNER,\\n      -- decrypt_char(R.DLVR_CRDNTR_TXT, MIS_REP_REF.MIS_REP_REF_CD) AS DELIVERY_COORDINATOR,\\n      -- decrypt_char(R.SLTN_DSGNR_TXT, MIS_REP_REF.MIS_REP_REF_CD) AS SOLUTION_DESIGNER,\\n       SF.ESTMTD_SLTN_HRS_NUM AS ESTIMATED_SOLUTION_HOURS, COALESCE(R.CLAIM_ACCNT_CD, \\'\\') AS CLAIM_ACCOUNT_CODE,\\n         --  CLAIM ACCOUNT CODE FOR REQUEST REPSPONSE\\n --TRIM(SUBSTRING(CL.CLAIM_WORKITEM_CODES,1,200)) AS CLAIM_WORKITEM_CODES, \\n SF.ESTMTD_IMPLMNT_HRS_NUM AS EST_IMPLEMENTATION_HOURS,\\n COALESCE(R.RSRC_SUMRY_PRSNL_INVOLV_IND, \\'\\') AS PERSONNEL_CHANGES,\\n COALESCE(R.RSRC_SUMRY_SERVER_COM_IND, \\'\\') AS SERVER_COMPONENT, COALESCE(R.RSRC_SUMRY_STRG_COM_IND, \\'\\') AS STORAGE_COMPONENT, \\n COALESCE(R.RSRC_SUMRY_HW_COM_IND, \\'\\') AS HARDWARE_COMPONENT,\\n COALESCE(R.RSRC_SUMRY_SOFTWR_COM_IND, \\'\\') AS SOFTWARE_COMPONENT, \\n CRNCY.CRNCY_NM AS LOCAL_CURRENCY, ALT_CUR.CRNCY_NM AS ALT_CURRENCY, \\n --PRICETYPE.PRICE_TYPE_LIST,  --  PRICE TYPE\\n SF.ESTMTD_OTC_AMT AS ESTIMATED_OTC_LOCAL_CURR,  --  ESTIMATED OTC IN LOCAL CURRENCY\\n SF.ESTMTD_TCV_AMT AS ESTIMATED_TCV_LOCAL_CURR,  --  ESTIMATED TCV IN LOCAL CURRENCY\\n SF.ESTMTD_PRPSL_PRDCT_COST_AMT AS ESTIMATED_PRPSL_PROD_COST_LOCAL_CURR,\\n  SF.ACTL_PRPSL_PRDCT_COST_AMT AS ACTUAL_PRPSL_PROD_COST_LOCAL_CURR, \\n  SF.NON_RECURG_COST_AMT AS NON_RECURRING_COST_LOCAL_CURR,  --  NON RECURRING COST IN LOCAL CURRENCY\\n SF.NON_RECURG_COST_ALT_CRNCY_AMT AS NON_RECURRING_COST_ALT_CURR,  --  NON RECURRING COST IN ALTERNATE CURRENCY\\n SF.MONTHLY_RECURG_COST_AMT AS MNTHLY_RECURRING_COST_LOCAL_CURR,  --  MONTHLY RECURRING COST IN LOCAL CURRENCY\\n SF.MTH_RECURG_COST_ALT_CRNCY_AMT AS MNTHLY_RECURRING_COST_ALT_CURR,  --  MONTHLY RECURRING COST IN ALTERNATE CURRENCY\\n SF.TOTAL_CNTRCT_COST_AMT AS TOTAL_CNTRCT_COST_LOCAL_CURR,  --  TOTAL CONTRACT COST IN LOCAL CURRENCY\\n SF.TOTAL_CNTRCT_COST_ALT_CRNCY_AMT AS TOTAL_CNTRCT_COST_ALT_CURR,  --  TOTAL CONTRACT COST IN ALTERNATE CURRENCY\\n SF.NON_RECURG_PRICE_AMT AS NON_RECURRING_PRICE_LOCAL_CURR,  --  NON RECURRING PRICE IN LOCAL CURRENCY\\n SF.NON_RECURG_PRCE_ALT_CRNCY_AMT AS NON_RECURRING_PRICE_ALT_CURR,  --  NON RECURRING PRICE IN ALTERNATE CURRENCY\\n SF.MONTHLY_RECURG_CHRGS_AMT AS MNTHLY_RECURRING_CHARGES_LOCAL_CURR,  --  MONTHLY RECURRING CHARGES IN LOCAL CURRENCY\\n SF.MTH_RECURG_CHRG_ALT_CRNCY_AMT AS MNTHLY_RECURRING_CHARGES_ALT_CURR, SF.TOTAL_CNTRCT_PRICE_AMT AS TOTAL_CNTRCT_PRICE_LOCAL_CURR,  --  TOTAL CONTRACT PRICE IN LOCAL CURRENCY\\n SF.TOTAL_CNTRCT_PRICE_ALT_CRNCY_AMT AS TOTAL_CNTRCT_PRICE_ALT_CURR,  --  TOTAL CONTRACT PRICE IN ALTERNATE CURRENCY\\n CLIENT_FP.USER_NAME AS CLIENT_FOCAL_POINT,  --  CLIENT FOCAL POINT\\n RFS_MGR.USER_NAME AS IBM_RFS_MANAGER,  --  IBM RFS MANAGER\\n IRTL.USER_NAME AS REQUIREMENTS_TEAM_LEAD,  --  REQUIREMENTS TEAM LEAD\\n COCR.USER_NAME AS CLIENT_CATALOG_ONLY_REQUESTOR,  --  CLIENT CATALOG ONLY REQUESTOR\\n COCA.USER_NAME AS CLIENT_CATALOG_ONLY_APPROVER,  --  CLIENT CATALOG ONLY APPROVER\\n COCBA.USER_NAME AS CLIENT_CTLG_ONLY_BUDGET_APPROVER,  --  CLIENT CATALOG ONLY BUDGET APPROVER\\n COCFP.USER_NAME AS CLIENT_CTLG_ONLY_FOCAL_POINT,  --  CLIENT CATALOG ONLY FOCAL POINT\\n SRC.SRC_SYS_CD, \\'\\' AS LEGACY_UNIQUE_ID, \\'\\' AS LEGACY_DISPLAYED_ID, TRIM(INOTES.NOTES_DESC) AS LATEST_IBM_ONLY_NOTE, TRIM(PNOTES.NOTES_DESC) AS LATEST_IBM_AND_CLIENT_NOTE, TRIM(SUBSTRING(L30_1.OFFRNG_DESC,1,80)) AS L30_OFFERING_1,  --  L30 OFFING CODE 1\\n TRIM(SUBSTRING(L30_2.OFFRNG_DESC,1,80)) AS L30_OFFERING_2,  --  L30 OFFING CODE 2\\n TRIM(SUBSTRING(L30_3.OFFRNG_DESC,1,80)) AS L30_OFFERING_3,  --  L30 OFFING CODE 3\\n SF.PRIOR_TOTAL_CNTRCT_PRICE_AMT,  --  PRIOR TOTAL CONTRACT PRICE IN LOCAL CURRENCY\\n CF.PRPSL_CHG_RSN_TXT,  --  REASON FOR PROPOSAL REQUESTED DATE CHANGE\\n CF.PRPSL_CHG_EXPLNN_TXT,  --  EXPLANATION FOR PROPOSAL REQUESTED DATE CHANGE\\n CF.IMPLMTN_CHG_RSN_TXT,  --  REASON FOR IMPLEMENTATION REQUESTED DATE CHANGE\\n CF.IMPLMTN_CHG_EXPLNN_TXT,  --  EXPLANATION FOR IMPLEMENTATION REQUESTED DATE CHANGE\\n CF.PRPSL_LATE_RSN_TXT,  --  REASON FOR MISSING THE PROPOSAL REQUESTED DATE\\n CF.PRPSL_LATE_EXPLNN_TXT,  --  EXPLANATION FOR MISSING THE PROPOSAL REQUESTED DATE\\n CF.IMPLMTN_LATE_RSN_TXT,  --  REASON FOR MISSING THE IMPLEMENTATION REQUESTED DATE\\n CF.IMPLMTN_LATE_EXPLNN_TXT,  --  EXPLANATION FOR MISSING THE IMPLEMENTATION REQUESTED DATE\\n CA.USER_NAME AS CLIENT_APPROVER, CBA.USER_NAME AS CLIENT_BUDGET_APPROVER, DATAIBM.IBM_ONLY_TEXT_1, DATAIBM.IBM_ONLY_TEXT_2, DATAIBM.IBM_ONLY_TEXT_3, DATAIBM.IBM_ONLY_TEXT_4, DATAIBM.IBM_ONLY_TEXT_5, DATAIBM.IBM_ONLY_TEXT_6, DATAIBM.IBM_ONLY_TEXT_7, DATAIBM.IBM_ONLY_TEXT_8, DATAIBM.IBM_ONLY_TEXT_9, DATAIBM.IBM_ONLY_TEXT_10, DATAIBM.IBM_ONLY_TEXT_11, DATAIBM.IBM_ONLY_TEXT_12, DATAIBM.IBM_ONLY_TEXT_13, DATAIBM.IBM_ONLY_TEXT_14, DATAIBM.IBM_ONLY_TEXT_15, DATAIBM.IBM_ONLY_TEXT_16, DATAIBM.IBM_ONLY_TEXT_17, DATAIBM.IBM_ONLY_TEXT_18, DATAIBM.IBM_ONLY_TEXT_19, DATAIBM.IBM_ONLY_TEXT_20, DATAIBM.IBM_ONLY_TEXT_21, DATAIBM.IBM_ONLY_TEXT_22, DATAIBM.IBM_ONLY_TEXT_23, DATAIBM.IBM_ONLY_TEXT_24, DATAIBM.IBM_ONLY_TEXT_25, DATAIBM.IBM_ONLY_DATE_1, DATAIBM.IBM_ONLY_DATE_2, DATAIBM.IBM_ONLY_DATE_3, DATAIBM.IBM_ONLY_DATE_4, DATAIBM.IBM_ONLY_DATE_5, DATAIBM.IBM_ONLY_DATE_6, DATAIBM.IBM_ONLY_DATE_7, DATAIBM.IBM_ONLY_DATE_8, DATAIBM.IBM_ONLY_DATE_9, DATAIBM.IBM_ONLY_DATE_10, DATAIBM.IBM_ONLY_DATE_11, DATAIBM.IBM_ONLY_DATE_12, DATAIBM.IBM_ONLY_DATE_13, DATAIBM.IBM_ONLY_DATE_14, DATAIBM.IBM_ONLY_DATE_15, DATAPBLIC.PUBLIC_TEXT_1, DATAPBLIC.PUBLIC_TEXT_2, DATAPBLIC.PUBLIC_TEXT_3, DATAPBLIC.PUBLIC_TEXT_4, DATAPBLIC.PUBLIC_TEXT_5, DATAPBLIC.PUBLIC_TEXT_6, DATAPBLIC.PUBLIC_TEXT_7, DATAPBLIC.PUBLIC_TEXT_8, DATAPBLIC.PUBLIC_TEXT_9, DATAPBLIC.PUBLIC_TEXT_10, DATAPBLIC.PUBLIC_TEXT_11, DATAPBLIC.PUBLIC_TEXT_12, DATAPBLIC.PUBLIC_TEXT_13, DATAPBLIC.PUBLIC_TEXT_14, DATAPBLIC.PUBLIC_TEXT_15, DATAPBLIC.PUBLIC_TEXT_16, DATAPBLIC.PUBLIC_TEXT_17, DATAPBLIC.PUBLIC_TEXT_18, DATAPBLIC.PUBLIC_TEXT_19, DATAPBLIC.PUBLIC_TEXT_20, DATAPBLIC.PUBLIC_TEXT_21, DATAPBLIC.PUBLIC_TEXT_22, DATAPBLIC.PUBLIC_TEXT_23, DATAPBLIC.PUBLIC_TEXT_24, DATAPBLIC.PUBLIC_TEXT_25, DATAPBLIC.PUBLIC_TEXT_26, DATAPBLIC.PUBLIC_TEXT_27, DATAPBLIC.PUBLIC_TEXT_28, DATAPBLIC.PUBLIC_TEXT_29, DATAPBLIC.PUBLIC_TEXT_30, DATAPBLIC.PUBLIC_TEXT_31, DATAPBLIC.PUBLIC_TEXT_32, DATAPBLIC.PUBLIC_TEXT_33, DATAPBLIC.PUBLIC_TEXT_34, DATAPBLIC.PUBLIC_TEXT_35, DATAPBLIC.PUBLIC_DATE_1, DATAPBLIC.PUBLIC_DATE_2, DATAPBLIC.PUBLIC_DATE_3, DATAPBLIC.PUBLIC_DATE_4, DATAPBLIC.PUBLIC_DATE_5, DATAPBLIC.PUBLIC_DATE_6, DATAPBLIC.PUBLIC_DATE_7, DATAPBLIC.PUBLIC_DATE_8, DATAPBLIC.PUBLIC_DATE_9, DATAPBLIC.PUBLIC_DATE_10, DATAPBLIC.PUBLIC_DATE_11, DATAPBLIC.PUBLIC_DATE_12, DATAPBLIC.PUBLIC_DATE_13, DATAPBLIC.PUBLIC_DATE_14, DATAPBLIC.PUBLIC_DATE_15, SOO.STAT_OF_ORGNZN_DESC AS STATE_OF_ORGANIZATION, SCTR.SCTR_NM\\n, R.ERO_CHCKD_DT, R.EA_CHCKD_DT, R.CE_CHCKD_DT, R.EXPORT_REG, R.ENVMNTL_AFF, R.CMPLNCE_ENGG, Z.CLIENT_UNIT_NM AS CLIENT_UNIT\\n\\nFROM PGMPDM.CNTRCT_DIM C\\n\\nINNER JOIN PGMPDM.PROC_DIM P ON P.CNTRCT_DIM_UID = C.CNTRCT_DIM_UID AND P.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.PCR_CRC_DIM R ON R.PCR_CRC_DIM_UID = P.PROC_DIM_UID AND R.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.PROC_HEADR_FCT H ON H.PROC_DIM_UID = P.PROC_DIM_UID AND H.ROW_STAT_CD <> \\'D\\'\\n\\nINNER JOIN PGMPDM.PCR_CRC_FCT F ON F.PROC_HEADR_FCT_UID =H.PROC_HEADR_FCT_UID AND F.ROW_STAT_CD <> \\'D\\'       \\nINNER JOIN PGMPDM.GEO_DIM GEO ON GEO.GEO_DIM_UID =H.GEO_DIM_UID \\n--AND GEO.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.SCTR_DIM SECTOR ON SECTOR.SCTR_DIM_UID = F.SCTR_DIM_UID AND SECTOR.ROW_STAT_CD <> \\'D\\'\\n\\nINNER JOIN PGMPDM.WKFLW_DEF_DIM WKFLW ON WKFLW.WKFLW_DEF_DIM_UID =H.WKFLW_DEF_DIM_UID AND WKFLW.ROW_STAT_CD <> \\'D\\' \\nINNER JOIN PGMPDM.WKFLW_STEP_DEF_DIM WKFLW_STEP ON WKFLW_STEP.WKFLW_STEP_DEF_DIM_UID =H.WKFLW_STEP_DEF_DIM_UID AND WKFLW_STEP.ROW_STAT_CD <> \\'D\\'    \\nINNER JOIN PGMPDM.CRNCY_DIM CRNCY ON CRNCY.CRNCY_DIM_UID = F.LOCAL_CRNCY_DIM_UID AND CRNCY.ROW_STAT_CD <> \\'D\\'\\n\\nINNER JOIN PGMPDM.CRNCY_DIM ALT_CUR ON ALT_CUR.CRNCY_DIM_UID = F.ALT_CRNCY_DIM_UID AND ALT_CUR.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.CMPLXTY_DIM CMPLXTY ON CMPLXTY.CMPLXTY_DIM_UID = R.CMPLXTY_DIM_UID AND CMPLXTY.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.RQST_TYPE_DIM RTYPE ON RTYPE.RQST_TYPE_DIM_UID = R.RQST_TYPE_DIM_UID AND RTYPE.ROW_STAT_CD <> \\'D\\'\\n\\nINNER JOIN PGMPDM.SRC_SYS_DIM SRC ON SRC.SRC_SYS_DIM_UID = R.SRC_SYS_DIM_UID AND SRC.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.STATE_DIM ST ON ST.STATE_DIM_UID =H.STATE_DIM_UID AND ST.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.PROC_STEP_DAT_DIM PSD ON P.PROC_DIM_UID = PSD.PROC_DIM_UID AND P.CURR_PROC_STEP_DAT_DIM_UID = PSD.PROC_STEP_DAT_DIM_UID AND PSD.ROW_STAT_CD <> \\'D\\'\\n\\nINNER JOIN PGMPDM.ON_HOLD_DIM ONHOLD ON ONHOLD.ON_HOLD_DIM_UID =H.ON_HOLD_DIM_UID AND ONHOLD.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.WINLOSS_DIM WINLOSS ON WINLOSS.WINLOSS_DIM_UID = P.WINLOSS_DIM_UID AND WINLOSS.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.PROC_TYPE_DIM PTYPE ON PTYPE.PROC_TYPE_DIM_UID =H.PROC_TYPE_DIM_UID AND PTYPE.ROW_STAT_CD <> \\'D\\'\\n\\nINNER JOIN PGMPDM.CNTRCT_TYPE_DIM CTYPE ON CTYPE.CNTRCT_TYPE_DIM_UID = C.CNTRCT_TYPE_DIM_UID AND CTYPE.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.WKFLW_DEF_DIM INITWDD ON INITWDD.WKFLW_DEF_DIM_UID =H.INIT_WKFLW_DEF_DIM_UID AND INITWDD.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.SCTR_DIM SCTR ON SCTR.SCTR_DIM_UID = C.SCTR_DIM_UID AND SCTR.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.MISC_REP_REF MIS_REP_REF ON MIS_REP_REF_UID = 3\\n\\nLEFT OUTER JOIN PGMPDM.WITHDRWN_DIM WD ON WD.WITHDRWN_DIM_UID =H.WITHDRWN_DIM_UID AND WD.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.CATLG_RQST_HEADR_DIM CTLG ON CTLG.PROC_DIM_UID = P.PROC_DIM_UID AND CTLG.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.STAT_DIM STATUS ON STATUS.STAT_DIM_UID =H.STAT_DIM_UID AND STATUS.ROW_STAT_CD <> \\'D\\'\\n\\nLEFT OUTER JOIN PGMPDM.NOTES_DIM INOTES ON INOTES.PROC_DIM_UID = P.PROC_DIM_UID AND INOTES.ROW_STAT_CD <> \\'D\\' AND INOTES.NOTE_TYPE_CD = \\'I\\' AND INOTES.LATEST_NOTE_IND = \\'Y\\'\\nLEFT OUTER JOIN PGMPDM.NOTES_DIM PNOTES ON PNOTES.PROC_DIM_UID = P.PROC_DIM_UID AND PNOTES.ROW_STAT_CD <> \\'D\\' AND PNOTES.NOTE_TYPE_CD = \\'P\\' AND PNOTES.LATEST_NOTE_IND = \\'Y\\'\\nLEFT OUTER JOIN PGMPDM.ADNC_DIM AUDIENCE ON AUDIENCE.ADNC_CD = P.ADNC_CD\\n\\nLEFT OUTER JOIN PGMPDM.PRPSL_PRICE_TERM_DIM PTERM ON PTERM.PRPSL_PRICE_TERM_DIM_CD = \\'F\\' AND PTERM.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM PUBLIC_DATA ON PUBLIC_DATA.PROC_DIM_UID = P.PROC_DIM_UID\\nLEFT OUTER JOIN PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM PRIVATE_DATA ON PRIVATE_DATA.PROC_DIM_UID = P.PROC_DIM_UID\\n\\nLEFT OUTER JOIN PGMPDM.RQST_PRICE_TYPE_FCT PRICETYPFCT ON PRICETYPFCT.PROC_DIM_UID = P.PROC_DIM_UID AND PRICETYPFCT.ROW_STAT_CD <> \\'D\\'\\n--LEFT OUTER JOIN PGMPDM.PRICE_TYPES_V PRICETYPE ON PRICETYPE.PROC_DIM_UID =H.PROC_DIM_UID\\nLEFT OUTER JOIN PGMPDM.ORG_CNTRCT_MAP_DIM ORG_MAP ON ORG_MAP.CNTRCT_DIM_UID = C.CNTRCT_DIM_UID\\n\\nLEFT OUTER JOIN PGMPDM.ORGNZN_DIM ORG ON ORG.ORGNZN_DIM_UID = ORG_MAP.ORGNZN_DIM_UID\\nLEFT OUTER JOIN PGMPDM.STAT_OF_ORGNZN_DIM SOO ON SOO.STAT_OF_ORGNZN_DIM_UID = ORG.STAT_OF_ORGNZN_DIM_UID\\nLEFT OUTER JOIN PGMPDM.ON_HOLD_RSN_DIM HOLD_RSN ON HOLD_RSN.ON_HOLD_RSN_ID = P.ON_HOLD_RSN_ID\\n\\nLEFT OUTER JOIN APPFUN.PROC_REGION_V RGN ON RGN.PROC_ID =H.PROC_DIM_UID AND RGN.INTERNAL_VAL <> \\'NONE\\'\\nLEFT OUTER JOIN PGMPDM.PROC_DIM PR ON PR.PROC_DIM_UID = P.PARNT_PROC_ID AND PR.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.PROC_DIM PROJECT ON PROJECT.PROC_DIM_UID = R.PRJCT_DIM_UID AND PROJECT.ROW_STAT_CD <> \\'D\\'\\n\\nLEFT OUTER JOIN PGMPDM.PROC_HEADR_FCT PRNT_HDR ON PRNT_HDR.PROC_DIM_UID = PR.PROC_DIM_UID AND PRNT_HDR.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.PROC_TYPE_DIM PRNT_TYPE ON PRNT_TYPE.PROC_TYPE_DIM_UID = PRNT_HDR.PROC_TYPE_DIM_UID AND PRNT_TYPE.ROW_STAT_CD <> \\'D\\'\\n--LEFT OUTER JOIN PGMPDM.PCR_REASONS_V PCR_REASONS ON PCR_REASONS.PROC_DIM_UID =H.PROC_DIM_UID\\n\\nLEFT OUTER JOIN PGMPDM.GEO_DIM RQST_GEO ON RQST_GEO.GEO_DIM_UID = R.GEO_DIM_UID AND RQST_GEO.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.SO_NONSO_DIM IS_NONIS ON IS_NONIS.SO_NONSO_DIM_UID = C.SO_NONSO_DIM_UID AND  IS_NONIS.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.USER_DIM CREATOR ON CREATOR.USER_DIM_UID =H.CRETD_BY_USER_DIM_UID AND CREATOR.ROW_STAT_CD <> \\'D\\'\\n\\nLEFT OUTER JOIN PGMPDM.USER_DIM ASSGNTO ON ASSGNTO.USER_ID = PSD.ASSGN_TO_NUM AND ASSGNTO.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.PRIORITY_DIM PD ON PD.PRIORITY_NUM = P.PRIORTY_NUM\\nLEFT OUTER JOIN PGMPDM.COND_DIM COND ON COND.COND_CD = P.CNDTN_CD\\n\\n--LEFT OUTER JOIN PGMPDM.CLAIM_WORKITEM_CODES_V CL ON CL.PROC_DIM_UID = P.PROC_DIM_UID\\n--LEFT OUTER JOIN PGMPDM.SERVICE_TYPES_V STYPES ON STYPES.PROC_DIM_UID = P.PROC_DIM_UID\\nLEFT OUTER JOIN PGMPDM.L30_OFFRNG_DIM L30_1 ON L30_1.OFFRNG_ID = R.L30_OFFRNG_ID_1 AND L30_1.ROW_STAT_CD <> \\'D\\'\\n\\nLEFT OUTER JOIN PGMPDM.L30_OFFRNG_DIM L30_2 ON L30_2.OFFRNG_ID = R.L30_OFFRNG_ID_2 AND L30_2.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.L30_OFFRNG_DIM L30_3 ON L30_3.OFFRNG_ID = R.L30_OFFRNG_ID_3 AND L30_3.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.DATE_CHG_RSN_DIM CF ON CF.DATE_CHG_RSN_DIM_UID = P.PROC_DIM_UID AND CF.ROW_STAT_CD <> \\'D\\'\\n\\nLEFT OUTER JOIN PGMPDM.PROC_ROLE_USER RFS_MGR ON RFS_MGR.ROLE_ID = 3 AND RFS_MGR.PROC_ID = P.PROC_DIM_UID AND RFS_MGR.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.PROC_ROLE_USER CLIENT_FP ON CLIENT_FP.ROLE_ID = 2 AND CLIENT_FP.PROC_ID = P.PROC_DIM_UID AND CLIENT_FP.ROW_STAT_CD <> \\'D\\'      \\nLEFT OUTER JOIN PGMPDM.PROC_ROLE_USER CA ON CA.ROLE_ID = 1 AND CA.PROC_ID = P.PROC_DIM_UID AND CA.ROW_STAT_CD <> \\'D\\'\\n\\nLEFT OUTER JOIN PGMPDM.PROC_ROLE_USER CBA ON CBA.ROLE_ID = 24 AND CBA.PROC_ID = P.PROC_DIM_UID AND CBA.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DATAPBLIC ON DATAPBLIC.PROC_DIM_UID = P.PROC_DIM_UID\\nLEFT OUTER JOIN PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DATAIBM ON DATAIBM.PROC_DIM_UID = P.PROC_DIM_UID\\n\\nLEFT OUTER JOIN PGMPDM.ZAUX_CNTRCT_GLBL_BUY_GRP_MAP Z ON Z.CNTRCT_DIM_UID = C.CNTRCT_DIM_UID\\n\\nLEFT OUTER JOIN ( \\nSELECT PROC_HEADR_FCT_UID AS PROC_DIM_UID, SUM(ESTMTD_PRPSL_PRDCT_COST_AMT) AS ESTMTD_PRPSL_PRDCT_COST_AMT, \\nSUM(ACTL_PRPSL_PRDCT_COST_AMT) AS ACTL_PRPSL_PRDCT_COST_AMT, SUM(NON_RECURG_COST_AMT) AS NON_RECURG_COST_AMT, \\nSUM(NON_RECURG_COST_ALT_CRNCY_AMT) AS NON_RECURG_COST_ALT_CRNCY_AMT, SUM(MONTHLY_RECURG_COST_AMT) AS MONTHLY_RECURG_COST_AMT,\\n SUM(MTH_RECURG_COST_ALT_CRNCY_AMT) AS MTH_RECURG_COST_ALT_CRNCY_AMT, SUM(TOTAL_CNTRCT_COST_AMT) AS TOTAL_CNTRCT_COST_AMT, \\n SUM(TOTAL_CNTRCT_COST_ALT_CRNCY_AMT) AS TOTAL_CNTRCT_COST_ALT_CRNCY_AMT, SUM(NON_RECURG_PRICE_AMT) AS NON_RECURG_PRICE_AMT, \\n SUM(NON_RECURG_PRCE_ALT_CRNCY_AMT) AS NON_RECURG_PRCE_ALT_CRNCY_AMT, SUM(MONTHLY_RECURG_CHRGS_AMT) AS MONTHLY_RECURG_CHRGS_AMT, \\n SUM(MTH_RECURG_CHRG_ALT_CRNCY_AMT) AS MTH_RECURG_CHRG_ALT_CRNCY_AMT, SUM(TOTAL_CNTRCT_PRICE_AMT) AS TOTAL_CNTRCT_PRICE_AMT,\\n SUM(TOTAL_CNTRCT_PRICE_ALT_CRNCY_AMT) AS TOTAL_CNTRCT_PRICE_ALT_CRNCY_AMT, SUM(ESTMTD_OTC_AMT) AS ESTMTD_OTC_AMT,\\n SUM(ESTMTD_TCV_AMT) AS ESTMTD_TCV_AMT, SUM(ESTMTD_SLTN_HRS_NUM) AS ESTMTD_SLTN_HRS_NUM, SUM(ESTMTD_IMPLMNT_HRS_NUM) AS ESTMTD_IMPLMNT_HRS_NUM, \\n SUM(PRIOR_TOTAL_CNTRCT_PRICE_AMT) AS PRIOR_TOTAL_CNTRCT_PRICE_AMT, SUM(LAG_DAYS_NUM) AS LAG_DAYS_NUM \\n FROM PGMPDM.PCR_CRC_FCT FCT GROUP BY FCT.PROC_HEADR_FCT_UID )  AS SF ON SF.PROC_DIM_UID =H.PROC_DIM_UID\\n\\nLEFT OUTER JOIN PGMPDM.PROC_ROLE_USER IRTL ON IRTL.ROLE_ID = 30 AND IRTL.PROC_ID = SF.PROC_DIM_UID AND IRTL.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.PROC_ROLE_USER COCR ON COCR.ROLE_ID = 26 AND COCR.PROC_ID = SF.PROC_DIM_UID AND COCR.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.PROC_ROLE_USER COCA ON COCA.ROLE_ID = 27 AND COCA.PROC_ID = SF.PROC_DIM_UID AND COCA.ROW_STAT_CD <> \\'D\\'\\n\\nLEFT OUTER JOIN PGMPDM.PROC_ROLE_USER COCBA ON COCBA.ROLE_ID = 29 AND COCBA.PROC_ID = SF.PROC_DIM_UID AND COCBA.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.PROC_ROLE_USER COCFP ON COCFP.ROLE_ID = 28 AND COCFP.PROC_ID = SF.PROC_DIM_UID AND COCFP.ROW_STAT_CD <> \\'D\\'\\n--LEFT OUTER JOIN PGMPDM.CNTRCT_TZ_V AF ON C.CNTRCT_DIM_UID = AF.CNTRCT_DIM_UID WHERE C.ROW_STAT_CD <> \\'D\\' \\nAND H.DELD_DT IS NULL AND SRC.SRC_SYS_CD = \\'PGMP\\'',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceDataforPCR",
						"getSourceDataforPCR alterRow(upsertIf(true())) ~> upsertRows",
						"upsertRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'PCR_RPT',",
						"     insertable: false,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> upsertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0003_CP_PROJECT_DELIVR_RPT')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job creation:05-06-2022\nJob name: DF_BALD0003_CP_PROJECT_DELIVR_RPT\nCreated By: Varaprasad",
				"folder": {
					"name": "PGMP/CP Summary & Custom Field audit"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceDataforPCR"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "upsertTable"
						}
					],
					"transformations": [
						{
							"name": "upsertRows"
						}
					],
					"scriptLines": [
						"source(output(",
						"          CNTRCT_DIM_UID as integer,",
						"          CONTRACT as string,",
						"          UNIQUE_ID as integer,",
						"          DISPLAY_ID as string,",
						"          GLBL_BUY_GRP_ID as string,",
						"          PARENT_UNIQUE_ID as integer,",
						"          PARENT_RECORD_TYPE as string,",
						"          PARENT_DISPLAY_ID as string,",
						"          PARENT_TITLE as string,",
						"          CLIENT_REFERENCE as string,",
						"          PROC_TITLE as string,",
						"          PROC_DESCRIPTION as string,",
						"          CURRENT_WORKFLOW as string,",
						"          CURRENT_WKFLW_STEP as string,",
						"          CURRENT_STEP_SEQ as integer,",
						"          STEP_DESCRIPTION as string,",
						"          STATE_DESC as string,",
						"          STATUS_DESC as string,",
						"          DUE_DT as date,",
						"          RVSD_DUE_DT as date,",
						"          AUDIENCE as string,",
						"          PRIORITY_TXT as string,",
						"          CONDITION as string,",
						"          REGION as string,",
						"          CREATED_DATE as timestamp,",
						"          LAST_UPDATED as timestamp,",
						"          COMPLETION_DATE as date,",
						"          CMPLTD_RSN_TXT as string,",
						"          RECEIVING_ORGANIZATION as string,",
						"          AGREEMENT_REFERENCE_DETAILS as string,",
						"          ACCEPT_OR_COMPLETE_CRITERIA as string,",
						"          ACTUAL_DATE_DELIVERED as date,",
						"          LOCATION_OF_DELIVERABLE as string,",
						"          CLIENT_ACCEPTANCE_DATE as date,",
						"          REMARKS as string,",
						"          SRC_SYS_CD as string,",
						"          LEGACY_UNIQUE_ID as string,",
						"          LEGACY_DISPLAYED_ID as string,",
						"          LATEST_IBM_ONLY_NOTE as string,",
						"          LATEST_IBM_AND_CLIENT_NOTE as string,",
						"          CNTRCT_TYPE_DESC as string,",
						"          PROC_TYPE_ID as string,",
						"          PROC_TYPE_DESC as string,",
						"          ON_HOLD_CD as string,",
						"          WITHDRWN_DIM_UID as integer,",
						"          WITHDRWN_DT as date,",
						"          ORGNZN_DIM_UID as integer,",
						"          IBM_ONLY_TEXT_1 as string,",
						"          IBM_ONLY_TEXT_2 as string,",
						"          IBM_ONLY_TEXT_3 as string,",
						"          IBM_ONLY_TEXT_4 as string,",
						"          IBM_ONLY_TEXT_5 as string,",
						"          IBM_ONLY_TEXT_6 as string,",
						"          IBM_ONLY_DATE_1 as date,",
						"          IBM_ONLY_DATE_2 as date,",
						"          IBM_ONLY_DATE_3 as date,",
						"          IBM_ONLY_DATE_4 as date,",
						"          IBM_ONLY_DATE_5 as date,",
						"          IBM_ONLY_DATE_6 as date,",
						"          PUBLIC_TEXT_1 as string,",
						"          PUBLIC_TEXT_2 as string,",
						"          PUBLIC_TEXT_3 as string,",
						"          PUBLIC_TEXT_4 as string,",
						"          PUBLIC_TEXT_5 as string,",
						"          PUBLIC_TEXT_6 as string,",
						"          PUBLIC_DATE_1 as date,",
						"          PUBLIC_DATE_2 as date,",
						"          PUBLIC_DATE_3 as date,",
						"          PUBLIC_DATE_4 as date,",
						"          PUBLIC_DATE_5 as date,",
						"          PUBLIC_DATE_6 as date,",
						"          STATE_OF_ORGANIZATION as string,",
						"          SCTR_NM as string,",
						"          CLIENT_UNIT as string,",
						"          COUNTRY as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'SELECT C.CNTRCT_DIM_UID,\\nC.CNTRCT_NM AS CONTRACT,\\nH.PROC_DIM_UID AS UNIQUE_ID,\\nP.PROC_DSPLY_ID AS DISPLAY_ID,\\nZACNTRCT.GLBL_BUY_GRP_ID,\\nPR.PROC_DIM_UID AS PARENT_UNIQUE_ID,      \\nPRNT_PTYPE.PROC_TYPE_DESC AS PARENT_RECORD_TYPE,   \\nPR.PROC_DSPLY_ID AS PARENT_DISPLAY_ID,    \\nPR.TITLE_TXT AS PARENT_TITLE,\\nP.CLNT_REF_NUM AS CLIENT_REFERENCE,\\nP.TITLE_TXT AS PROC_TITLE,\\nP.PROC_DESC AS PROC_DESCRIPTION,\\nWKFLW.WKFLW_DEF_ID AS CURRENT_WORKFLOW,\\nWKFLW_STEP.WKFLW_STEP_DEF_ID AS CURRENT_WKFLW_STEP,\\nWKFLW_STEP.WKFLW_STEP_SEQ_NUM AS CURRENT_STEP_SEQ, \\nWKFLW_STEP.WKFLW_STEP_DESC AS STEP_DESCRIPTION,    \\nST.STATE_TITLE_TXT STATE_DESC,\\nSTATUS.STAT_IBM_DESC AS STATUS_DESC,\\nFCT.DUE_DT,\\nFCT.RVSD_DUE_DT,/*\\nDAYS(NVL(FCT.DUE_DT,FCT.RVSD_DUE_DT)) - DAYS((CURRENT_TIMESTAMP - CURRENT_TIMEZONE) + AF.TZ HOURS) AS DUE_IN_DAY_CNT,\\nCASE WHEN DAYS(NVL(FCT.RVSD_DUE_DT,FCT.DUE_DT)) - DAYS((CURRENT_TIMESTAMP - CURRENT_TIMEZONE) + AF.TZ HOURS) < 0 THEN \\'Y\\' ELSE \\'N\\' END AS OVERDUE, */      \\n--decrypt_char(ASSGNTO.CONCAT_NM_LOGIN,MIS_REP_REF.MIS_REP_REF_CD) AS ASSIGNED_TO,\\nAUDIENCE.ADNC_TXT AS AUDIENCE,\\nPD.PRIORITY_TXT,\\nCOND.COND_TXT AS CONDITION,\\nCOALESCE(RGN.REGION,GEO.KYNDRYL_RGN) AS REGION,\\n--decrypt_char(P.RQSTR_TXT,MIS_REP_REF.MIS_REP_REF_CD) AS REQUESTOR,\\n--decrypt_char(CREATOR.CONCAT_NM_LOGIN,MIS_REP_REF.MIS_REP_REF_CD) AS CREATED_BY,\\nH.SRC_CRETD_TMS AS CREATED_DATE,\\nH.SRC_UPDTD_TMS AS LAST_UPDATED,\\nH.CMPLTD_DT AS COMPLETION_DATE,\\nP.CMPLTD_RSN_TXT,\\n--decrypt_char(PDELIVER.DELVRBL_OWNR_TXT,\\n--MIS_REP_REF.MIS_REP_REF_CD) AS DELIVERABLE_OWNER,\\nCASE PDELIVER.RCVNG_ORG WHEN \\'I\\' THEN \\'IBM\\' WHEN \\'C\\' THEN \\'Client\\' ELSE \\'\\' END AS RECEIVING_ORGANIZATION,\\n--decrypt_char(PDELIVER.DELVRBL_RCPNT_TXT,MIS_REP_REF.MIS_REP_REF_CD) AS DELIVERABLE_RECIPIENT,\\nPDELIVER.AGRMT_REF_DTL_TXT AS AGREEMENT_REFERENCE_DETAILS,\\nPDELIVER.ACCPTD_COMPL_CRTR_TXT AS ACCEPT_OR_COMPLETE_CRITERIA,\\nFCT.ACT_DELVRD_DT AS ACTUAL_DATE_DELIVERED,\\nPDELIVER.LCTN_OF_DELVRBL_TXT AS LOCATION_OF_DELIVERABLE,\\nFCT.CLNT_ACCPTD_DT AS CLIENT_ACCEPTANCE_DATE,\\nPDELIVER.PRJCTDLVBL_REM_TXT AS REMARKS,\\nSRC.SRC_SYS_CD,\\n\\' \\' AS LEGACY_UNIQUE_ID,\\n\\' \\' AS LEGACY_DISPLAYED_ID,\\nTRIM(INOTES.NOTES_DESC) AS LATEST_IBM_ONLY_NOTE,\\nTRIM(PNOTES.NOTES_DESC) AS LATEST_IBM_AND_CLIENT_NOTE,\\nCTYPE.CNTRCT_TYPE_DESC,\\nPTYPE.PROC_TYPE_ID,\\nPTYPE.PROC_TYPE_DESC,\\nONHOLD.ON_HOLD_CD,\\nH.WITHDRWN_DIM_UID,\\nH.WITHDRWN_DT,\\nH.ORGNZN_DIM_UID,\\nDATAIBM.IBM_ONLY_TEXT_1,\\nDATAIBM.IBM_ONLY_TEXT_2,\\nDATAIBM.IBM_ONLY_TEXT_3,\\nDATAIBM.IBM_ONLY_TEXT_4,\\nDATAIBM.IBM_ONLY_TEXT_5,\\nDATAIBM.IBM_ONLY_TEXT_6,\\nDATAIBM.IBM_ONLY_DATE_1,\\nDATAIBM.IBM_ONLY_DATE_2,\\nDATAIBM.IBM_ONLY_DATE_3,\\nDATAIBM.IBM_ONLY_DATE_4,\\nDATAIBM.IBM_ONLY_DATE_5,\\nDATAIBM.IBM_ONLY_DATE_6,\\nDATAPBLIC.PUBLIC_TEXT_1,\\nDATAPBLIC.PUBLIC_TEXT_2,\\nDATAPBLIC.PUBLIC_TEXT_3,\\nDATAPBLIC.PUBLIC_TEXT_4,\\nDATAPBLIC.PUBLIC_TEXT_5,\\nDATAPBLIC.PUBLIC_TEXT_6,\\nDATAPBLIC.PUBLIC_DATE_1,\\nDATAPBLIC.PUBLIC_DATE_2,\\nDATAPBLIC.PUBLIC_DATE_3,\\nDATAPBLIC.PUBLIC_DATE_4,\\nDATAPBLIC.PUBLIC_DATE_5,\\nDATAPBLIC.PUBLIC_DATE_6,\\nSOO.STAT_OF_ORGNZN_DESC AS STATE_OF_ORGANIZATION,\\nSCTR.SCTR_NM,\\nZACNTRCT.CLIENT_UNIT_NM AS CLIENT_UNIT,\\nGEO.SRGNCTRY_NM AS COUNTRY\\n\\nFROM PGMPDM.PRJCTDLVBL_DIM PDELIVER\\n\\nINNER JOIN PGMPDM.PROC_HEADR_FCT H ON H.PROC_DIM_UID = PDELIVER.PRJCTDLVBL_DIM_UID AND H.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.PROC_TYPE_DIM PTYPE ON PTYPE.PROC_TYPE_DIM_UID = H.PROC_TYPE_DIM_UID AND PTYPE.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.PROC_DIM P ON P.PROC_DIM_UID = H.PROC_DIM_UID AND P.ROW_STAT_CD <> \\'D\\'\\n\\nINNER JOIN PGMPDM.CNTRCT_DIM C ON C.CNTRCT_DIM_UID = P.CNTRCT_DIM_UID AND C.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.CNTRCT_TYPE_DIM CTYPE ON CTYPE.CNTRCT_TYPE_DIM_UID = C.CNTRCT_TYPE_DIM_UID AND CTYPE.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.WKFLW_DEF_DIM INITWDD ON INITWDD.WKFLW_DEF_DIM_UID = H.INIT_WKFLW_DEF_DIM_UID AND INITWDD.ROW_STAT_CD <> \\'D\\' \\n\\nINNER JOIN PGMPDM.WKFLW_DEF_DIM WKFLW ON WKFLW.WKFLW_DEF_DIM_UID = H.WKFLW_DEF_DIM_UID AND WKFLW.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.WKFLW_STEP_DEF_DIM WKFLW_STEP ON WKFLW_STEP.WKFLW_STEP_DEF_DIM_UID = H.WKFLW_STEP_DEF_DIM_UID AND WKFLW_STEP.ROW_STAT_CD <> \\'D\\'     \\nINNER JOIN PGMPDM.STATE_DIM ST ON ST.STATE_DIM_UID = H.STATE_DIM_UID AND ST.ROW_STAT_CD <> \\'D\\'\\n\\nINNER JOIN PGMPDM.PROC_STEP_DAT_DIM PSD ON P.PROC_DIM_UID = PSD.PROC_DIM_UID AND P.CURR_PROC_STEP_DAT_DIM_UID = PSD.PROC_STEP_DAT_DIM_UID AND PSD.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.GEO_DIM GEO ON GEO.GEO_DIM_UID = H.GEO_DIM_UID \\n--AND GEO.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.MISC_FCT FCT ON FCT.PROC_HEADR_FCT_UID = H.PROC_HEADR_FCT_UID AND FCT.ROW_STAT_CD <> \\'D\\'\\n\\nINNER JOIN PGMPDM.SRC_SYS_DIM SRC ON SRC.SRC_SYS_DIM_UID = H.SRC_SYS_DIM_UID AND SRC.ROW_STAT_CD <> \\'D\\'\\nINNER JOIN PGMPDM.SCTR_DIM SCTR ON SCTR.SCTR_DIM_UID = C.SCTR_DIM_UID AND SCTR.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.STAT_DIM STATUS ON STATUS.STAT_DIM_UID = H.STAT_DIM_UID AND STATUS.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.MISC_REP_REF MIS_REP_REF ON MIS_REP_REF_UID = 3\\n\\nLEFT OUTER JOIN PGMPDM.USER_DIM ASSGNTO ON ASSGNTO.USER_ID = PSD.ASSGN_TO_NUM AND ASSGNTO.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.ADNC_DIM AUDIENCE ON AUDIENCE.ADNC_CD = P.ADNC_CD\\nLEFT OUTER JOIN PGMPDM.PRIORITY_DIM PD ON PD.PRIORITY_NUM = P.PRIORTY_NUM\\n\\nLEFT OUTER JOIN PGMPDM.COND_DIM COND ON COND.COND_CD = P.CNDTN_CD\\nLEFT OUTER JOIN APPFUN.PROC_REGION_V RGN ON RGN.PROC_ID = P.PROC_DIM_UID AND RGN.INTERNAL_VAL <> \\'NONE\\'\\nLEFT OUTER JOIN PGMPDM.USER_DIM CREATOR ON CREATOR.USER_DIM_UID = H.CRETD_BY_USER_DIM_UID AND CREATOR.ROW_STAT_CD <> \\'D\\'\\n\\nLEFT OUTER JOIN PGMPDM.NOTES_DIM INOTES ON INOTES.PROC_DIM_UID = P.PROC_DIM_UID AND INOTES.ROW_STAT_CD <> \\'D\\' AND INOTES.NOTE_TYPE_CD = \\'I\\'  AND INOTES.LATEST_NOTE_IND = \\'Y\\'\\nLEFT OUTER JOIN PGMPDM.NOTES_DIM PNOTES ON PNOTES.PROC_DIM_UID = P.PROC_DIM_UID AND PNOTES.ROW_STAT_CD <> \\'D\\' AND PNOTES.NOTE_TYPE_CD = \\'P\\'  AND PNOTES.LATEST_NOTE_IND = \\'Y\\'\\nLEFT OUTER JOIN PGMPDM.ON_HOLD_DIM ONHOLD ON ONHOLD.ON_HOLD_DIM_UID = H.ON_HOLD_DIM_UID AND ONHOLD.ROW_STAT_CD <> \\'D\\'\\n\\nLEFT OUTER JOIN PGMPDM.PROC_DIM PR ON PR.PROC_DIM_UID = P.PARNT_PROC_ID AND PR.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.PROC_HEADR_FCT PARENT_FCT ON PARENT_FCT.PROC_HEADR_FCT_UID = PR.PROC_DIM_UID AND PARENT_FCT.ROW_STAT_CD <> \\'D\\'\\nLEFT OUTER JOIN PGMPDM.PROC_TYPE_DIM PRNT_PTYPE ON PRNT_PTYPE.PROC_TYPE_DIM_UID = PARENT_FCT.PROC_TYPE_DIM_UID AND PRNT_PTYPE.ROW_STAT_CD <> \\'D\\'\\n\\nLEFT OUTER JOIN PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DATAPBLIC ON DATAPBLIC.PROC_DIM_UID = P.PROC_DIM_UID\\nLEFT OUTER JOIN PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DATAIBM ON DATAIBM.PROC_DIM_UID = P.PROC_DIM_UID\\nLEFT OUTER JOIN PGMPDM.ORG_CNTRCT_MAP_DIM ORG_MAP ON ORG_MAP.CNTRCT_DIM_UID = C.CNTRCT_DIM_UID\\n\\nLEFT OUTER JOIN PGMPDM.ORGNZN_DIM ORG ON ORG.ORGNZN_DIM_UID = ORG_MAP.ORGNZN_DIM_UID\\nLEFT OUTER JOIN PGMPDM.STAT_OF_ORGNZN_DIM SOO ON SOO.STAT_OF_ORGNZN_DIM_UID = ORG.STAT_OF_ORGNZN_DIM_UID\\nLEFT OUTER JOIN PGMPDM.ZAUX_CNTRCT_GLBL_BUY_GRP_MAP ZACNTRCT ON ZACNTRCT.CNTRCT_DIM_UID = C.CNTRCT_DIM_UID\\n\\n--LEFT OUTER JOIN PGMPDM.CNTRCT_TZ_V AF ON C.CNTRCT_DIM_UID = AF.CNTRCT_DIM_UID \\n\\nWHERE PDELIVER.ROW_STAT_CD <> \\'D\\' AND PTYPE.PROC_TYPE_ID = \\'PDELIVER\\' AND P.ADNC_ACCSS_CD <> \\'C\\' AND H.DELD_DT IS NULL AND SRC.SRC_SYS_CD = \\'PGMP\\'\\n',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceDataforPCR",
						"getSourceDataforPCR alterRow(insertIf(true())) ~> upsertRows",
						"upsertRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'PROJECT_DELIVR_RPT',",
						"     insertable: false,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: true,",
						"     keys:['UNIQUE_ID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> upsertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0006_CP_RISK_RPT')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 10-06-2022\nJob Name: DF_BALD0006_CP_RISK_RPT\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/CP Summary & Custom Field audit"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "upsertTable"
						}
					],
					"transformations": [
						{
							"name": "upsertRows"
						}
					],
					"scriptLines": [
						"source(output(",
						"          CNTRCT_DIM_UID as integer,",
						"          CONTRACT as string,",
						"          UNIQUE_ID as integer,",
						"          CLIENT_REFERENCE as string,",
						"          DISPLAY_ID as string,",
						"          GLBL_BUY_GRP_ID as string,",
						"          PARENT_UNIQUE_ID as integer,",
						"          PARENT_DISPLAY_ID as string,",
						"          PARENT_TITLE as string,",
						"          PROC_TITLE as string,",
						"          WKFLW_DEF_ID as string,",
						"          INIT_WKFLW_DEF_ID_DESCR as string,",
						"          PROC_DESCRIPTION as string,",
						"          CURRENT_WORKFLOW as string,",
						"          CURRENT_WKFLW_STEP as string,",
						"          CURRENT_STEP_SEQ as integer,",
						"          STEP_DESCRIPTION as string,",
						"          STATE_DESC as string,",
						"          STATUS_DESC as string,",
						"          ASSIGNED_TO as binary,",
						"          AUDIENCE as string,",
						"          PRIORITY_TXT as string,",
						"          CONDITION as string,",
						"          REGION as string,",
						"          REQUESTOR as binary,",
						"          CREATED_BY as binary,",
						"          CREATED_DATE as timestamp,",
						"          LAST_UPDATED as timestamp,",
						"          RISK_RESPONSE_DUE_DATE as date,",
						"          REVISED_RISK_RESPONSE_DUE_DATE as date,",
						"          DUE_IN_DAY_CNT as integer,",
						"          OVERDUE as string,",
						"          COMPLETION_DATE as date,",
						"          COMPLETION_REASON as string,",
						"          COUNTRY as string,",
						"          RISK_SOURCE as string,",
						"          REMARKS as string,",
						"          ORIGINATING_ORG as string,",
						"          RISK_OWNER as binary,",
						"          PROBABILITY as integer,",
						"          PROBABILITY_SMPL as string,",
						"          IMPACT as string,",
						"          RESPONSE_PLAN as string,",
						"          LOCAL_CURRENCY as string,",
						"          IMPACT_AMOUNT_LOCAL_CURRENCY as decimal(15,2),",
						"          GS_RISK_ID as string,",
						"          BUSINESS_CONTROLS_RISK as string,",
						"          WWBCIT_REFERENCE as string,",
						"          RISK_ANLYSS_DUE_DT as date,",
						"          RVSD_RISK_ANLYSS_DUE_DT as date,",
						"          RISK_RSPNS_TYPE_CD as string,",
						"          RISK_OCCURRED_CD as string,",
						"          RISK_CLOSE_RSN_TXT as string,",
						"          SRC_SYS_CD as string,",
						"          LEGACY_UNIQUE_ID as string,",
						"          LEGACY_DISPLAYED_ID as string,",
						"          LATEST_IBM_ONLY_NOTE as string,",
						"          LATEST_IBM_AND_CLIENT_NOTE as string,",
						"          CNTRCT_TYPE_DESC as string,",
						"          PROC_TYPE_ID as string,",
						"          PROC_TYPE_DESC as string,",
						"          ON_HOLD_CD as string,",
						"          WITHDRWN_DIM_UID as integer,",
						"          WITHDRWN_DT as date,",
						"          ORGNZN_DIM_UID as integer,",
						"          IBM_ONLY_TEXT_1 as string,",
						"          IBM_ONLY_TEXT_2 as string,",
						"          IBM_ONLY_TEXT_3 as string,",
						"          IBM_ONLY_TEXT_4 as string,",
						"          IBM_ONLY_TEXT_5 as string,",
						"          IBM_ONLY_TEXT_6 as string,",
						"          IBM_ONLY_DATE_1 as date,",
						"          IBM_ONLY_DATE_2 as date,",
						"          IBM_ONLY_DATE_3 as date,",
						"          IBM_ONLY_DATE_4 as date,",
						"          IBM_ONLY_DATE_5 as date,",
						"          IBM_ONLY_DATE_6 as date,",
						"          PUBLIC_TEXT_1 as string,",
						"          PUBLIC_TEXT_2 as string,",
						"          PUBLIC_TEXT_3 as string,",
						"          PUBLIC_TEXT_4 as string,",
						"          PUBLIC_TEXT_5 as string,",
						"          PUBLIC_TEXT_6 as string,",
						"          PUBLIC_DATE_1 as date,",
						"          PUBLIC_DATE_2 as date,",
						"          PUBLIC_DATE_3 as date,",
						"          PUBLIC_DATE_4 as date,",
						"          PUBLIC_DATE_5 as date,",
						"          PUBLIC_DATE_6 as date,",
						"          STATE_OF_ORGANIZATION as string,",
						"          SCTR_NM as string,",
						"          CLIENT_UNIT as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'SELECT \\n     C.CNTRCT_DIM_UID,\\n     C.CNTRCT_NM AS CONTRACT,\\n     HFCT.PROC_DIM_UID AS UNIQUE_ID,\\n     PROCS.CLNT_REF_NUM AS CLIENT_REFERENCE,\\n     PROCS.PROC_DSPLY_ID AS DISPLAY_ID,\\n     ZACNTRCT.GLBL_BUY_GRP_ID,\\n     PARENT_PROC.PROC_DIM_UID AS PARENT_UNIQUE_ID,\\n     PARENT_PROC.PROC_DSPLY_ID AS PARENT_DISPLAY_ID,\\n     PARENT_PROC.TITLE_TXT AS PARENT_TITLE,\\n     PROCS.TITLE_TXT AS PROC_TITLE,\\n     INITWDD.WKFLW_DEF_ID,\\n     INITWDD.\"DESC\" AS INIT_WKFLW_DEF_ID_DESCR,\\n     PROCS.PROC_DESC AS PROC_DESCRIPTION,\\n     WKFLW.WKFLW_DEF_ID AS CURRENT_WORKFLOW,\\n     WKFLW_STEP.WKFLW_STEP_DEF_ID AS CURRENT_WKFLW_STEP,\\n     WKFLW_STEP.WKFLW_STEP_SEQ_NUM AS CURRENT_STEP_SEQ,\\n     WKFLW_STEP.WKFLW_STEP_DESC AS STEP_DESCRIPTION,\\n     ST.STATE_TITLE_TXT STATE_DESC,\\n     STAT.STAT_IBM_DESC AS STATUS_DESC,\\n     ASSGNTO.CONCAT_NM_LOGIN AS ASSIGNED_TO, --decrypt_char(ASSGNTO.CONCAT_NM_LOGIN,MIS_REP_REF.MIS_REP_REF_CD) AS ASSIGNED_TO,\\n     AUDIENCE.ADNC_TXT AS AUDIENCE,\\n     P.PRIORITY_TXT,\\n     COND.COND_TXT AS CONDITION,\\n     COALESCE(RGN.REGION,GEO.KYNDRYL_RGN) AS REGION,\\n     PROCS.RQSTR_TXT AS REQUESTOR, --decrypt_char(PROCS.RQSTR_TXT,MIS_REP_REF.MIS_REP_REF_CD) AS REQUESTOR,\\n     CREATOR.CONCAT_NM_LOGIN AS CREATED_BY, --decrypt_char(CREATOR.CONCAT_NM_LOGIN,MIS_REP_REF.MIS_REP_REF_CD) AS CREATED_BY,\\n     HFCT.SRC_CRETD_TMS AS CREATED_DATE,\\n     HFCT.SRC_UPDTD_TMS AS LAST_UPDATED,\\n     FCT.RISK_RSPNS_DUE_DT AS RISK_RESPONSE_DUE_DATE,\\n     FCT.RVSD_RISK_RSPNS_DUE_DT AS REVISED_RISK_RESPONSE_DUE_DATE,\\n     DATEDIFF(day, \\'2000/01/01\\', ISNULL(FCT.RVSD_RISK_RSPNS_DUE_DT,FCT.RISK_RSPNS_DUE_DT)) - DATEDIFF(day, \\'2000/01/01\\', DATEADD(hh, AF.TZ, CURRENT_TIMESTAMP)) AS DUE_IN_DAY_CNT,\\n     CASE \\n          WHEN DATEDIFF(day, \\'2000/01/01\\', ISNULL(FCT.RVSD_RISK_RSPNS_DUE_DT,FCT.RISK_RSPNS_DUE_DT)) - DATEDIFF(day, \\'2000/01/01\\', DATEADD(hh, AF.TZ, CURRENT_TIMESTAMP)) < 0 THEN \\'Y\\' \\n          ELSE \\'N\\' \\n     END AS OVERDUE,\\n     HFCT.CMPLTD_DT AS COMPLETION_DATE,\\n     PROCS.CMPLTD_RSN_TXT AS COMPLETION_REASON,\\n     GEO.SRGNCTRY_NM AS COUNTRY,\\n     RISKSRC.RISK_SRC_TXT AS RISK_SOURCE,\\n     RISK.RISK_REM_TXT AS REMARKS,\\n     RISK.ORGNTG_ORGNZN_TXT AS ORIGINATING_ORG,\\n     RISK.RISK_OWNR_TXT AS RISK_OWNER, --decrypt_char(RISK.RISK_OWNR_TXT,MIS_REP_REF.MIS_REP_REF_CD) AS RISK_OWNER,\\n     FCT.PRBBLTY_NUM AS PROBABILITY,\\n     CASE \\n          WHEN FCT.PROBABILITY_SMPL =  \\'Z\\' THEN \\'10\\' \\n          WHEN FCT.PROBABILITY_SMPL =  \\'D\\' THEN \\'25\\' \\n          WHEN FCT.PROBABILITY_SMPL =  \\'C\\' THEN \\'50\\' \\n          WHEN FCT.PROBABILITY_SMPL =  \\'S\\' THEN \\'75\\' \\n     END AS PROBABILITY_SMPL ,\\n     FCT.IMPCT_CD AS IMPACT,\\n     RISK.MTGTN_RSPNS_PLAN_TXT AS RESPONSE_PLAN,\\n     CRNCY.CRNCY_NM AS LOCAL_CURRENCY,\\n     FCT.IMPCT_AMT AS IMPACT_AMOUNT_LOCAL_CURRENCY,\\n     RISK.GS_RISK_ID ,\\n     CASE \\n          WHEN RISK.BUS_CNTRL_RISK_IND=\\'Y\\' THEN \\'YES\\' \\n          WHEN RISK.BUS_CNTRL_RISK_IND=\\'N\\' THEN \\'NO\\' \\n     END AS BUSINESS_CONTROLS_RISK,\\n     RISK.WWRADB_REF_TXT AS WWBCIT_REFERENCE,\\n     FCT.RISK_ANLYSS_DUE_DT,\\n     FCT.RVSD_RISK_ANLYSS_DUE_DT,\\n     CASE \\n          WHEN RISK.RISK_RSPNS_TYPE_CD = \\'ACC\\' THEN \\'Accept/Retain\\' \\n          WHEN RISK.RISK_RSPNS_TYPE_CD = \\'AVO\\' THEN \\'Avoid\\' \\n          WHEN RISK.RISK_RSPNS_TYPE_CD = \\'CON\\' THEN \\'Contain/Reduce\\' \\n          WHEN RISK.RISK_RSPNS_TYPE_CD = \\'INS\\' THEN \\'Use Insurance\\'  \\n          WHEN RISK.RISK_RSPNS_TYPE_CD = \\'XFR\\' THEN \\'Transfer\\' \\n          WHEN RISK.RISK_RSPNS_TYPE_CD = \\'RES\\' THEN \\'Use Risk Reserve\\' \\n          ELSE RISK.RISK_RSPNS_TYPE_CD \\n     END AS RISK_RSPNS_TYPE_CD,\\n     RISK.RISK_OCCURRED_CD,\\n     RISK.RISK_CLOSE_RSN_TXT,\\n     SRC.SRC_SYS_CD,\\n     \\'\\' AS LEGACY_UNIQUE_ID,\\n     \\'\\' AS LEGACY_DISPLAYED_ID,\\n     TRIM(INOTES.NOTES_DESC) AS LATEST_IBM_ONLY_NOTE,\\n     TRIM(PNOTES.NOTES_DESC) AS LATEST_IBM_AND_CLIENT_NOTE,\\n     CTYPE.CNTRCT_TYPE_DESC,\\n     PTYPE.PROC_TYPE_ID,\\n     PTYPE.PROC_TYPE_DESC,\\n     ONHOLD.ON_HOLD_CD,\\n     HFCT.WITHDRWN_DIM_UID,\\n     HFCT.WITHDRWN_DT,\\n     HFCT.ORGNZN_DIM_UID,\\n     DATAIBM.IBM_ONLY_TEXT_1,\\n     DATAIBM.IBM_ONLY_TEXT_2,\\n     DATAIBM.IBM_ONLY_TEXT_3,\\n     DATAIBM.IBM_ONLY_TEXT_4,\\n     DATAIBM.IBM_ONLY_TEXT_5,\\n     DATAIBM.IBM_ONLY_TEXT_6,\\n     DATAIBM.IBM_ONLY_DATE_1,\\n     DATAIBM.IBM_ONLY_DATE_2,\\n     DATAIBM.IBM_ONLY_DATE_3,\\n     DATAIBM.IBM_ONLY_DATE_4,\\n     DATAIBM.IBM_ONLY_DATE_5,\\n     DATAIBM.IBM_ONLY_DATE_6,\\n     DATAPBLIC.PUBLIC_TEXT_1,\\n     DATAPBLIC.PUBLIC_TEXT_2,\\n     DATAPBLIC.PUBLIC_TEXT_3,\\n     DATAPBLIC.PUBLIC_TEXT_4,\\n     DATAPBLIC.PUBLIC_TEXT_5,\\n     DATAPBLIC.PUBLIC_TEXT_6,\\n     DATAPBLIC.PUBLIC_DATE_1,\\n     DATAPBLIC.PUBLIC_DATE_2,\\n     DATAPBLIC.PUBLIC_DATE_3,\\n     DATAPBLIC.PUBLIC_DATE_4,\\n     DATAPBLIC.PUBLIC_DATE_5,\\n     DATAPBLIC.PUBLIC_DATE_6,\\n     SOO.STAT_OF_ORGNZN_DESC AS STATE_OF_ORGANIZATION,\\n     SCTR.SCTR_NM,\\n     ZACNTRCT.CLIENT_UNIT_NM AS CLIENT_UNIT\\n\\nFROM \\n     PGMPDM.RISK_DIM RISK\\n     INNER JOIN PGMPDM.PROC_HEADR_FCT HFCT ON HFCT.PROC_DIM_UID = RISK.RISK_DIM_UID AND HFCT.ROW_STAT_CD <> \\'D\\'\\n     INNER JOIN PGMPDM.PROC_TYPE_DIM PTYPE ON PTYPE.PROC_TYPE_DIM_UID = HFCT.PROC_TYPE_DIM_UID AND PTYPE.ROW_STAT_CD <> \\'D\\'\\n     INNER JOIN PGMPDM.PROC_DIM PROCS ON PROCS.PROC_DIM_UID = HFCT.PROC_DIM_UID AND PROCS.ROW_STAT_CD <> \\'D\\'\\n     INNER JOIN PGMPDM.CNTRCT_DIM C ON C.CNTRCT_DIM_UID = PROCS.CNTRCT_DIM_UID AND C.ROW_STAT_CD <> \\'D\\'\\n     INNER JOIN PGMPDM.CNTRCT_TYPE_DIM CTYPE ON CTYPE.CNTRCT_TYPE_DIM_UID = C.CNTRCT_TYPE_DIM_UID AND CTYPE.ROW_STAT_CD <> \\'D\\'\\n     INNER JOIN PGMPDM.WKFLW_DEF_DIM INITWDD ON INITWDD.WKFLW_DEF_DIM_UID = HFCT.INIT_WKFLW_DEF_DIM_UID AND INITWDD.ROW_STAT_CD <> \\'D\\'\\n     INNER JOIN PGMPDM.WKFLW_DEF_DIM WKFLW ON WKFLW.WKFLW_DEF_DIM_UID = HFCT.WKFLW_DEF_DIM_UID AND WKFLW.ROW_STAT_CD <> \\'D\\'\\n     INNER JOIN PGMPDM.WKFLW_STEP_DEF_DIM WKFLW_STEP ON WKFLW_STEP.WKFLW_STEP_DEF_DIM_UID = HFCT.WKFLW_STEP_DEF_DIM_UID AND WKFLW_STEP.ROW_STAT_CD <> \\'D\\' \\n     INNER JOIN PGMPDM.STATE_DIM ST ON ST.STATE_DIM_UID = HFCT.STATE_DIM_UID AND ST.ROW_STAT_CD <> \\'D\\'\\n     INNER JOIN PGMPDM.PROC_STEP_DAT_DIM PSD ON PROCS.PROC_DIM_UID = PSD.PROC_DIM_UID AND PROCS.CURR_PROC_STEP_DAT_DIM_UID = PSD.PROC_STEP_DAT_DIM_UID AND PSD.ROW_STAT_CD <> \\'D\\'\\n     INNER JOIN PGMPDM.GEO_DIM GEO ON GEO.GEO_DIM_UID = HFCT.GEO_DIM_UID -- AND GEO.ROW_STAT_CD <> \\'D\\'\\n     INNER JOIN PGMPDM.MISC_FCT FCT ON FCT.PROC_HEADR_FCT_UID = HFCT.PROC_HEADR_FCT_UID AND FCT.ROW_STAT_CD <> \\'D\\'\\n     INNER JOIN PGMPDM.SRC_SYS_DIM SRC ON SRC.SRC_SYS_DIM_UID = HFCT.SRC_SYS_DIM_UID AND SRC.ROW_STAT_CD <> \\'D\\'\\n     INNER JOIN PGMPDM.SCTR_DIM SCTR ON SCTR.SCTR_DIM_UID = C.SCTR_DIM_UID AND SCTR.ROW_STAT_CD <> \\'D\\'\\n     LEFT OUTER JOIN PGMPDM.STAT_DIM STAT ON STAT.STAT_DIM_UID = HFCT.STAT_DIM_UID AND STAT.ROW_STAT_CD <> \\'D\\'\\n     LEFT OUTER JOIN PGMPDM.MISC_REP_REF MIS_REP_REF ON MIS_REP_REF_UID = 3\\n     LEFT OUTER JOIN PGMPDM.USER_DIM ASSGNTO ON ASSGNTO.USER_ID = PSD.ASSGN_TO_NUM AND ASSGNTO.ROW_STAT_CD <> \\'D\\'\\n     LEFT OUTER JOIN PGMPDM.ADNC_DIM AUDIENCE ON AUDIENCE.ADNC_CD = PROCS.ADNC_CD\\n     LEFT OUTER JOIN PGMPDM.PRIORITY_DIM P ON P.PRIORITY_NUM = PROCS.PRIORTY_NUM\\n     LEFT OUTER JOIN PGMPDM.COND_DIM COND ON COND.COND_CD = PROCS.CNDTN_CD\\n     LEFT OUTER JOIN APPFUN.PROC_REGION_V RGN ON RGN.PROC_ID = PROCS.PROC_DIM_UID AND RGN.INTERNAL_VAL <> \\'NONE\\'\\n     LEFT OUTER JOIN PGMPDM.USER_DIM CREATOR ON CREATOR.USER_DIM_UID = HFCT.CRETD_BY_USER_DIM_UID AND CREATOR.ROW_STAT_CD <> \\'D\\'\\n     LEFT OUTER JOIN PGMPDM.NOTES_DIM INOTES ON INOTES.PROC_DIM_UID = PROCS.PROC_DIM_UID AND INOTES.ROW_STAT_CD <> \\'D\\' AND INOTES.NOTE_TYPE_CD = \\'I\\' AND INOTES.LATEST_NOTE_IND = \\'Y\\'\\n     LEFT OUTER JOIN PGMPDM.NOTES_DIM PNOTES ON PNOTES.PROC_DIM_UID = PROCS.PROC_DIM_UID AND PNOTES.ROW_STAT_CD <> \\'D\\' AND PNOTES.NOTE_TYPE_CD = \\'P\\' AND PNOTES.LATEST_NOTE_IND = \\'Y\\'\\n     LEFT OUTER JOIN PGMPDM.ON_HOLD_DIM ONHOLD ON ONHOLD.ON_HOLD_DIM_UID = HFCT.ON_HOLD_DIM_UID AND ONHOLD.ROW_STAT_CD <> \\'D\\'\\n     LEFT OUTER JOIN PGMPDM.ON_HOLD_RSN_DIM HOLDRSN ON HOLDRSN.ON_HOLD_RSN_ID = PROCS.ON_HOLD_RSN_ID AND HOLDRSN.ROW_STAT_CD <> \\'D\\'\\n     LEFT OUTER JOIN PGMPDM.CRNCY_DIM CRNCY ON CRNCY.CRNCY_DIM_UID = FCT.LOCAL_CRNCY_DIM_UID AND CRNCY.ROW_STAT_CD <> \\'D\\'\\n     LEFT OUTER JOIN PGMPDM.PROC_DIM PARENT_PROC ON PARENT_PROC.PROC_DIM_UID = PROCS.PARNT_PROC_ID AND PARENT_PROC.ROW_STAT_CD <> \\'D\\'\\n     LEFT OUTER JOIN PGMPDM.RISK_SRC_DIM RISKSRC ON RISKSRC.RISK_SRC_DIM_CD = RISK.RISK_SRC_IND AND RISKSRC.ROW_STAT_CD <> \\'D\\'\\n     LEFT OUTER JOIN PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DATAPBLIC ON DATAPBLIC.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n     LEFT OUTER JOIN PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DATAIBM ON DATAIBM.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n     LEFT OUTER JOIN PGMPDM.ORG_CNTRCT_MAP_DIM ORG_MAP ON ORG_MAP.CNTRCT_DIM_UID = C.CNTRCT_DIM_UID\\n     LEFT OUTER JOIN PGMPDM.ORGNZN_DIM ORG ON ORG.ORGNZN_DIM_UID = ORG_MAP.ORGNZN_DIM_UID\\n     LEFT OUTER JOIN PGMPDM.STAT_OF_ORGNZN_DIM SOO ON SOO.STAT_OF_ORGNZN_DIM_UID = ORG.STAT_OF_ORGNZN_DIM_UID\\n     --LEFT OUTER JOIN PGMPDM.CNTRCT_TZ_V AF ON C.CNTRCT_DIM_UID = AF.CNTRCT_DIM_UID\\n     LEFT OUTER JOIN (\\n          SELECT DISTINCT \\n               C.CNTRCT_DIM_UID,\\n              CASE\\n                  WHEN B.TMZONE_DESC LIKE \\'(GMT)%\\' THEN 0\\n                  WHEN B.TMZONE_DESC LIKE \\'(GMT+%\\' THEN CAST(SUBSTRING(B.TMZONE_DESC, 6, 2) AS INT)\\n                  WHEN B.TMZONE_DESC LIKE \\'(GMT-%\\' THEN CAST(SUBSTRING(B.TMZONE_DESC, 6, 2) AS INT) * -1\\n                  ELSE 0\\n              END TZ\\n          FROM\\n              PGMPDM.ORGNZN_DIM A,\\n              PGMPDM.TMZONE_DIM B,\\n              PGMPDM.CNTRCT_DIM C,\\n              PGMPDM.ORG_CNTRCT_MAP_DIM D\\n          WHERE\\n              A.TMZONE_DIM_UID = B.TMZONE_DIM_UID\\n              AND A.ORGNZN_DIM_UID = D.ORGNZN_DIM_UID\\n              AND C.CNTRCT_DIM_UID = D.CNTRCT_DIM_UID \\n      ) AF ON C.CNTRCT_DIM_UID = AF.CNTRCT_DIM_UID\\n     LEFT OUTER JOIN PGMPDM.ZAUX_CNTRCT_GLBL_BUY_GRP_MAP ZACNTRCT ON ZACNTRCT.CNTRCT_DIM_UID = C.CNTRCT_DIM_UID \\n\\nWHERE \\n     RISK.ROW_STAT_CD <> \\'D\\' \\n     AND PTYPE.PROC_TYPE_ID IN (\\'RISK\\', \\'RISKNEW\\') \\n     AND PROCS.ADNC_ACCSS_CD <> \\'C\\' \\n     AND HFCT.DELD_DT IS NULL \\n     AND SRC.SRC_SYS_CD = \\'PGMP\\'',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"getSourceData alterRow(upsertIf(true())) ~> upsertRows",
						"upsertRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'RISK_RPT',",
						"     insertable: true,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['UNIQUE_ID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> upsertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0007_CSTM_DATA_PUBLIC_AUDIT_RPT')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 13-06-2022\nJob Name: DF_BALD0007_CSTM_DATA_PUBLIC_AUDIT_RPT\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/CP Summary & Custom Field audit"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "upsertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedSourceData"
						},
						{
							"name": "upsertRows"
						}
					],
					"scriptLines": [
						"source(output(",
						"          PROC_DIM_UID as integer,",
						"          CNTRCT_DIM_UID as integer,",
						"          CNTRCT_NM as string,",
						"          PUBLIC_TEXT_1 as string,",
						"          UTS_PT1 as timestamp,",
						"          UUID_PT1 as binary,",
						"          PUBLIC_TEXT_2 as string,",
						"          UTS_PT2 as timestamp,",
						"          UUID_PT2 as binary,",
						"          PUBLIC_TEXT_3 as string,",
						"          UTS_PT3 as timestamp,",
						"          UUID_PT3 as binary,",
						"          PUBLIC_TEXT_4 as string,",
						"          UTS_PT4 as timestamp,",
						"          UUID_PT4 as binary,",
						"          PUBLIC_TEXT_5 as string,",
						"          UTS_PT5 as timestamp,",
						"          UUID_PT5 as binary,",
						"          PUBLIC_TEXT_6 as string,",
						"          UTS_PT6 as timestamp,",
						"          UUID_PT6 as binary,",
						"          PUBLIC_TEXT_7 as string,",
						"          UTS_PT7 as timestamp,",
						"          UUID_PT7 as binary,",
						"          PUBLIC_TEXT_8 as string,",
						"          UTS_PT8 as timestamp,",
						"          UUID_PT8 as binary,",
						"          PUBLIC_TEXT_9 as string,",
						"          UTS_PT9 as timestamp,",
						"          UUID_PT9 as binary,",
						"          PUBLIC_TEXT_10 as string,",
						"          UTS_PT10 as timestamp,",
						"          UUID_PT10 as binary,",
						"          PUBLIC_TEXT_11 as string,",
						"          UTS_PT11 as timestamp,",
						"          UUID_PT11 as binary,",
						"          PUBLIC_TEXT_12 as string,",
						"          UTS_PT12 as timestamp,",
						"          UUID_PT12 as binary,",
						"          PUBLIC_TEXT_13 as string,",
						"          UTS_PT13 as timestamp,",
						"          UUID_PT13 as binary,",
						"          PUBLIC_TEXT_14 as string,",
						"          UTS_PT14 as timestamp,",
						"          UUID_PT14 as binary,",
						"          PUBLIC_TEXT_15 as string,",
						"          UTS_PT15 as timestamp,",
						"          UUID_PT15 as binary,",
						"          PUBLIC_TEXT_16 as string,",
						"          UTS_PT16 as timestamp,",
						"          UUID_PT16 as binary,",
						"          PUBLIC_TEXT_17 as string,",
						"          UTS_PT17 as timestamp,",
						"          UUID_PT17 as binary,",
						"          PUBLIC_TEXT_18 as string,",
						"          UTS_PT18 as timestamp,",
						"          UUID_PT18 as binary,",
						"          PUBLIC_TEXT_19 as string,",
						"          UTS_PT19 as timestamp,",
						"          UUID_PT19 as binary,",
						"          PUBLIC_TEXT_20 as string,",
						"          UTS_PT20 as timestamp,",
						"          UUID_PT20 as binary,",
						"          PUBLIC_TEXT_21 as string,",
						"          UTS_PT21 as timestamp,",
						"          UUID_PT21 as binary,",
						"          PUBLIC_TEXT_22 as string,",
						"          UTS_PT22 as timestamp,",
						"          UUID_PT22 as binary,",
						"          PUBLIC_TEXT_23 as string,",
						"          UTS_PT23 as timestamp,",
						"          UUID_PT23 as binary,",
						"          PUBLIC_TEXT_24 as string,",
						"          UTS_PT24 as timestamp,",
						"          UUID_PT24 as binary,",
						"          PUBLIC_TEXT_25 as string,",
						"          UTS_PT25 as timestamp,",
						"          UUID_PT25 as binary,",
						"          PUBLIC_TEXT_26 as string,",
						"          UTS_PT26 as timestamp,",
						"          UUID_PT26 as binary,",
						"          PUBLIC_TEXT_27 as string,",
						"          UTS_PT27 as timestamp,",
						"          UUID_PT27 as binary,",
						"          PUBLIC_TEXT_28 as string,",
						"          UTS_PT28 as timestamp,",
						"          UUID_PT28 as binary,",
						"          PUBLIC_TEXT_29 as string,",
						"          UTS_PT29 as timestamp,",
						"          UUID_PT29 as binary,",
						"          PUBLIC_TEXT_30 as string,",
						"          UTS_PT30 as timestamp,",
						"          UUID_PT30 as binary,",
						"          PUBLIC_TEXT_31 as string,",
						"          UTS_PT31 as timestamp,",
						"          UUID_PT31 as binary,",
						"          PUBLIC_TEXT_32 as string,",
						"          UTS_PT32 as timestamp,",
						"          UUID_PT32 as binary,",
						"          PUBLIC_TEXT_33 as string,",
						"          UTS_PT33 as timestamp,",
						"          UUID_PT33 as binary,",
						"          PUBLIC_TEXT_34 as string,",
						"          UTS_PT34 as timestamp,",
						"          UUID_PT34 as binary,",
						"          PUBLIC_TEXT_35 as string,",
						"          UTS_PT35 as timestamp,",
						"          UUID_PT35 as binary,",
						"          PUBLIC_TEXT_36 as string,",
						"          UTS_PT36 as timestamp,",
						"          UUID_PT36 as binary,",
						"          PUBLIC_TEXT_37 as string,",
						"          UTS_PT37 as timestamp,",
						"          UUID_PT37 as binary,",
						"          PUBLIC_DATE_1 as date,",
						"          UTS_PD1 as timestamp,",
						"          UUID_PD1 as binary,",
						"          PUBLIC_DATE_2 as date,",
						"          UTS_PD2 as timestamp,",
						"          UUID_PD2 as binary,",
						"          PUBLIC_DATE_3 as date,",
						"          UTS_PD3 as timestamp,",
						"          UUID_PD3 as binary,",
						"          PUBLIC_DATE_4 as date,",
						"          UTS_PD4 as timestamp,",
						"          UUID_PD4 as binary,",
						"          PUBLIC_DATE_5 as date,",
						"          UTS_PD5 as timestamp,",
						"          UUID_PD5 as binary,",
						"          PUBLIC_DATE_6 as date,",
						"          UTS_PD6 as timestamp,",
						"          UUID_PD6 as binary,",
						"          PUBLIC_DATE_7 as date,",
						"          UTS_PD7 as timestamp,",
						"          UUID_PD7 as binary,",
						"          PUBLIC_DATE_8 as date,",
						"          UTS_PD8 as timestamp,",
						"          UUID_PD8 as binary,",
						"          PUBLIC_DATE_9 as date,",
						"          UTS_PD9 as timestamp,",
						"          UUID_PD9 as binary,",
						"          PUBLIC_DATE_10 as date,",
						"          UTS_PD10 as timestamp,",
						"          UUID_PD10 as binary,",
						"          PUBLIC_DATE_11 as date,",
						"          UTS_PD11 as timestamp,",
						"          UUID_PD11 as binary,",
						"          PUBLIC_DATE_12 as date,",
						"          UTS_PD12 as timestamp,",
						"          UUID_PD12 as binary,",
						"          PUBLIC_DATE_13 as date,",
						"          UTS_PD13 as timestamp,",
						"          UUID_PD13 as binary,",
						"          PUBLIC_DATE_14 as date,",
						"          UTS_PD14 as timestamp,",
						"          UUID_PD14 as binary,",
						"          PUBLIC_DATE_15 as date,",
						"          UTS_PD15 as timestamp,",
						"          UUID_PD15 as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select distinct\\n     PROCS.PROC_DIM_UID,\\n     C.CNTRCT_DIM_UID,\\n     C.CNTRCT_NM,\\n    PUBLIC_TEXT_1.PUBLIC_TEXT_1,\\n     PUBLIC_TEXT_1.UPDATED_TIMESTAMP AS UTS_PT1,\\n     PUBLIC_TEXT_1.USER_ID AS UUID_PT1,\\n    PUBLIC_TEXT_2.PUBLIC_TEXT_2,\\n     PUBLIC_TEXT_2.UPDATED_TIMESTAMP AS UTS_PT2,\\n     PUBLIC_TEXT_2.USER_ID AS UUID_PT2,\\n    PUBLIC_TEXT_3.PUBLIC_TEXT_3,\\n     PUBLIC_TEXT_3.UPDATED_TIMESTAMP AS UTS_PT3,\\n     PUBLIC_TEXT_3.USER_ID AS UUID_PT3,\\n    PUBLIC_TEXT_4.PUBLIC_TEXT_4,\\n     PUBLIC_TEXT_4.UPDATED_TIMESTAMP AS UTS_PT4,\\n     PUBLIC_TEXT_4.USER_ID AS UUID_PT4,\\n    PUBLIC_TEXT_5.PUBLIC_TEXT_5,\\n     PUBLIC_TEXT_5.UPDATED_TIMESTAMP AS UTS_PT5,\\n     PUBLIC_TEXT_5.USER_ID AS UUID_PT5,\\n    PUBLIC_TEXT_6.PUBLIC_TEXT_6,\\n     PUBLIC_TEXT_6.UPDATED_TIMESTAMP AS UTS_PT6,\\n     PUBLIC_TEXT_6.USER_ID AS UUID_PT6,\\n    PUBLIC_TEXT_7.PUBLIC_TEXT_7,\\n     PUBLIC_TEXT_7.UPDATED_TIMESTAMP AS UTS_PT7,\\n     PUBLIC_TEXT_7.USER_ID AS UUID_PT7,\\n    PUBLIC_TEXT_8.PUBLIC_TEXT_8,\\n     PUBLIC_TEXT_8.UPDATED_TIMESTAMP AS UTS_PT8,\\n     PUBLIC_TEXT_8.USER_ID AS UUID_PT8,\\n    PUBLIC_TEXT_9.PUBLIC_TEXT_9,\\n     PUBLIC_TEXT_9.UPDATED_TIMESTAMP AS UTS_PT9,\\n     PUBLIC_TEXT_9.USER_ID AS UUID_PT9,\\n    PUBLIC_TEXT_10.PUBLIC_TEXT_10,\\n     PUBLIC_TEXT_10.UPDATED_TIMESTAMP AS UTS_PT10,\\n     PUBLIC_TEXT_10.USER_ID AS UUID_PT10,\\n    PUBLIC_TEXT_11.PUBLIC_TEXT_11,\\n     PUBLIC_TEXT_11.UPDATED_TIMESTAMP AS UTS_PT11,\\n     PUBLIC_TEXT_11.USER_ID AS UUID_PT11,\\n    PUBLIC_TEXT_12.PUBLIC_TEXT_12,\\n     PUBLIC_TEXT_12.UPDATED_TIMESTAMP AS UTS_PT12,\\n     PUBLIC_TEXT_12.USER_ID AS UUID_PT12,\\n    PUBLIC_TEXT_13.PUBLIC_TEXT_13,\\n     PUBLIC_TEXT_13.UPDATED_TIMESTAMP AS UTS_PT13,\\n     PUBLIC_TEXT_13.USER_ID AS UUID_PT13,\\n    PUBLIC_TEXT_14.PUBLIC_TEXT_14,\\n     PUBLIC_TEXT_14.UPDATED_TIMESTAMP AS UTS_PT14,\\n     PUBLIC_TEXT_14.USER_ID AS UUID_PT14,\\n    PUBLIC_TEXT_15.PUBLIC_TEXT_15,\\n     PUBLIC_TEXT_15.UPDATED_TIMESTAMP AS UTS_PT15,\\n     PUBLIC_TEXT_15.USER_ID AS UUID_PT15,\\n    PUBLIC_TEXT_16.PUBLIC_TEXT_16,\\n     PUBLIC_TEXT_16.UPDATED_TIMESTAMP AS UTS_PT16,\\n     PUBLIC_TEXT_16.USER_ID AS UUID_PT16,\\n    PUBLIC_TEXT_17.PUBLIC_TEXT_17,\\n     PUBLIC_TEXT_17.UPDATED_TIMESTAMP AS UTS_PT17,\\n     PUBLIC_TEXT_17.USER_ID AS UUID_PT17,\\n    PUBLIC_TEXT_18.PUBLIC_TEXT_18,\\n     PUBLIC_TEXT_18.UPDATED_TIMESTAMP AS UTS_PT18,\\n     PUBLIC_TEXT_18.USER_ID AS UUID_PT18,\\n    PUBLIC_TEXT_19.PUBLIC_TEXT_19,\\n     PUBLIC_TEXT_19.UPDATED_TIMESTAMP AS UTS_PT19,\\n     PUBLIC_TEXT_19.USER_ID AS UUID_PT19,\\n    PUBLIC_TEXT_20.PUBLIC_TEXT_20,\\n     PUBLIC_TEXT_20.UPDATED_TIMESTAMP AS UTS_PT20,\\n     PUBLIC_TEXT_20.USER_ID AS UUID_PT20,\\n    PUBLIC_TEXT_21.PUBLIC_TEXT_21,\\n     PUBLIC_TEXT_21.UPDATED_TIMESTAMP AS UTS_PT21,\\n     PUBLIC_TEXT_21.USER_ID AS UUID_PT21,\\n    PUBLIC_TEXT_22.PUBLIC_TEXT_22,\\n     PUBLIC_TEXT_22.UPDATED_TIMESTAMP AS UTS_PT22,\\n     PUBLIC_TEXT_22.USER_ID AS UUID_PT22,\\n    PUBLIC_TEXT_23.PUBLIC_TEXT_23,\\n     PUBLIC_TEXT_23.UPDATED_TIMESTAMP AS UTS_PT23,\\n     PUBLIC_TEXT_23.USER_ID AS UUID_PT23,\\n    PUBLIC_TEXT_24.PUBLIC_TEXT_24,\\n     PUBLIC_TEXT_24.UPDATED_TIMESTAMP AS UTS_PT24,\\n     PUBLIC_TEXT_24.USER_ID AS UUID_PT24,\\n    PUBLIC_TEXT_25.PUBLIC_TEXT_25,\\n     PUBLIC_TEXT_25.UPDATED_TIMESTAMP AS UTS_PT25,\\n     PUBLIC_TEXT_25.USER_ID AS UUID_PT25,\\n    PUBLIC_TEXT_26.PUBLIC_TEXT_26,\\n     PUBLIC_TEXT_26.UPDATED_TIMESTAMP AS UTS_PT26,\\n     PUBLIC_TEXT_26.USER_ID AS UUID_PT26,\\n    PUBLIC_TEXT_27.PUBLIC_TEXT_27,\\n     PUBLIC_TEXT_27.UPDATED_TIMESTAMP AS UTS_PT27,\\n     PUBLIC_TEXT_27.USER_ID AS UUID_PT27,\\n     PUBLIC_TEXT_28.PUBLIC_TEXT_28,\\n     PUBLIC_TEXT_28.UPDATED_TIMESTAMP AS UTS_PT28,\\n     PUBLIC_TEXT_28.USER_ID AS UUID_PT28,\\n    PUBLIC_TEXT_29.PUBLIC_TEXT_29,\\n     PUBLIC_TEXT_29.UPDATED_TIMESTAMP AS UTS_PT29,\\n     PUBLIC_TEXT_29.USER_ID AS UUID_PT29,\\n    PUBLIC_TEXT_30.PUBLIC_TEXT_30,\\n     PUBLIC_TEXT_30.UPDATED_TIMESTAMP AS UTS_PT30,\\n     PUBLIC_TEXT_30.USER_ID AS UUID_PT30,\\n    PUBLIC_TEXT_31.PUBLIC_TEXT_31,\\n     PUBLIC_TEXT_31.UPDATED_TIMESTAMP AS UTS_PT31,\\n     PUBLIC_TEXT_31.USER_ID AS UUID_PT31,\\n    PUBLIC_TEXT_32.PUBLIC_TEXT_32,\\n     PUBLIC_TEXT_32.UPDATED_TIMESTAMP AS UTS_PT32,\\n     PUBLIC_TEXT_32.USER_ID AS UUID_PT32,\\n    PUBLIC_TEXT_33.PUBLIC_TEXT_33,\\n     PUBLIC_TEXT_33.UPDATED_TIMESTAMP AS UTS_PT33,\\n     PUBLIC_TEXT_33.USER_ID AS UUID_PT33,\\n    PUBLIC_TEXT_34.PUBLIC_TEXT_34,\\n     PUBLIC_TEXT_34.UPDATED_TIMESTAMP AS UTS_PT34,\\n     PUBLIC_TEXT_34.USER_ID AS UUID_PT34,\\n    PUBLIC_TEXT_35.PUBLIC_TEXT_35,\\n     PUBLIC_TEXT_35.UPDATED_TIMESTAMP AS UTS_PT35,\\n     PUBLIC_TEXT_35.USER_ID AS UUID_PT35,\\n    PUBLIC_TEXT_36.PUBLIC_TEXT_36,\\n     PUBLIC_TEXT_36.UPDATED_TIMESTAMP AS UTS_PT36,\\n     PUBLIC_TEXT_36.USER_ID AS UUID_PT36,\\n    PUBLIC_TEXT_37.PUBLIC_TEXT_37,\\n     PUBLIC_TEXT_37.UPDATED_TIMESTAMP AS UTS_PT37,\\n     PUBLIC_TEXT_37.USER_ID AS UUID_PT37,\\n    PUBLIC_DATE_1.PUBLIC_DATE_1,\\n     PUBLIC_DATE_1.UPDATED_TIMESTAMP AS UTS_PD1,\\n     PUBLIC_DATE_1.USER_ID AS UUID_PD1,\\n    PUBLIC_DATE_2.PUBLIC_DATE_2,\\n     PUBLIC_DATE_2.UPDATED_TIMESTAMP AS UTS_PD2,\\n     PUBLIC_DATE_2.USER_ID AS UUID_PD2,\\n    PUBLIC_DATE_3.PUBLIC_DATE_3,\\n     PUBLIC_DATE_3.UPDATED_TIMESTAMP AS UTS_PD3,\\n     PUBLIC_DATE_3.USER_ID AS UUID_PD3,\\n    PUBLIC_DATE_4.PUBLIC_DATE_4,\\n     PUBLIC_DATE_4.UPDATED_TIMESTAMP AS UTS_PD4,\\n     PUBLIC_DATE_4.USER_ID AS UUID_PD4,\\n    PUBLIC_DATE_5.PUBLIC_DATE_5,\\n     PUBLIC_DATE_5.UPDATED_TIMESTAMP AS UTS_PD5,\\n     PUBLIC_DATE_5.USER_ID AS UUID_PD5,\\n    PUBLIC_DATE_6.PUBLIC_DATE_6,\\n     PUBLIC_DATE_6.UPDATED_TIMESTAMP AS UTS_PD6,\\n     PUBLIC_DATE_6.USER_ID AS UUID_PD6,\\n    PUBLIC_DATE_7.PUBLIC_DATE_7,\\n     PUBLIC_DATE_7.UPDATED_TIMESTAMP AS UTS_PD7,\\n     PUBLIC_DATE_7.USER_ID AS UUID_PD7,\\n    PUBLIC_DATE_8.PUBLIC_DATE_8,\\n     PUBLIC_DATE_8.UPDATED_TIMESTAMP AS UTS_PD8,\\n     PUBLIC_DATE_8.USER_ID AS UUID_PD8,\\n    PUBLIC_DATE_9.PUBLIC_DATE_9,\\n     PUBLIC_DATE_9.UPDATED_TIMESTAMP AS UTS_PD9,\\n     PUBLIC_DATE_9.USER_ID AS UUID_PD9,\\n    PUBLIC_DATE_10.PUBLIC_DATE_10,\\n     PUBLIC_DATE_10.UPDATED_TIMESTAMP AS UTS_PD10,\\n     PUBLIC_DATE_10.USER_ID AS UUID_PD10,\\n    PUBLIC_DATE_11.PUBLIC_DATE_11,\\n     PUBLIC_DATE_11.UPDATED_TIMESTAMP AS UTS_PD11,\\n     PUBLIC_DATE_11.USER_ID AS UUID_PD11,\\n    PUBLIC_DATE_12.PUBLIC_DATE_12,\\n     PUBLIC_DATE_12.UPDATED_TIMESTAMP AS UTS_PD12,\\n     PUBLIC_DATE_12.USER_ID AS UUID_PD12,\\n    PUBLIC_DATE_13.PUBLIC_DATE_13,\\n     PUBLIC_DATE_13.UPDATED_TIMESTAMP AS UTS_PD13,\\n     PUBLIC_DATE_13.USER_ID AS UUID_PD13,\\n    PUBLIC_DATE_14.PUBLIC_DATE_14,\\n     PUBLIC_DATE_14.UPDATED_TIMESTAMP AS UTS_PD14,\\n     PUBLIC_DATE_14.USER_ID AS UUID_PD14,\\n    PUBLIC_DATE_15.PUBLIC_DATE_15,\\n     PUBLIC_DATE_15.UPDATED_TIMESTAMP AS UTS_PD15,\\n     PUBLIC_DATE_15.USER_ID AS UUID_PD15\\nFrom\\n     PGMPDM.PROC_DIM PROCS\\n     INNER JOIN PGMPDM.CNTRCT_DIM C ON C.CNTRCT_DIM_UID = PROCS.CNTRCT_DIM_UID\\n     INNER JOIN (\\n          Select distinct\\n               PROC_DIM_UID\\n          From     \\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM\\n          Where     \\n               COALESCE(\\n                    PUBLIC_TEXT_1,\\n                    PUBLIC_TEXT_2,\\n                    PUBLIC_TEXT_3,\\n                    PUBLIC_TEXT_4,\\n                    PUBLIC_TEXT_5,\\n                    PUBLIC_TEXT_6,\\n                    PUBLIC_TEXT_7,\\n                    PUBLIC_TEXT_8,\\n                    PUBLIC_TEXT_9,\\n                    PUBLIC_TEXT_10,\\n                    PUBLIC_TEXT_11,\\n                    PUBLIC_TEXT_12,\\n                    PUBLIC_TEXT_13,\\n                    PUBLIC_TEXT_14,\\n                    PUBLIC_TEXT_15,\\n                    PUBLIC_TEXT_16,\\n                    PUBLIC_TEXT_17,\\n                    PUBLIC_TEXT_18,\\n                    PUBLIC_TEXT_19,\\n                    PUBLIC_TEXT_20,\\n                    PUBLIC_TEXT_21,\\n                    PUBLIC_TEXT_22,\\n                    PUBLIC_TEXT_23,\\n                    PUBLIC_TEXT_24,\\n                    PUBLIC_TEXT_25,\\n                    PUBLIC_TEXT_26,\\n                    PUBLIC_TEXT_27,\\n                    PUBLIC_TEXT_28,\\n                    PUBLIC_TEXT_29,\\n                    PUBLIC_TEXT_30,\\n                    PUBLIC_TEXT_31,\\n                    PUBLIC_TEXT_32,\\n                    PUBLIC_TEXT_33,\\n                    PUBLIC_TEXT_34,\\n                    PUBLIC_TEXT_35,\\n                    --PUBLIC_TEXT_36,\\n                    --PUBLIC_TEXT_37,\\n                    CAST(PUBLIC_DATE_1 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_2 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_3 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_4 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_5 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_6 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_7 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_8 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_9 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_10 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_11 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_12 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_13 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_14 AS VARCHAR),\\n                    CAST(PUBLIC_DATE_15 AS VARCHAR)\\n               ) NOT IN (\\'\\', \\'NULL\\')\\n     ) PPROC ON PPROC.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n     LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_1 AS PUBLIC_TEXT_1,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_1,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_1                         \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_1 = DP.PUBLIC_TEXT_1)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_1 not in (\\'\\') \\n     ) PUBLIC_TEXT_1 ON PUBLIC_TEXT_1.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_2 AS PUBLIC_TEXT_2,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_2,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_2                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_2 = DP.PUBLIC_TEXT_2)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_2 not in (\\'\\') \\n     )PUBLIC_TEXT_2 ON PUBLIC_TEXT_2.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_3 AS PUBLIC_TEXT_3,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_3,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_3                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_3 = DP.PUBLIC_TEXT_3)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_3 not in (\\'\\') \\n     ) PUBLIC_TEXT_3 ON PUBLIC_TEXT_3.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_4 AS PUBLIC_TEXT_4,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_4,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP               \\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_4                              \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_4 = DP.PUBLIC_TEXT_4)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_4 not in (\\'\\') \\n     ) PUBLIC_TEXT_4 ON PUBLIC_TEXT_4.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_5 AS PUBLIC_TEXT_5,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_5,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_5               \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_5 = DP.PUBLIC_TEXT_5)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_5 not in (\\'\\') \\n     ) PUBLIC_TEXT_5 ON PUBLIC_TEXT_5.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_6 AS PUBLIC_TEXT_6,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_6,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_6                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_6 = DP.PUBLIC_TEXT_6)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_6 not in (\\'\\') \\n     ) PUBLIC_TEXT_6 ON PUBLIC_TEXT_6.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_7 AS PUBLIC_TEXT_7,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_7,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_7                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_7 = DP.PUBLIC_TEXT_7)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_7 not in (\\'\\') \\n     ) PUBLIC_TEXT_7 ON PUBLIC_TEXT_7.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_8 AS PUBLIC_TEXT_8,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_8,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_8                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_8 = DP.PUBLIC_TEXT_8)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_8 not in (\\'\\') \\n     ) PUBLIC_TEXT_8 ON PUBLIC_TEXT_8.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_9 AS PUBLIC_TEXT_9,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_9,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_9                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_9 = DP.PUBLIC_TEXT_9)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_9 not in (\\'\\') \\n     ) PUBLIC_TEXT_9 ON PUBLIC_TEXT_9.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_10 AS PUBLIC_TEXT_10,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_10,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_10                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_10 = DP.PUBLIC_TEXT_10)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_10 not in (\\'\\') \\n     ) PUBLIC_TEXT_10 ON PUBLIC_TEXT_10.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_11 AS PUBLIC_TEXT_11,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_11,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_11                         \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_11 = DP.PUBLIC_TEXT_11)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_11 not in (\\'\\') \\n     ) PUBLIC_TEXT_11 ON PUBLIC_TEXT_11.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_12 AS PUBLIC_TEXT_12,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_12,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_12                         \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_12 = DP.PUBLIC_TEXT_12)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_12 not in (\\'\\') \\n     ) PUBLIC_TEXT_12 ON PUBLIC_TEXT_12.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_13 AS PUBLIC_TEXT_13,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_13,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_13                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_13 = DP.PUBLIC_TEXT_13)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_13 not in (\\'\\') \\n     ) PUBLIC_TEXT_13 ON PUBLIC_TEXT_13.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_14 AS PUBLIC_TEXT_14,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_14,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_14\\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_14 = DP.PUBLIC_TEXT_14)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_14 not in (\\'\\') \\n     ) PUBLIC_TEXT_14 ON PUBLIC_TEXT_14.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_15 AS PUBLIC_TEXT_15,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_15,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_15                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_15 = DP.PUBLIC_TEXT_15)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_15 not in (\\'\\') \\n     ) PUBLIC_TEXT_15 ON PUBLIC_TEXT_15.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_16 AS PUBLIC_TEXT_16,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_16,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_16                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_16 = DP.PUBLIC_TEXT_16)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_16 not in (\\'\\') \\n     ) PUBLIC_TEXT_16 ON PUBLIC_TEXT_16.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_17 AS PUBLIC_TEXT_17,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_17,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_17                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_17 = DP.PUBLIC_TEXT_17)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_17 not in (\\'\\') \\n     ) PUBLIC_TEXT_17 ON PUBLIC_TEXT_17.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_18 AS PUBLIC_TEXT_18,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_18,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_18                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_18 = DP.PUBLIC_TEXT_18)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_18 not in (\\'\\') \\n     ) PUBLIC_TEXT_18 ON PUBLIC_TEXT_18.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_19 AS PUBLIC_TEXT_19,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_19,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_19                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_19 = DP.PUBLIC_TEXT_19)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_19 not in (\\'\\') \\n     ) PUBLIC_TEXT_19 ON PUBLIC_TEXT_19.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_20 AS PUBLIC_TEXT_20,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_20,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_20                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_20 = DP.PUBLIC_TEXT_20)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_20 not in (\\'\\') \\n     ) PUBLIC_TEXT_20 ON PUBLIC_TEXT_20.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_21 AS PUBLIC_TEXT_21,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_21,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_21                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_21 = DP.PUBLIC_TEXT_21)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_21 not in (\\'\\') \\n     ) PUBLIC_TEXT_21 ON PUBLIC_TEXT_21.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_22 AS PUBLIC_TEXT_22,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_22,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_22                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_22 = DP.PUBLIC_TEXT_22)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_22 not in (\\'\\') \\n     ) PUBLIC_TEXT_22 ON PUBLIC_TEXT_22.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_23 AS PUBLIC_TEXT_23,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_23,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_23                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_23 = DP.PUBLIC_TEXT_23)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_23 not in (\\'\\') \\n     ) PUBLIC_TEXT_23 ON PUBLIC_TEXT_23.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_24 AS PUBLIC_TEXT_24,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_24,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_24                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_24 = DP.PUBLIC_TEXT_24)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_24 not in (\\'\\') \\n     ) PUBLIC_TEXT_24 ON PUBLIC_TEXT_24.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_25 AS PUBLIC_TEXT_25,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_25,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_25                         \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_25 = DP.PUBLIC_TEXT_25)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_25 not in (\\'\\') \\n     ) PUBLIC_TEXT_25 ON PUBLIC_TEXT_25.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_26 AS PUBLIC_TEXT_26,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_26,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_26                         \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_26 = DP.PUBLIC_TEXT_26)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_26 not in (\\'\\') \\n     ) PUBLIC_TEXT_26 ON PUBLIC_TEXT_26.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_27 AS PUBLIC_TEXT_27,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_27,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_27               \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_27 = DP.PUBLIC_TEXT_27)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_27 not in (\\'\\') \\n     ) PUBLIC_TEXT_27 ON PUBLIC_TEXT_27.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_28 AS PUBLIC_TEXT_28,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_28,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_28                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_28 = DP.PUBLIC_TEXT_28)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_28 not in (\\'\\') \\n     ) PUBLIC_TEXT_28 ON PUBLIC_TEXT_28.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_29 AS PUBLIC_TEXT_29,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_29,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_29                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_29 = DP.PUBLIC_TEXT_29)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_29 not in (\\'\\') \\n     ) PUBLIC_TEXT_29 ON PUBLIC_TEXT_29.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_30 AS PUBLIC_TEXT_30,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_30,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_30                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_30 = DP.PUBLIC_TEXT_30)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_30 not in (\\'\\') \\n     ) PUBLIC_TEXT_30 ON PUBLIC_TEXT_30.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_31 AS PUBLIC_TEXT_31,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_31,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_31                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_31 = DP.PUBLIC_TEXT_31)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_31 not in (\\'\\') \\n     ) PUBLIC_TEXT_31 ON PUBLIC_TEXT_31.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_32 AS PUBLIC_TEXT_32,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_32,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_32                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_32 = DP.PUBLIC_TEXT_32)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_32 not in (\\'\\') \\n     ) PUBLIC_TEXT_32 ON PUBLIC_TEXT_32.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_33 AS PUBLIC_TEXT_33,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_33,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_33                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_33 = DP.PUBLIC_TEXT_33)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_33 not in (\\'\\') \\n     ) PUBLIC_TEXT_33 ON PUBLIC_TEXT_33.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_34 AS PUBLIC_TEXT_34,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_34,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_34                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_34 = DP.PUBLIC_TEXT_34)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_34 not in (\\'\\') \\n     ) PUBLIC_TEXT_34 ON PUBLIC_TEXT_34.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_35 AS PUBLIC_TEXT_35,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_35,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_35                         \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_35 = DP.PUBLIC_TEXT_35)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_35 not in (\\'\\') \\n     ) PUBLIC_TEXT_35 ON PUBLIC_TEXT_35.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n     LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_36 AS PUBLIC_TEXT_36,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_36,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_36                         \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_36 = DP.PUBLIC_TEXT_36)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_36 not in (\\'\\') \\n     ) PUBLIC_TEXT_36 ON PUBLIC_TEXT_36.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_TEXT_37 AS PUBLIC_TEXT_37,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_TEXT_37,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_TEXT_37               \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_TEXT_37 = DP.PUBLIC_TEXT_37)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_TEXT_37 not in (\\'\\') \\n     ) PUBLIC_TEXT_37 ON PUBLIC_TEXT_37.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n     LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_1 AS PUBLIC_DATE_1,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_1,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_1                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_1 = DP.PUBLIC_DATE_1)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_1 not like (\\'\\') \\n     ) PUBLIC_DATE_1 ON PUBLIC_DATE_1.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_2 AS PUBLIC_DATE_2,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_2,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_2                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_2 = DP.PUBLIC_DATE_2)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_2 not like (\\'\\') \\n     ) PUBLIC_DATE_2 ON PUBLIC_DATE_2.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_3 AS PUBLIC_DATE_3,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_3,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_3                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_3 = DP.PUBLIC_DATE_3)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_3 not like (\\'\\') \\n     ) PUBLIC_DATE_3 ON PUBLIC_DATE_3.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_4 AS PUBLIC_DATE_4,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_4,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP     \\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_4                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_4 = DP.PUBLIC_DATE_4)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_4 not like (\\'\\') \\n     ) PUBLIC_DATE_4 ON PUBLIC_DATE_4.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_5 AS PUBLIC_DATE_5,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_5,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_5                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_5 = DP.PUBLIC_DATE_5)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_5 not like (\\'\\') \\n     ) PUBLIC_DATE_5 ON PUBLIC_DATE_5.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_6 AS PUBLIC_DATE_6,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_6,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_6                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_6 = DP.PUBLIC_DATE_6)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_6 not like (\\'\\') \\n     ) PUBLIC_DATE_6 ON PUBLIC_DATE_6.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_7 AS PUBLIC_DATE_7,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_7,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_7                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_7 = DP.PUBLIC_DATE_7)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_7 not like (\\'\\') \\n     ) PUBLIC_DATE_7 ON PUBLIC_DATE_7.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_8 AS PUBLIC_DATE_8,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_8,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_8                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_8 = DP.PUBLIC_DATE_8)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_8 not like (\\'\\') \\n     ) PUBLIC_DATE_8 ON PUBLIC_DATE_8.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_9 AS PUBLIC_DATE_9,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_9,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_9                         \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_9 = DP.PUBLIC_DATE_9)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_9 not like (\\'\\') \\n     ) PUBLIC_DATE_9 ON PUBLIC_DATE_9.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_10 AS PUBLIC_DATE_10,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_10,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_10                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_10 = DP.PUBLIC_DATE_10)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_10 not like (\\'\\') \\n     ) PUBLIC_DATE_10 ON PUBLIC_DATE_10.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_11 AS PUBLIC_DATE_11,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_11,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_11                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_11 = DP.PUBLIC_DATE_11)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_11 not like (\\'\\') \\n     ) PUBLIC_DATE_11 ON PUBLIC_DATE_11.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_12 AS PUBLIC_DATE_12,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_12,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_12                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_12 = DP.PUBLIC_DATE_12)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_12 not like (\\'\\') \\n     ) PUBLIC_DATE_12 ON PUBLIC_DATE_12.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_13 AS PUBLIC_DATE_13,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_13,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_13                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_13 = DP.PUBLIC_DATE_13)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_13 not like (\\'\\') \\n     ) PUBLIC_DATE_13 ON PUBLIC_DATE_13.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_14 AS PUBLIC_DATE_14,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_14,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_14                         \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_14 = DP.PUBLIC_DATE_14)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_14 not like (\\'\\') \\n     ) PUBLIC_DATE_14 ON PUBLIC_DATE_14.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_DATE_15 AS PUBLIC_DATE_15,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_PUBLIC_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_DATE_15,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_DATE_15                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_DATE_15 = DP.PUBLIC_DATE_15)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_DATE_15 not like (\\'\\') \\n     ) PUBLIC_DATE_15 ON PUBLIC_DATE_15.PROC_DIM_UID = PROCS.PROC_DIM_UID',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"getSourceData sort(asc(PROC_DIM_UID, false)) ~> sortedSourceData",
						"sortedSourceData alterRow(upsertIf(true())) ~> upsertRows",
						"upsertRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'CSTM_DATA_PUBLIC_AUDIT_RPT',",
						"     insertable: true,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['PROC_DIM_UID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> upsertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0008_CSTM_DATA_KYNDRTL_ONLY_AUDIT_RPT')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 13-06-2022\nJob Name: DF_BALD0008_CSTM_DATA_KYNDRTL_ONLY_AUDIT_RPT\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/CP Summary & Custom Field audit"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "upsertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedSourceData"
						},
						{
							"name": "upsertRows"
						}
					],
					"scriptLines": [
						"source(output(",
						"          PROC_DIM_UID as integer,",
						"          CNTRCT_DIM_UID as integer,",
						"          CNTRCT_NM as string,",
						"          KYNDRYL_ONLY_TEXT_1 as string,",
						"          UTS_KoT1 as timestamp,",
						"          UUID_KoT1 as binary,",
						"          KYNDRYL_ONLY_TEXT_2 as string,",
						"          UTS_KoT2 as timestamp,",
						"          UUID_KoT2 as binary,",
						"          KYNDRYL_ONLY_TEXT_3 as string,",
						"          UTS_KoT3 as timestamp,",
						"          UUID_KoT3 as binary,",
						"          KYNDRYL_ONLY_TEXT_4 as string,",
						"          UTS_KoT4 as timestamp,",
						"          UUID_KoT4 as binary,",
						"          KYNDRYL_ONLY_TEXT_5 as string,",
						"          UTS_KoT5 as timestamp,",
						"          UUID_KoT5 as binary,",
						"          KYNDRYL_ONLY_TEXT_6 as string,",
						"          UTS_KoT6 as timestamp,",
						"          UUID_KoT6 as binary,",
						"          KYNDRYL_ONLY_TEXT_7 as string,",
						"          UTS_KoT7 as timestamp,",
						"          UUID_KoT7 as binary,",
						"          KYNDRYL_ONLY_TEXT_8 as string,",
						"          UTS_KoT8 as timestamp,",
						"          UUID_KoT8 as binary,",
						"          KYNDRYL_ONLY_TEXT_9 as string,",
						"          UTS_KoT9 as timestamp,",
						"          UUID_KoT9 as binary,",
						"          KYNDRYL_ONLY_TEXT_10 as string,",
						"          UTS_KoT10 as timestamp,",
						"          UUID_KoT10 as binary,",
						"          KYNDRYL_ONLY_TEXT_11 as string,",
						"          UTS_KoT11 as timestamp,",
						"          UUID_KoT11 as binary,",
						"          KYNDRYL_ONLY_TEXT_12 as string,",
						"          UTS_KoT12 as timestamp,",
						"          UUID_KoT12 as binary,",
						"          KYNDRYL_ONLY_TEXT_13 as string,",
						"          UTS_KoT13 as timestamp,",
						"          UUID_KoT13 as binary,",
						"          KYNDRYL_ONLY_TEXT_14 as string,",
						"          UTS_KoT14 as timestamp,",
						"          UUID_KoT14 as binary,",
						"          KYNDRYL_ONLY_TEXT_15 as string,",
						"          UTS_KoT15 as timestamp,",
						"          UUID_KoT15 as binary,",
						"          KYNDRYL_ONLY_TEXT_16 as string,",
						"          UTS_KoT16 as timestamp,",
						"          UUID_KoT16 as binary,",
						"          KYNDRYL_ONLY_TEXT_17 as string,",
						"          UTS_KoT17 as timestamp,",
						"          UUID_KoT17 as binary,",
						"          KYNDRYL_ONLY_TEXT_18 as string,",
						"          UTS_KoT18 as timestamp,",
						"          UUID_KoT18 as binary,",
						"          KYNDRYL_ONLY_TEXT_19 as string,",
						"          UTS_KoT19 as timestamp,",
						"          UUID_KoT19 as binary,",
						"          KYNDRYL_ONLY_TEXT_20 as string,",
						"          UTS_KoT20 as timestamp,",
						"          UUID_KoT20 as binary,",
						"          KYNDRYL_ONLY_TEXT_21 as string,",
						"          UTS_KoT21 as timestamp,",
						"          UUID_KoT21 as binary,",
						"          KYNDRYL_ONLY_TEXT_22 as string,",
						"          UTS_KoT22 as timestamp,",
						"          UUID_KoT22 as binary,",
						"          KYNDRYL_ONLY_TEXT_23 as string,",
						"          UTS_KoT23 as timestamp,",
						"          UUID_KoT23 as binary,",
						"          KYNDRYL_ONLY_TEXT_24 as string,",
						"          UTS_KoT24 as timestamp,",
						"          UUID_KoT24 as binary,",
						"          KYNDRYL_ONLY_TEXT_25 as string,",
						"          UTS_KoT25 as timestamp,",
						"          UUID_KoT25 as binary,",
						"          KYNDRYL_ONLY_TEXT_26 as string,",
						"          UTS_KoT26 as timestamp,",
						"          UUID_KoT26 as binary,",
						"          KYNDRYL_ONLY_TEXT_27 as string,",
						"          UTS_KoT27 as timestamp,",
						"          UUID_KoT27 as binary,",
						"          KYNDRYL_ONLY_DATE_1 as date,",
						"          UTS_KoD1 as timestamp,",
						"          UUID_KoD1 as binary,",
						"          KYNDRYL_ONLY_DATE_2 as date,",
						"          UTS_KoD2 as timestamp,",
						"          UUID_KoD2 as binary,",
						"          KYNDRYL_ONLY_DATE_3 as date,",
						"          UTS_KoD3 as timestamp,",
						"          UUID_KoD3 as binary,",
						"          KYNDRYL_ONLY_DATE_4 as date,",
						"          UTS_KoD4 as timestamp,",
						"          UUID_KoD4 as binary,",
						"          KYNDRYL_ONLY_DATE_5 as date,",
						"          UTS_KoD5 as timestamp,",
						"          UUID_KoD5 as binary,",
						"          KYNDRYL_ONLY_DATE_6 as date,",
						"          UTS_KoD6 as timestamp,",
						"          UUID_KoD6 as binary,",
						"          KYNDRYL_ONLY_DATE_7 as date,",
						"          UTS_KoD7 as timestamp,",
						"          UUID_KoD7 as binary,",
						"          KYNDRYL_ONLY_DATE_8 as date,",
						"          UTS_KoD8 as timestamp,",
						"          UUID_KoD8 as binary,",
						"          KYNDRYL_ONLY_DATE_9 as date,",
						"          UTS_KoD9 as timestamp,",
						"          UUID_KoD9 as binary,",
						"          KYNDRYL_ONLY_DATE_10 as date,",
						"          UTS_KoD10 as timestamp,",
						"          UUID_KoD10 as binary,",
						"          KYNDRYL_ONLY_DATE_11 as date,",
						"          UTS_KoD11 as timestamp,",
						"          UUID_KoD11 as binary,",
						"          KYNDRYL_ONLY_DATE_12 as date,",
						"          UTS_KoD12 as timestamp,",
						"          UUID_KoD12 as binary,",
						"          KYNDRYL_ONLY_DATE_13 as date,",
						"          UTS_KoD13 as timestamp,",
						"          UUID_KoD13 as binary,",
						"          KYNDRYL_ONLY_DATE_14 as date,",
						"          UTS_KoD14 as timestamp,",
						"          UUID_KoD14 as binary,",
						"          KYNDRYL_ONLY_DATE_15 as date,",
						"          UTS_KoD15 as timestamp,",
						"          UUID_KoD15 as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select distinct\\n     PROCS.PROC_DIM_UID,\\n     C.CNTRCT_DIM_UID,\\n     C.CNTRCT_NM,\\n    KYNDRYL_ONLY_TEXT_1.KYNDRYL_ONLY_TEXT_1,\\n     KYNDRYL_ONLY_TEXT_1.UPDATED_TIMESTAMP AS UTS_KoT1,\\n     KYNDRYL_ONLY_TEXT_1.USER_ID AS UUID_KoT1,\\n    KYNDRYL_ONLY_TEXT_2.KYNDRYL_ONLY_TEXT_2,\\n     KYNDRYL_ONLY_TEXT_2.UPDATED_TIMESTAMP AS UTS_KoT2,\\n     KYNDRYL_ONLY_TEXT_2.USER_ID AS UUID_KoT2,\\n    KYNDRYL_ONLY_TEXT_3.KYNDRYL_ONLY_TEXT_3,\\n     KYNDRYL_ONLY_TEXT_3.UPDATED_TIMESTAMP AS UTS_KoT3,\\n     KYNDRYL_ONLY_TEXT_3.USER_ID AS UUID_KoT3,\\n    KYNDRYL_ONLY_TEXT_4.KYNDRYL_ONLY_TEXT_4,\\n     KYNDRYL_ONLY_TEXT_4.UPDATED_TIMESTAMP AS UTS_KoT4,\\n     KYNDRYL_ONLY_TEXT_4.USER_ID AS UUID_KoT4,\\n    KYNDRYL_ONLY_TEXT_5.KYNDRYL_ONLY_TEXT_5,\\n     KYNDRYL_ONLY_TEXT_5.UPDATED_TIMESTAMP AS UTS_KoT5,\\n     KYNDRYL_ONLY_TEXT_5.USER_ID AS UUID_KoT5,\\n    KYNDRYL_ONLY_TEXT_6.KYNDRYL_ONLY_TEXT_6,\\n     KYNDRYL_ONLY_TEXT_6.UPDATED_TIMESTAMP AS UTS_KoT6,\\n     KYNDRYL_ONLY_TEXT_6.USER_ID AS UUID_KoT6,\\n    KYNDRYL_ONLY_TEXT_7.KYNDRYL_ONLY_TEXT_7,\\n     KYNDRYL_ONLY_TEXT_7.UPDATED_TIMESTAMP AS UTS_KoT7,\\n     KYNDRYL_ONLY_TEXT_7.USER_ID AS UUID_KoT7,\\n    KYNDRYL_ONLY_TEXT_8.KYNDRYL_ONLY_TEXT_8,\\n     KYNDRYL_ONLY_TEXT_8.UPDATED_TIMESTAMP AS UTS_KoT8,\\n     KYNDRYL_ONLY_TEXT_8.USER_ID AS UUID_KoT8,\\n    KYNDRYL_ONLY_TEXT_9.KYNDRYL_ONLY_TEXT_9,\\n     KYNDRYL_ONLY_TEXT_9.UPDATED_TIMESTAMP AS UTS_KoT9,\\n     KYNDRYL_ONLY_TEXT_9.USER_ID AS UUID_KoT9,\\n    KYNDRYL_ONLY_TEXT_10.KYNDRYL_ONLY_TEXT_10,\\n     KYNDRYL_ONLY_TEXT_10.UPDATED_TIMESTAMP AS UTS_KoT10,\\n     KYNDRYL_ONLY_TEXT_10.USER_ID AS UUID_KoT10,\\n    KYNDRYL_ONLY_TEXT_11.KYNDRYL_ONLY_TEXT_11,\\n     KYNDRYL_ONLY_TEXT_11.UPDATED_TIMESTAMP AS UTS_KoT11,\\n     KYNDRYL_ONLY_TEXT_11.USER_ID AS UUID_KoT11,\\n    KYNDRYL_ONLY_TEXT_12.KYNDRYL_ONLY_TEXT_12,\\n     KYNDRYL_ONLY_TEXT_12.UPDATED_TIMESTAMP AS UTS_KoT12,\\n     KYNDRYL_ONLY_TEXT_12.USER_ID AS UUID_KoT12,\\n    KYNDRYL_ONLY_TEXT_13.KYNDRYL_ONLY_TEXT_13,\\n     KYNDRYL_ONLY_TEXT_13.UPDATED_TIMESTAMP AS UTS_KoT13,\\n     KYNDRYL_ONLY_TEXT_13.USER_ID AS UUID_KoT13,\\n    KYNDRYL_ONLY_TEXT_14.KYNDRYL_ONLY_TEXT_14,\\n     KYNDRYL_ONLY_TEXT_14.UPDATED_TIMESTAMP AS UTS_KoT14,\\n     KYNDRYL_ONLY_TEXT_14.USER_ID AS UUID_KoT14,\\n    KYNDRYL_ONLY_TEXT_15.KYNDRYL_ONLY_TEXT_15,\\n     KYNDRYL_ONLY_TEXT_15.UPDATED_TIMESTAMP AS UTS_KoT15,\\n     KYNDRYL_ONLY_TEXT_15.USER_ID AS UUID_KoT15,\\n    KYNDRYL_ONLY_TEXT_16.KYNDRYL_ONLY_TEXT_16,\\n     KYNDRYL_ONLY_TEXT_16.UPDATED_TIMESTAMP AS UTS_KoT16,\\n     KYNDRYL_ONLY_TEXT_16.USER_ID AS UUID_KoT16,\\n    KYNDRYL_ONLY_TEXT_17.KYNDRYL_ONLY_TEXT_17,\\n     KYNDRYL_ONLY_TEXT_17.UPDATED_TIMESTAMP AS UTS_KoT17,\\n     KYNDRYL_ONLY_TEXT_17.USER_ID AS UUID_KoT17,\\n    KYNDRYL_ONLY_TEXT_18.KYNDRYL_ONLY_TEXT_18,\\n     KYNDRYL_ONLY_TEXT_18.UPDATED_TIMESTAMP AS UTS_KoT18,\\n     KYNDRYL_ONLY_TEXT_18.USER_ID AS UUID_KoT18,\\n    KYNDRYL_ONLY_TEXT_19.KYNDRYL_ONLY_TEXT_19,\\n     KYNDRYL_ONLY_TEXT_19.UPDATED_TIMESTAMP AS UTS_KoT19,\\n     KYNDRYL_ONLY_TEXT_19.USER_ID AS UUID_KoT19,\\n    KYNDRYL_ONLY_TEXT_20.KYNDRYL_ONLY_TEXT_20,\\n     KYNDRYL_ONLY_TEXT_20.UPDATED_TIMESTAMP AS UTS_KoT20,\\n     KYNDRYL_ONLY_TEXT_20.USER_ID AS UUID_KoT20,\\n    KYNDRYL_ONLY_TEXT_21.KYNDRYL_ONLY_TEXT_21,\\n     KYNDRYL_ONLY_TEXT_21.UPDATED_TIMESTAMP AS UTS_KoT21,\\n     KYNDRYL_ONLY_TEXT_21.USER_ID AS UUID_KoT21,\\n    KYNDRYL_ONLY_TEXT_22.KYNDRYL_ONLY_TEXT_22,\\n     KYNDRYL_ONLY_TEXT_22.UPDATED_TIMESTAMP AS UTS_KoT22,\\n     KYNDRYL_ONLY_TEXT_22.USER_ID AS UUID_KoT22,\\n    KYNDRYL_ONLY_TEXT_23.KYNDRYL_ONLY_TEXT_23,\\n     KYNDRYL_ONLY_TEXT_23.UPDATED_TIMESTAMP AS UTS_KoT23,\\n     KYNDRYL_ONLY_TEXT_23.USER_ID AS UUID_KoT23,\\n    KYNDRYL_ONLY_TEXT_24.KYNDRYL_ONLY_TEXT_24,\\n     KYNDRYL_ONLY_TEXT_24.UPDATED_TIMESTAMP AS UTS_KoT24,\\n     KYNDRYL_ONLY_TEXT_24.USER_ID AS UUID_KoT24,\\n    KYNDRYL_ONLY_TEXT_25.KYNDRYL_ONLY_TEXT_25,\\n     KYNDRYL_ONLY_TEXT_25.UPDATED_TIMESTAMP AS UTS_KoT25,\\n     KYNDRYL_ONLY_TEXT_25.USER_ID AS UUID_KoT25,\\n    KYNDRYL_ONLY_TEXT_26.KYNDRYL_ONLY_TEXT_26,\\n     KYNDRYL_ONLY_TEXT_26.UPDATED_TIMESTAMP AS UTS_KoT26,\\n     KYNDRYL_ONLY_TEXT_26.USER_ID AS UUID_KoT26,\\n    KYNDRYL_ONLY_TEXT_27.KYNDRYL_ONLY_TEXT_27,\\n     KYNDRYL_ONLY_TEXT_27.UPDATED_TIMESTAMP AS UTS_KoT27,\\n     KYNDRYL_ONLY_TEXT_27.USER_ID AS UUID_KoT27,\\n    KYNDRYL_ONLY_DATE_1.KYNDRYL_ONLY_DATE_1,\\n     KYNDRYL_ONLY_DATE_1.UPDATED_TIMESTAMP AS UTS_KoD1,\\n     KYNDRYL_ONLY_DATE_1.USER_ID AS UUID_KoD1,\\n    KYNDRYL_ONLY_DATE_2.KYNDRYL_ONLY_DATE_2,\\n     KYNDRYL_ONLY_DATE_2.UPDATED_TIMESTAMP AS UTS_KoD2,\\n     KYNDRYL_ONLY_DATE_2.USER_ID AS UUID_KoD2,\\n    KYNDRYL_ONLY_DATE_3.KYNDRYL_ONLY_DATE_3,\\n     KYNDRYL_ONLY_DATE_3.UPDATED_TIMESTAMP AS UTS_KoD3,\\n     KYNDRYL_ONLY_DATE_3.USER_ID AS UUID_KoD3,\\n    KYNDRYL_ONLY_DATE_4.KYNDRYL_ONLY_DATE_4,\\n     KYNDRYL_ONLY_DATE_4.UPDATED_TIMESTAMP AS UTS_KoD4,\\n     KYNDRYL_ONLY_DATE_4.USER_ID AS UUID_KoD4,\\n    KYNDRYL_ONLY_DATE_5.KYNDRYL_ONLY_DATE_5,\\n     KYNDRYL_ONLY_DATE_5.UPDATED_TIMESTAMP AS UTS_KoD5,\\n     KYNDRYL_ONLY_DATE_5.USER_ID AS UUID_KoD5,\\n    KYNDRYL_ONLY_DATE_6.KYNDRYL_ONLY_DATE_6,\\n     KYNDRYL_ONLY_DATE_6.UPDATED_TIMESTAMP AS UTS_KoD6,\\n     KYNDRYL_ONLY_DATE_6.USER_ID AS UUID_KoD6,\\n    KYNDRYL_ONLY_DATE_7.KYNDRYL_ONLY_DATE_7,\\n     KYNDRYL_ONLY_DATE_7.UPDATED_TIMESTAMP AS UTS_KoD7,\\n     KYNDRYL_ONLY_DATE_7.USER_ID AS UUID_KoD7,\\n    KYNDRYL_ONLY_DATE_8.KYNDRYL_ONLY_DATE_8,\\n     KYNDRYL_ONLY_DATE_8.UPDATED_TIMESTAMP AS UTS_KoD8,\\n     KYNDRYL_ONLY_DATE_8.USER_ID AS UUID_KoD8,\\n    KYNDRYL_ONLY_DATE_9.KYNDRYL_ONLY_DATE_9,\\n     KYNDRYL_ONLY_DATE_9.UPDATED_TIMESTAMP AS UTS_KoD9,\\n     KYNDRYL_ONLY_DATE_9.USER_ID AS UUID_KoD9,\\n    KYNDRYL_ONLY_DATE_10.KYNDRYL_ONLY_DATE_10,\\n     KYNDRYL_ONLY_DATE_10.UPDATED_TIMESTAMP AS UTS_KoD10,\\n     KYNDRYL_ONLY_DATE_10.USER_ID AS UUID_KoD10,\\n    KYNDRYL_ONLY_DATE_11.KYNDRYL_ONLY_DATE_11,\\n     KYNDRYL_ONLY_DATE_11.UPDATED_TIMESTAMP AS UTS_KoD11,\\n     KYNDRYL_ONLY_DATE_11.USER_ID AS UUID_KoD11,\\n    KYNDRYL_ONLY_DATE_12.KYNDRYL_ONLY_DATE_12,\\n     KYNDRYL_ONLY_DATE_12.UPDATED_TIMESTAMP AS UTS_KoD12,\\n     KYNDRYL_ONLY_DATE_12.USER_ID AS UUID_KoD12,\\n    KYNDRYL_ONLY_DATE_13.KYNDRYL_ONLY_DATE_13,\\n     KYNDRYL_ONLY_DATE_13.UPDATED_TIMESTAMP AS UTS_KoD13,\\n     KYNDRYL_ONLY_DATE_13.USER_ID AS UUID_KoD13,\\n    KYNDRYL_ONLY_DATE_14.KYNDRYL_ONLY_DATE_14,\\n     KYNDRYL_ONLY_DATE_14.UPDATED_TIMESTAMP AS UTS_KoD14,\\n     KYNDRYL_ONLY_DATE_14.USER_ID AS UUID_KoD14,\\n    KYNDRYL_ONLY_DATE_15.KYNDRYL_ONLY_DATE_15,\\n     KYNDRYL_ONLY_DATE_15.UPDATED_TIMESTAMP AS UTS_KoD15,\\n     KYNDRYL_ONLY_DATE_15.USER_ID AS UUID_KoD15\\nFrom\\n     PGMPDM.PROC_DIM PROCS\\n     INNER JOIN PGMPDM.CNTRCT_DIM C ON C.CNTRCT_DIM_UID = PROCS.CNTRCT_DIM_UID\\n     INNER JOIN (\\n          Select distinct\\n               PROC_DIM_UID\\n          From     \\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM\\n          Where     \\n               COALESCE(\\n                    IBM_ONLY_TEXT_1,\\n                    IBM_ONLY_TEXT_2,\\n                    IBM_ONLY_TEXT_3,\\n                    IBM_ONLY_TEXT_4,\\n                    IBM_ONLY_TEXT_5,\\n                    IBM_ONLY_TEXT_6,\\n                    IBM_ONLY_TEXT_7,\\n                    IBM_ONLY_TEXT_8,\\n                    IBM_ONLY_TEXT_9,\\n                    IBM_ONLY_TEXT_10,\\n                    IBM_ONLY_TEXT_11,\\n                    IBM_ONLY_TEXT_12,\\n                    IBM_ONLY_TEXT_13,\\n                    IBM_ONLY_TEXT_14,\\n                    IBM_ONLY_TEXT_15,\\n                    IBM_ONLY_TEXT_16,\\n                    IBM_ONLY_TEXT_17,\\n                    IBM_ONLY_TEXT_18,\\n                    IBM_ONLY_TEXT_19,\\n                    IBM_ONLY_TEXT_20,\\n                    IBM_ONLY_TEXT_21,\\n                    IBM_ONLY_TEXT_22,\\n                    IBM_ONLY_TEXT_23,\\n                    IBM_ONLY_TEXT_24,\\n                    IBM_ONLY_TEXT_25,\\n                    IBM_ONLY_TEXT_26,\\n                    IBM_ONLY_TEXT_27,\\n                    CAST(IBM_ONLY_DATE_1 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_2 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_3 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_4 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_5 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_6 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_7 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_8 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_9 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_10 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_11 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_12 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_13 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_14 AS VARCHAR),\\n                    CAST(IBM_ONLY_DATE_15 AS VARCHAR)\\n               ) NOT IN (\\'\\', \\'NULL\\')\\n     ) KoPROC ON KoPROC.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n     LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_1 AS KYNDRYL_ONLY_TEXT_1,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_1,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_1                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_1 = DKo.IBM_ONLY_TEXT_1)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_1 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_1 ON KYNDRYL_ONLY_TEXT_1.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_2 AS KYNDRYL_ONLY_TEXT_2,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_2,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_2                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_2 = DKo.IBM_ONLY_TEXT_2)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_2 not in (\\'\\') \\n     )KYNDRYL_ONLY_TEXT_2 ON KYNDRYL_ONLY_TEXT_2.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_3 AS KYNDRYL_ONLY_TEXT_3,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_3,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_3                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_3 = DKo.IBM_ONLY_TEXT_3)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_3 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_3 ON KYNDRYL_ONLY_TEXT_3.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_4 AS KYNDRYL_ONLY_TEXT_4,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_4,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP               \\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_4                              \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_4 = DKo.IBM_ONLY_TEXT_4)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_4 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_4 ON KYNDRYL_ONLY_TEXT_4.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_5 AS KYNDRYL_ONLY_TEXT_5,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_5,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_5               \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_5 = DKo.IBM_ONLY_TEXT_5)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_5 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_5 ON KYNDRYL_ONLY_TEXT_5.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_6 AS KYNDRYL_ONLY_TEXT_6,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_6,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_6                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_6 = DKo.IBM_ONLY_TEXT_6)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_6 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_6 ON KYNDRYL_ONLY_TEXT_6.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_7 AS KYNDRYL_ONLY_TEXT_7,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_7,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_7                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_7 = DKo.IBM_ONLY_TEXT_7)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_7 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_7 ON KYNDRYL_ONLY_TEXT_7.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_8 AS KYNDRYL_ONLY_TEXT_8,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_8,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_8                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_8 = DKo.IBM_ONLY_TEXT_8)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_8 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_8 ON KYNDRYL_ONLY_TEXT_8.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_9 AS KYNDRYL_ONLY_TEXT_9,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_9,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_9                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_9 = DKo.IBM_ONLY_TEXT_9)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_9 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_9 ON KYNDRYL_ONLY_TEXT_9.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_10 AS KYNDRYL_ONLY_TEXT_10,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_10,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_10                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_10 = DKo.IBM_ONLY_TEXT_10)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_10 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_10 ON KYNDRYL_ONLY_TEXT_10.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_11 AS KYNDRYL_ONLY_TEXT_11,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_11,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_11                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_11 = DKo.IBM_ONLY_TEXT_11)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_11 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_11 ON KYNDRYL_ONLY_TEXT_11.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_12 AS KYNDRYL_ONLY_TEXT_12,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_12,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_12                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_12 = DKo.IBM_ONLY_TEXT_12)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_12 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_12 ON KYNDRYL_ONLY_TEXT_12.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_13 AS KYNDRYL_ONLY_TEXT_13,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_13,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_13                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_13 = DKo.IBM_ONLY_TEXT_13)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_13 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_13 ON KYNDRYL_ONLY_TEXT_13.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_14 AS KYNDRYL_ONLY_TEXT_14,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_14,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_14\\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_14 = DKo.IBM_ONLY_TEXT_14)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_14 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_14 ON KYNDRYL_ONLY_TEXT_14.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_15 AS KYNDRYL_ONLY_TEXT_15,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_15,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_15                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_15 = DKo.IBM_ONLY_TEXT_15)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_15 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_15 ON KYNDRYL_ONLY_TEXT_15.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_16 AS KYNDRYL_ONLY_TEXT_16,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_16,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_16                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_16 = DKo.IBM_ONLY_TEXT_16)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_16 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_16 ON KYNDRYL_ONLY_TEXT_16.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_17 AS KYNDRYL_ONLY_TEXT_17,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_17,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_17                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_17 = DKo.IBM_ONLY_TEXT_17)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_17 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_17 ON KYNDRYL_ONLY_TEXT_17.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_18 AS KYNDRYL_ONLY_TEXT_18,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_18,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_18                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_18 = DKo.IBM_ONLY_TEXT_18)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_18 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_18 ON KYNDRYL_ONLY_TEXT_18.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_19 AS KYNDRYL_ONLY_TEXT_19,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_19,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_19                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_19 = DKo.IBM_ONLY_TEXT_19)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_19 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_19 ON KYNDRYL_ONLY_TEXT_19.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_20 AS KYNDRYL_ONLY_TEXT_20,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_20,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_20                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_20 = DKo.IBM_ONLY_TEXT_20)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_20 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_20 ON KYNDRYL_ONLY_TEXT_20.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_21 AS KYNDRYL_ONLY_TEXT_21,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_21,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_21                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_21 = DKo.IBM_ONLY_TEXT_21)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_21 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_21 ON KYNDRYL_ONLY_TEXT_21.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_22 AS KYNDRYL_ONLY_TEXT_22,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_22,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_22                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_22 = DKo.IBM_ONLY_TEXT_22)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_22 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_22 ON KYNDRYL_ONLY_TEXT_22.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_23 AS KYNDRYL_ONLY_TEXT_23,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_23,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_23                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_23 = DKo.IBM_ONLY_TEXT_23)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_23 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_23 ON KYNDRYL_ONLY_TEXT_23.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_24 AS KYNDRYL_ONLY_TEXT_24,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_24,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_24                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_24 = DKo.IBM_ONLY_TEXT_24)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_24 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_24 ON KYNDRYL_ONLY_TEXT_24.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_25 AS KYNDRYL_ONLY_TEXT_25,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_25,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_25                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_25 = DKo.IBM_ONLY_TEXT_25)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_25 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_25 ON KYNDRYL_ONLY_TEXT_25.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n     LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_26 AS KYNDRYL_ONLY_TEXT_26,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_26,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_26                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_26 = DKo.IBM_ONLY_TEXT_26)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_26 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_26 ON KYNDRYL_ONLY_TEXT_26.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_TEXT_27 AS KYNDRYL_ONLY_TEXT_27,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_TEXT_27,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_TEXT_27               \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_TEXT_27 = DKo.IBM_ONLY_TEXT_27)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_TEXT_27 not in (\\'\\') \\n     ) KYNDRYL_ONLY_TEXT_27 ON KYNDRYL_ONLY_TEXT_27.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_1 AS KYNDRYL_ONLY_DATE_1,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_1,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_1                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_1 = DKo.IBM_ONLY_DATE_1)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_1 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_1 ON KYNDRYL_ONLY_DATE_1.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_2 AS KYNDRYL_ONLY_DATE_2,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_2,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_2                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_2 = DKo.IBM_ONLY_DATE_2)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_2 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_2 ON KYNDRYL_ONLY_DATE_2.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_3 AS KYNDRYL_ONLY_DATE_3,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_3,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_3                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_3 = DKo.IBM_ONLY_DATE_3)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_3 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_3 ON KYNDRYL_ONLY_DATE_3.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_4 AS KYNDRYL_ONLY_DATE_4,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_4,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP     \\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_4                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_4 = DKo.IBM_ONLY_DATE_4)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_4 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_4 ON KYNDRYL_ONLY_DATE_4.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_5 AS KYNDRYL_ONLY_DATE_5,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_5,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_5                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_5 = DKo.IBM_ONLY_DATE_5)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_5 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_5 ON KYNDRYL_ONLY_DATE_5.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_6 AS KYNDRYL_ONLY_DATE_6,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_6,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_6                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_6 = DKo.IBM_ONLY_DATE_6)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_6 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_6 ON KYNDRYL_ONLY_DATE_6.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_7 AS KYNDRYL_ONLY_DATE_7,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_7,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_7                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_7 = DKo.IBM_ONLY_DATE_7)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_7 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_7 ON KYNDRYL_ONLY_DATE_7.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_8 AS KYNDRYL_ONLY_DATE_8,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_8,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_8                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_8 = DKo.IBM_ONLY_DATE_8)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_8 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_8 ON KYNDRYL_ONLY_DATE_8.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_9 AS KYNDRYL_ONLY_DATE_9,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_9,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_9                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_9 = DKo.IBM_ONLY_DATE_9)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_9 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_9 ON KYNDRYL_ONLY_DATE_9.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_10 AS KYNDRYL_ONLY_DATE_10,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_10,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_10                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_10 = DKo.IBM_ONLY_DATE_10)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_10 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_10 ON KYNDRYL_ONLY_DATE_10.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_11 AS KYNDRYL_ONLY_DATE_11,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_11,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_11                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_11 = DKo.IBM_ONLY_DATE_11)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_11 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_11 ON KYNDRYL_ONLY_DATE_11.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_12 AS KYNDRYL_ONLY_DATE_12,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_12,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_12                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_12 = DKo.IBM_ONLY_DATE_12)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_12 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_12 ON KYNDRYL_ONLY_DATE_12.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_13 AS KYNDRYL_ONLY_DATE_13,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_13,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_13                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_13 = DKo.IBM_ONLY_DATE_13)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_13 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_13 ON KYNDRYL_ONLY_DATE_13.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_14 AS KYNDRYL_ONLY_DATE_14,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_14,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_14                         \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_14 = DKo.IBM_ONLY_DATE_14)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_14 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_14 ON KYNDRYL_ONLY_DATE_14.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKo.PROC_DIM_UID,\\n               DKo.IBM_ONLY_DATE_15 AS KYNDRYL_ONLY_DATE_15,\\n               DKoTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM DKo\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_DATE_15,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_DATE_15                    \\n               )DKoTime ON (DKoTime.PROC_ID = DKo.PROC_DIM_UID and DKoTime.IBM_ONLY_DATE_15 = DKo.IBM_ONLY_DATE_15)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKoUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_H DKoUser\\n                    Where (DKoUser.PROC_ID = DKoTime.PROC_ID and DKoUser.UPDATED_TS = DKoTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKoTime.IBM_ONLY_DATE_15 not like (\\'\\') \\n     ) KYNDRYL_ONLY_DATE_15 ON KYNDRYL_ONLY_DATE_15.PROC_DIM_UID = PROCS.PROC_DIM_UID',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"getSourceData sort(asc(PROC_DIM_UID, false)) ~> sortedSourceData",
						"sortedSourceData alterRow(upsertIf(true())) ~> upsertRows",
						"upsertRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'CSTM_DATA_KYNDRYL_ONLY_AUDIT_RPT',",
						"     insertable: true,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['PROC_DIM_UID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> upsertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0009_CNTRY_CSTM_DATA_PUBLIC_AUDIT_RPT')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 14-06-2022\nJob Name: DF_BALD0009_CNTRY_CSTM_DATA_PUBLIC_AUDIT_RPT\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/CP Summary & Custom Field audit"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "upsertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedSourceData"
						},
						{
							"name": "upsertRows"
						}
					],
					"scriptLines": [
						"source(output(",
						"          PROC_DIM_UID as integer,",
						"          CNTRCT_DIM_UID as integer,",
						"          CNTRCT_NM as string,",
						"          PUBLIC_CNTRY_TEXT_1 as string,",
						"          UTS_PCT1 as timestamp,",
						"          UUID_PCT1 as binary,",
						"          PUBLIC_CNTRY_TEXT_2 as string,",
						"          UTS_PCT2 as timestamp,",
						"          UUID_PCT2 as binary,",
						"          PUBLIC_CNTRY_TEXT_3 as string,",
						"          UTS_PCT3 as timestamp,",
						"          UUID_PCT3 as binary,",
						"          PUBLIC_CNTRY_TEXT_4 as string,",
						"          UTS_PCT4 as timestamp,",
						"          UUID_PCT4 as binary,",
						"          PUBLIC_CNTRY_TEXT_5 as string,",
						"          UTS_PCT5 as timestamp,",
						"          UUID_PCT5 as binary,",
						"          PUBLIC_CNTRY_TEXT_6 as string,",
						"          UTS_PCT6 as timestamp,",
						"          UUID_PCT6 as binary,",
						"          PUBLIC_CNTRY_TEXT_7 as string,",
						"          UTS_PCT7 as timestamp,",
						"          UUID_PCT7 as binary,",
						"          PUBLIC_CNTRY_TEXT_8 as string,",
						"          UTS_PCT8 as timestamp,",
						"          UUID_PCT8 as binary,",
						"          PUBLIC_CNTRY_TEXT_9 as string,",
						"          UTS_PCT9 as timestamp,",
						"          UUID_PCT9 as binary,",
						"          PUBLIC_CNTRY_TEXT_10 as string,",
						"          UTS_PCT10 as timestamp,",
						"          UUID_PCT10 as binary,",
						"          PUBLIC_CNTRY_DATE_1 as date,",
						"          UTS_PCD1 as timestamp,",
						"          UUID_PCD1 as binary,",
						"          PUBLIC_CNTRY_DATE_2 as date,",
						"          UTS_PCD2 as timestamp,",
						"          UUID_PCD2 as binary,",
						"          PUBLIC_CNTRY_DATE_3 as date,",
						"          UTS_PCD3 as timestamp,",
						"          UUID_PCD3 as binary,",
						"          PUBLIC_CNTRY_DATE_4 as date,",
						"          UTS_PCD4 as timestamp,",
						"          UUID_PCD4 as binary,",
						"          PUBLIC_CNTRY_DATE_5 as date,",
						"          UTS_PCD5 as timestamp,",
						"          UUID_PCD5 as binary,",
						"          PUBLIC_CNTRY_DATE_6 as date,",
						"          UTS_PCD6 as timestamp,",
						"          UUID_PCD6 as binary,",
						"          PUBLIC_CNTRY_DATE_7 as date,",
						"          UTS_PCD7 as timestamp,",
						"          UUID_PCD7 as binary,",
						"          PUBLIC_CNTRY_DATE_8 as date,",
						"          UTS_PCD8 as timestamp,",
						"          UUID_PCD8 as binary,",
						"          PUBLIC_CNTRY_DATE_9 as date,",
						"          UTS_PCD9 as timestamp,",
						"          UUID_PCD9 as binary,",
						"          PUBLIC_CNTRY_DATE_10 as date,",
						"          UTS_PCD10 as timestamp,",
						"          UUID_PCD10 as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select distinct\\n     PROCS.PROC_DIM_UID,\\n     C.CNTRCT_DIM_UID,\\n     C.CNTRCT_NM,\\n    PUBLIC_CNTRY_TEXT_1.PUBLIC_CNTRY_TEXT_1,\\n     PUBLIC_CNTRY_TEXT_1.UPDATED_TIMESTAMP AS UTS_PCT1,\\n     PUBLIC_CNTRY_TEXT_1.USER_ID AS UUID_PCT1,\\n    PUBLIC_CNTRY_TEXT_2.PUBLIC_CNTRY_TEXT_2,\\n     PUBLIC_CNTRY_TEXT_2.UPDATED_TIMESTAMP AS UTS_PCT2,\\n     PUBLIC_CNTRY_TEXT_2.USER_ID AS UUID_PCT2,\\n    PUBLIC_CNTRY_TEXT_3.PUBLIC_CNTRY_TEXT_3,\\n     PUBLIC_CNTRY_TEXT_3.UPDATED_TIMESTAMP AS UTS_PCT3,\\n     PUBLIC_CNTRY_TEXT_3.USER_ID AS UUID_PCT3,\\n    PUBLIC_CNTRY_TEXT_4.PUBLIC_CNTRY_TEXT_4,\\n     PUBLIC_CNTRY_TEXT_4.UPDATED_TIMESTAMP AS UTS_PCT4,\\n     PUBLIC_CNTRY_TEXT_4.USER_ID AS UUID_PCT4,\\n    PUBLIC_CNTRY_TEXT_5.PUBLIC_CNTRY_TEXT_5,\\n     PUBLIC_CNTRY_TEXT_5.UPDATED_TIMESTAMP AS UTS_PCT5,\\n     PUBLIC_CNTRY_TEXT_5.USER_ID AS UUID_PCT5,\\n    PUBLIC_CNTRY_TEXT_6.PUBLIC_CNTRY_TEXT_6,\\n     PUBLIC_CNTRY_TEXT_6.UPDATED_TIMESTAMP AS UTS_PCT6,\\n     PUBLIC_CNTRY_TEXT_6.USER_ID AS UUID_PCT6,\\n    PUBLIC_CNTRY_TEXT_7.PUBLIC_CNTRY_TEXT_7,\\n     PUBLIC_CNTRY_TEXT_7.UPDATED_TIMESTAMP AS UTS_PCT7,\\n     PUBLIC_CNTRY_TEXT_7.USER_ID AS UUID_PCT7,\\n    PUBLIC_CNTRY_TEXT_8.PUBLIC_CNTRY_TEXT_8,\\n     PUBLIC_CNTRY_TEXT_8.UPDATED_TIMESTAMP AS UTS_PCT8,\\n     PUBLIC_CNTRY_TEXT_8.USER_ID AS UUID_PCT8,\\n    PUBLIC_CNTRY_TEXT_9.PUBLIC_CNTRY_TEXT_9,\\n     PUBLIC_CNTRY_TEXT_9.UPDATED_TIMESTAMP AS UTS_PCT9,\\n     PUBLIC_CNTRY_TEXT_9.USER_ID AS UUID_PCT9,\\n    PUBLIC_CNTRY_TEXT_10.PUBLIC_CNTRY_TEXT_10,\\n     PUBLIC_CNTRY_TEXT_10.UPDATED_TIMESTAMP AS UTS_PCT10,\\n     PUBLIC_CNTRY_TEXT_10.USER_ID AS UUID_PCT10,\\n    PUBLIC_CNTRY_DATE_1.PUBLIC_CNTRY_DATE_1,\\n     PUBLIC_CNTRY_DATE_1.UPDATED_TIMESTAMP AS UTS_PCD1,\\n     PUBLIC_CNTRY_DATE_1.USER_ID AS UUID_PCD1,\\n    PUBLIC_CNTRY_DATE_2.PUBLIC_CNTRY_DATE_2,\\n     PUBLIC_CNTRY_DATE_2.UPDATED_TIMESTAMP AS UTS_PCD2,\\n     PUBLIC_CNTRY_DATE_2.USER_ID AS UUID_PCD2,\\n    PUBLIC_CNTRY_DATE_3.PUBLIC_CNTRY_DATE_3,\\n     PUBLIC_CNTRY_DATE_3.UPDATED_TIMESTAMP AS UTS_PCD3,\\n     PUBLIC_CNTRY_DATE_3.USER_ID AS UUID_PCD3,\\n    PUBLIC_CNTRY_DATE_4.PUBLIC_CNTRY_DATE_4,\\n     PUBLIC_CNTRY_DATE_4.UPDATED_TIMESTAMP AS UTS_PCD4,\\n     PUBLIC_CNTRY_DATE_4.USER_ID AS UUID_PCD4,\\n    PUBLIC_CNTRY_DATE_5.PUBLIC_CNTRY_DATE_5,\\n     PUBLIC_CNTRY_DATE_5.UPDATED_TIMESTAMP AS UTS_PCD5,\\n     PUBLIC_CNTRY_DATE_5.USER_ID AS UUID_PCD5,\\n    PUBLIC_CNTRY_DATE_6.PUBLIC_CNTRY_DATE_6,\\n     PUBLIC_CNTRY_DATE_6.UPDATED_TIMESTAMP AS UTS_PCD6,\\n     PUBLIC_CNTRY_DATE_6.USER_ID AS UUID_PCD6,\\n    PUBLIC_CNTRY_DATE_7.PUBLIC_CNTRY_DATE_7,\\n     PUBLIC_CNTRY_DATE_7.UPDATED_TIMESTAMP AS UTS_PCD7,\\n     PUBLIC_CNTRY_DATE_7.USER_ID AS UUID_PCD7,\\n    PUBLIC_CNTRY_DATE_8.PUBLIC_CNTRY_DATE_8,\\n     PUBLIC_CNTRY_DATE_8.UPDATED_TIMESTAMP AS UTS_PCD8,\\n     PUBLIC_CNTRY_DATE_8.USER_ID AS UUID_PCD8,\\n    PUBLIC_CNTRY_DATE_9.PUBLIC_CNTRY_DATE_9,\\n     PUBLIC_CNTRY_DATE_9.UPDATED_TIMESTAMP AS UTS_PCD9,\\n     PUBLIC_CNTRY_DATE_9.USER_ID AS UUID_PCD9,\\n    PUBLIC_CNTRY_DATE_10.PUBLIC_CNTRY_DATE_10,\\n     PUBLIC_CNTRY_DATE_10.UPDATED_TIMESTAMP AS UTS_PCD10,\\n     PUBLIC_CNTRY_DATE_10.USER_ID AS UUID_PCD10\\nFrom\\n     PGMPDM.PROC_DIM PROCS\\n     INNER JOIN PGMPDM.CNTRCT_DIM C ON C.CNTRCT_DIM_UID = PROCS.CNTRCT_DIM_UID\\n     INNER JOIN (\\n          Select distinct\\n               PROC_DIM_UID\\n          From     \\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM\\n          Where     \\n               COALESCE(\\n                    PUBLIC_CNTRY_TEXT_1,\\n                    PUBLIC_CNTRY_TEXT_2,\\n                    PUBLIC_CNTRY_TEXT_3,\\n                    PUBLIC_CNTRY_TEXT_4,\\n                    PUBLIC_CNTRY_TEXT_5,\\n                    PUBLIC_CNTRY_TEXT_6,\\n                    PUBLIC_CNTRY_TEXT_7,\\n                    PUBLIC_CNTRY_TEXT_8,\\n                    PUBLIC_CNTRY_TEXT_9,\\n                    PUBLIC_CNTRY_TEXT_10,\\n                    CAST(PUBLIC_CNTRY_DATE_1 AS VARCHAR),\\n                    CAST(PUBLIC_CNTRY_DATE_2 AS VARCHAR),\\n                    CAST(PUBLIC_CNTRY_DATE_3 AS VARCHAR),\\n                    CAST(PUBLIC_CNTRY_DATE_4 AS VARCHAR),\\n                    CAST(PUBLIC_CNTRY_DATE_5 AS VARCHAR),\\n                    CAST(PUBLIC_CNTRY_DATE_6 AS VARCHAR),\\n                    CAST(PUBLIC_CNTRY_DATE_7 AS VARCHAR),\\n                    CAST(PUBLIC_CNTRY_DATE_8 AS VARCHAR),\\n                    CAST(PUBLIC_CNTRY_DATE_9 AS VARCHAR),\\n                    CAST(PUBLIC_CNTRY_DATE_10 AS VARCHAR)\\n               ) NOT IN (\\'\\', \\'NULL\\')\\n     ) PPROC ON PPROC.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n     LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_CNTRY_TEXT_1 AS PUBLIC_CNTRY_TEXT_1,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_CNTRY_TEXT_1,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_CNTRY_TEXT_1                         \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_CNTRY_TEXT_1 = DP.PUBLIC_CNTRY_TEXT_1)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_CNTRY_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_CNTRY_TEXT_1 not in (\\'\\') \\n     ) PUBLIC_CNTRY_TEXT_1 ON PUBLIC_CNTRY_TEXT_1.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_CNTRY_TEXT_2 AS PUBLIC_CNTRY_TEXT_2,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_CNTRY_TEXT_2,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_CNTRY_TEXT_2                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_CNTRY_TEXT_2 = DP.PUBLIC_CNTRY_TEXT_2)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_CNTRY_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_CNTRY_TEXT_2 not in (\\'\\') \\n     ) PUBLIC_CNTRY_TEXT_2 ON PUBLIC_CNTRY_TEXT_2.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_CNTRY_TEXT_3 AS PUBLIC_CNTRY_TEXT_3,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_CNTRY_TEXT_3,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_CNTRY_TEXT_3                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_CNTRY_TEXT_3 = DP.PUBLIC_CNTRY_TEXT_3)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_CNTRY_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_CNTRY_TEXT_3 not in (\\'\\') \\n     ) PUBLIC_CNTRY_TEXT_3 ON PUBLIC_CNTRY_TEXT_3.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_CNTRY_TEXT_4 AS PUBLIC_CNTRY_TEXT_4,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_CNTRY_TEXT_4,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP               \\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_CNTRY_TEXT_4                              \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_CNTRY_TEXT_4 = DP.PUBLIC_CNTRY_TEXT_4)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_CNTRY_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_CNTRY_TEXT_4 not in (\\'\\') \\n     ) PUBLIC_CNTRY_TEXT_4 ON PUBLIC_CNTRY_TEXT_4.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_CNTRY_TEXT_5 AS PUBLIC_CNTRY_TEXT_5,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_CNTRY_TEXT_5,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_CNTRY_TEXT_5               \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_CNTRY_TEXT_5 = DP.PUBLIC_CNTRY_TEXT_5)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_CNTRY_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_CNTRY_TEXT_5 not in (\\'\\') \\n     ) PUBLIC_CNTRY_TEXT_5 ON PUBLIC_CNTRY_TEXT_5.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_CNTRY_TEXT_6 AS PUBLIC_CNTRY_TEXT_6,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_CNTRY_TEXT_6,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_CNTRY_TEXT_6                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_CNTRY_TEXT_6 = DP.PUBLIC_CNTRY_TEXT_6)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_CNTRY_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_CNTRY_TEXT_6 not in (\\'\\') \\n     ) PUBLIC_CNTRY_TEXT_6 ON PUBLIC_CNTRY_TEXT_6.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_CNTRY_TEXT_7 AS PUBLIC_CNTRY_TEXT_7,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_CNTRY_TEXT_7,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_CNTRY_TEXT_7                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_CNTRY_TEXT_7 = DP.PUBLIC_CNTRY_TEXT_7)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_CNTRY_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_CNTRY_TEXT_7 not in (\\'\\') \\n     ) PUBLIC_CNTRY_TEXT_7 ON PUBLIC_CNTRY_TEXT_7.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_CNTRY_TEXT_8 AS PUBLIC_CNTRY_TEXT_8,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_CNTRY_TEXT_8,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_CNTRY_TEXT_8                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_CNTRY_TEXT_8 = DP.PUBLIC_CNTRY_TEXT_8)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_CNTRY_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_CNTRY_TEXT_8 not in (\\'\\') \\n     ) PUBLIC_CNTRY_TEXT_8 ON PUBLIC_CNTRY_TEXT_8.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_CNTRY_TEXT_9 AS PUBLIC_CNTRY_TEXT_9,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_CNTRY_TEXT_9,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_CNTRY_TEXT_9                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_CNTRY_TEXT_9 = DP.PUBLIC_CNTRY_TEXT_9)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_CNTRY_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_CNTRY_TEXT_9 not in (\\'\\') \\n     ) PUBLIC_CNTRY_TEXT_9 ON PUBLIC_CNTRY_TEXT_9.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_CNTRY_TEXT_10 AS PUBLIC_CNTRY_TEXT_10,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_CNTRY_TEXT_10,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_CNTRY_TEXT_10                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_CNTRY_TEXT_10 = DP.PUBLIC_CNTRY_TEXT_10)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_CNTRY_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_CNTRY_TEXT_10 not in (\\'\\') \\n     ) PUBLIC_CNTRY_TEXT_10 ON PUBLIC_CNTRY_TEXT_10.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_CNTRY_DATE_1 AS PUBLIC_CNTRY_DATE_1,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_CNTRY_DATE_1,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_CNTRY_DATE_1                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_CNTRY_DATE_1 = DP.PUBLIC_CNTRY_DATE_1)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_CNTRY_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_CNTRY_DATE_1 not like (\\'\\') \\n     ) PUBLIC_CNTRY_DATE_1 ON PUBLIC_CNTRY_DATE_1.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_CNTRY_DATE_2 AS PUBLIC_CNTRY_DATE_2,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_CNTRY_DATE_2,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_CNTRY_DATE_2                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_CNTRY_DATE_2 = DP.PUBLIC_CNTRY_DATE_2)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_CNTRY_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_CNTRY_DATE_2 not like (\\'\\') \\n     ) PUBLIC_CNTRY_DATE_2 ON PUBLIC_CNTRY_DATE_2.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_CNTRY_DATE_3 AS PUBLIC_CNTRY_DATE_3,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_CNTRY_DATE_3,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_CNTRY_DATE_3                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_CNTRY_DATE_3 = DP.PUBLIC_CNTRY_DATE_3)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_CNTRY_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_CNTRY_DATE_3 not like (\\'\\') \\n     ) PUBLIC_CNTRY_DATE_3 ON PUBLIC_CNTRY_DATE_3.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_CNTRY_DATE_4 AS PUBLIC_CNTRY_DATE_4,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_CNTRY_DATE_4,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP     \\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_CNTRY_DATE_4                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_CNTRY_DATE_4 = DP.PUBLIC_CNTRY_DATE_4)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_CNTRY_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_CNTRY_DATE_4 not like (\\'\\') \\n     ) PUBLIC_CNTRY_DATE_4 ON PUBLIC_CNTRY_DATE_4.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_CNTRY_DATE_5 AS PUBLIC_CNTRY_DATE_5,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_CNTRY_DATE_5,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_CNTRY_DATE_5                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_CNTRY_DATE_5 = DP.PUBLIC_CNTRY_DATE_5)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_CNTRY_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_CNTRY_DATE_5 not like (\\'\\') \\n     ) PUBLIC_CNTRY_DATE_5 ON PUBLIC_CNTRY_DATE_5.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_CNTRY_DATE_6 AS PUBLIC_CNTRY_DATE_6,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_CNTRY_DATE_6,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_CNTRY_DATE_6                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_CNTRY_DATE_6 = DP.PUBLIC_CNTRY_DATE_6)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_CNTRY_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_CNTRY_DATE_6 not like (\\'\\') \\n     ) PUBLIC_CNTRY_DATE_6 ON PUBLIC_CNTRY_DATE_6.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_CNTRY_DATE_7 AS PUBLIC_CNTRY_DATE_7,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_CNTRY_DATE_7,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_CNTRY_DATE_7                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_CNTRY_DATE_7 = DP.PUBLIC_CNTRY_DATE_7)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_CNTRY_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_CNTRY_DATE_7 not like (\\'\\') \\n     ) PUBLIC_CNTRY_DATE_7 ON PUBLIC_CNTRY_DATE_7.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_CNTRY_DATE_8 AS PUBLIC_CNTRY_DATE_8,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_CNTRY_DATE_8,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_CNTRY_DATE_8                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_CNTRY_DATE_8 = DP.PUBLIC_CNTRY_DATE_8)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_CNTRY_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_CNTRY_DATE_8 not like (\\'\\') \\n     ) PUBLIC_CNTRY_DATE_8 ON PUBLIC_CNTRY_DATE_8.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_CNTRY_DATE_9 AS PUBLIC_CNTRY_DATE_9,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_CNTRY_DATE_9,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_CNTRY_DATE_9                         \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_CNTRY_DATE_9 = DP.PUBLIC_CNTRY_DATE_9)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_CNTRY_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_CNTRY_DATE_9 not like (\\'\\') \\n     ) PUBLIC_CNTRY_DATE_9 ON PUBLIC_CNTRY_DATE_9.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DP.PROC_DIM_UID,\\n               DP.PUBLIC_CNTRY_DATE_10 AS PUBLIC_CNTRY_DATE_10,\\n               DPTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DP\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         PUBLIC_CNTRY_DATE_10,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_PUBLIC_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         PUBLIC_CNTRY_DATE_10                    \\n               )DPTime ON (DPTime.PROC_ID = DP.PROC_DIM_UID and DPTime.PUBLIC_CNTRY_DATE_10 = DP.PUBLIC_CNTRY_DATE_10)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DPUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_PUBLIC_CNTRY_H DPUser\\n                    Where (DPUser.PROC_ID = DPTime.PROC_ID and DPUser.UPDATED_TS = DPTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DPTime.PUBLIC_CNTRY_DATE_10 not like (\\'\\') \\n     ) PUBLIC_CNTRY_DATE_10 ON PUBLIC_CNTRY_DATE_10.PROC_DIM_UID = PROCS.PROC_DIM_UID',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"getSourceData sort(asc(PROC_DIM_UID, false)) ~> sortedSourceData",
						"sortedSourceData alterRow(upsertIf(true())) ~> upsertRows",
						"upsertRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'CNTRY_CSTM_DATA_PUBLIC_AUDIT_RPT',",
						"     insertable: true,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['PROC_DIM_UID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> upsertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0010_ACCTRPTS_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 30-05-2022\nJob Name: DF_BALD0010_ACCTRPTS_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						}
					],
					"scriptLines": [
						"parameters{",
						"     current_user as string",
						"}",
						"source(output(",
						"          ACCTRPTS_DIM_UID as integer,",
						"          PRJCT_ID as string,",
						"          ACCTRPTS_REM_TXT as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     ACC.PROC_ID as ACCTRPTS_DIM_UID,\\n     ACC.PROJECT_ID as PRJCT_ID,\\n     cast(ACC.REMARKS as varchar(1024)) as ACCTRPTS_REM_TXT,\\n     ACC.CREATED_TS as SRC_CRETD_TMS,\\n     ACC.CREATED_USERID as SRC_CRETD_USER_ID,\\n     ACC.UPDATED_TS as SRC_UPDTD_TMS,\\n     --ACC.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     \\'PGMPELT\\' as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED\\nfrom\\n     APPFUN.ACCTRPTS ACC\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on ACC.PROC_ID = ZDT.PROC_ID\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on ACC.PROC_ID = PDEL.PROC_ID\\n     left join\\n     (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          from\\n               PGMPDM.ZAUX_ETL_JOBS\\n          where\\n               ETL_JOB_NM = \\'BALD0010_ACCTRPTS_DIM\\'          \\n     ) as JOB\\n     on \\'1\\' = \\'1\\'\\n     left join\\n     (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          where\\n               IS_CURR_IND = \\'Y\\'\\n     ) as FIL\\n     on \\'1\\' = \\'1\\'\\n     left join\\n     (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          from\\n               PGMPDM.SRC_SYS_DIM\\n          where\\n               SRC_SYS_CD = \\'PGMP\\'               \\n     ) as SYS\\n     on \\'1\\' = \\'1\\'\\n',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          LKP_ACCTRPTS_DIM_UID as integer,",
						"          LKP_PRJCT_ID as string,",
						"          LKP_ACCTRPTS_REM_TXT as string,",
						"          LKP_SRC_CRETD_TMS as timestamp,",
						"          LKP_SRC_CRETD_USER_ID as string,",
						"          LKP_SRC_UPDTD_TMS as timestamp,",
						"          LKP_SRC_UPDTD_USER_ID as string,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_ETL_JOB_ID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     S.ACCTRPTS_DIM_UID as LKP_ACCTRPTS_DIM_UID,\\n     S.PRJCT_ID as LKP_PRJCT_ID,\\n     S.ACCTRPTS_REM_TXT as LKP_ACCTRPTS_REM_TXT,\\n     S.SRC_CRETD_TMS as LKP_SRC_CRETD_TMS,\\n     S.SRC_CRETD_USER_ID as LKP_SRC_CRETD_USER_ID,\\n     S.SRC_UPDTD_TMS as LKP_SRC_UPDTD_TMS,\\n     S.SRC_UPDTD_USER_ID as LKP_SRC_UPDTD_USER_ID,\\n     S.SRC_SYS_DIM_UID as LKP_SRC_SYS_DIM_UID,\\n     S.ETL_JOB_ID as LKP_ETL_JOB_ID\\nfrom\\n     PGMPDM.ACCTRPTS_DIM S\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on S.ACCTRPTS_DIM_UID = ZDT.PROC_ID     \\n     ',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"getLookupData sort(asc(LKP_ACCTRPTS_DIM_UID, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"mergedData split(ACCTRPTS_DIM_UID==LKP_ACCTRPTS_DIM_UID,",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(ACCTRPTS_DIM_UID == LKP_ACCTRPTS_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          ACCTRPTS_DIM_UID,",
						"          PRJCT_ID,",
						"          ACCTRPTS_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'U'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'I'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          ACCTRPTS_DIM_UID,",
						"          PRJCT_ID,",
						"          ACCTRPTS_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'ACCTRPTS_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['ACCTRPTS_DIM_UID'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'ACCTRPTS_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0010_CNTRY_CSTM_DATA_KYNDRTL_ONLY_AUDIT_RPT')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 14-06-2022\nJob Name: DF_BALD0010_CNTRY_CSTM_DATA_KYNDRTL_ONLY_AUDIT_RPT\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/CP Summary & Custom Field audit"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "upsertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedSourceData"
						},
						{
							"name": "upsertRows"
						}
					],
					"scriptLines": [
						"source(output(",
						"          PROC_DIM_UID as integer,",
						"          CNTRCT_DIM_UID as integer,",
						"          CNTRCT_NM as string,",
						"          KYNDRYL_ONLY_CNTRY_TEXT_1 as string,",
						"          UTS_KOCT1 as timestamp,",
						"          UUID_KOCT1 as binary,",
						"          KYNDRYL_ONLY_CNTRY_TEXT_2 as string,",
						"          UTS_KOCT2 as timestamp,",
						"          UUID_KOCT2 as binary,",
						"          KYNDRYL_ONLY_CNTRY_TEXT_3 as string,",
						"          UTS_KOCT3 as timestamp,",
						"          UUID_KOCT3 as binary,",
						"          KYNDRYL_ONLY_CNTRY_TEXT_4 as string,",
						"          UTS_KOCT4 as timestamp,",
						"          UUID_KOCT4 as binary,",
						"          KYNDRYL_ONLY_CNTRY_TEXT_5 as string,",
						"          UTS_KOCT5 as timestamp,",
						"          UUID_KOCT5 as binary,",
						"          KYNDRYL_ONLY_CNTRY_TEXT_6 as string,",
						"          UTS_KOCT6 as timestamp,",
						"          UUID_KOCT6 as binary,",
						"          KYNDRYL_ONLY_CNTRY_TEXT_7 as string,",
						"          UTS_KOCT7 as timestamp,",
						"          UUID_KOCT7 as binary,",
						"          KYNDRYL_ONLY_CNTRY_TEXT_8 as string,",
						"          UTS_KOCT8 as timestamp,",
						"          UUID_KOCT8 as binary,",
						"          KYNDRYL_ONLY_CNTRY_TEXT_9 as string,",
						"          UTS_KOCT9 as timestamp,",
						"          UUID_KOCT9 as binary,",
						"          KYNDRYL_ONLY_CNTRY_TEXT_10 as string,",
						"          UTS_KOCT10 as timestamp,",
						"          UUID_KOCT10 as binary,",
						"          KYNDRYL_ONLY_CNTRY_DATE_1 as date,",
						"          UTS_KOCD1 as timestamp,",
						"          UUID_KOCD1 as binary,",
						"          KYNDRYL_ONLY_CNTRY_DATE_2 as date,",
						"          UTS_KOCD2 as timestamp,",
						"          UUID_KOCD2 as binary,",
						"          KYNDRYL_ONLY_CNTRY_DATE_3 as date,",
						"          UTS_KOCD3 as timestamp,",
						"          UUID_KOCD3 as binary,",
						"          KYNDRYL_ONLY_CNTRY_DATE_4 as date,",
						"          UTS_KOCD4 as timestamp,",
						"          UUID_KOCD4 as binary,",
						"          KYNDRYL_ONLY_CNTRY_DATE_5 as date,",
						"          UTS_KOCD5 as timestamp,",
						"          UUID_KOCD5 as binary,",
						"          KYNDRYL_ONLY_CNTRY_DATE_6 as date,",
						"          UTS_KOCD6 as timestamp,",
						"          UUID_KOCD6 as binary,",
						"          KYNDRYL_ONLY_CNTRY_DATE_7 as date,",
						"          UTS_KOCD7 as timestamp,",
						"          UUID_KOCD7 as binary,",
						"          KYNDRYL_ONLY_CNTRY_DATE_8 as date,",
						"          UTS_KOCD8 as timestamp,",
						"          UUID_KOCD8 as binary,",
						"          KYNDRYL_ONLY_CNTRY_DATE_9 as date,",
						"          UTS_KOCD9 as timestamp,",
						"          UUID_KOCD9 as binary,",
						"          KYNDRYL_ONLY_CNTRY_DATE_10 as date,",
						"          UTS_KOCD10 as timestamp,",
						"          UUID_KOCD10 as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select distinct\\n     PROCS.PROC_DIM_UID,\\n     C.CNTRCT_DIM_UID,\\n     C.CNTRCT_NM,\\n    KYNDRYL_ONLY_CNTRY_TEXT_1.KYNDRYL_ONLY_CNTRY_TEXT_1,\\n     KYNDRYL_ONLY_CNTRY_TEXT_1.UPDATED_TIMESTAMP AS UTS_KOCT1,\\n     KYNDRYL_ONLY_CNTRY_TEXT_1.USER_ID AS UUID_KOCT1,\\n    KYNDRYL_ONLY_CNTRY_TEXT_2.KYNDRYL_ONLY_CNTRY_TEXT_2,\\n     KYNDRYL_ONLY_CNTRY_TEXT_2.UPDATED_TIMESTAMP AS UTS_KOCT2,\\n     KYNDRYL_ONLY_CNTRY_TEXT_2.USER_ID AS UUID_KOCT2,\\n    KYNDRYL_ONLY_CNTRY_TEXT_3.KYNDRYL_ONLY_CNTRY_TEXT_3,\\n     KYNDRYL_ONLY_CNTRY_TEXT_3.UPDATED_TIMESTAMP AS UTS_KOCT3,\\n     KYNDRYL_ONLY_CNTRY_TEXT_3.USER_ID AS UUID_KOCT3,\\n    KYNDRYL_ONLY_CNTRY_TEXT_4.KYNDRYL_ONLY_CNTRY_TEXT_4,\\n     KYNDRYL_ONLY_CNTRY_TEXT_4.UPDATED_TIMESTAMP AS UTS_KOCT4,\\n     KYNDRYL_ONLY_CNTRY_TEXT_4.USER_ID AS UUID_KOCT4,\\n    KYNDRYL_ONLY_CNTRY_TEXT_5.KYNDRYL_ONLY_CNTRY_TEXT_5,\\n     KYNDRYL_ONLY_CNTRY_TEXT_5.UPDATED_TIMESTAMP AS UTS_KOCT5,\\n     KYNDRYL_ONLY_CNTRY_TEXT_5.USER_ID AS UUID_KOCT5,\\n    KYNDRYL_ONLY_CNTRY_TEXT_6.KYNDRYL_ONLY_CNTRY_TEXT_6,\\n     KYNDRYL_ONLY_CNTRY_TEXT_6.UPDATED_TIMESTAMP AS UTS_KOCT6,\\n     KYNDRYL_ONLY_CNTRY_TEXT_6.USER_ID AS UUID_KOCT6,\\n    KYNDRYL_ONLY_CNTRY_TEXT_7.KYNDRYL_ONLY_CNTRY_TEXT_7,\\n     KYNDRYL_ONLY_CNTRY_TEXT_7.UPDATED_TIMESTAMP AS UTS_KOCT7,\\n     KYNDRYL_ONLY_CNTRY_TEXT_7.USER_ID AS UUID_KOCT7,\\n    KYNDRYL_ONLY_CNTRY_TEXT_8.KYNDRYL_ONLY_CNTRY_TEXT_8,\\n     KYNDRYL_ONLY_CNTRY_TEXT_8.UPDATED_TIMESTAMP AS UTS_KOCT8,\\n     KYNDRYL_ONLY_CNTRY_TEXT_8.USER_ID AS UUID_KOCT8,\\n    KYNDRYL_ONLY_CNTRY_TEXT_9.KYNDRYL_ONLY_CNTRY_TEXT_9,\\n     KYNDRYL_ONLY_CNTRY_TEXT_9.UPDATED_TIMESTAMP AS UTS_KOCT9,\\n     KYNDRYL_ONLY_CNTRY_TEXT_9.USER_ID AS UUID_KOCT9,\\n    KYNDRYL_ONLY_CNTRY_TEXT_10.KYNDRYL_ONLY_CNTRY_TEXT_10,\\n     KYNDRYL_ONLY_CNTRY_TEXT_10.UPDATED_TIMESTAMP AS UTS_KOCT10,\\n     KYNDRYL_ONLY_CNTRY_TEXT_10.USER_ID AS UUID_KOCT10,\\n    KYNDRYL_ONLY_CNTRY_DATE_1.KYNDRYL_ONLY_CNTRY_DATE_1,\\n     KYNDRYL_ONLY_CNTRY_DATE_1.UPDATED_TIMESTAMP AS UTS_KOCD1,\\n     KYNDRYL_ONLY_CNTRY_DATE_1.USER_ID AS UUID_KOCD1,\\n    KYNDRYL_ONLY_CNTRY_DATE_2.KYNDRYL_ONLY_CNTRY_DATE_2,\\n     KYNDRYL_ONLY_CNTRY_DATE_2.UPDATED_TIMESTAMP AS UTS_KOCD2,\\n     KYNDRYL_ONLY_CNTRY_DATE_2.USER_ID AS UUID_KOCD2,\\n    KYNDRYL_ONLY_CNTRY_DATE_3.KYNDRYL_ONLY_CNTRY_DATE_3,\\n     KYNDRYL_ONLY_CNTRY_DATE_3.UPDATED_TIMESTAMP AS UTS_KOCD3,\\n     KYNDRYL_ONLY_CNTRY_DATE_3.USER_ID AS UUID_KOCD3,\\n    KYNDRYL_ONLY_CNTRY_DATE_4.KYNDRYL_ONLY_CNTRY_DATE_4,\\n     KYNDRYL_ONLY_CNTRY_DATE_4.UPDATED_TIMESTAMP AS UTS_KOCD4,\\n     KYNDRYL_ONLY_CNTRY_DATE_4.USER_ID AS UUID_KOCD4,\\n    KYNDRYL_ONLY_CNTRY_DATE_5.KYNDRYL_ONLY_CNTRY_DATE_5,\\n     KYNDRYL_ONLY_CNTRY_DATE_5.UPDATED_TIMESTAMP AS UTS_KOCD5,\\n     KYNDRYL_ONLY_CNTRY_DATE_5.USER_ID AS UUID_KOCD5,\\n    KYNDRYL_ONLY_CNTRY_DATE_6.KYNDRYL_ONLY_CNTRY_DATE_6,\\n     KYNDRYL_ONLY_CNTRY_DATE_6.UPDATED_TIMESTAMP AS UTS_KOCD6,\\n     KYNDRYL_ONLY_CNTRY_DATE_6.USER_ID AS UUID_KOCD6,\\n    KYNDRYL_ONLY_CNTRY_DATE_7.KYNDRYL_ONLY_CNTRY_DATE_7,\\n     KYNDRYL_ONLY_CNTRY_DATE_7.UPDATED_TIMESTAMP AS UTS_KOCD7,\\n     KYNDRYL_ONLY_CNTRY_DATE_7.USER_ID AS UUID_KOCD7,\\n    KYNDRYL_ONLY_CNTRY_DATE_8.KYNDRYL_ONLY_CNTRY_DATE_8,\\n     KYNDRYL_ONLY_CNTRY_DATE_8.UPDATED_TIMESTAMP AS UTS_KOCD8,\\n     KYNDRYL_ONLY_CNTRY_DATE_8.USER_ID AS UUID_KOCD8,\\n    KYNDRYL_ONLY_CNTRY_DATE_9.KYNDRYL_ONLY_CNTRY_DATE_9,\\n     KYNDRYL_ONLY_CNTRY_DATE_9.UPDATED_TIMESTAMP AS UTS_KOCD9,\\n     KYNDRYL_ONLY_CNTRY_DATE_9.USER_ID AS UUID_KOCD9,\\n    KYNDRYL_ONLY_CNTRY_DATE_10.KYNDRYL_ONLY_CNTRY_DATE_10,\\n     KYNDRYL_ONLY_CNTRY_DATE_10.UPDATED_TIMESTAMP AS UTS_KOCD10,\\n     KYNDRYL_ONLY_CNTRY_DATE_10.USER_ID AS UUID_KOCD10\\nFrom\\n     PGMPDM.PROC_DIM PROCS\\n     INNER JOIN PGMPDM.CNTRCT_DIM C ON C.CNTRCT_DIM_UID = PROCS.CNTRCT_DIM_UID\\n     INNER JOIN (\\n          Select distinct\\n               PROC_DIM_UID\\n          From     \\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM\\n          Where     \\n               COALESCE(\\n                    IBM_ONLY_CNTRY_TEXT_1,\\n                    IBM_ONLY_CNTRY_TEXT_2,\\n                    IBM_ONLY_CNTRY_TEXT_3,\\n                    IBM_ONLY_CNTRY_TEXT_4,\\n                    IBM_ONLY_CNTRY_TEXT_5,\\n                    IBM_ONLY_CNTRY_TEXT_6,\\n                    IBM_ONLY_CNTRY_TEXT_7,\\n                    IBM_ONLY_CNTRY_TEXT_8,\\n                    IBM_ONLY_CNTRY_TEXT_9,\\n                    IBM_ONLY_CNTRY_TEXT_10,\\n                    CAST(IBM_ONLY_CNTRY_DATE_1 AS VARCHAR),\\n                    CAST(IBM_ONLY_CNTRY_DATE_2 AS VARCHAR),\\n                    CAST(IBM_ONLY_CNTRY_DATE_3 AS VARCHAR),\\n                    CAST(IBM_ONLY_CNTRY_DATE_4 AS VARCHAR),\\n                    CAST(IBM_ONLY_CNTRY_DATE_5 AS VARCHAR),\\n                    CAST(IBM_ONLY_CNTRY_DATE_6 AS VARCHAR),\\n                    CAST(IBM_ONLY_CNTRY_DATE_7 AS VARCHAR),\\n                    CAST(IBM_ONLY_CNTRY_DATE_8 AS VARCHAR),\\n                    CAST(IBM_ONLY_CNTRY_DATE_9 AS VARCHAR),\\n                    CAST(IBM_ONLY_CNTRY_DATE_10 AS VARCHAR)\\n               ) NOT IN (\\'\\', \\'NULL\\')\\n     ) KOPROC ON KOPROC.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n     LEFT OUTER JOIN (\\n          Select distinct\\n               DKO.PROC_DIM_UID,\\n               DKO.IBM_ONLY_CNTRY_TEXT_1 AS KYNDRYL_ONLY_CNTRY_TEXT_1,\\n               DKOTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DKO\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_CNTRY_TEXT_1,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_CNTRY_TEXT_1                         \\n               )DKOTime ON (DKOTime.PROC_ID = DKO.PROC_DIM_UID and DKOTime.IBM_ONLY_CNTRY_TEXT_1 = DKO.IBM_ONLY_CNTRY_TEXT_1)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKOUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H DKOUser\\n                    Where (DKOUser.PROC_ID = DKOTime.PROC_ID and DKOUser.UPDATED_TS = DKOTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKOTime.IBM_ONLY_CNTRY_TEXT_1 not in (\\'\\') \\n     ) KYNDRYL_ONLY_CNTRY_TEXT_1 ON KYNDRYL_ONLY_CNTRY_TEXT_1.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKO.PROC_DIM_UID,\\n               DKO.IBM_ONLY_CNTRY_TEXT_2 AS KYNDRYL_ONLY_CNTRY_TEXT_2,\\n               DKOTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DKO\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_CNTRY_TEXT_2,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_CNTRY_TEXT_2                    \\n               )DKOTime ON (DKOTime.PROC_ID = DKO.PROC_DIM_UID and DKOTime.IBM_ONLY_CNTRY_TEXT_2 = DKO.IBM_ONLY_CNTRY_TEXT_2)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKOUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H DKOUser\\n                    Where (DKOUser.PROC_ID = DKOTime.PROC_ID and DKOUser.UPDATED_TS = DKOTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKOTime.IBM_ONLY_CNTRY_TEXT_2 not in (\\'\\') \\n     ) KYNDRYL_ONLY_CNTRY_TEXT_2 ON KYNDRYL_ONLY_CNTRY_TEXT_2.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKO.PROC_DIM_UID,\\n               DKO.IBM_ONLY_CNTRY_TEXT_3 AS KYNDRYL_ONLY_CNTRY_TEXT_3,\\n               DKOTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DKO\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_CNTRY_TEXT_3,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_CNTRY_TEXT_3                    \\n               )DKOTime ON (DKOTime.PROC_ID = DKO.PROC_DIM_UID and DKOTime.IBM_ONLY_CNTRY_TEXT_3 = DKO.IBM_ONLY_CNTRY_TEXT_3)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKOUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H DKOUser\\n                    Where (DKOUser.PROC_ID = DKOTime.PROC_ID and DKOUser.UPDATED_TS = DKOTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKOTime.IBM_ONLY_CNTRY_TEXT_3 not in (\\'\\') \\n     ) KYNDRYL_ONLY_CNTRY_TEXT_3 ON KYNDRYL_ONLY_CNTRY_TEXT_3.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKO.PROC_DIM_UID,\\n               DKO.IBM_ONLY_CNTRY_TEXT_4 AS KYNDRYL_ONLY_CNTRY_TEXT_4,\\n               DKOTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DKO\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_CNTRY_TEXT_4,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP               \\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_CNTRY_TEXT_4                              \\n               )DKOTime ON (DKOTime.PROC_ID = DKO.PROC_DIM_UID and DKOTime.IBM_ONLY_CNTRY_TEXT_4 = DKO.IBM_ONLY_CNTRY_TEXT_4)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKOUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H DKOUser\\n                    Where (DKOUser.PROC_ID = DKOTime.PROC_ID and DKOUser.UPDATED_TS = DKOTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKOTime.IBM_ONLY_CNTRY_TEXT_4 not in (\\'\\') \\n     ) KYNDRYL_ONLY_CNTRY_TEXT_4 ON KYNDRYL_ONLY_CNTRY_TEXT_4.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKO.PROC_DIM_UID,\\n               DKO.IBM_ONLY_CNTRY_TEXT_5 AS KYNDRYL_ONLY_CNTRY_TEXT_5,\\n               DKOTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DKO\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_CNTRY_TEXT_5,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_CNTRY_TEXT_5               \\n               )DKOTime ON (DKOTime.PROC_ID = DKO.PROC_DIM_UID and DKOTime.IBM_ONLY_CNTRY_TEXT_5 = DKO.IBM_ONLY_CNTRY_TEXT_5)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKOUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H DKOUser\\n                    Where (DKOUser.PROC_ID = DKOTime.PROC_ID and DKOUser.UPDATED_TS = DKOTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKOTime.IBM_ONLY_CNTRY_TEXT_5 not in (\\'\\') \\n     ) KYNDRYL_ONLY_CNTRY_TEXT_5 ON KYNDRYL_ONLY_CNTRY_TEXT_5.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKO.PROC_DIM_UID,\\n               DKO.IBM_ONLY_CNTRY_TEXT_6 AS KYNDRYL_ONLY_CNTRY_TEXT_6,\\n               DKOTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DKO\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_CNTRY_TEXT_6,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_CNTRY_TEXT_6                    \\n               )DKOTime ON (DKOTime.PROC_ID = DKO.PROC_DIM_UID and DKOTime.IBM_ONLY_CNTRY_TEXT_6 = DKO.IBM_ONLY_CNTRY_TEXT_6)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKOUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H DKOUser\\n                    Where (DKOUser.PROC_ID = DKOTime.PROC_ID and DKOUser.UPDATED_TS = DKOTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKOTime.IBM_ONLY_CNTRY_TEXT_6 not in (\\'\\') \\n     ) KYNDRYL_ONLY_CNTRY_TEXT_6 ON KYNDRYL_ONLY_CNTRY_TEXT_6.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKO.PROC_DIM_UID,\\n               DKO.IBM_ONLY_CNTRY_TEXT_7 AS KYNDRYL_ONLY_CNTRY_TEXT_7,\\n               DKOTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DKO\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_CNTRY_TEXT_7,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_CNTRY_TEXT_7                    \\n               )DKOTime ON (DKOTime.PROC_ID = DKO.PROC_DIM_UID and DKOTime.IBM_ONLY_CNTRY_TEXT_7 = DKO.IBM_ONLY_CNTRY_TEXT_7)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKOUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H DKOUser\\n                    Where (DKOUser.PROC_ID = DKOTime.PROC_ID and DKOUser.UPDATED_TS = DKOTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKOTime.IBM_ONLY_CNTRY_TEXT_7 not in (\\'\\') \\n     ) KYNDRYL_ONLY_CNTRY_TEXT_7 ON KYNDRYL_ONLY_CNTRY_TEXT_7.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKO.PROC_DIM_UID,\\n               DKO.IBM_ONLY_CNTRY_TEXT_8 AS KYNDRYL_ONLY_CNTRY_TEXT_8,\\n               DKOTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DKO\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_CNTRY_TEXT_8,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_CNTRY_TEXT_8                    \\n               )DKOTime ON (DKOTime.PROC_ID = DKO.PROC_DIM_UID and DKOTime.IBM_ONLY_CNTRY_TEXT_8 = DKO.IBM_ONLY_CNTRY_TEXT_8)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKOUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H DKOUser\\n                    Where (DKOUser.PROC_ID = DKOTime.PROC_ID and DKOUser.UPDATED_TS = DKOTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKOTime.IBM_ONLY_CNTRY_TEXT_8 not in (\\'\\') \\n     ) KYNDRYL_ONLY_CNTRY_TEXT_8 ON KYNDRYL_ONLY_CNTRY_TEXT_8.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKO.PROC_DIM_UID,\\n               DKO.IBM_ONLY_CNTRY_TEXT_9 AS KYNDRYL_ONLY_CNTRY_TEXT_9,\\n               DKOTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DKO\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_CNTRY_TEXT_9,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_CNTRY_TEXT_9                    \\n               )DKOTime ON (DKOTime.PROC_ID = DKO.PROC_DIM_UID and DKOTime.IBM_ONLY_CNTRY_TEXT_9 = DKO.IBM_ONLY_CNTRY_TEXT_9)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKOUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H DKOUser\\n                    Where (DKOUser.PROC_ID = DKOTime.PROC_ID and DKOUser.UPDATED_TS = DKOTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKOTime.IBM_ONLY_CNTRY_TEXT_9 not in (\\'\\') \\n     ) KYNDRYL_ONLY_CNTRY_TEXT_9 ON KYNDRYL_ONLY_CNTRY_TEXT_9.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKO.PROC_DIM_UID,\\n               DKO.IBM_ONLY_CNTRY_TEXT_10 AS KYNDRYL_ONLY_CNTRY_TEXT_10,\\n               DKOTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DKO\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_CNTRY_TEXT_10,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_CNTRY_TEXT_10                    \\n               )DKOTime ON (DKOTime.PROC_ID = DKO.PROC_DIM_UID and DKOTime.IBM_ONLY_CNTRY_TEXT_10 = DKO.IBM_ONLY_CNTRY_TEXT_10)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKOUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H DKOUser\\n                    Where (DKOUser.PROC_ID = DKOTime.PROC_ID and DKOUser.UPDATED_TS = DKOTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKOTime.IBM_ONLY_CNTRY_TEXT_10 not in (\\'\\') \\n     ) KYNDRYL_ONLY_CNTRY_TEXT_10 ON KYNDRYL_ONLY_CNTRY_TEXT_10.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKO.PROC_DIM_UID,\\n               DKO.IBM_ONLY_CNTRY_DATE_1 AS KYNDRYL_ONLY_CNTRY_DATE_1,\\n               DKOTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DKO\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_CNTRY_DATE_1,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_CNTRY_DATE_1                    \\n               )DKOTime ON (DKOTime.PROC_ID = DKO.PROC_DIM_UID and DKOTime.IBM_ONLY_CNTRY_DATE_1 = DKO.IBM_ONLY_CNTRY_DATE_1)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKOUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H DKOUser\\n                    Where (DKOUser.PROC_ID = DKOTime.PROC_ID and DKOUser.UPDATED_TS = DKOTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKOTime.IBM_ONLY_CNTRY_DATE_1 not like (\\'\\') \\n     ) KYNDRYL_ONLY_CNTRY_DATE_1 ON KYNDRYL_ONLY_CNTRY_DATE_1.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKO.PROC_DIM_UID,\\n               DKO.IBM_ONLY_CNTRY_DATE_2 AS KYNDRYL_ONLY_CNTRY_DATE_2,\\n               DKOTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DKO\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_CNTRY_DATE_2,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_CNTRY_DATE_2                    \\n               )DKOTime ON (DKOTime.PROC_ID = DKO.PROC_DIM_UID and DKOTime.IBM_ONLY_CNTRY_DATE_2 = DKO.IBM_ONLY_CNTRY_DATE_2)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKOUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H DKOUser\\n                    Where (DKOUser.PROC_ID = DKOTime.PROC_ID and DKOUser.UPDATED_TS = DKOTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKOTime.IBM_ONLY_CNTRY_DATE_2 not like (\\'\\') \\n     ) KYNDRYL_ONLY_CNTRY_DATE_2 ON KYNDRYL_ONLY_CNTRY_DATE_2.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKO.PROC_DIM_UID,\\n               DKO.IBM_ONLY_CNTRY_DATE_3 AS KYNDRYL_ONLY_CNTRY_DATE_3,\\n               DKOTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DKO\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_CNTRY_DATE_3,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_CNTRY_DATE_3                    \\n               )DKOTime ON (DKOTime.PROC_ID = DKO.PROC_DIM_UID and DKOTime.IBM_ONLY_CNTRY_DATE_3 = DKO.IBM_ONLY_CNTRY_DATE_3)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKOUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H DKOUser\\n                    Where (DKOUser.PROC_ID = DKOTime.PROC_ID and DKOUser.UPDATED_TS = DKOTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKOTime.IBM_ONLY_CNTRY_DATE_3 not like (\\'\\') \\n     ) KYNDRYL_ONLY_CNTRY_DATE_3 ON KYNDRYL_ONLY_CNTRY_DATE_3.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKO.PROC_DIM_UID,\\n               DKO.IBM_ONLY_CNTRY_DATE_4 AS KYNDRYL_ONLY_CNTRY_DATE_4,\\n               DKOTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DKO\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_CNTRY_DATE_4,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP     \\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_CNTRY_DATE_4                    \\n               )DKOTime ON (DKOTime.PROC_ID = DKO.PROC_DIM_UID and DKOTime.IBM_ONLY_CNTRY_DATE_4 = DKO.IBM_ONLY_CNTRY_DATE_4)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKOUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H DKOUser\\n                    Where (DKOUser.PROC_ID = DKOTime.PROC_ID and DKOUser.UPDATED_TS = DKOTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKOTime.IBM_ONLY_CNTRY_DATE_4 not like (\\'\\') \\n     ) KYNDRYL_ONLY_CNTRY_DATE_4 ON KYNDRYL_ONLY_CNTRY_DATE_4.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKO.PROC_DIM_UID,\\n               DKO.IBM_ONLY_CNTRY_DATE_5 AS KYNDRYL_ONLY_CNTRY_DATE_5,\\n               DKOTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DKO\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_CNTRY_DATE_5,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_CNTRY_DATE_5                    \\n               )DKOTime ON (DKOTime.PROC_ID = DKO.PROC_DIM_UID and DKOTime.IBM_ONLY_CNTRY_DATE_5 = DKO.IBM_ONLY_CNTRY_DATE_5)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKOUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H DKOUser\\n                    Where (DKOUser.PROC_ID = DKOTime.PROC_ID and DKOUser.UPDATED_TS = DKOTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKOTime.IBM_ONLY_CNTRY_DATE_5 not like (\\'\\') \\n     ) KYNDRYL_ONLY_CNTRY_DATE_5 ON KYNDRYL_ONLY_CNTRY_DATE_5.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKO.PROC_DIM_UID,\\n               DKO.IBM_ONLY_CNTRY_DATE_6 AS KYNDRYL_ONLY_CNTRY_DATE_6,\\n               DKOTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DKO\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_CNTRY_DATE_6,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_CNTRY_DATE_6                    \\n               )DKOTime ON (DKOTime.PROC_ID = DKO.PROC_DIM_UID and DKOTime.IBM_ONLY_CNTRY_DATE_6 = DKO.IBM_ONLY_CNTRY_DATE_6)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKOUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H DKOUser\\n                    Where (DKOUser.PROC_ID = DKOTime.PROC_ID and DKOUser.UPDATED_TS = DKOTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKOTime.IBM_ONLY_CNTRY_DATE_6 not like (\\'\\') \\n     ) KYNDRYL_ONLY_CNTRY_DATE_6 ON KYNDRYL_ONLY_CNTRY_DATE_6.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKO.PROC_DIM_UID,\\n               DKO.IBM_ONLY_CNTRY_DATE_7 AS KYNDRYL_ONLY_CNTRY_DATE_7,\\n               DKOTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DKO\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_CNTRY_DATE_7,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_CNTRY_DATE_7                    \\n               )DKOTime ON (DKOTime.PROC_ID = DKO.PROC_DIM_UID and DKOTime.IBM_ONLY_CNTRY_DATE_7 = DKO.IBM_ONLY_CNTRY_DATE_7)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKOUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H DKOUser\\n                    Where (DKOUser.PROC_ID = DKOTime.PROC_ID and DKOUser.UPDATED_TS = DKOTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKOTime.IBM_ONLY_CNTRY_DATE_7 not like (\\'\\') \\n     ) KYNDRYL_ONLY_CNTRY_DATE_7 ON KYNDRYL_ONLY_CNTRY_DATE_7.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKO.PROC_DIM_UID,\\n               DKO.IBM_ONLY_CNTRY_DATE_8 AS KYNDRYL_ONLY_CNTRY_DATE_8,\\n               DKOTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DKO\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_CNTRY_DATE_8,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_CNTRY_DATE_8                    \\n               )DKOTime ON (DKOTime.PROC_ID = DKO.PROC_DIM_UID and DKOTime.IBM_ONLY_CNTRY_DATE_8 = DKO.IBM_ONLY_CNTRY_DATE_8)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKOUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H DKOUser\\n                    Where (DKOUser.PROC_ID = DKOTime.PROC_ID and DKOUser.UPDATED_TS = DKOTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKOTime.IBM_ONLY_CNTRY_DATE_8 not like (\\'\\') \\n     ) KYNDRYL_ONLY_CNTRY_DATE_8 ON KYNDRYL_ONLY_CNTRY_DATE_8.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKO.PROC_DIM_UID,\\n               DKO.IBM_ONLY_CNTRY_DATE_9 AS KYNDRYL_ONLY_CNTRY_DATE_9,\\n               DKOTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DKO\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_CNTRY_DATE_9,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_CNTRY_DATE_9                         \\n               )DKOTime ON (DKOTime.PROC_ID = DKO.PROC_DIM_UID and DKOTime.IBM_ONLY_CNTRY_DATE_9 = DKO.IBM_ONLY_CNTRY_DATE_9)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKOUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H DKOUser\\n                    Where (DKOUser.PROC_ID = DKOTime.PROC_ID and DKOUser.UPDATED_TS = DKOTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKOTime.IBM_ONLY_CNTRY_DATE_9 not like (\\'\\') \\n     ) KYNDRYL_ONLY_CNTRY_DATE_9 ON KYNDRYL_ONLY_CNTRY_DATE_9.PROC_DIM_UID = PROCS.PROC_DIM_UID\\n    LEFT OUTER JOIN (\\n          Select distinct\\n               DKO.PROC_DIM_UID,\\n               DKO.IBM_ONLY_CNTRY_DATE_10 AS KYNDRYL_ONLY_CNTRY_DATE_10,\\n               DKOTime.UPDATED_TIMESTAMP,\\n               USER_DIM.CONCAT_NM_LOGIN AS USER_ID -- decrypt_char(USER_DIM.CONCAT_NM_LOGIN, MISC_REP_REF.MIS_REP_REF_CD) AS USER_ID\\n          From\\n               PGMPDM.CSTM_DATA_PUBLIC_CNTRY_DIM DKO\\n               INNER JOIN (\\n                    Select distinct\\n                         PROC_ID,\\n                         IBM_ONLY_CNTRY_DATE_10,\\n                         MIN(UPDATED_TS) AS UPDATED_TIMESTAMP\\n                    From\\n                         APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H\\n                    Group By\\n                         PROC_ID, \\n                         IBM_ONLY_CNTRY_DATE_10                    \\n               )DKOTime ON (DKOTime.PROC_ID = DKO.PROC_DIM_UID and DKOTime.IBM_ONLY_CNTRY_DATE_10 = DKO.IBM_ONLY_CNTRY_DATE_10)\\n               LEFT OUTER JOIN PGMPDM.USER_DIM as USER_DIM ON USER_DIM.USER_ID = (\\n                    Select distinct DKOUser.UPDATED_USERID \\n                    From APPFUN.CSTM_DATA_IBM_CLIENT_CNTRY_H DKOUser\\n                    Where (DKOUser.PROC_ID = DKOTime.PROC_ID and DKOUser.UPDATED_TS = DKOTime.UPDATED_TIMESTAMP))\\n               LEFT OUTER JOIN PGMPDM.MISC_REP_REF as MISC_REP_REF ON MISC_REP_REF.MIS_REP_REF_UID = 3\\n          Where \\n               DKOTime.IBM_ONLY_CNTRY_DATE_10 not like (\\'\\') \\n     ) KYNDRYL_ONLY_CNTRY_DATE_10 ON KYNDRYL_ONLY_CNTRY_DATE_10.PROC_DIM_UID = PROCS.PROC_DIM_UID',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"getSourceData sort(asc(PROC_DIM_UID, false)) ~> sortedSourceData",
						"sortedSourceData alterRow(upsertIf(true())) ~> upsertRows",
						"upsertRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'CNTRY_CSTM_DATA_KYNDRYL_ONLY_AUDIT_RPT',",
						"     insertable: true,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['PROC_DIM_UID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> upsertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0020_ACCTSPFC_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job creation:05-06-2022\nJob name: Df_BALD0020ACCTSPF_DIM\nCreatedBy: Varaprasad",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "appfunacctspcf"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "LkpTgtAcctspcfDim"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "TgtAcctspfDimupd"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "TgtpgmdmacctspfDim"
						}
					],
					"transformations": [
						{
							"name": "acctspfmrgd"
						},
						{
							"name": "SrtTgtacctspfdim"
						},
						{
							"name": "mergdAcctspf"
						},
						{
							"name": "Addrowstat"
						},
						{
							"name": "select1"
						},
						{
							"name": "alterRow1"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "select2"
						}
					],
					"scriptLines": [
						"source(output(",
						"          ACCTSPFC_DIM_UID as integer,",
						"          PRJCT_ID as string,",
						"          ACCTSPFC_REM_TXT as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n     ACCTS.PROC_ID as ACCTSPFC_DIM_UID,\\n     --ACCTS.PROJECT_ID as PRJCT_ID,\\n     \\'RFSETL\\' as PRJCT_ID,\\n     cast(ACCTS.REMARKS as varchar(4096)) as ACCTSPFC_REM_TXT,\\n     ACCTS.CREATED_TS as SRC_CRETD_TMS,\\n     ACCTS.CREATED_USERID as SRC_CRETD_USER_ID,\\n     ACCTS.UPDATED_TS as SRC_UPDTD_TMS,\\n     ACCTS.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED\\nfrom\\n     APPFUN.ACCTSPFC ACCTS\\n    /* inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on ACCTS.PROC_ID = ZDT.PROC_ID*/\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on ACCTS.PROC_ID = PDEL.PROC_ID\\n     left join\\n     (select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          from\\n               PGMPDM.ZAUX_ETL_JOBS\\n          where\\n               ETL_JOB_NM = \\'#DSJobName#\\')JOB\\n     on 1 = 1\\n     left join\\n     (select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          where\\n               IS_CURR_IND = \\'Y\\') FIL\\n     on 1 = 1\\n     left join\\n     (select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          from\\n               PGMPDM.SRC_SYS_DIM\\n          where\\n               SRC_SYS_CD = \\'PGMP\\')SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> appfunacctspcf",
						"source(output(",
						"          LKP_ACCTSPFC_DIM_UID as integer,",
						"          LKP_PRJCT_ID as string,",
						"          LKP_ as string,",
						"          LKP_SRC_CRETD_TMS as timestamp,",
						"          LKP_SRC_CRETD_USER_ID as string,",
						"          LKP_SRC_UPDTD_TMS as timestamp,",
						"          LKP_SRC_UPDTD_USER_ID as string,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_ETL_JOB_ID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n     S.ACCTSPFC_DIM_UID as LKP_ACCTSPFC_DIM_UID,\\n     S.PRJCT_ID AS LKP_PRJCT_ID,\\n     S.ACCTSPFC_REM_TXT AS LKP_,\\n     S.SRC_CRETD_TMS AS LKP_SRC_CRETD_TMS,\\n     S.SRC_CRETD_USER_ID AS LKP_SRC_CRETD_USER_ID,\\n     S.SRC_UPDTD_TMS AS LKP_SRC_UPDTD_TMS,\\n     S.SRC_UPDTD_USER_ID AS LKP_SRC_UPDTD_USER_ID,\\n     S.SRC_SYS_DIM_UID AS LKP_SRC_SYS_DIM_UID,\\n     S.ETL_JOB_ID AS LKP_ETL_JOB_ID\\nfrom\\n     PGMPDM.ACCTSPFC_DIM S\\n     /*inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on S.ACCTSPFC_DIM_UID = ZDT.PROC_ID*/\\n',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> LkpTgtAcctspcfDim",
						"appfunacctspcf, SrtTgtacctspfdim join(ACCTSPFC_DIM_UID == LKP_ACCTSPFC_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> acctspfmrgd",
						"LkpTgtAcctspcfDim sort(asc(LKP_ACCTSPFC_DIM_UID, true)) ~> SrtTgtacctspfdim",
						"acctspfmrgd split(ACCTSPFC_DIM_UID==LKP_ACCTSPFC_DIM_UID,",
						"     disjoint: false) ~> mergdAcctspf@(datatobeupdated, datatobeinserted)",
						"mergdAcctspf@datatobeupdated derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'U'),",
						"          DM_CRETD_USER_ID = \"dsadm\",",
						"          DM_UPDTD_USER_ID = \"dsadm\",",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> Addrowstat",
						"Addrowstat select(mapColumn(",
						"          ACCTSPFC_DIM_UID,",
						"          PRJCT_ID,",
						"          ACCTSPFC_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          IS_DELETED,",
						"          LKP_ACCTSPFC_DIM_UID,",
						"          LKP_PRJCT_ID,",
						"          LKP_,",
						"          LKP_SRC_CRETD_TMS,",
						"          LKP_SRC_CRETD_USER_ID,",
						"          LKP_SRC_UPDTD_TMS,",
						"          LKP_SRC_UPDTD_USER_ID,",
						"          LKP_SRC_SYS_DIM_UID,",
						"          LKP_ETL_JOB_ID,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 alterRow(updateIf(true())) ~> alterRow1",
						"mergdAcctspf@datatobeinserted derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'I'),",
						"          DM_CRETD_USER_ID = \"dsadm\",",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> derivedColumn1",
						"derivedColumn1 select(mapColumn(",
						"          ACCTSPFC_DIM_UID,",
						"          PRJCT_ID,",
						"          ACCTSPFC_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          IS_DELETED,",
						"          LKP_ACCTSPFC_DIM_UID,",
						"          LKP_PRJCT_ID,",
						"          LKP_,",
						"          LKP_SRC_CRETD_TMS,",
						"          LKP_SRC_CRETD_USER_ID,",
						"          LKP_SRC_UPDTD_TMS,",
						"          LKP_SRC_UPDTD_USER_ID,",
						"          LKP_SRC_SYS_DIM_UID,",
						"          LKP_ETL_JOB_ID,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select2",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'ACCTSPFC_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['ACCTSPFC_DIM_UID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> TgtAcctspfDimupd",
						"select2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'ACCTSPFC_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> TgtpgmdmacctspfDim"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0030_ACTN_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job creation:05-06-2022\nJob name: Df_BALD0020ACCTSPF_DIM\nCreatedBy: Varaprasad",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "appfunacct"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "LkpTgtAcctDim"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "TgtAcctspfDimupd"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "TgtpgmdmacctspfDim"
						}
					],
					"transformations": [
						{
							"name": "actnmrgd"
						},
						{
							"name": "SrtTgtacctdim"
						},
						{
							"name": "mergdActn"
						},
						{
							"name": "Addrowstat"
						},
						{
							"name": "select1"
						},
						{
							"name": "alterRow1"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "select2"
						}
					],
					"scriptLines": [
						"source(output(",
						"          ACTN_DIM_UID as integer,",
						"          ACTN_REM_TXT as string,",
						"          ACTN_OUTCOME_TXT as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n     A.PROC_ID as ACTN_DIM_UID,\\n     cast(A.REMARKS as varchar(1000)) as ACTN_REM_TXT,\\n     cast(A.OUTCOME as varchar(1000))  as ACTN_OUTCOME_TXT,\\n     A.CREATED_TS as SRC_CRETD_TMS,\\n     --A.CREATED_USERID as SRC_CRETD_USER_ID,\\n     COALESCE(A.CREATED_USERID,\\'pgmpetl\\')as SRC_CRETD_USER_ID,\\n     A.UPDATED_TS as SRC_UPDTD_TMS,\\n     A.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED\\nfrom\\n     APPFUN.ACTION A\\n     /*inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on A.PROC_ID = ZDT.PROC_ID*/\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on A.PROC_ID = PDEL.PROC_ID\\n     left join\\n     (select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          from\\n               PGMPDM.ZAUX_ETL_JOBS\\n          where\\n               ETL_JOB_NM = \\'#DSJobName#\\')JOB\\n     on 1 = 1\\n     left join\\n     (select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          where\\n               IS_CURR_IND = \\'Y\\')FIL\\n     on 1 = 1\\n     left join\\n     (select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          from\\n               PGMPDM.SRC_SYS_DIM\\n          where\\n               SRC_SYS_CD = \\'PGMP\\')SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> appfunacct",
						"source(output(",
						"          LKP_ACTN_DIM_UID as integer,",
						"          LKP_ACTN_REM_TXT as string,",
						"          LKP_ACTN_OUTCOME_TXT as string,",
						"          LKP_SRC_CRETD_TMS as timestamp,",
						"          LKP_SRC_CRETD_USER_ID as string,",
						"          LKP_SRC_UPDTD_TMS as timestamp,",
						"          LKP_SRC_UPDTD_USER_ID as string,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_ETL_JOB_ID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n     S.ACTN_DIM_UID AS LKP_ACTN_DIM_UID\\n     ,S.ACTN_REM_TXT AS LKP_ACTN_REM_TXT\\n     ,S.ACTN_OUTCOME_TXT AS LKP_ACTN_OUTCOME_TXT\\n     ,S.SRC_CRETD_TMS AS LKP_SRC_CRETD_TMS\\n     ,S.SRC_CRETD_USER_ID AS LKP_SRC_CRETD_USER_ID\\n     ,S.SRC_UPDTD_TMS AS LKP_SRC_UPDTD_TMS\\n     ,S.SRC_UPDTD_USER_ID AS LKP_SRC_UPDTD_USER_ID\\n     ,S.SRC_SYS_DIM_UID AS LKP_SRC_SYS_DIM_UID\\n     ,S.ETL_JOB_ID AS LKP_ETL_JOB_ID\\nfrom\\n     PGMPDM.ACTN_DIM S\\n   /*  inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on S.ACTN_DIM_UID = ZDT.PROC_ID     */',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> LkpTgtAcctDim",
						"appfunacct, LkpTgtAcctDim join(ACTN_DIM_UID == LKP_ACTN_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> actnmrgd",
						"LkpTgtAcctDim sort(asc(LKP_ACTN_DIM_UID, true)) ~> SrtTgtacctdim",
						"actnmrgd split(ACTN_DIM_UID==LKP_ACTN_DIM_UID,",
						"     disjoint: false) ~> mergdActn@(datatobeupdated, datatobeinserted)",
						"mergdActn@datatobeupdated derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'U'),",
						"          DM_CRETD_USER_ID = \"dsadm\",",
						"          DM_UPDTD_USER_ID = \"dsadm\",",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> Addrowstat",
						"Addrowstat select(mapColumn(",
						"          ACTN_DIM_UID,",
						"          ACTN_REM_TXT,",
						"          ACTN_OUTCOME_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          IS_DELETED,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 alterRow(updateIf(true())) ~> alterRow1",
						"mergdActn@datatobeinserted derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'I'),",
						"          DM_CRETD_USER_ID = \"dsadm\",",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> derivedColumn1",
						"derivedColumn1 select(mapColumn(",
						"          ACTN_DIM_UID,",
						"          ACTN_REM_TXT,",
						"          ACTN_OUTCOME_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          IS_DELETED,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select2",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'ACTN_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['ACTN_DIM_UID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          ACTN_DIM_UID,",
						"          ACTN_REM_TXT,",
						"          ACTN_OUTCOME_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          IS_DELETED,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     )) ~> TgtAcctspfDimupd",
						"select2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'ACTN_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> TgtpgmdmacctspfDim"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0040_ADTNL_INFRMTN_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 15-06-2022\nJob Name: DF_BALD0040_ADTNL_INFRMTN_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						}
					],
					"scriptLines": [
						"parameters{",
						"     current_user as string",
						"}",
						"source(output(",
						"          ADTNL_INFRMTN_DIM_UID as integer,",
						"          ADTNL_INFRMTN_TXT as string,",
						"          ADTNL_INFRMTN_PRIOR_STEP_PBLC_TXT as string,",
						"          ADTNL_INFRMTN_PRIOR_STEP_IBM_TXT as string,",
						"          ADTNL_INFRMTN_PRIOR_STEP_CLNT_TXT as string,",
						"          ADTNL_INFRMTN_IBM_ONLY_TXT as string,",
						"          ADTNL_INFRMTN_CLNT_ONLY_TXT as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     AI.PROC_ID as ADTNL_INFRMTN_DIM_UID,\\n     cast(AI.ADTNL_INF as varchar(4096)) as ADTNL_INFRMTN_TXT,\\n     cast(AI.ADTNL_INF_PRIOR_STEP_PUBLIC as varchar(4096)) as ADTNL_INFRMTN_PRIOR_STEP_PBLC_TXT,\\n     cast(AI.ADTNL_INF_PRIOR_STEP_IBM as varchar(4096)) as ADTNL_INFRMTN_PRIOR_STEP_IBM_TXT,\\n     cast(AI.ADTNL_INF_PRIOR_STEP_CLIENT as varchar(4096)) as ADTNL_INFRMTN_PRIOR_STEP_CLNT_TXT,\\n     cast(AI.ADTNL_INF_IBM_ONLY as varchar(4096)) as ADTNL_INFRMTN_IBM_ONLY_TXT,\\n     cast(AI.ADTNL_INF_CLIENT_ONLY as varchar(4096)) as ADTNL_INFRMTN_CLNT_ONLY_TXT,\\n     AI.CREATED_TS as SRC_CRETD_TMS,\\n     AI.CREATED_USERID as SRC_CRETD_USER_ID,\\n     AI.UPDATED_TS as SRC_UPDTD_TMS,\\n     AI.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED\\nFrom\\n     APPFUN.ADD_INFO AI\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on AI.PROC_ID = ZDT.PROC_ID\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on AI.PROC_ID = PDEL.PROC_ID\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD0040_ADTNL_INFRMTN_DIM\\'\\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'\\n     ) SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          LKP_ADTNL_INFRMTN_DIM_UID as integer,",
						"          LKP_ADTNL_INFRMTN_TXT as string,",
						"          LKP_ADTNL_INFRMTN_PRIOR_STEP_PBLC_TXT as string,",
						"          LKP_ADTNL_INFRMTN_PRIOR_STEP_IBM_TXT as string,",
						"          LKP_ADTNL_INFRMTN_PRIOR_STEP_CLNT_TXT as string,",
						"          LKP_ADTNL_INFRMTN_IBM_ONLY_TXT as string,",
						"          LKP_ADTNL_INFRMTN_CLNT_ONLY_TXT as string,",
						"          LKP_SRC_CRETD_TMS as timestamp,",
						"          LKP_SRC_CRETD_USER_ID as string,",
						"          LKP_SRC_UPDTD_TMS as timestamp,",
						"          LKP_SRC_UPDTD_USER_ID as string,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_ETL_JOB_ID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     S.ADTNL_INFRMTN_DIM_UID as LKP_ADTNL_INFRMTN_DIM_UID,\\n     S.ADTNL_INFRMTN_TXT as LKP_ADTNL_INFRMTN_TXT,\\n     S.ADTNL_INFRMTN_PRIOR_STEP_PBLC_TXT as LKP_ADTNL_INFRMTN_PRIOR_STEP_PBLC_TXT,\\n     S.ADTNL_INFRMTN_PRIOR_STEP_IBM_TXT as LKP_ADTNL_INFRMTN_PRIOR_STEP_IBM_TXT,\\n     S.ADTNL_INFRMTN_PRIOR_STEP_CLNT_TXT as LKP_ADTNL_INFRMTN_PRIOR_STEP_CLNT_TXT,\\n     S.ADTNL_INFRMTN_IBM_ONLY_TXT as LKP_ADTNL_INFRMTN_IBM_ONLY_TXT,\\n     S.ADTNL_INFRMTN_CLNT_ONLY_TXT as LKP_ADTNL_INFRMTN_CLNT_ONLY_TXT,\\n     S.SRC_CRETD_TMS as LKP_SRC_CRETD_TMS,\\n     S.SRC_CRETD_USER_ID as LKP_SRC_CRETD_USER_ID,\\n     S.SRC_UPDTD_TMS as LKP_SRC_UPDTD_TMS,\\n     S.SRC_UPDTD_USER_ID as LKP_SRC_UPDTD_USER_ID,\\n     S.SRC_SYS_DIM_UID as LKP_SRC_SYS_DIM_UID,\\n     S.ETL_JOB_ID as LKP_ETL_JOB_ID\\nFrom\\n     PGMPDM.ADTNL_INFRMTN_DIM S\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on S.ADTNL_INFRMTN_DIM_UID = ZDT.PROC_ID',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"getLookupData sort(asc(LKP_ADTNL_INFRMTN_DIM_UID, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"mergedData split(ADTNL_INFRMTN_DIM_UID==LKP_ADTNL_INFRMTN_DIM_UID,",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(ADTNL_INFRMTN_DIM_UID == LKP_ADTNL_INFRMTN_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          ADTNL_INFRMTN_DIM_UID,",
						"          ADTNL_INFRMTN_TXT,",
						"          ADTNL_INFRMTN_PRIOR_STEP_PBLC_TXT,",
						"          ADTNL_INFRMTN_PRIOR_STEP_IBM_TXT,",
						"          ADTNL_INFRMTN_PRIOR_STEP_CLNT_TXT,",
						"          ADTNL_INFRMTN_IBM_ONLY_TXT,",
						"          ADTNL_INFRMTN_CLNT_ONLY_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'U'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'I'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          ADTNL_INFRMTN_DIM_UID,",
						"          ADTNL_INFRMTN_TXT,",
						"          ADTNL_INFRMTN_PRIOR_STEP_PBLC_TXT,",
						"          ADTNL_INFRMTN_PRIOR_STEP_IBM_TXT,",
						"          ADTNL_INFRMTN_PRIOR_STEP_CLNT_TXT,",
						"          ADTNL_INFRMTN_IBM_ONLY_TXT,",
						"          ADTNL_INFRMTN_CLNT_ONLY_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'ADTNL_INFRMTN_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['ADTNL_INFRMTN_DIM_UID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'ADTNL_INFRMTN_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0050_APPRVL_STATE_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 15-06-2022\nJob Name: DF_BALD0050_APPRVL_STATE_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						},
						{
							"name": "renamedColumns"
						},
						{
							"name": "CDCval"
						}
					],
					"scriptLines": [
						"parameters{",
						"     current_user as string ('rfsetl')",
						"}",
						"source(output(",
						"          APPRVL_STATE_DIM_UID as integer,",
						"          APPRVL_STATE_CD as string,",
						"          APPRVL_STATE_DESC as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     APP.APPRVL_STATE_DIM_UID,\\n     cast(APP.APPRVL_STATE_CD as char(1)) as APPRVL_STATE_CD,\\n     APP.APPRVL_STATE_DESC,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID\\nFrom\\n     (\\n          Select\\n               -1 as APPRVL_STATE_DIM_UID,\\n               \\'UNK\\' as APPRVL_STATE_CD,\\n               \\'Unknown\\' as APPRVL_STATE_DESC\\n          From\\n               sys.dm_db_resource_stats\\n          union all\\n          Select\\n               0 as APPRVL_STATE_DIM_UID,\\n               \\'P\\' as APPRVL_STATE_CD,\\n               \\'Pending\\' as APPRVL_STATE_DESC\\n          From\\n               sys.dm_db_resource_stats\\n          union all\\n          Select\\n               1 as APPRVL_STATE_DIM_UID,\\n               \\'A\\' as APPRVL_STATE_CD,\\n               \\'Approved\\' as APPRVL_STATE_DESC\\n          From\\n               sys.dm_db_resource_stats\\n          union all\\n          Select\\n               2 as APPRVL_STATE_DIM_UID,\\n               \\'D\\' as APPRVL_STATE_CD,\\n               \\'Disapproved\\' as APPRVL_STATE_DESC\\n          From\\n               sys.dm_db_resource_stats\\n     ) APP\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD0050_APPRVL_STATE_DIM\\'\\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'\\n     ) SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          APPRVL_STATE_DIM_UID as integer,",
						"          APPRVL_STATE_CD as string,",
						"          APPRVL_STATE_DESC as string,",
						"          SRC_SYS_DIM_UID as integer,",
						"          ETL_JOB_ID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     APPRVL_STATE_DIM_UID,\\n     APPRVL_STATE_CD,\\n     APPRVL_STATE_DESC,\\n     SRC_SYS_DIM_UID,\\n     ETL_JOB_ID\\nFrom\\n     PGMPDM.APPRVL_STATE_DIM',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"renamedColumns sort(asc(LKP_APPRVL_STATE_DIM_UID, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"CDCval split(APPRVL_STATE_DIM_UID==LKP_APPRVL_STATE_DIM_UID,",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(APPRVL_STATE_DIM_UID == LKP_APPRVL_STATE_DIM_UID,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          APPRVL_STATE_DIM_UID,",
						"          APPRVL_STATE_CD,",
						"          APPRVL_STATE_DESC,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD = ROW_STATUS,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(DM_CRETD_USER_ID = $current_user,",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(DM_CRETD_USER_ID = $current_user,",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          APPRVL_STATE_DIM_UID,",
						"          APPRVL_STATE_CD,",
						"          APPRVL_STATE_DESC,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD = ROW_STATUS,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"getLookupData select(mapColumn(",
						"          each(match(true()),",
						"               'LKP_'+$$ = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> renamedColumns",
						"mergedData derive(ROW_STATUS = case(APPRVL_STATE_DIM_UID == toInteger(null()) && LKP_APPRVL_STATE_DIM_UID != toInteger(null()), 'D', case(APPRVL_STATE_DIM_UID != toInteger(null()) && LKP_APPRVL_STATE_DIM_UID == toInteger(null()), 'I', 'U'))) ~> CDCval",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'APPRVL_STATE_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['APPRVL_STATE_DIM_UID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'APPRVL_STATE_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0060_BASELN_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 06-06-2022\nJob Name: DF_BALD0060_BASELN_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						}
					],
					"scriptLines": [
						"parameters{",
						"     current_user as string",
						"}",
						"source(output(",
						"          BASELN_DIM_UID as integer,",
						"          PRJCT_ID as string,",
						"          BASELN_REM_TXT as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     B.PROC_ID as BASELN_DIM_UID,\\n     B.PROJECT_ID as PRJCT_ID,\\n     cast(B.REMARKS as varchar(2048)) as BASELN_REM_TXT,\\n     B.CREATED_TS as SRC_CRETD_TMS,\\n     B.CREATED_USERID as SRC_CRETD_USER_ID,\\n     B.UPDATED_TS as SRC_UPDTD_TMS,\\n     B.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED\\nFrom\\n     APPFUN.BASELINE B\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on B.PROC_ID = ZDT.PROC_ID\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on B.PROC_ID = PDEL.PROC_ID\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD0060_BASELN_DIM\\'\\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'\\n     ) SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          LKP_BASELN_DIM_UID as integer,",
						"          LKP_PRJCT_ID as string,",
						"          LKP_BASELN_REM_TXT as string,",
						"          LKP_SRC_CRETD_TMS as timestamp,",
						"          LKP_SRC_CRETD_USER_ID as string,",
						"          LKP_SRC_UPDTD_TMS as timestamp,",
						"          LKP_SRC_UPDTD_USER_ID as string,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_ETL_JOB_ID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     S.BASELN_DIM_UID as LKP_BASELN_DIM_UID,\\n     S.PRJCT_ID as LKP_PRJCT_ID,\\n     S.BASELN_REM_TXT as LKP_BASELN_REM_TXT,\\n     S.SRC_CRETD_TMS as LKP_SRC_CRETD_TMS,\\n     S.SRC_CRETD_USER_ID as LKP_SRC_CRETD_USER_ID,\\n     S.SRC_UPDTD_TMS as LKP_SRC_UPDTD_TMS,\\n     S.SRC_UPDTD_USER_ID as LKP_SRC_UPDTD_USER_ID,\\n     S.SRC_SYS_DIM_UID as LKP_SRC_SYS_DIM_UID,\\n     S.ETL_JOB_ID as LKP_ETL_JOB_ID\\nFrom\\n     PGMPDM.BASELN_DIM S\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on S.BASELN_DIM_UID = ZDT.PROC_ID     ',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"getLookupData sort(asc(LKP_BASELN_DIM_UID, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"mergedData split(BASELN_DIM_UID==LKP_BASELN_DIM_UID,",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(BASELN_DIM_UID == LKP_BASELN_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          BASELN_DIM_UID,",
						"          PRJCT_ID,",
						"          BASELN_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'U'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'I'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          BASELN_DIM_UID,",
						"          PRJCT_ID,",
						"          BASELN_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'BASELN_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['BASELN_DIM_UID'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'BASELN_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0070_BOTE_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 06-06-2022\nJob Name: DF_BALD0070_BOTE_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						}
					],
					"scriptLines": [
						"parameters{",
						"     current_user as string",
						"}",
						"source(output(",
						"          BOTE_DIM_UID as integer,",
						"          PRJCT_ID as string,",
						"          BOTE_REM_TXT as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     B.PROC_ID as BOTE_DIM_UID,\\n     B.PROJECT_ID as PRJCT_ID,\\n     cast(B.REMARKS as varchar(2048)) as BOTE_REM_TXT,\\n     B.CREATED_TS as SRC_CRETD_TMS,\\n     B.CREATED_USERID as SRC_CRETD_USER_ID,\\n     B.UPDATED_TS as SRC_UPDTD_TMS,\\n     B.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED\\nFrom\\n     APPFUN.BOTE B\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on B.PROC_ID = ZDT.PROC_ID\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on B.PROC_ID = PDEL.PROC_ID\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD0070_BOTE_DIM\\'\\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'\\n     ) SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          LKP_BOTE_DIM_UID as integer,",
						"          LKP_PRJCT_ID as string,",
						"          LKP_BOTE_REM_TXT as string,",
						"          LKP_SRC_CRETD_TMS as timestamp,",
						"          LKP_SRC_CRETD_USER_ID as string,",
						"          LKP_SRC_UPDTD_TMS as timestamp,",
						"          LKP_SRC_UPDTD_USER_ID as string,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_ETL_JOB_ID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     S.BOTE_DIM_UID as LKP_BOTE_DIM_UID,\\n     S.PRJCT_ID as LKP_PRJCT_ID,\\n     S.BOTE_REM_TXT as LKP_BOTE_REM_TXT,\\n     S.SRC_CRETD_TMS as LKP_SRC_CRETD_TMS,\\n     S.SRC_CRETD_USER_ID as LKP_SRC_CRETD_USER_ID,\\n     S.SRC_UPDTD_TMS as LKP_SRC_UPDTD_TMS,\\n     S.SRC_UPDTD_USER_ID as LKP_SRC_UPDTD_USER_ID,\\n     S.SRC_SYS_DIM_UID as LKP_SRC_SYS_DIM_UID,\\n     S.ETL_JOB_ID as LKP_ETL_JOB_ID\\nFrom\\n     PGMPDM.BOTE_DIM S\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on S.BOTE_DIM_UID = ZDT.PROC_ID',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"getLookupData sort(asc(LKP_BOTE_DIM_UID, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"mergedData split(BOTE_DIM_UID==LKP_BOTE_DIM_UID,",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(BOTE_DIM_UID == LKP_BOTE_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          BOTE_DIM_UID,",
						"          PRJCT_ID,",
						"          BOTE_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'U'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'I'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          BOTE_DIM_UID,",
						"          PRJCT_ID,",
						"          BOTE_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'BOTE_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['BOTE_DIM_UID'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'BOTE_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0080_CATLG_BRAND_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 27-06-2022\nJob Name: DF_BALD0080_CATLG_BRAND_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						},
						{
							"name": "CDCval"
						},
						{
							"name": "renamedColumns"
						}
					],
					"scriptLines": [
						"parameters{",
						"     current_user as string",
						"}",
						"source(output(",
						"          MAX_ID as integer,",
						"          ROW_NUM as long,",
						"          CATLG_BRAND_CD as string,",
						"          CATLG_BRAND_DESC as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     CB.MAX_ID as MAX_ID,\\n     CB.ROW_NUM as ROW_NUM,\\n     CB.CATLG_BRAND_CD,\\n     CB.CATLG_BRAND_DESC,\\n     CB.SRC_CRETD_TMS,\\n     CB.SRC_CRETD_USER_ID,\\n     CB.SRC_UPDTD_TMS,\\n     CB.SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID\\nFrom\\n     (\\n          Select\\n               (Select coalesce(max(CATLG_BRAND_DIM_UID), 0) from PGMPDM.CATLG_BRAND_DIM) as MAX_ID,\\n               ROW_NUMBER() over (order by CTLG_BRND_CODE) as ROW_NUM,\\n               CTLG_BRND_CODE as CATLG_BRAND_CD,\\n               CTLG_BRND_DESC as CATLG_BRAND_DESC,\\n               CREATED_TS as SRC_CRETD_TMS,\\n               CREATED_USERID as SRC_CRETD_USER_ID,\\n               UPDATED_TS as SRC_UPDTD_TMS,\\n               UPDATED_USERID as SRC_UPDTD_USER_ID\\n          From\\n               APPFUN.CATALOG_BRAND\\n          union all\\n          Select\\n               0 as MAX_ID,\\n               -1 as ROW_NUM,\\n               \\'UNK\\' as CATLG_BRAND_CD,\\n               \\'Unknown\\' as CATLG_BRAND_DESC,\\n               CURRENT_TIMESTAMP as SRC_CRETD_TMS,\\n               CURRENT_USER as SRC_CRETD_USER_ID,\\n               CURRENT_TIMESTAMP as SRC_UPDTD_TMS,\\n               CURRENT_USER as SRC_UPDTD_USER_ID\\n          From\\n               sys.dm_db_resource_stats\\n     ) CB\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD0080_CATLG_BRAND_DIM\\'\\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'\\n     ) SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          CATLG_BRAND_CD as string,",
						"          CATLG_BRAND_DESC as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          SRC_SYS_DIM_UID as integer,",
						"          ETL_JOB_ID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     CATLG_BRAND_CD,\\n     CATLG_BRAND_DESC,\\n     SRC_CRETD_TMS,\\n     SRC_CRETD_USER_ID,\\n     SRC_UPDTD_TMS,\\n     SRC_UPDTD_USER_ID,\\n     SRC_SYS_DIM_UID,\\n     ETL_JOB_ID\\nFrom\\n     PGMPDM.CATLG_BRAND_DIM',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"renamedColumns sort(asc(LKP_CATLG_BRAND_CD, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"CDCval split(ROW_STATUS!='I',",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(CATLG_BRAND_CD == LKP_CATLG_BRAND_CD,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          CATLG_BRAND_DIM_UID,",
						"          CATLG_BRAND_CD,",
						"          CATLG_BRAND_DESC,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD = ROW_STATUS,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(DM_CRETD_USER_ID = $current_user,",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(CATLG_BRAND_DIM_UID = MAX_ID+ROW_NUM,",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          CATLG_BRAND_CD,",
						"          CATLG_BRAND_DESC,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD = ROW_STATUS,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"mergedData derive(ROW_STATUS = case(CATLG_BRAND_CD == toString(null()) && LKP_CATLG_BRAND_CD != toString(null()), 'D', case(CATLG_BRAND_CD != toString(null()) && LKP_CATLG_BRAND_CD == toString(null()), 'I', 'U'))) ~> CDCval",
						"getLookupData select(mapColumn(",
						"          each(match(true()),",
						"               'LKP_'+$$ = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> renamedColumns",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'CATLG_BRAND_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['CNTRCT_TYPE_DIM_UID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'CATLG_BRAND_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0130_CLNT_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 06-07-2022\nJob Name: DF_BALD0130_CLNT_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						},
						{
							"name": "CDCval"
						},
						{
							"name": "dropUnchangedRows"
						}
					],
					"scriptLines": [
						"parameters{",
						"     current_user as string",
						"}",
						"source(output(",
						"          CLNT_DIM_UID as integer,",
						"          PST_CLNT_ACCT_ID as integer,",
						"          CLNT_NM as string,",
						"          CMR_LCT_TXT as string,",
						"          CLNT_ACTV_IND as string,",
						"          CLNT_TYPE_CD as string,",
						"          CLNT_PREFX_TXT as string,",
						"          CTRY_CD as string,",
						"          CLNT_MSTR_AGRMT_LCTN_TXT as string,",
						"          CLNT_ACCT_DTL_VRFD_IND as string,",
						"          CLNT_SRVC_PRVDR_NUM as integer,",
						"          CLNT_RGN_NUM as integer,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          HASHED_VAL as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     C.CLIENT_ID as CLNT_DIM_UID,\\n     C.CLIENT_ID as PST_CLNT_ACCT_ID,\\n     C.NAME as CLNT_NM,\\n     C.CMR_LOCATE as CMR_LCT_TXT,\\n     cast(coalesce(C.CLNT_ACTV_FLAG, PC.CLIENT_ACTV_FLAG, \\'N\\') as char(1)) as CLNT_ACTV_IND,\\n     PC.CLIENT_TYPE_CD as CLNT_TYPE_CD,\\n     PC.CLIENT_PREFIX as CLNT_PREFX_TXT,\\n     PC.COUNTRY_CD as CTRY_CD,\\n     PC.CLNT_MSTR_AGRMNT_LCTN_TXT as CLNT_MSTR_AGRMT_LCTN_TXT,\\n     PC.CLNT_ACCT_DTLS_VRFD_IND as CLNT_ACCT_DTL_VRFD_IND,\\n     PC.CLNT_SRVC_PRVDR_NUM as CLNT_SRVC_PRVDR_NUM,\\n     PC.CLNT_RGN_NUM as CLNT_RGN_NUM,\\n     C.CREATED_TS as SRC_CRETD_TMS,\\n     C.CREATED_USERID as SRC_CRETD_USER_ID,\\n     C.UPDATED_TS as SRC_UPDTD_TMS,\\n     C.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     HashBytes(\\'SHA2_256\\', coalesce(\\n          cast(C.CLIENT_ID as varchar) + \\n          C.NAME + \\n          C.CMR_LOCATE + \\n          cast(coalesce(C.CLNT_ACTV_FLAG, PC.CLIENT_ACTV_FLAG, \\'N\\') as char(1)) + \\n          PC.CLIENT_TYPE_CD + \\n          PC.CLIENT_PREFIX + \\n          PC.COUNTRY_CD + \\n          PC.CLNT_MSTR_AGRMNT_LCTN_TXT + \\n          PC.CLNT_ACCT_DTLS_VRFD_IND + \\n          cast(PC.CLNT_SRVC_PRVDR_NUM as varchar) + \\n          cast(PC.CLNT_RGN_NUM as varchar) + \\n          cast(C.CREATED_TS as varchar) + \\n          C.CREATED_USERID + \\n          cast(C.UPDATED_TS as varchar) + \\n          C.UPDATED_USERID + \\n          cast(JOB.ETL_JOB_ID as varchar) + \\n          cast(SYS.SRC_SYS_DIM_UID as varchar)\\n          , \\'\\')\\n     ) as HASHED_VAL\\n\\nFrom\\n     APPFUN.CLIENT C\\n     left join\\n     APPFUN.PST_CLIENT_ACCT PC\\n     on C.CLIENT_ID = PST_CLIENT_ACCT_ID\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD0130_CLNT_DIM\\'\\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          From\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join  (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'\\n     ) SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          LKP_CLNT_DIM_UID as integer,",
						"          LKP_PST_CLNT_ACCT_ID as integer,",
						"          LKP_CLNT_NM as string,",
						"          LKP_CMR_LCT_TXT as string,",
						"          LKP_CLNT_ACTV_IND as string,",
						"          LKP_CLNT_TYPE_CD as string,",
						"          LKP_CLNT_PREFX_TXT as string,",
						"          LKP_CTRY_CD as string,",
						"          LKP_CLNT_MSTR_AGRMT_LCTN_TXT as string,",
						"          LKP_CLNT_ACCT_DTL_VRFD_IND as string,",
						"          LKP_CLNT_SRVC_PRVDR_NUM as integer,",
						"          LKP_CLNT_RGN_NUM as integer,",
						"          LKP_SRC_CRETD_TMS as timestamp,",
						"          LKP_SRC_CRETD_USER_ID as string,",
						"          LKP_SRC_UPDTD_TMS as timestamp,",
						"          LKP_SRC_UPDTD_USER_ID as string,",
						"          LKP_ETL_JOB_ID as integer,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_HASHED_VAL as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     CLNT_DIM_UID as LKP_CLNT_DIM_UID,\\n     PST_CLNT_ACCT_ID as LKP_PST_CLNT_ACCT_ID,\\n     CLNT_NM as LKP_CLNT_NM,\\n     CMR_LCT_TXT as LKP_CMR_LCT_TXT,\\n     CLNT_ACTV_IND as LKP_CLNT_ACTV_IND,\\n     CLNT_TYPE_CD as LKP_CLNT_TYPE_CD,\\n     CLNT_PREFX_TXT as LKP_CLNT_PREFX_TXT,\\n     CTRY_CD as LKP_CTRY_CD,\\n     CLNT_MSTR_AGRMT_LCTN_TXT as LKP_CLNT_MSTR_AGRMT_LCTN_TXT,\\n     CLNT_ACCT_DTL_VRFD_IND as LKP_CLNT_ACCT_DTL_VRFD_IND,\\n     CLNT_SRVC_PRVDR_NUM as LKP_CLNT_SRVC_PRVDR_NUM,\\n     CLNT_RGN_NUM as LKP_CLNT_RGN_NUM,\\n     SRC_CRETD_TMS as LKP_SRC_CRETD_TMS,\\n     SRC_CRETD_USER_ID as LKP_SRC_CRETD_USER_ID,\\n     SRC_UPDTD_TMS as LKP_SRC_UPDTD_TMS,\\n     SRC_UPDTD_USER_ID as LKP_SRC_UPDTD_USER_ID,\\n     ETL_JOB_ID as LKP_ETL_JOB_ID,\\n     SRC_SYS_DIM_UID as LKP_SRC_SYS_DIM_UID,\\n     HashBytes(\\'SHA2_256\\', coalesce(\\n          cast(PST_CLNT_ACCT_ID as varchar) + \\n          CLNT_NM + \\n          CMR_LCT_TXT + \\n          CLNT_ACTV_IND + \\n          CLNT_TYPE_CD + \\n          CLNT_PREFX_TXT + \\n          CTRY_CD + \\n          CLNT_MSTR_AGRMT_LCTN_TXT + \\n          CLNT_ACCT_DTL_VRFD_IND + \\n          cast(CLNT_SRVC_PRVDR_NUM as varchar) + \\n          cast(CLNT_RGN_NUM as varchar) + \\n          cast(SRC_CRETD_TMS as varchar) + \\n          SRC_CRETD_USER_ID + \\n          cast(SRC_UPDTD_TMS as varchar) + \\n          SRC_UPDTD_USER_ID + \\n          cast(ETL_JOB_ID as varchar) + \\n          cast(SRC_SYS_DIM_UID as varchar)\\n          , \\'\\')\\n     ) as LKP_HASHED_VAL\\nFrom\\n     PGMPDM.CLNT_DIM',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"getLookupData sort(asc(LKP_CLNT_DIM_UID, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"dropUnchangedRows split(ROW_STATUS!='I',",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(CLNT_DIM_UID == LKP_CLNT_DIM_UID,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          CLNT_DIM_UID,",
						"          PST_CLNT_ACCT_ID,",
						"          CLNT_NM,",
						"          CMR_LCT_TXT,",
						"          CLNT_ACTV_IND,",
						"          CLNT_TYPE_CD,",
						"          CLNT_PREFX_TXT,",
						"          CTRY_CD,",
						"          CLNT_MSTR_AGRMT_LCTN_TXT,",
						"          CLNT_ACCT_DTL_VRFD_IND,",
						"          CLNT_SRVC_PRVDR_NUM,",
						"          CLNT_RGN_NUM,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD = ROW_STATUS,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(DM_CRETD_USER_ID = $current_user,",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(DM_CRETD_USER_ID = $current_user,",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          CLNT_DIM_UID,",
						"          PST_CLNT_ACCT_ID,",
						"          CLNT_NM,",
						"          CMR_LCT_TXT,",
						"          CLNT_ACTV_IND,",
						"          CLNT_TYPE_CD,",
						"          CLNT_PREFX_TXT,",
						"          CTRY_CD,",
						"          CLNT_MSTR_AGRMT_LCTN_TXT,",
						"          CLNT_ACCT_DTL_VRFD_IND,",
						"          CLNT_SRVC_PRVDR_NUM,",
						"          CLNT_RGN_NUM,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD = ROW_STATUS,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"mergedData derive(ROW_STATUS = case(CLNT_DIM_UID == toInteger(null()) && LKP_CLNT_DIM_UID != toInteger(null()), 'D', case(CLNT_DIM_UID != toInteger(null()) && LKP_CLNT_DIM_UID == toInteger(null()), 'I', case(HASHED_VAL == LKP_HASHED_VAL, 'L', 'U')))) ~> CDCval",
						"CDCval filter(ROW_STATUS != 'L') ~> dropUnchangedRows",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'CLNT_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['CLNT_DIM_UID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'CLNT_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0150_CNTRCT_CHG_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 28-06-2022\nJob Name: DF_BALD0150_CNTRCT_CHG_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						},
						{
							"name": "renameColumns"
						}
					],
					"scriptLines": [
						"parameters{",
						"     current_user as string",
						"}",
						"source(output(",
						"          CNTRCT_CHG_DIM_UID as integer,",
						"          CNTRCT_REF_TXT as string,",
						"          IBM_APPRVR_TXT as string,",
						"          CLNT_APPRVR_TXT as string,",
						"          FRML_AMNDMNT_NBR_TXT as string,",
						"          CNTRCT_DELVRBL_CNFRMN_IND as string,",
						"          CNTRCT_BSLN_CNFRMN_IND as string,",
						"          STAFFG_SKILL_CNFRMN_IND as string,",
						"          FIN_CNFRMN_IND as string,",
						"          RJCT_RSN_ID as integer,",
						"          RJCT_EXPLNN_TXT as string,",
						"          CNTRCT_REM_TXT as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer,",
						"          ORIG_ORG as string,",
						"          RJCT_RSN_TXT as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     C.PROC_ID as CNTRCT_CHG_DIM_UID,\\n     C.CNTRCT_REF as CNTRCT_REF_TXT,\\n     C.IBM_APPROVER as IBM_APPRVR_TXT, -- SYSIBM.ENCRYPT(#$RFSSCHM#.DECRYPTDATA(C.IBM_APPROVER, PWD.MIS_REP_REF_CD), PWD.MIS_REP_REF_CD) as IBM_APPRVR_TXT,\\n     C.CLIENT_APPROVER as CLNT_APPRVR_TXT, -- SYSIBM.ENCRYPT(#$RFSSCHM#.DECRYPTDATA(C.CLIENT_APPROVER, PWD.MIS_REP_REF_CD), PWD.MIS_REP_REF_CD) as CLNT_APPRVR_TXT,\\n     C.FRML_AMNDMNT_NBR as FRML_AMNDMNT_NBR_TXT,\\n     C.CNTRCT_DELVRBL_CONF as CNTRCT_DELVRBL_CNFRMN_IND,\\n     C.CNTRCT_BSLN_CONF as CNTRCT_BSLN_CNFRMN_IND,\\n     C.STAFFING_SKILL_CONF as STAFFG_SKILL_CNFRMN_IND,\\n     C.FIN_CONF as FIN_CNFRMN_IND,\\n     case \\n          when (C.REJECT_REASON is null or ltrim(C.REJECT_REASON) = \\'\\') then -1 \\n          else cast(C.REJECT_REASON as Integer) \\n     end as RJCT_RSN_ID,\\n     C.REJECT_EXPLNTN as RJCT_EXPLNN_TXT,\\n     cast(C.REMARKS as varchar(2048)) as CNTRCT_REM_TXT,\\n     C.CREATED_TS as SRC_CRETD_TMS,\\n     C.CREATED_USERID as SRC_CRETD_USER_ID,\\n     C.UPDATED_TS as SRC_UPDTD_TMS,\\n     C.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED,\\n     C.ORIG_ORG,\\n     R.REJECT_RSN_TXT as RJCT_RSN_TXT\\nFrom\\n     APPFUN.CNTRCTCHNG C\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on C.PROC_ID = ZDT.PROC_ID\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on C.PROC_ID = PDEL.PROC_ID\\n     left join  (\\n          Select\\n               SELLIST.INTERNAL_VAL AS REJECT_RSN_CD, T.TEXT AS REJECT_RSN_TXT\\n          From \\n               APPFUN.UI_SEL_LIST_DEF SELLIST, APPFUN.MULTILANG_TEXT_DEF T\\n          Where\\n               UI_SEL_LIST_ID = 95518\\n               AND SELLIST.ACTIVE_IND = \\'Y\\'\\n               AND T.MULTILANG_TEXT_ID = SELLIST.MULTILANG_SEL_TEXT_ID\\n               AND T.LANG_ID = 1 \\n     ) R\\n     on C.REJECT_REASON = R.REJECT_RSN_CD\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD0150_CNTRCT_CHG_DIM\\'\\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          From\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'\\n     ) SYS\\n     on 1 = 1\\n     left join (\\n          Select\\n               MIS_REP_REF_UID,\\n               MIS_REP_REF_CD\\n          From\\n               PGMPDM.MISC_REP_REF               \\n     ) PWD\\n     on PWD.MIS_REP_REF_UID = 3',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          CNTRCT_CHG_DIM_UID as integer,",
						"          CNTRCT_REF_TXT as string,",
						"          IBM_APPRVR_TXT as binary,",
						"          CLNT_APPRVR_TXT as binary,",
						"          FRML_AMNDMNT_NBR_TXT as string,",
						"          CNTRCT_DELVRBL_CNFRMN_IND as string,",
						"          CNTRCT_BSLN_CNFRMN_IND as string,",
						"          STAFFG_SKILL_CNFRMN_IND as string,",
						"          FIN_CNFRMN_IND as string,",
						"          RJCT_RSN_ID as integer,",
						"          RJCT_EXPLNN_TXT as string,",
						"          CNTRCT_REM_TXT as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          ORIG_ORG as string,",
						"          RJCT_RSN_TXT as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     S.CNTRCT_CHG_DIM_UID,\\n     S.CNTRCT_REF_TXT,\\n     S.IBM_APPRVR_TXT,\\n     S.CLNT_APPRVR_TXT,\\n     S.FRML_AMNDMNT_NBR_TXT,\\n     S.CNTRCT_DELVRBL_CNFRMN_IND,\\n     S.CNTRCT_BSLN_CNFRMN_IND,\\n     S.STAFFG_SKILL_CNFRMN_IND,\\n     S.FIN_CNFRMN_IND,\\n     S.RJCT_RSN_ID,\\n     S.RJCT_EXPLNN_TXT,\\n     S.CNTRCT_REM_TXT,\\n     S.SRC_UPDTD_TMS,\\n     S.SRC_UPDTD_USER_ID,\\n     S.ETL_JOB_ID,\\n     S.SRC_SYS_DIM_UID,\\n     S.ORIG_ORG,\\n     S.RJCT_RSN_TXT\\nFrom\\n     PGMPDM.CNTRCT_CHG_DIM S\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on S.CNTRCT_CHG_DIM_UID = ZDT.PROC_ID',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"renameColumns sort(asc(LKP_CNTRCT_CHG_DIM_UID, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"mergedData split(CNTRCT_CHG_DIM_UID==LKP_CNTRCT_CHG_DIM_UID,",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(CNTRCT_CHG_DIM_UID == LKP_CNTRCT_CHG_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          CNTRCT_CHG_DIM_UID,",
						"          CNTRCT_REF_TXT,",
						"          IBM_APPRVR_TXT,",
						"          CLNT_APPRVR_TXT,",
						"          FRML_AMNDMNT_NBR_TXT,",
						"          CNTRCT_DELVRBL_CNFRMN_IND,",
						"          CNTRCT_BSLN_CNFRMN_IND,",
						"          STAFFG_SKILL_CNFRMN_IND,",
						"          FIN_CNFRMN_IND,",
						"          RJCT_RSN_ID,",
						"          RJCT_EXPLNN_TXT,",
						"          CNTRCT_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ORIG_ORG,",
						"          RJCT_RSN_TXT,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'U'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'I'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          CNTRCT_CHG_DIM_UID,",
						"          CNTRCT_REF_TXT,",
						"          IBM_APPRVR_TXT,",
						"          CLNT_APPRVR_TXT,",
						"          FRML_AMNDMNT_NBR_TXT,",
						"          CNTRCT_DELVRBL_CNFRMN_IND,",
						"          CNTRCT_BSLN_CNFRMN_IND,",
						"          STAFFG_SKILL_CNFRMN_IND,",
						"          FIN_CNFRMN_IND,",
						"          RJCT_RSN_ID,",
						"          RJCT_EXPLNN_TXT,",
						"          CNTRCT_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ORIG_ORG,",
						"          RJCT_RSN_TXT,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"getLookupData select(mapColumn(",
						"          each(match(true()),",
						"               'LKP_'+$$ = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> renameColumns",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'CNTRCT_CHG_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['CNTRCT_CHG_DIM_UID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'CNTRCT_CHG_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0160_CNTRCT_DLVRBL_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 29-06-2022\nJob Name: DF_BALD0160_CNTRCT_DLVRBL_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						}
					],
					"scriptLines": [
						"parameters{",
						"     current_user as string",
						"}",
						"source(output(",
						"          CNTRCT_DLVRBL_DIM_UID as integer,",
						"          DELVRBL_OWNR_TXT as string,",
						"          DELVRBL_RCPNT_TXT as string,",
						"          AGRMT_REF_DTL_TXT as string,",
						"          DELVRBL_CTGRY_CD as string,",
						"          ACCPTD_CMPLTD_CRTR_TXT as string,",
						"          FREQ_TXT as string,",
						"          LCTN_OF_DELVRBL_TXT as string,",
						"          CNTRCT_DLVRBL_REM_TXT as string,",
						"          DELVRBL_BDGT_AMT as decimal(15,2),",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer,",
						"          RCVNG_ORG as string,",
						"          DLVR_CAT_NM as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     CNT.PROC_ID as CNTRCT_DLVRBL_DIM_UID,\\n     CNT.DELVRBL_OWNER as DELVRBL_OWNR_TXT, -- SYSIBM.ENCRYPT(APPFUN.DECRYPTDATA(CNT.DELVRBL_OWNER, PWD.MIS_REP_REF_CD), PWD.MIS_REP_REF_CD) as DELVRBL_OWNR_TXT,\\n     CNT.DELVRBL_RCPNT as DELVRBL_RCPNT_TXT, -- SYSIBM.ENCRYPT(APPFUN.DECRYPTDATA(CNT.DELVRBL_RCPNT, PWD.MIS_REP_REF_CD), PWD.MIS_REP_REF_CD) as DELVRBL_RCPNT_TXT,\\n     cast(CNT.AGRMNT_REF_DETAILS as char(200)) as AGRMT_REF_DTL_TXT,\\n     cast(CNT.DELVRBL_CATGRY as char(1)) as DELVRBL_CTGRY_CD,\\n     cast(CNT.ACCPT_COMPL_CRITERIA as varchar(4096)) as ACCPTD_CMPLTD_CRTR_TXT,\\n     CNT.FREQUENCY as FREQ_TXT,\\n     CNT.LOC_OF_DELVRBL as LCTN_OF_DELVRBL_TXT,\\n     cast(CNT.REMARKS as varchar(1000)) as CNTRCT_DLVRBL_REM_TXT,\\n     CNT.DELVRBL_BUDGET as DELVRBL_BDGT_AMT,\\n     CNT.CREATED_TS as SRC_CRETD_TMS,\\n     CNT.CREATED_USERID as SRC_CRETD_USER_ID,\\n     CNT.UPDATED_TS as SRC_UPDTD_TMS,\\n     CNT.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED,\\n    CNT.RECVNG_ORG as RCVNG_ORG,\\n    DC.DLVR_CAT_NM\\nFrom\\n     APPFUN.CNTRCTDLVRBL CNT \\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on CNT.PROC_ID = ZDT.PROC_ID\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on CNT.PROC_ID = PDEL.PROC_ID\\n    left join (\\n         Select\\n              SELLIST.INTERNAL_VAL AS DELVRBL_CATGRY,\\n            T.TEXT AS DLVR_CAT_NM\\n        From \\n            APPFUN.UI_SEL_LIST_DEF SELLIST, APPFUN.MULTILANG_TEXT_DEF T\\n         Where \\n            UI_SEL_LIST_ID = 95551\\n            AND T.MULTILANG_TEXT_ID = SELLIST.MULTILANG_SEL_TEXT_ID\\n             AND T.LANG_ID = 1\\n    ) DC\\n    on CNT.DELVRBL_CATGRY = DC.DELVRBL_CATGRY\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD0160_CNTRCT_DLVRBL_DIM\\'\\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          From\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'\\n     ) SYS\\n     on 1 = 1\\n     left join (\\n          Select\\n               MIS_REP_REF_UID,\\n               MIS_REP_REF_CD\\n          From\\n               PGMPDM.MISC_REP_REF               \\n     ) PWD\\n     on PWD.MIS_REP_REF_UID = 3',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          LKP_CNTRCT_DLVRBL_DIM_UID as integer,",
						"          LKP_DELVRBL_OWNR_TXT as binary,",
						"          LKP_DELVRBL_RCPNT_TXT as binary,",
						"          LKP_AGRMT_REF_DTL_TXT as string,",
						"          LKP_DELVRBL_CTGRY_CD as string,",
						"          LKP_ACCPTD_CMPLTD_CRTR_TXT as string,",
						"          LKP_FREQ_TXT as string,",
						"          LKP_LCTN_OF_DELVRBL_TXT as string,",
						"          LKP_CNTRCT_DLVRBL_REM_TXT as string,",
						"          LKP_DELVRBL_BDGT_AMT as decimal(15,2),",
						"          LKP_SRC_CRETD_TMS as timestamp,",
						"          LKP_SRC_CRETD_USER_ID as string,",
						"          LKP_SRC_UPDTD_TMS as timestamp,",
						"          LKP_SRC_UPDTD_USER_ID as string,",
						"          LKP_ETL_JOB_ID as integer,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_RCVNG_ORG as string,",
						"          LKP_DLVR_CAT_NM as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     S.CNTRCT_DLVRBL_DIM_UID as LKP_CNTRCT_DLVRBL_DIM_UID,\\n     S.DELVRBL_OWNR_TXT as LKP_DELVRBL_OWNR_TXT,\\n     S.DELVRBL_RCPNT_TXT as LKP_DELVRBL_RCPNT_TXT,\\n     S.AGRMT_REF_DTL_TXT as LKP_AGRMT_REF_DTL_TXT,\\n     S.DELVRBL_CTGRY_CD as LKP_DELVRBL_CTGRY_CD,\\n     S.ACCPTD_CMPLTD_CRTR_TXT as LKP_ACCPTD_CMPLTD_CRTR_TXT,\\n     S.FREQ_TXT as LKP_FREQ_TXT,\\n     S.LCTN_OF_DELVRBL_TXT as LKP_LCTN_OF_DELVRBL_TXT,\\n     S.CNTRCT_DLVRBL_REM_TXT as LKP_CNTRCT_DLVRBL_REM_TXT,\\n     S.DELVRBL_BDGT_AMT as LKP_DELVRBL_BDGT_AMT,\\n     S.SRC_CRETD_TMS as LKP_SRC_CRETD_TMS,\\n     S.SRC_CRETD_USER_ID as LKP_SRC_CRETD_USER_ID,\\n     S.SRC_UPDTD_TMS as LKP_SRC_UPDTD_TMS,\\n     S.SRC_UPDTD_USER_ID as LKP_SRC_UPDTD_USER_ID,\\n     S.ETL_JOB_ID as LKP_ETL_JOB_ID,\\n     S.SRC_SYS_DIM_UID as LKP_SRC_SYS_DIM_UID,\\n     S.RCVNG_ORG as LKP_RCVNG_ORG,\\n     S.DLVR_CAT_NM as LKP_DLVR_CAT_NM\\nFrom\\n     PGMPDM.CNTRCT_DLVRBL_DIM S\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on S.CNTRCT_DLVRBL_DIM_UID = ZDT.PROC_ID',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"getLookupData sort(asc(LKP_CNTRCT_DLVRBL_DIM_UID, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"mergedData split(CNTRCT_DLVRBL_DIM_UID==LKP_CNTRCT_DLVRBL_DIM_UID,",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(CNTRCT_DLVRBL_DIM_UID == LKP_CNTRCT_DLVRBL_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          CNTRCT_DLVRBL_DIM_UID,",
						"          DELVRBL_OWNR_TXT,",
						"          DELVRBL_RCPNT_TXT,",
						"          AGRMT_REF_DTL_TXT,",
						"          DELVRBL_CTGRY_CD,",
						"          ACCPTD_CMPLTD_CRTR_TXT,",
						"          FREQ_TXT,",
						"          LCTN_OF_DELVRBL_TXT,",
						"          CNTRCT_DLVRBL_REM_TXT,",
						"          DELVRBL_BDGT_AMT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          RCVNG_ORG,",
						"          DLVR_CAT_NM,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'U'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'I'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          CNTRCT_DLVRBL_DIM_UID,",
						"          DELVRBL_OWNR_TXT,",
						"          DELVRBL_RCPNT_TXT,",
						"          AGRMT_REF_DTL_TXT,",
						"          DELVRBL_CTGRY_CD,",
						"          ACCPTD_CMPLTD_CRTR_TXT,",
						"          FREQ_TXT,",
						"          LCTN_OF_DELVRBL_TXT,",
						"          CNTRCT_DLVRBL_REM_TXT,",
						"          DELVRBL_BDGT_AMT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          RCVNG_ORG,",
						"          DLVR_CAT_NM,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'CNTRCT_DLVRBL_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['CNTRCT_DLVRBL_DIM_UID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'CNTRCT_DLVRBL_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0170_CNTRCT_TYPE_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 30-06-2022\nJob Name: DF_BALD0170_CNTRCT_TYPE_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						},
						{
							"name": "CDCval"
						},
						{
							"name": "renamedColumns"
						}
					],
					"scriptLines": [
						"parameters{",
						"     current_user as string",
						"}",
						"source(output(",
						"          CNTRCT_TYPE_DIM_UID as integer,",
						"          CNTRCT_TYPE_DESC as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     CT.CNTRCT_TYPE_DIM_UID,\\n     CT.CNTRCT_TYPE_DESC,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID\\nFrom\\n     (\\n          Select\\n               -1 as CNTRCT_TYPE_DIM_UID,\\n               \\'Unknown\\' as CNTRCT_TYPE_DESC\\n          From\\n               sys.dm_db_resource_stats\\n          union all\\n          Select\\n               1 as CNTRCT_TYPE_DIM_UID,\\n               \\'Inactive Contract\\' as CNTRCT_TYPE_DESC\\n          From\\n               sys.dm_db_resource_stats\\n          union all\\n          Select\\n               2 as CNTRCT_TYPE_DIM_UID,\\n               \\'Production Contract\\' as CNTRCT_TYPE_DESC\\n          From\\n               sys.dm_db_resource_stats\\n          union all\\n          Select\\n               3 as CNTRCT_TYPE_DIM_UID,\\n               \\'Test Contract\\' as CNTRCT_TYPE_DESC\\n          From\\n               sys.dm_db_resource_stats     \\n     ) CT\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD0170_CNTRCT_TYPE_DIM\\'\\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'\\n     ) SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          CNTRCT_TYPE_DIM_UID as integer,",
						"          CNTRCT_TYPE_DESC as string,",
						"          ETL_JOB_ID as integer,",
						"          SRC_SYS_DIM_UID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     CNTRCT_TYPE_DIM_UID,\\n     CNTRCT_TYPE_DESC,\\n     ETL_JOB_ID,\\n     SRC_SYS_DIM_UID\\nFrom\\n     PGMPDM.CNTRCT_TYPE_DIM',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"renamedColumns sort(asc(LKP_CNTRCT_TYPE_DIM_UID, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"CDCval split(ROW_STATUS!='I',",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(CNTRCT_TYPE_DIM_UID == LKP_CNTRCT_TYPE_DIM_UID,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          CNTRCT_TYPE_DIM_UID,",
						"          CNTRCT_TYPE_DESC,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD = ROW_STATUS,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(DM_CRETD_USER_ID = $current_user,",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(DM_CRETD_USER_ID = $current_user,",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          CNTRCT_TYPE_DIM_UID,",
						"          CNTRCT_TYPE_DESC,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD = ROW_STATUS,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"mergedData derive(ROW_STATUS = case(CNTRCT_TYPE_DIM_UID == toInteger(null()) && LKP_CNTRCT_TYPE_DIM_UID != toInteger(null()), 'D', case(CNTRCT_TYPE_DIM_UID != toInteger(null()) && LKP_CNTRCT_TYPE_DIM_UID == toInteger(null()), 'I', 'U'))) ~> CDCval",
						"getLookupData select(mapColumn(",
						"          each(match(true()),",
						"               'LKP_'+$$ = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> renamedColumns",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'CNTRCT_TYPE_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['CNTRCT_TYPE_DIM_UID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'CNTRCT_TYPE_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0180_CRNCY_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 30-06-2022\nJob Name: DF_BALD0180_CRNCY_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						},
						{
							"name": "CDCval"
						},
						{
							"name": "renamedColumns"
						},
						{
							"name": "dropUnchangedRows"
						}
					],
					"scriptLines": [
						"parameters{",
						"     current_user as string",
						"}",
						"source(output(",
						"          MAX_ID as integer,",
						"          ROW_NUM as long,",
						"          CRNCY_CD as string,",
						"          CRNCY_NM as string,",
						"          LOCAL_CURRENCY_AS_BASE as decimal(18,10),",
						"          USD_AS_BASE as decimal(18,10),",
						"          TO_USD_DIVISOR_SPOT as decimal(26,13),",
						"          SPOT_DATE as date,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          HASHED_VAL as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     (Select coalesce(max(CRNCY_DIM_UID), 0) from PGMPDM.CRNCY_DIM) as MAX_ID,\\n     ROW_NUMBER() over (order by C.CRNCY_CD) as ROW_NUM,\\n     cast(C.CRNCY_CD as char(3)) as CRNCY_CD,\\n     C.CRNCY_NM,\\n     C.LOCAL_CURRENCY_AS_BASE,\\n     C.USD_AS_BASE,\\n     C.TO_USD_DIVISOR_SPOT,\\n     C.SPOT_DATE,\\n     C.SRC_CRETD_TMS,\\n     C.SRC_CRETD_USER_ID,\\n     C.SRC_UPDTD_TMS,\\n     C.SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     HashBytes(\\'SHA2_256\\', \\n          coalesce(CRNCY_CD, \\'\\') + \\n          coalesce(CRNCY_NM, \\'\\') + \\n          coalesce(cast(LOCAL_CURRENCY_AS_BASE as varchar), \\'\\') + \\n          coalesce(cast(USD_AS_BASE as varchar), \\'\\') + \\n          coalesce(cast(TO_USD_DIVISOR_SPOT as varchar), \\'\\') + \\n          cast(SRC_CRETD_TMS as varchar) + \\n          cast(SRC_CRETD_USER_ID as varchar) + \\n          cast(SRC_UPDTD_TMS as varchar) + \\n          cast(SRC_UPDTD_USER_ID as varchar) + \\n          cast(ETL_JOB_ID as varchar) + \\n          cast(SRC_SYS_DIM_UID as varchar)\\n     ) as HASHED_VAL\\nFrom\\n     (\\n          Select\\n               C.CURRENCY_CD as CRNCY_CD,\\n               C.CURRENCY_NAME as CRNCY_NM,\\n               CPR.LOCAL_CURRENCY_AS_BASE,\\n               cast(C.TO_USD_DIVISOR as decimal(18,10)) as USD_AS_BASE,\\n               C.TO_USD_DIVISOR_SPOT as TO_USD_DIVISOR_SPOT,\\n               C.SPOT_DATE as SPOT_DATE,\\n               C.CREATED_TS as SRC_CRETD_TMS,\\n               C.CREATED_USERID as SRC_CRETD_USER_ID,\\n               C.UPDATED_TS as SRC_UPDTD_TMS,\\n               C.UPDATED_USERID as SRC_UPDTD_USER_ID\\n          From\\n               APPFUN.CURRENCY as C\\n               left join\\n               APPFUN.CURRENCY_PLAN_RATE as CPR\\n               on C.CURRENCY_CD = CPR.CURRENCY_CD\\n          union all\\n          Select\\n               \\'UNK\\' as CRNCY_CD,\\n               \\'Unknown\\' as CRNCY_NM,\\n               1.0 as LOCAL_CURRENCY_AS_BASE,\\n               1.0 as USD_AS_BASE,\\n               1.0 as TO_USD_DIVISOR_SPOT,\\n               NULL as SPOT_DATE,\\n               CURRENT_TIMESTAMP as SRC_CRETD_TMS,\\n               CURRENT_USER as SRC_CRETD_USER_ID,\\n               CURRENT_TIMESTAMP as SRC_UPDTD_TMS,\\n               CURRENT_USER as SRC_UPDTD_USER_ID\\n     ) C\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD0180_CRNCY_DIM\\'\\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'\\n     ) SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          CRNCY_CD as string,",
						"          CRNCY_NM as string,",
						"          LOCAL_CURRENCY_AS_BASE as decimal(18,10),",
						"          USD_AS_BASE as decimal(18,10),",
						"          TO_USD_DIVISOR_SPOT as decimal(26,13),",
						"          SPOT_DATE as date,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          HASHED_VAL as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     CRNCY_CD,\\n     CRNCY_NM,\\n     LOCAL_CURRENCY_AS_BASE,\\n     USD_AS_BASE,\\n     TO_USD_DIVISOR_SPOT,\\n     SPOT_DATE,\\n     SRC_CRETD_TMS,\\n     SRC_CRETD_USER_ID,\\n     SRC_UPDTD_TMS,\\n     SRC_UPDTD_USER_ID,\\n     ETL_JOB_ID,\\n     SRC_SYS_DIM_UID,\\n     HashBytes(\\'SHA2_256\\', \\n          coalesce(CRNCY_CD, \\'\\') + \\n          coalesce(CRNCY_NM, \\'\\') + \\n          coalesce(cast(LOCAL_CURRENCY_AS_BASE as varchar), \\'\\') + \\n          coalesce(cast(USD_AS_BASE as varchar), \\'\\') + \\n          coalesce(cast(TO_USD_DIVISOR_SPOT as varchar), \\'\\') + \\n          cast(SRC_CRETD_TMS as varchar) + \\n          cast(SRC_CRETD_USER_ID as varchar) + \\n          cast(SRC_UPDTD_TMS as varchar) + \\n          cast(SRC_UPDTD_USER_ID as varchar) + \\n          cast(ETL_JOB_ID as varchar) + \\n          cast(SRC_SYS_DIM_UID as varchar)\\n     ) as HASHED_VAL\\nFrom\\n     PGMPDM.CRNCY_DIM\\n',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"renamedColumns sort(asc(LKP_CRNCY_CD, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"dropUnchangedRows split(ROW_STATUS!='I',",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(CRNCY_CD == LKP_CRNCY_CD,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          CRNCY_DIM_UID,",
						"          CRNCY_CD,",
						"          CRNCY_NM,",
						"          LOCAL_CURRENCY_AS_BASE,",
						"          USD_AS_BASE,",
						"          TO_USD_DIVISOR_SPOT,",
						"          SPOT_DATE,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STATUS,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(DM_CRETD_USER_ID = $current_user,",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(CRNCY_DIM_UID = MAX_ID+ROW_NUM,",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          CRNCY_CD,",
						"          CRNCY_NM,",
						"          LOCAL_CURRENCY_AS_BASE,",
						"          USD_AS_BASE,",
						"          TO_USD_DIVISOR_SPOT,",
						"          SPOT_DATE,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STATUS,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"mergedData derive(ROW_STATUS = case(CRNCY_CD == toString(null()) && LKP_CRNCY_CD != toString(null()), 'D', case(CRNCY_CD != toString(null()) && LKP_CRNCY_CD == toString(null()), 'I', case(HASHED_VAL == LKP_HASHED_VAL, 'L', 'U')))) ~> CDCval",
						"getLookupData select(mapColumn(",
						"          each(match(true()),",
						"               'LKP_'+$$ = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> renamedColumns",
						"CDCval filter(ROW_STATUS != 'L') ~> dropUnchangedRows",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'CRNCY_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['CRNCY_CD'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'CRNCY_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0190_CSTM_FIELDS_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 01-07-2022\nJob Name: DF_BALD0190_CSTM_FIELDS_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						},
						{
							"name": "renamedColumns"
						}
					],
					"scriptLines": [
						"parameters{",
						"     current_user as string",
						"}",
						"source(output(",
						"          CSTM_FIELDS_DIM_UID as integer,",
						"          PBLC_TXT_1 as string,",
						"          PBLC_TXT_2 as string,",
						"          PBLC_TXT_3 as string,",
						"          PBLC_TXT_4 as string,",
						"          PBLC_TXT_5 as string,",
						"          PBLC_TXT_6 as string,",
						"          PBLC_DT_1 as date,",
						"          PBLC_DT_2 as date,",
						"          PBLC_DT_3 as date,",
						"          PBLC_DT_4 as date,",
						"          PBLC_DT_5 as date,",
						"          PBLC_DT_6 as date,",
						"          IBM_ONLY_TXT_1 as string,",
						"          IBM_ONLY_TXT_2 as string,",
						"          IBM_ONLY_TXT_3 as string,",
						"          IBM_ONLY_TXT_4 as string,",
						"          IBM_ONLY_TXT_5 as string,",
						"          IBM_ONLY_TXT_6 as string,",
						"          IBM_ONLY_DT_1 as date,",
						"          IBM_ONLY_DT_2 as date,",
						"          IBM_ONLY_DT_3 as date,",
						"          IBM_ONLY_DT_4 as date,",
						"          IBM_ONLY_DT_5 as date,",
						"          IBM_ONLY_DT_6 as date,",
						"          CLNT_ONLY_TXT_1 as string,",
						"          CLNT_ONLY_TXT_2 as string,",
						"          CLNT_ONLY_TXT_3 as string,",
						"          CLNT_ONLY_TXT_4 as string,",
						"          CLNT_ONLY_TXT_5 as string,",
						"          CLNT_ONLY_TXT_6 as string,",
						"          CLNT_ONLY_DT_1 as date,",
						"          CLNT_ONLY_DT_2 as date,",
						"          CLNT_ONLY_DT_3 as date,",
						"          CLNT_ONLY_DT_4 as date,",
						"          CLNT_ONLY_DT_5 as date,",
						"          CLNT_ONLY_DT_6 as date,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          PBLC_TXT_7 as string,",
						"          PBLC_TXT_8 as string,",
						"          PBLC_TXT_9 as string,",
						"          PBLC_TXT_10 as string,",
						"          PBLC_TXT_11 as string,",
						"          PBLC_TXT_12 as string,",
						"          PBLC_TXT_13 as string,",
						"          PBLC_TXT_14 as string,",
						"          PBLC_TXT_15 as string,",
						"          PBLC_TXT_16 as string,",
						"          PBLC_TXT_17 as string,",
						"          PBLC_TXT_18 as string,",
						"          PBLC_TXT_19 as string,",
						"          PBLC_TXT_20 as string,",
						"          PBLC_TXT_21 as string,",
						"          PBLC_TXT_22 as string,",
						"          PBLC_TXT_23 as string,",
						"          PBLC_TXT_24 as string,",
						"          PBLC_TXT_25 as string,",
						"          PBLC_DT_7 as date,",
						"          PBLC_DT_8 as date,",
						"          PBLC_DT_9 as date,",
						"          PBLC_DT_10 as date,",
						"          PBLC_DT_11 as date,",
						"          PBLC_DT_12 as date,",
						"          PBLC_DT_13 as date,",
						"          PBLC_DT_14 as date,",
						"          PBLC_DT_15 as date,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     CD.PROC_ID as CSTM_FIELDS_DIM_UID,\\n     CD.PUBLIC_TEXT_1 as PBLC_TXT_1,\\n     CD.PUBLIC_TEXT_2 as PBLC_TXT_2,\\n     CD.PUBLIC_TEXT_3 as PBLC_TXT_3,\\n     CD.PUBLIC_TEXT_4 as PBLC_TXT_4,\\n     CD.PUBLIC_TEXT_5 as PBLC_TXT_5,\\n     CD.PUBLIC_TEXT_6 as PBLC_TXT_6,\\n     CD.PUBLIC_DATE_1 as PBLC_DT_1,\\n     CD.PUBLIC_DATE_2 as PBLC_DT_2,\\n     CD.PUBLIC_DATE_3 as PBLC_DT_3,\\n     CD.PUBLIC_DATE_4 as PBLC_DT_4,\\n     CD.PUBLIC_DATE_5 as PBLC_DT_5,\\n     CD.PUBLIC_DATE_6 as PBLC_DT_6,\\n     CD.IBM_ONLY_TEXT_1 as IBM_ONLY_TXT_1,\\n     CD.IBM_ONLY_TEXT_2 as IBM_ONLY_TXT_2,\\n     CD.IBM_ONLY_TEXT_3 as IBM_ONLY_TXT_3,\\n     CD.IBM_ONLY_TEXT_4 as IBM_ONLY_TXT_4,\\n     CD.IBM_ONLY_TEXT_5 as IBM_ONLY_TXT_5,\\n     CD.IBM_ONLY_TEXT_6 as IBM_ONLY_TXT_6,\\n     CD.IBM_ONLY_DATE_1 as IBM_ONLY_DT_1,\\n     CD.IBM_ONLY_DATE_2 as IBM_ONLY_DT_2,\\n     CD.IBM_ONLY_DATE_3 as IBM_ONLY_DT_3,\\n     CD.IBM_ONLY_DATE_4 as IBM_ONLY_DT_4,\\n     CD.IBM_ONLY_DATE_5 as IBM_ONLY_DT_5,\\n     CD.IBM_ONLY_DATE_6 as IBM_ONLY_DT_6,\\n     CD.CLIENT_ONLY_TEXT_1 as CLNT_ONLY_TXT_1,\\n     CD.CLIENT_ONLY_TEXT_2 as CLNT_ONLY_TXT_2,\\n     CD.CLIENT_ONLY_TEXT_3 as CLNT_ONLY_TXT_3,\\n     CD.CLIENT_ONLY_TEXT_4 as CLNT_ONLY_TXT_4,\\n     CD.CLIENT_ONLY_TEXT_5 as CLNT_ONLY_TXT_5,\\n     CD.CLIENT_ONLY_TEXT_6 as CLNT_ONLY_TXT_6,\\n     CD.CLIENT_ONLY_DATE_1 as CLNT_ONLY_DT_1,\\n     CD.CLIENT_ONLY_DATE_2 as CLNT_ONLY_DT_2,\\n     CD.CLIENT_ONLY_DATE_3 as CLNT_ONLY_DT_3,\\n     CD.CLIENT_ONLY_DATE_4 as CLNT_ONLY_DT_4,\\n     CD.CLIENT_ONLY_DATE_5 as CLNT_ONLY_DT_5,\\n     CD.CLIENT_ONLY_DATE_6 as CLNT_ONLY_DT_6,\\n     CD.CREATED_TS as SRC_CRETD_TMS,\\n     CD.CREATED_USERID as SRC_CRETD_USER_ID,\\n     CD.UPDATED_TS as SRC_UPDTD_TMS,\\n     CD.UPDATED_USERID as SRC_UPDTD_USER_ID,     \\n     CD.PUBLIC_TEXT_7 as PBLC_TXT_7,\\n     CD.PUBLIC_TEXT_8 as PBLC_TXT_8,\\n     CD.PUBLIC_TEXT_9 as PBLC_TXT_9,\\n     CD.PUBLIC_TEXT_10 as PBLC_TXT_10,\\n     CD.PUBLIC_TEXT_11 as PBLC_TXT_11,\\n     CD.PUBLIC_TEXT_12 as PBLC_TXT_12,\\n     CD.PUBLIC_TEXT_13 as PBLC_TXT_13,\\n     CD.PUBLIC_TEXT_14 as PBLC_TXT_14,\\n     CD.PUBLIC_TEXT_15 as PBLC_TXT_15,\\n     CD.PUBLIC_TEXT_16 as PBLC_TXT_16,\\n     CD.PUBLIC_TEXT_17 as PBLC_TXT_17,\\n     CD.PUBLIC_TEXT_18 as PBLC_TXT_18,\\n     CD.PUBLIC_TEXT_19 as PBLC_TXT_19,\\n     CD.PUBLIC_TEXT_20 as PBLC_TXT_20,\\n     CD.PUBLIC_TEXT_21 as PBLC_TXT_21,\\n     CD.PUBLIC_TEXT_22 as PBLC_TXT_22,\\n     CD.PUBLIC_TEXT_23 as PBLC_TXT_23,\\n     CD.PUBLIC_TEXT_24 as PBLC_TXT_24,\\n     CD.PUBLIC_TEXT_25 as PBLC_TXT_25,\\n     CD.PUBLIC_DATE_7 as PBLC_DT_7,\\n     CD.PUBLIC_DATE_8 as PBLC_DT_8,\\n     CD.PUBLIC_DATE_9 as PBLC_DT_9,\\n     CD.PUBLIC_DATE_10 as PBLC_DT_10,\\n     CD.PUBLIC_DATE_11 as PBLC_DT_11,\\n     CD.PUBLIC_DATE_12 as PBLC_DT_12,\\n     CD.PUBLIC_DATE_13 as PBLC_DT_13,\\n     CD.PUBLIC_DATE_14 as PBLC_DT_14,\\n     CD.PUBLIC_DATE_15 as PBLC_DT_15,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED\\nFrom\\n     APPFUN.CSTM_DATA CD\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on CD.PROC_ID = ZDT.PROC_ID\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on CD.PROC_ID = PDEL.PROC_ID\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD0190_CSTM_FIELDS_DIM\\'\\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'\\n     ) SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          CSTM_FIELDS_DIM_UID as integer,",
						"          PBLC_TXT_1 as string,",
						"          PBLC_TXT_2 as string,",
						"          PBLC_TXT_3 as string,",
						"          PBLC_TXT_4 as string,",
						"          PBLC_TXT_5 as string,",
						"          PBLC_TXT_6 as string,",
						"          PBLC_DT_1 as date,",
						"          PBLC_DT_2 as date,",
						"          PBLC_DT_3 as date,",
						"          PBLC_DT_4 as date,",
						"          PBLC_DT_5 as date,",
						"          PBLC_DT_6 as date,",
						"          IBM_ONLY_TXT_1 as string,",
						"          IBM_ONLY_TXT_2 as string,",
						"          IBM_ONLY_TXT_3 as string,",
						"          IBM_ONLY_TXT_4 as string,",
						"          IBM_ONLY_TXT_5 as string,",
						"          IBM_ONLY_TXT_6 as string,",
						"          IBM_ONLY_DT_1 as date,",
						"          IBM_ONLY_DT_2 as date,",
						"          IBM_ONLY_DT_3 as date,",
						"          IBM_ONLY_DT_4 as date,",
						"          IBM_ONLY_DT_5 as date,",
						"          IBM_ONLY_DT_6 as date,",
						"          CLNT_ONLY_TXT_1 as string,",
						"          CLNT_ONLY_TXT_2 as string,",
						"          CLNT_ONLY_TXT_3 as string,",
						"          CLNT_ONLY_TXT_4 as string,",
						"          CLNT_ONLY_TXT_5 as string,",
						"          CLNT_ONLY_TXT_6 as string,",
						"          CLNT_ONLY_DT_1 as date,",
						"          CLNT_ONLY_DT_2 as date,",
						"          CLNT_ONLY_DT_3 as date,",
						"          CLNT_ONLY_DT_4 as date,",
						"          CLNT_ONLY_DT_5 as date,",
						"          CLNT_ONLY_DT_6 as date,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          PBLC_TXT_7 as string,",
						"          PBLC_TXT_8 as string,",
						"          PBLC_TXT_9 as string,",
						"          PBLC_TXT_10 as string,",
						"          PBLC_TXT_11 as string,",
						"          PBLC_TXT_12 as string,",
						"          PBLC_TXT_13 as string,",
						"          PBLC_TXT_14 as string,",
						"          PBLC_TXT_15 as string,",
						"          PBLC_TXT_16 as string,",
						"          PBLC_TXT_17 as string,",
						"          PBLC_TXT_18 as string,",
						"          PBLC_TXT_19 as string,",
						"          PBLC_TXT_20 as string,",
						"          PBLC_TXT_21 as string,",
						"          PBLC_TXT_22 as string,",
						"          PBLC_TXT_23 as string,",
						"          PBLC_TXT_24 as string,",
						"          PBLC_TXT_25 as string,",
						"          PBLC_DT_7 as date,",
						"          PBLC_DT_8 as date,",
						"          PBLC_DT_9 as date,",
						"          PBLC_DT_10 as date,",
						"          PBLC_DT_11 as date,",
						"          PBLC_DT_12 as date,",
						"          PBLC_DT_13 as date,",
						"          PBLC_DT_14 as date,",
						"          PBLC_DT_15 as date,",
						"          SRC_SYS_DIM_UID as integer,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     S.CSTM_FIELDS_DIM_UID,\\n     S.PBLC_TXT_1,\\n     S.PBLC_TXT_2,\\n     S.PBLC_TXT_3,\\n     S.PBLC_TXT_4,\\n     S.PBLC_TXT_5,\\n     S.PBLC_TXT_6,\\n     S.PBLC_DT_1,\\n     S.PBLC_DT_2,\\n     S.PBLC_DT_3,\\n     S.PBLC_DT_4,\\n     S.PBLC_DT_5,\\n     S.PBLC_DT_6,\\n     S.IBM_ONLY_TXT_1,\\n     S.IBM_ONLY_TXT_2,\\n     S.IBM_ONLY_TXT_3,\\n     S.IBM_ONLY_TXT_4,\\n     S.IBM_ONLY_TXT_5,\\n     S.IBM_ONLY_TXT_6,\\n     S.IBM_ONLY_DT_1,\\n     S.IBM_ONLY_DT_2,\\n     S.IBM_ONLY_DT_3,\\n     S.IBM_ONLY_DT_4,\\n     S.IBM_ONLY_DT_5,\\n     S.IBM_ONLY_DT_6,\\n     S.CLNT_ONLY_TXT_1,\\n     S.CLNT_ONLY_TXT_2,\\n     S.CLNT_ONLY_TXT_3,\\n     S.CLNT_ONLY_TXT_4,\\n     S.CLNT_ONLY_TXT_5,\\n     S.CLNT_ONLY_TXT_6,\\n     S.CLNT_ONLY_DT_1,\\n     S.CLNT_ONLY_DT_2,\\n     S.CLNT_ONLY_DT_3,\\n     S.CLNT_ONLY_DT_4,\\n     S.CLNT_ONLY_DT_5,\\n     S.CLNT_ONLY_DT_6,\\n     S.SRC_CRETD_TMS,\\n     S.SRC_CRETD_USER_ID,\\n     S.SRC_UPDTD_TMS,\\n     S.SRC_UPDTD_USER_ID,\\n     S.PBLC_TXT_7,\\n     S.PBLC_TXT_8,\\n     S.PBLC_TXT_9,\\n     S.PBLC_TXT_10,\\n     S.PBLC_TXT_11,\\n     S.PBLC_TXT_12,\\n     S.PBLC_TXT_13,\\n     S.PBLC_TXT_14,\\n     S.PBLC_TXT_15,\\n     S.PBLC_TXT_16,\\n     S.PBLC_TXT_17,\\n     S.PBLC_TXT_18,\\n     S.PBLC_TXT_19,\\n     S.PBLC_TXT_20,\\n     S.PBLC_TXT_21,\\n     S.PBLC_TXT_22,\\n     S.PBLC_TXT_23,\\n     S.PBLC_TXT_24,\\n     S.PBLC_TXT_25,\\n     S.PBLC_DT_7,\\n     S.PBLC_DT_8,\\n     S.PBLC_DT_9,\\n     S.PBLC_DT_10,\\n     S.PBLC_DT_11,\\n     S.PBLC_DT_12,\\n     S.PBLC_DT_13,\\n     S.PBLC_DT_14,\\n     S.PBLC_DT_15,\\n     S.SRC_SYS_DIM_UID,\\n     S.ETL_JOB_ID,\\n     S.ETL_EXCTN_ID\\nFrom\\n     PGMPDM.CSTM_FIELDS_DIM S\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on S.CSTM_FIELDS_DIM_UID = ZDT.PROC_ID     ',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"renamedColumns sort(asc(LKP_CSTM_FIELDS_DIM_UID, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"mergedData split(CSTM_FIELDS_DIM_UID==LKP_CSTM_FIELDS_DIM_UID,",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(CSTM_FIELDS_DIM_UID == LKP_CSTM_FIELDS_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          CSTM_FIELDS_DIM_UID,",
						"          PBLC_TXT_1,",
						"          PBLC_TXT_2,",
						"          PBLC_TXT_3,",
						"          PBLC_TXT_4,",
						"          PBLC_TXT_5,",
						"          PBLC_TXT_6,",
						"          PBLC_DT_1,",
						"          PBLC_DT_2,",
						"          PBLC_DT_3,",
						"          PBLC_DT_4,",
						"          PBLC_DT_5,",
						"          PBLC_DT_6,",
						"          IBM_ONLY_TXT_1,",
						"          IBM_ONLY_TXT_2,",
						"          IBM_ONLY_TXT_3,",
						"          IBM_ONLY_TXT_4,",
						"          IBM_ONLY_TXT_5,",
						"          IBM_ONLY_TXT_6,",
						"          IBM_ONLY_DT_1,",
						"          IBM_ONLY_DT_2,",
						"          IBM_ONLY_DT_3,",
						"          IBM_ONLY_DT_4,",
						"          IBM_ONLY_DT_5,",
						"          IBM_ONLY_DT_6,",
						"          CLNT_ONLY_TXT_1,",
						"          CLNT_ONLY_TXT_2,",
						"          CLNT_ONLY_TXT_3,",
						"          CLNT_ONLY_TXT_4,",
						"          CLNT_ONLY_TXT_5,",
						"          CLNT_ONLY_TXT_6,",
						"          CLNT_ONLY_DT_1,",
						"          CLNT_ONLY_DT_2,",
						"          CLNT_ONLY_DT_3,",
						"          CLNT_ONLY_DT_4,",
						"          CLNT_ONLY_DT_5,",
						"          CLNT_ONLY_DT_6,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          PBLC_TXT_7,",
						"          PBLC_TXT_8,",
						"          PBLC_TXT_9,",
						"          PBLC_TXT_10,",
						"          PBLC_TXT_11,",
						"          PBLC_TXT_12,",
						"          PBLC_TXT_13,",
						"          PBLC_TXT_14,",
						"          PBLC_TXT_15,",
						"          PBLC_TXT_16,",
						"          PBLC_TXT_17,",
						"          PBLC_TXT_18,",
						"          PBLC_TXT_19,",
						"          PBLC_TXT_20,",
						"          PBLC_TXT_21,",
						"          PBLC_TXT_22,",
						"          PBLC_TXT_23,",
						"          PBLC_TXT_24,",
						"          PBLC_TXT_25,",
						"          PBLC_DT_7,",
						"          PBLC_DT_8,",
						"          PBLC_DT_9,",
						"          PBLC_DT_10,",
						"          PBLC_DT_11,",
						"          PBLC_DT_12,",
						"          PBLC_DT_13,",
						"          PBLC_DT_14,",
						"          PBLC_DT_15,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'U'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'I'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          CSTM_FIELDS_DIM_UID,",
						"          PBLC_TXT_1,",
						"          PBLC_TXT_2,",
						"          PBLC_TXT_3,",
						"          PBLC_TXT_4,",
						"          PBLC_TXT_5,",
						"          PBLC_TXT_6,",
						"          PBLC_DT_1,",
						"          PBLC_DT_2,",
						"          PBLC_DT_3,",
						"          PBLC_DT_4,",
						"          PBLC_DT_5,",
						"          PBLC_DT_6,",
						"          IBM_ONLY_TXT_1,",
						"          IBM_ONLY_TXT_2,",
						"          IBM_ONLY_TXT_3,",
						"          IBM_ONLY_TXT_4,",
						"          IBM_ONLY_TXT_5,",
						"          IBM_ONLY_TXT_6,",
						"          IBM_ONLY_DT_1,",
						"          IBM_ONLY_DT_2,",
						"          IBM_ONLY_DT_3,",
						"          IBM_ONLY_DT_4,",
						"          IBM_ONLY_DT_5,",
						"          IBM_ONLY_DT_6,",
						"          CLNT_ONLY_TXT_1,",
						"          CLNT_ONLY_TXT_2,",
						"          CLNT_ONLY_TXT_3,",
						"          CLNT_ONLY_TXT_4,",
						"          CLNT_ONLY_TXT_5,",
						"          CLNT_ONLY_TXT_6,",
						"          CLNT_ONLY_DT_1,",
						"          CLNT_ONLY_DT_2,",
						"          CLNT_ONLY_DT_3,",
						"          CLNT_ONLY_DT_4,",
						"          CLNT_ONLY_DT_5,",
						"          CLNT_ONLY_DT_6,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          PBLC_TXT_7,",
						"          PBLC_TXT_8,",
						"          PBLC_TXT_9,",
						"          PBLC_TXT_10,",
						"          PBLC_TXT_11,",
						"          PBLC_TXT_12,",
						"          PBLC_TXT_13,",
						"          PBLC_TXT_14,",
						"          PBLC_TXT_15,",
						"          PBLC_TXT_16,",
						"          PBLC_TXT_17,",
						"          PBLC_TXT_18,",
						"          PBLC_TXT_19,",
						"          PBLC_TXT_20,",
						"          PBLC_TXT_21,",
						"          PBLC_TXT_22,",
						"          PBLC_TXT_23,",
						"          PBLC_TXT_24,",
						"          PBLC_TXT_25,",
						"          PBLC_DT_7,",
						"          PBLC_DT_8,",
						"          PBLC_DT_9,",
						"          PBLC_DT_10,",
						"          PBLC_DT_11,",
						"          PBLC_DT_12,",
						"          PBLC_DT_13,",
						"          PBLC_DT_14,",
						"          PBLC_DT_15,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"getLookupData select(mapColumn(",
						"          each(match(true()),",
						"               'LKP_'+$$ = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> renamedColumns",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'CSTM_FIELDS_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['CSTM_FIELDS_DIM_UID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'CSTM_FIELDS_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0200_CUSTSTSFCN_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 08-06-2022\nJob Name: DF_BALD0200_CUSTSTSFCN_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						}
					],
					"scriptLines": [
						"parameters{",
						"     current_user as string",
						"}",
						"source(output(",
						"          CUSTSTSFCN_DIM_UID as integer,",
						"          PRJCT_ID as string,",
						"          CUSTSTSFCN_REM_TXT as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer,",
						"          ORIG_ORG as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     CS.PROC_ID as CUSTSTSFCN_DIM_UID,\\n     CS.PROJECT_ID as PRJCT_ID,\\n     cast(CS.REMARKS as varchar(2048)) as CUSTSTSFCN_REM_TXT,\\n     CS.CREATED_TS as SRC_CRETD_TMS,\\n     CS.CREATED_USERID as SRC_CRETD_USER_ID,\\n     CS.UPDATED_TS as SRC_UPDTD_TMS,\\n     CS.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED,\\n     CS.ORIG_ORG\\nFrom\\n     APPFUN.CUSTSAT CS\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on CS.PROC_ID = ZDT.PROC_ID\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on CS.PROC_ID = PDEL.PROC_ID\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD0200_CUSTSTSFCN_DIM\\'\\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'\\n     ) SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          LKP_CUSTSTSFCN_DIM_UID as integer,",
						"          LKP_PRJCT_ID as string,",
						"          LKP_CUSTSTSFCN_REM_TXT as string,",
						"          LKP_SRC_CRETD_TMS as timestamp,",
						"          LKP_SRC_CRETD_USER_ID as string,",
						"          LKP_SRC_UPDTD_TMS as timestamp,",
						"          LKP_SRC_UPDTD_USER_ID as string,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_ETL_JOB_ID as integer,",
						"          LKP_ORIG_ORG as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     S.CUSTSTSFCN_DIM_UID as LKP_CUSTSTSFCN_DIM_UID,\\n     S.PRJCT_ID as LKP_PRJCT_ID,\\n     S.CUSTSTSFCN_REM_TXT as LKP_CUSTSTSFCN_REM_TXT,\\n     S.SRC_CRETD_TMS as LKP_SRC_CRETD_TMS,\\n     S.SRC_CRETD_USER_ID as LKP_SRC_CRETD_USER_ID,\\n     S.SRC_UPDTD_TMS as LKP_SRC_UPDTD_TMS,\\n     S.SRC_UPDTD_USER_ID as LKP_SRC_UPDTD_USER_ID,\\n     S.SRC_SYS_DIM_UID as LKP_SRC_SYS_DIM_UID,\\n     S.ETL_JOB_ID as LKP_ETL_JOB_ID,\\n     S.ORIG_ORG as LKP_ORIG_ORG\\nFrom\\n     PGMPDM.CUSTSTSFCN_DIM S\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on S.CUSTSTSFCN_DIM_UID = ZDT.PROC_ID',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"getLookupData sort(asc(LKP_CUSTSTSFCN_DIM_UID, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"mergedData split(CUSTSTSFCN_DIM_UID==LKP_CUSTSTSFCN_DIM_UID,",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(CUSTSTSFCN_DIM_UID == LKP_CUSTSTSFCN_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          CUSTSTSFCN_DIM_UID,",
						"          PRJCT_ID,",
						"          CUSTSTSFCN_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ORIG_ORG,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'U'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'I'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          CUSTSTSFCN_DIM_UID,",
						"          PRJCT_ID,",
						"          CUSTSTSFCN_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ORIG_ORG,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'CUSTSTSFCN_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['CUSTSTSFCN_DIM_UID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'CUSTSTSFCN_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0205_DATE_CHG_RSN_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 08-06-2022\nJob Name: DF_BALD0205_DATE_CHG_RSN_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceRqstData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourcePCRData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						},
						{
							"name": "sourceRqstPCRData"
						},
						{
							"name": "renamedColumns"
						},
						{
							"name": "sortedLookupData"
						}
					],
					"scriptLines": [
						"parameters{",
						"     current_user as string",
						"}",
						"source(output(",
						"          DATE_CHG_RSN_DIM_UID as integer,",
						"          PRPSL_CHG_RSN_CD as string,",
						"          PRPSL_CHG_RSN_TXT as string,",
						"          PRPSL_CHG_EXPLNN_TXT as string,",
						"          IMPLMTN_CHG_RSN_CD as string,",
						"          IMPLMTN_CHG_RSN_TXT as string,",
						"          IMPLMTN_CHG_EXPLNN_TXT as string,",
						"          PRPSL_LATE_RSN_CD as string,",
						"          PRPSL_LATE_RSN_TXT as string,",
						"          PRPSL_LATE_EXPLNN_TXT as string,",
						"          IMPLMTN_LATE_RSN_CD as string,",
						"          IMPLMTN_LATE_RSN_TXT as string,",
						"          IMPLMTN_LATE_EXPLNN_TXT as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     R.PROC_ID as DATE_CHG_RSN_DIM_UID,\\n     \\n     -- Proposal Change\\n     cast(coalesce(R.PRPSL_CHG_REASON_CD, \\'\\') as varchar(8)) as PRPSL_CHG_RSN_CD,\\n--     cast(coalesce(PRPSL_CHANGES.CHANGE_TXT, \\'\\') as varchar(400)) as PRPSL_CHG_RSN_TXT, \\n     cast(coalesce(PRPSL_CHANGES.PRPSL_CHG_RSN_TXT, \\'\\') as varchar(400)) as PRPSL_CHG_RSN_TXT,           \\n     cast(coalesce(R.PRPSL_CHG_EXPLN, \\'\\') as varchar(200)) as PRPSL_CHG_EXPLNN_TXT,\\n           \\n     -- Implementation Change\\n     cast(coalesce(R.IMPL_CHG_REASON_CD, \\'\\') as varchar(8)) as IMPLMTN_CHG_RSN_CD, \\n--     cast(coalesce(IMPL_CHANGES.CHANGE_TXT, \\'\\') as varchar(400)) as IMPLMTN_CHG_RSN_TXT, \\n     cast(coalesce(IMPL_CHANGES.IMPL_CHG_RSN_TXT, \\'\\') as varchar(400)) as IMPLMTN_CHG_RSN_TXT,           \\n     cast(coalesce(R.IMPL_CHG_EXPLN, \\'\\') as varchar(200)) as IMPLMTN_CHG_EXPLNN_TXT, \\n          \\n     -- Proposal Late\\n     cast(coalesce(R.PRPSL_LATE_REASON_CD, \\'\\') as varchar(8)) as PRPSL_LATE_RSN_CD, \\n--     cast(coalesce(PRPSL_LATE.PRPSL_LATE_RSN_TXT, \\'\\') as varchar(400)) as PRPSL_LATE_RSN_TXT, \\n     cast(coalesce(PRPSL_LATE.PRPSL_LATE_RSN_TXT, \\'\\') as varchar(400)) as PRPSL_LATE_RSN_TXT,           \\n     cast(coalesce(R.PRPSL_LATE_EXPLN, \\'\\') as varchar(200)) as PRPSL_LATE_EXPLNN_TXT, \\n     \\n     -- Implementation Late\\n     cast(coalesce(R.IMPL_LATE_REASON_CD, \\'\\') as varchar(8)) as IMPLMTN_LATE_RSN_CD, \\n--     cast(coalesce(IMPL_LATE.CHANGE_TXT, \\'\\') as varchar(400)) as IMPLMTN_LATE_RSN_TXT, \\n     cast(coalesce(IMPL_LATE.IMPL_LATE_RSN_TXT, \\'\\') as varchar(400)) as IMPLMTN_LATE_RSN_TXT,           \\n     cast(coalesce(R.IMPL_LATE_EXPLN, \\'\\') as varchar(200)) as IMPLMTN_LATE_EXPLNN_TXT,\\n          \\n     -- Control fields\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED          \\nFrom\\n     APPFUN.RQST R\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on R.PROC_ID = ZDT.PROC_ID\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on R.PROC_ID = PDEL.PROC_ID\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD0205_DATE_CHG_RSN_DIM\\'\\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          From\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'\\n     ) SYS\\n     on 1 = 1\\n     Left join(\\n          SELECT \\n               SELLIST.INTERNAL_VAL AS PRPSL_CHG_REASON_CD, \\n               T.TEXT AS PRPSL_CHG_RSN_TXT\\n          FROM \\n               APPFUN.UI_SEL_LIST_DEF_WW_UV SELLIST\\n               INNER JOIN\\n               APPFUN.MULTILANG_TEXT_DEF_WW_UV T\\n               ON T.MULTILANG_TEXT_ID = SELLIST.MULTILANG_SEL_TEXT_ID \\n          WHERE \\n               SELLIST.UI_SEL_LIST_ID IN(95592)  -- code for  PRPSL_CHG_CD\\n               -- AND SELLIST.ACTIVE_IND = \\'Y\\'        this condition is commented because the history data still have codes with the active_ind = \\'N\\'\\n               AND T.LANG_ID = 1      \\n     ) PRPSL_CHANGES\\n     on R.PRPSL_CHG_REASON_CD = PRPSL_CHANGES.PRPSL_CHG_REASON_CD\\n     Left join (\\n          SELECT \\n               SELLIST.INTERNAL_VAL AS IMPL_CHG_REASON_CD, \\n               T.TEXT AS IMPL_CHG_RSN_TXT\\n          FROM \\n               APPFUN.UI_SEL_LIST_DEF_WW_UV SELLIST\\n               INNER JOIN\\n               APPFUN.MULTILANG_TEXT_DEF_WW_UV T\\n               ON T.MULTILANG_TEXT_ID = SELLIST.MULTILANG_SEL_TEXT_ID \\n          WHERE \\n               SELLIST.UI_SEL_LIST_ID IN(95599)  -- code for  IMPLMNT_CHG_CD\\n               -- AND SELLIST.ACTIVE_IND = \\'Y\\'        this condition is commented because the history data still have codes with the active_ind = \\'N\\'\\n               AND T.LANG_ID = 1      \\n     ) IMPL_CHANGES\\n     on R.IMPL_CHG_REASON_CD = IMPL_CHANGES.IMPL_CHG_REASON_CD\\n     Left join (\\n          SELECT \\n               SELLIST.INTERNAL_VAL AS PRPSL_LATE_REASON_CD, \\n               T.TEXT AS PRPSL_LATE_RSN_TXT\\n          FROM \\n               APPFUN.UI_SEL_LIST_DEF_WW_UV SELLIST\\n               INNER JOIN\\n               APPFUN.MULTILANG_TEXT_DEF_WW_UV T\\n               ON T.MULTILANG_TEXT_ID = SELLIST.MULTILANG_SEL_TEXT_ID \\n          WHERE \\n               SELLIST.UI_SEL_LIST_ID IN(95591)  -- code for  PRPSL_LATE_CHG_CD\\n               -- AND SELLIST.ACTIVE_IND = \\'Y\\'        this condition is commented because the history data still have codes with the active_ind = \\'N\\'\\n               AND T.LANG_ID = 1      \\n     ) PRPSL_LATE\\n     on R.PRPSL_LATE_REASON_CD = PRPSL_LATE.PRPSL_LATE_REASON_CD\\n     Left join (\\n          SELECT \\n               SELLIST.INTERNAL_VAL AS IMPL_LATE_REASON_CD, \\n               T.TEXT AS IMPL_LATE_RSN_TXT\\n          FROM \\n               APPFUN.UI_SEL_LIST_DEF_WW_UV SELLIST\\n               INNER JOIN\\n               APPFUN.MULTILANG_TEXT_DEF_WW_UV T\\n               ON T.MULTILANG_TEXT_ID = SELLIST.MULTILANG_SEL_TEXT_ID \\n          WHERE \\n               SELLIST.UI_SEL_LIST_ID IN(95600)  -- code for  IMPL_LATE_REASON_CD\\n               -- AND SELLIST.ACTIVE_IND = \\'Y\\'        this condition is commented because the history data still have codes with the active_ind = \\'N\\'\\n               AND T.LANG_ID = 1      \\n     ) IMPL_LATE\\n     on R.IMPL_LATE_REASON_CD = IMPL_LATE.IMPL_LATE_REASON_CD',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceRqstData",
						"source(output(",
						"          DATE_CHG_RSN_DIM_UID as integer,",
						"          PRPSL_CHG_RSN_CD as string,",
						"          PRPSL_CHG_RSN_TXT as string,",
						"          PRPSL_CHG_EXPLNN_TXT as string,",
						"          IMPLMTN_CHG_RSN_CD as string,",
						"          IMPLMTN_CHG_RSN_TXT as string,",
						"          IMPLMTN_CHG_EXPLNN_TXT as string,",
						"          PRPSL_LATE_RSN_CD as string,",
						"          PRPSL_LATE_RSN_TXT as string,",
						"          PRPSL_LATE_EXPLNN_TXT as string,",
						"          IMPLMTN_LATE_RSN_CD as string,",
						"          IMPLMTN_LATE_RSN_TXT as string,",
						"          IMPLMTN_LATE_EXPLNN_TXT as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     R.PROC_ID as DATE_CHG_RSN_DIM_UID,\\n          \\n     -- Proposal Change\\n     cast(coalesce(R.PRPSL_CHG_REASON_CD, \\'\\') as varchar(8)) as PRPSL_CHG_RSN_CD,\\n--     cast(coalesce(PRPSL_CHANGES.CHANGE_TXT, \\'\\') as varchar(400)) as PRPSL_CHG_RSN_TXT, \\n     cast(coalesce(PRPSL_CHANGES.PRPSL_CHG_RSN_TXT, \\'\\') as varchar(400)) as PRPSL_CHG_RSN_TXT,           \\n     cast(coalesce(R.PRPSL_CHG_EXPLN, \\'\\') as varchar(200)) as PRPSL_CHG_EXPLNN_TXT,\\n           \\n     -- Implementation Change\\n     cast(coalesce(R.IMPL_CHG_REASON_CD, \\'\\') as varchar(8)) as IMPLMTN_CHG_RSN_CD, \\n--     cast(coalesce(IMPL_CHANGES.CHANGE_TXT, \\'\\') as varchar(400)) as IMPLMTN_CHG_RSN_TXT, \\n     cast(coalesce(IMPL_CHANGES.IMPL_CHG_RSN_TXT, \\'\\') as varchar(400)) as IMPLMTN_CHG_RSN_TXT,           \\n     cast(coalesce(R.IMPL_CHG_EXPLN, \\'\\') as varchar(200)) as IMPLMTN_CHG_EXPLNN_TXT, \\n          \\n     -- Proposal Late\\n     cast(coalesce(R.PRPSL_LATE_REASON_CD, \\'\\') as varchar(8)) as PRPSL_LATE_RSN_CD, \\n--     cast(coalesce(PRPSL_LATE.PRPSL_LATE_RSN_TXT, \\'\\') as varchar(400)) as PRPSL_LATE_RSN_TXT, \\n     cast(coalesce(PRPSL_LATE.PRPSL_LATE_RSN_TXT, \\'\\') as varchar(400)) as PRPSL_LATE_RSN_TXT,           \\n     cast(coalesce(R.PRPSL_LATE_EXPLN, \\'\\') as varchar(200)) as PRPSL_LATE_EXPLNN_TXT, \\n     \\n     -- Implementation Late\\n     cast(coalesce(R.IMPL_LATE_REASON_CD, \\'\\') as varchar(8)) as IMPLMTN_LATE_RSN_CD, \\n--     cast(coalesce(IMPL_LATE.CHANGE_TXT, \\'\\') as varchar(400)) as IMPLMTN_LATE_RSN_TXT, \\n     cast(coalesce(IMPL_LATE.IMPL_LATE_RSN_TXT, \\'\\') as varchar(400)) as IMPLMTN_LATE_RSN_TXT,           \\n     cast(coalesce(R.IMPL_LATE_EXPLN, \\'\\') as varchar(200)) as IMPLMTN_LATE_EXPLNN_TXT,\\n          \\n     -- Control fields\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED          \\nFrom\\n     APPFUN.PCR R\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on R.PROC_ID = ZDT.PROC_ID\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on R.PROC_ID = PDEL.PROC_ID\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD0205_DATE_CHG_RSN_DIM\\'\\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          From\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'\\n     ) SYS\\n     on 1 = 1\\n     Left join (\\n          SELECT \\n               SELLIST.INTERNAL_VAL AS PRPSL_CHG_REASON_CD, \\n               T.TEXT AS PRPSL_CHG_RSN_TXT\\n          FROM \\n               APPFUN.UI_SEL_LIST_DEF_WW_UV SELLIST\\n               INNER JOIN\\n               APPFUN.MULTILANG_TEXT_DEF_WW_UV T\\n               ON T.MULTILANG_TEXT_ID = SELLIST.MULTILANG_SEL_TEXT_ID \\n          WHERE \\n               SELLIST.UI_SEL_LIST_ID IN(95592)  -- code for  PRPSL_CHG_CD\\n               -- AND SELLIST.ACTIVE_IND = \\'Y\\'        this condition is commented because the history data still have codes with the active_ind = \\'N\\'\\n               AND T.LANG_ID = 1      \\n     ) PRPSL_CHANGES\\n     on R.PRPSL_CHG_REASON_CD = PRPSL_CHANGES.PRPSL_CHG_REASON_CD\\n     Left join (\\n          SELECT \\n               SELLIST.INTERNAL_VAL AS IMPL_CHG_REASON_CD, \\n               T.TEXT AS IMPL_CHG_RSN_TXT\\n          FROM \\n               APPFUN.UI_SEL_LIST_DEF_WW_UV SELLIST\\n               INNER JOIN\\n               APPFUN.MULTILANG_TEXT_DEF_WW_UV T\\n               ON T.MULTILANG_TEXT_ID = SELLIST.MULTILANG_SEL_TEXT_ID \\n          WHERE \\n               SELLIST.UI_SEL_LIST_ID IN(95599)  -- code for  IMPLMNT_CHG_CD\\n               -- AND SELLIST.ACTIVE_IND = \\'Y\\'        this condition is commented because the history data still have codes with the active_ind = \\'N\\'\\n               AND T.LANG_ID = 1      \\n     ) IMPL_CHANGES\\n     on R.IMPL_CHG_REASON_CD = IMPL_CHANGES.IMPL_CHG_REASON_CD\\n     Left join (\\n          SELECT \\n               SELLIST.INTERNAL_VAL AS PRPSL_LATE_REASON_CD, \\n               T.TEXT AS PRPSL_LATE_RSN_TXT\\n          FROM \\n               APPFUN.UI_SEL_LIST_DEF_WW_UV SELLIST\\n               INNER JOIN\\n               APPFUN.MULTILANG_TEXT_DEF_WW_UV T\\n               ON T.MULTILANG_TEXT_ID = SELLIST.MULTILANG_SEL_TEXT_ID\\n          WHERE \\n               SELLIST.UI_SEL_LIST_ID = 95591\\n               AND SELLIST.ACTIVE_IND = \\'Y\\'\\n               AND T.LANG_ID = 1  \\n     ) PRPSL_LATE\\n     on R.PRPSL_LATE_REASON_CD = PRPSL_LATE.PRPSL_LATE_REASON_CD\\n     Left join (\\n          SELECT \\n               SELLIST.INTERNAL_VAL AS IMPL_LATE_REASON_CD, \\n               T.TEXT AS IMPL_LATE_RSN_TXT\\n          FROM \\n               APPFUN.UI_SEL_LIST_DEF_WW_UV SELLIST\\n               INNER JOIN\\n               APPFUN.MULTILANG_TEXT_DEF_WW_UV T\\n               ON T.MULTILANG_TEXT_ID = SELLIST.MULTILANG_SEL_TEXT_ID \\n          WHERE \\n               SELLIST.UI_SEL_LIST_ID IN(95600)  -- code for  IMPL_LATE_REASON_CD\\n               -- AND SELLIST.ACTIVE_IND = \\'Y\\'        this condition is commented because the history data still have codes with the active_ind = \\'N\\'\\n               AND T.LANG_ID = 1      \\n     ) IMPL_LATE\\n     on R.IMPL_LATE_REASON_CD = IMPL_LATE.IMPL_LATE_REASON_CD',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourcePCRData",
						"source(output(",
						"          DATE_CHG_RSN_DIM_UID as integer,",
						"          PRPSL_CHG_RSN_CD as string,",
						"          PRPSL_CHG_RSN_TXT as string,",
						"          PRPSL_CHG_EXPLNN_TXT as string,",
						"          IMPLMTN_CHG_RSN_CD as string,",
						"          IMPLMTN_CHG_RSN_TXT as string,",
						"          IMPLMTN_CHG_EXPLNN_TXT as string,",
						"          PRPSL_LATE_RSN_CD as string,",
						"          PRPSL_LATE_RSN_TXT as string,",
						"          PRPSL_LATE_EXPLNN_TXT as string,",
						"          IMPLMTN_LATE_RSN_CD as string,",
						"          IMPLMTN_LATE_RSN_TXT as string,",
						"          IMPLMTN_LATE_EXPLNN_TXT as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     DATE_CHG_RSN_DIM_UID,\\n     PRPSL_CHG_RSN_CD,\\n     PRPSL_CHG_RSN_TXT,\\n     PRPSL_CHG_EXPLNN_TXT,\\n     IMPLMTN_CHG_RSN_CD,\\n     IMPLMTN_CHG_RSN_TXT,\\n     IMPLMTN_CHG_EXPLNN_TXT,\\n     PRPSL_LATE_RSN_CD,\\n     PRPSL_LATE_RSN_TXT,\\n     PRPSL_LATE_EXPLNN_TXT,\\n     IMPLMTN_LATE_RSN_CD,\\n     IMPLMTN_LATE_RSN_TXT,\\n     IMPLMTN_LATE_EXPLNN_TXT\\nFrom\\n     PGMPDM.DATE_CHG_RSN_DIM S\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on S.DATE_CHG_RSN_DIM_UID = ZDT.PROC_ID',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"mergedData split(DATE_CHG_RSN_DIM_UID==LKP_DATE_CHG_RSN_DIM_UID,",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"sourceRqstPCRData, sortedLookupData join(DATE_CHG_RSN_DIM_UID == LKP_DATE_CHG_RSN_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          DATE_CHG_RSN_DIM_UID,",
						"          PRPSL_CHG_RSN_CD,",
						"          PRPSL_CHG_RSN_TXT,",
						"          PRPSL_CHG_EXPLNN_TXT,",
						"          IMPLMTN_CHG_RSN_CD,",
						"          IMPLMTN_CHG_RSN_TXT,",
						"          IMPLMTN_CHG_EXPLNN_TXT,",
						"          PRPSL_LATE_RSN_CD,",
						"          PRPSL_LATE_RSN_TXT,",
						"          PRPSL_LATE_EXPLNN_TXT,",
						"          IMPLMTN_LATE_RSN_CD,",
						"          IMPLMTN_LATE_RSN_TXT,",
						"          IMPLMTN_LATE_EXPLNN_TXT,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'U'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'I'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          DATE_CHG_RSN_DIM_UID,",
						"          PRPSL_CHG_RSN_CD,",
						"          PRPSL_CHG_RSN_TXT,",
						"          PRPSL_CHG_EXPLNN_TXT,",
						"          IMPLMTN_CHG_RSN_CD,",
						"          IMPLMTN_CHG_RSN_TXT,",
						"          IMPLMTN_CHG_EXPLNN_TXT,",
						"          PRPSL_LATE_RSN_CD,",
						"          PRPSL_LATE_RSN_TXT,",
						"          PRPSL_LATE_EXPLNN_TXT,",
						"          IMPLMTN_LATE_RSN_CD,",
						"          IMPLMTN_LATE_RSN_TXT,",
						"          IMPLMTN_LATE_EXPLNN_TXT,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"getSourceRqstData, getSourcePCRData union(byName: true)~> sourceRqstPCRData",
						"getLookupData select(mapColumn(",
						"          each(match(true()),",
						"               'LKP_'+$$ = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> renamedColumns",
						"renamedColumns sort(asc(LKP_DATE_CHG_RSN_DIM_UID, true)) ~> sortedLookupData",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'DATE_CHG_RSN_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['DATE_CHG_RSN_DIM_UID'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'DATE_CHG_RSN_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0210_DELD_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 09-06-2022\nJob Name: DF_BALD0210_DELD_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						},
						{
							"name": "CDCval"
						},
						{
							"name": "renamedColumns"
						}
					],
					"scriptLines": [
						"parameters{",
						"     current_user as string",
						"}",
						"source(output(",
						"          DELD_DIM_UID as integer,",
						"          DELD_CD as string,",
						"          DELD_DESC as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     D.DELD_DIM_UID,\\n     cast(D.DELD_CD as varchar(20)) as DELD_CD,\\n     cast(D.DELD_DESC as varchar(128)) as DELD_DESC,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID\\nFrom\\n     (\\n          Select\\n               -1 as DELD_DIM_UID,\\n               \\'UNK\\' as DELD_CD,\\n               \\'Unknown\\' as DELD_DESC\\n          union all\\n          Select\\n               0 as DELD_DIM_UID,\\n               \\'NOT_DELETED\\' as DELD_CD,\\n               \\'Not Deleted\\' as DELD_DESC\\n          union all\\n          Select\\n               1 as DELD_DIM_UID,\\n               \\'DELETED\\' as DELD_CD,\\n               \\'Deleted\\' as DELD_DESC\\n     ) D\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD0210_DELD_DIM\\'\\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'\\n     ) SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          DELD_DIM_UID as integer,",
						"          DELD_CD as string,",
						"          DELD_DESC as string,",
						"          SRC_SYS_DIM_UID as integer,",
						"          ETL_JOB_ID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     DELD_DIM_UID,\\n     DELD_CD,\\n     DELD_DESC,\\n     SRC_SYS_DIM_UID,\\n     ETL_JOB_ID\\nFrom\\n     PGMPDM.DELD_DIM',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"renamedColumns sort(asc(LKP_DELD_DIM_UID, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"CDCval split(ROW_STATUS!='I',",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(DELD_DIM_UID == LKP_DELD_DIM_UID,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          DELD_DIM_UID,",
						"          DELD_CD,",
						"          DELD_DESC,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STATUS,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(DM_CRETD_USER_ID = $current_user,",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(DM_CRETD_USER_ID = $current_user,",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          DELD_DIM_UID,",
						"          DELD_CD,",
						"          DELD_DESC,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STATUS,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"mergedData derive(ROW_STATUS = case(DELD_DIM_UID == toInteger(null()) && LKP_DELD_DIM_UID != toInteger(null()), 'D', case(DELD_DIM_UID != toInteger(null()) && LKP_DELD_DIM_UID == toInteger(null()), 'I', 'U'))) ~> CDCval",
						"getLookupData select(mapColumn(",
						"          each(match(true()),",
						"               'LKP_'+$$ = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> renamedColumns",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'DELD_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['DELD_DIM_UID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'DELD_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0220_DELIVOPS_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 16-06-2022\nJob Name: DF_BALD0220_DELIVOPS_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						}
					],
					"scriptLines": [
						"parameters{",
						"     current_user as string",
						"}",
						"source(output(",
						"          DELIVOPS_DIM_UID as integer,",
						"          PRJCT_ID as string,",
						"          DELIVOPS_REM_TXT as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer,",
						"          ORIG_ORG as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     D.PROC_ID as DELIVOPS_DIM_UID,\\n     D.PROJECT_ID as PRJCT_ID,\\n     cast(D.REMARKS as varchar(2048)) as DELIVOPS_REM_TXT,\\n     D.CREATED_TS as SRC_CRETD_TMS,\\n     D.CREATED_USERID as SRC_CRETD_USER_ID,\\n     D.UPDATED_TS as SRC_UPDTD_TMS,\\n     D.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED,\\n     D.ORIG_ORG\\nFrom\\n     APPFUN.DELIVOPS D\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on D.PROC_ID = ZDT.PROC_ID\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on D.PROC_ID = PDEL.PROC_ID\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD0220_DELIVOPS_DIM\\'\\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'\\n     ) SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          LKP_DELIVOPS_DIM_UID as integer,",
						"          LKP_PRJCT_ID as string,",
						"          LKP_DELIVOPS_REM_TXT as string,",
						"          LKP_SRC_CRETD_TMS as timestamp,",
						"          LKP_SRC_CRETD_USER_ID as string,",
						"          LKP_SRC_UPDTD_TMS as timestamp,",
						"          LKP_SRC_UPDTD_USER_ID as string,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_ETL_JOB_ID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     S.DELIVOPS_DIM_UID as LKP_DELIVOPS_DIM_UID,\\n     S.PRJCT_ID as LKP_PRJCT_ID,\\n     S.DELIVOPS_REM_TXT as LKP_DELIVOPS_REM_TXT,\\n     S.SRC_CRETD_TMS as LKP_SRC_CRETD_TMS,\\n     S.SRC_CRETD_USER_ID as LKP_SRC_CRETD_USER_ID,\\n     S.SRC_UPDTD_TMS as LKP_SRC_UPDTD_TMS,\\n     S.SRC_UPDTD_USER_ID as LKP_SRC_UPDTD_USER_ID,\\n     S.SRC_SYS_DIM_UID as LKP_SRC_SYS_DIM_UID,\\n     S.ETL_JOB_ID as LKP_ETL_JOB_ID\\nFrom\\n     PGMPDM.DELIVOPS_DIM S\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on S.DELIVOPS_DIM_UID = ZDT.PROC_ID     ',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"getLookupData sort(asc(LKP_DELIVOPS_DIM_UID, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"mergedData split(DELIVOPS_DIM_UID==LKP_DELIVOPS_DIM_UID,",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(DELIVOPS_DIM_UID == LKP_DELIVOPS_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          DELIVOPS_DIM_UID,",
						"          PRJCT_ID,",
						"          DELIVOPS_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ORIG_ORG,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'U'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'I'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          DELIVOPS_DIM_UID,",
						"          PRJCT_ID,",
						"          DELIVOPS_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ORIG_ORG,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'DELIVOPS_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['DELIVOPS_DIM_UID'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'DELIVOPS_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0230_DOCMTCTL_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 16-06-2022\nJob Name: DF_BALD0230_DOCMTCTL_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						}
					],
					"scriptLines": [
						"parameters{",
						"     current_user as string",
						"}",
						"source(output(",
						"          DOCMTCTL_DIM_UID as integer,",
						"          PRJCT_ID as string,",
						"          DOCMTCTL_REM_TXT as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer,",
						"          ORIG_ORG as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     D.PROC_ID as DOCMTCTL_DIM_UID,\\n     D.PROJECT_ID as PRJCT_ID,\\n     cast(D.REMARKS as varchar(6096)) as DOCMTCTL_REM_TXT,\\n     D.CREATED_TS as SRC_CRETD_TMS,\\n     D.CREATED_USERID as SRC_CRETD_USER_ID,\\n     D.UPDATED_TS as SRC_UPDTD_TMS,\\n     D.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED,\\n     D.ORIG_ORG\\nfrom\\n     APPFUN.DOCMTCTL D\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on D.PROC_ID = ZDT.PROC_ID\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on D.PROC_ID = PDEL.PROC_ID\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD0230_DOCMTCTL_DIM\\'\\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'\\n     ) SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          LKP_DOCMTCTL_DIM_UID as integer,",
						"          LKP_PRJCT_ID as string,",
						"          LKP_DOCMTCTL_REM_TXT as string,",
						"          LKP_SRC_CRETD_TMS as timestamp,",
						"          LKP_SRC_CRETD_USER_ID as string,",
						"          LKP_SRC_UPDTD_TMS as timestamp,",
						"          LKP_SRC_UPDTD_USER_ID as string,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_ETL_JOB_ID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     S.DOCMTCTL_DIM_UID as LKP_DOCMTCTL_DIM_UID,\\n     S.PRJCT_ID as LKP_PRJCT_ID,\\n     S.DOCMTCTL_REM_TXT as LKP_DOCMTCTL_REM_TXT,\\n     S.SRC_CRETD_TMS as LKP_SRC_CRETD_TMS,\\n     S.SRC_CRETD_USER_ID as LKP_SRC_CRETD_USER_ID,\\n     S.SRC_UPDTD_TMS as LKP_SRC_UPDTD_TMS,\\n     S.SRC_UPDTD_USER_ID as LKP_SRC_UPDTD_USER_ID,\\n     S.SRC_SYS_DIM_UID as LKP_SRC_SYS_DIM_UID,\\n     S.ETL_JOB_ID as LKP_ETL_JOB_ID\\nFrom\\n     PGMPDM.DOCMTCTL_DIM S\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on S.DOCMTCTL_DIM_UID = ZDT.PROC_ID     ',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"getLookupData sort(asc(LKP_DOCMTCTL_DIM_UID, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"mergedData split(DOCMTCTL_DIM_UID==LKP_DOCMTCTL_DIM_UID,",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(DOCMTCTL_DIM_UID == LKP_DOCMTCTL_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          DOCMTCTL_DIM_UID,",
						"          PRJCT_ID,",
						"          DOCMTCTL_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ORIG_ORG,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'U'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'I'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          DOCMTCTL_DIM_UID,",
						"          PRJCT_ID,",
						"          DOCMTCTL_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ORIG_ORG,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'DOCMTCTL_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['DOCMTCTL_DIM_UID'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'DOCMTCTL_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0240_FINNC_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 17-06-2022\nJob Name: DF_BALD0240_FINNC_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						}
					],
					"scriptLines": [
						"parameters{",
						"     current_user as string",
						"}",
						"source(output(",
						"          FINNC_DIM_UID as integer,",
						"          PRJCT_ID as string,",
						"          FINNC_REM_TXT as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer,",
						"          ORIG_ORG as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     FI.PROC_ID as FINNC_DIM_UID,\\n     FI.PROJECT_ID as PRJCT_ID,\\n     cast(FI.REMARKS as varchar(1024)) as FINNC_REM_TXT,\\n     FI.CREATED_TS as SRC_CRETD_TMS,\\n     FI.CREATED_USERID as SRC_CRETD_USER_ID,\\n     FI.UPDATED_TS as SRC_UPDTD_TMS,\\n     FI.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED,\\n     FI.ORIG_ORG\\nFrom\\n     APPFUN.FINANCE FI\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on FI.PROC_ID = ZDT.PROC_ID\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on FI.PROC_ID = PDEL.PROC_ID\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD0240_FINNC_DIM\\'\\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'\\n     ) SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          LKP_FINNC_DIM_UID as integer,",
						"          LKP_PRJCT_ID as string,",
						"          LKP_FINNC_REM_TXT as string,",
						"          LKP_SRC_CRETD_TMS as timestamp,",
						"          LKP_SRC_CRETD_USER_ID as string,",
						"          LKP_SRC_UPDTD_TMS as timestamp,",
						"          LKP_SRC_UPDTD_USER_ID as string,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_ETL_JOB_ID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     S.FINNC_DIM_UID as LKP_FINNC_DIM_UID,\\n     S.PRJCT_ID as LKP_PRJCT_ID,\\n     S.FINNC_REM_TXT as LKP_FINNC_REM_TXT,\\n     S.SRC_CRETD_TMS as LKP_SRC_CRETD_TMS,\\n     S.SRC_CRETD_USER_ID as LKP_SRC_CRETD_USER_ID,\\n     S.SRC_UPDTD_TMS as LKP_SRC_UPDTD_TMS,\\n     S.SRC_UPDTD_USER_ID as LKP_SRC_UPDTD_USER_ID,\\n     S.SRC_SYS_DIM_UID as LKP_SRC_SYS_DIM_UID,\\n     S.ETL_JOB_ID as LKP_ETL_JOB_ID\\nFrom\\n     PGMPDM.FINNC_DIM S\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on S.FINNC_DIM_UID = ZDT.PROC_ID',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"getLookupData sort(asc(LKP_FINNC_DIM_UID, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"mergedData split(FINNC_DIM_UID==LKP_FINNC_DIM_UID,",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(FINNC_DIM_UID == LKP_FINNC_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          FINNC_DIM_UID,",
						"          PRJCT_ID,",
						"          FINNC_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ORIG_ORG,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'U'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'I'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          FINNC_DIM_UID,",
						"          PRJCT_ID,",
						"          FINNC_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ORIG_ORG,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'FINNC_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['FINNC_DIM_UID'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'FINNC_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0250_FMLCRSPN_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 20-06-2022\nJob Name: DF_BALD0250_FMLCRSPN_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						}
					],
					"scriptLines": [
						"parameters{",
						"     current_user as string",
						"}",
						"source(output(",
						"          FMLCRSPN_DIM_UID as integer,",
						"          PRJCT_ID as string,",
						"          FMLCRSPN_REM_TXT as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer,",
						"          ORIG_ORG as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     FML.PROC_ID as FMLCRSPN_DIM_UID,\\n     FML.PROJECT_ID as PRJCT_ID,\\n     cast(FML.REMARKS as varchar(5000)) as FMLCRSPN_REM_TXT,\\n     FML.CREATED_TS as SRC_CRETD_TMS,\\n     FML.CREATED_USERID as SRC_CRETD_USER_ID,\\n     FML.UPDATED_TS as SRC_UPDTD_TMS,\\n     FML.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED,\\n     FML.ORIG_ORG\\nFrom\\n     APPFUN.FMLCRSPN FML\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on FML.PROC_ID = ZDT.PROC_ID\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on FML.PROC_ID = PDEL.PROC_ID\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD0250_FMLCRSPN_DIM\\'\\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'\\n     ) SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          LKP_FMLCRSPN_DIM_UID as integer,",
						"          LKP_PRJCT_ID as string,",
						"          LKP_FMLCRSPN_REM_TXT as string,",
						"          LKP_SRC_CRETD_TMS as timestamp,",
						"          LKP_SRC_CRETD_USER_ID as string,",
						"          LKP_SRC_UPDTD_TMS as timestamp,",
						"          LKP_SRC_UPDTD_USER_ID as string,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_ETL_JOB_ID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     S.FMLCRSPN_DIM_UID as LKP_FMLCRSPN_DIM_UID,\\n     S.PRJCT_ID as LKP_PRJCT_ID,\\n     S.FMLCRSPN_REM_TXT as LKP_FMLCRSPN_REM_TXT,\\n     S.SRC_CRETD_TMS as LKP_SRC_CRETD_TMS,\\n     S.SRC_CRETD_USER_ID as LKP_SRC_CRETD_USER_ID,\\n     S.SRC_UPDTD_TMS as LKP_SRC_UPDTD_TMS,\\n     S.SRC_UPDTD_USER_ID as LKP_SRC_UPDTD_USER_ID,\\n     S.SRC_SYS_DIM_UID as LKP_SRC_SYS_DIM_UID,\\n     S.ETL_JOB_ID as LKP_ETL_JOB_ID\\nFrom\\n     PGMPDM.FMLCRSPN_DIM S\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on S.FMLCRSPN_DIM_UID = ZDT.PROC_ID     ',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"getLookupData sort(asc(LKP_FMLCRSPN_DIM_UID, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"mergedData split(FMLCRSPN_DIM_UID==LKP_FMLCRSPN_DIM_UID,",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(FMLCRSPN_DIM_UID == LKP_FMLCRSPN_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          FMLCRSPN_DIM_UID,",
						"          PRJCT_ID,",
						"          FMLCRSPN_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ORIG_ORG,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'U'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'I'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          FMLCRSPN_DIM_UID,",
						"          PRJCT_ID,",
						"          FMLCRSPN_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ORIG_ORG,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'FMLCRSPN_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['FMLCRSPN_DIM_UID'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'FMLCRSPN_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0260_GEO_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job creation:05-06-2022\nJob name: Df_BAL0001_GEO_DIM\nCreatedBy: Varaprasad\n\nThis job will pull all GEO_TMF and loading into PGMPDM.GEO_DIM table",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "srcgeo"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "srtdtgtgeolkp"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "Tgtupdgeodimtable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "InsTgtGeoDim"
						}
					],
					"transformations": [
						{
							"name": "mergeddata"
						},
						{
							"name": "srtdtgtgeodata"
						},
						{
							"name": "split"
						},
						{
							"name": "rowstat"
						},
						{
							"name": "select1"
						},
						{
							"name": "alterRow1"
						},
						{
							"name": "rowstatIns"
						},
						{
							"name": "select2"
						}
					],
					"scriptLines": [
						"source(output(",
						"          GEO_DIM_UID as integer,",
						"          IOT_RGN_CD as string,",
						"          IOT_RGN_NM as string,",
						"          IMT_RGN_CD as string,",
						"          IMT_SRGN_NM as string,",
						"          SRGN_CD as string,",
						"          SRGNCTRY_CD as string,",
						"          SRGNCTRY_NM as string,",
						"          IOTSORT_NUM as integer,",
						"          GEO_CD as string,",
						"          GEO_TYP_CD as string,",
						"          GMR_RGN_CD as string,",
						"          GMR_RGN_NM as string,",
						"          IOT_SHORT_NM as string,",
						"          IMT_SHORT_NM as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n     G.GEO_DIM_UID,\\n     G.IOT_RGN_CD,\\n     G.IOT_RGN_NM,\\n     G.IMT_RGN_CD,\\n     G.IMT_SRGN_NM,\\n     G.SRGN_CD,\\n     G.SRGNCTRY_CD,\\n     G.SRGNCTRY_NM,\\n     G.IOTSORT_NUM,\\n     G.GEO_CD,\\n     G.GEO_TYP_CD,\\n     G.GMR_RGN_CD,\\n     G.GMR_RGN_NM,\\n     G.IOT_SHORT_NM,\\n     G.IMT_SHORT_NM,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID\\nfrom\\n     PGMPDM.GEO_DIM_TMF G\\n     left join\\n     (select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          from\\n               PGMPDM.ZAUX_ETL_JOBS\\n          where\\n               ETL_JOB_NM = \\'#DSJobName#\\'     )JOB\\n     on 1 = 1\\n     left join\\n     (select\\n               ETL_EXCTN_ID\\n          from\\n               PGMPDM.[ZAUX_ETL_EXCTN]\\n          where\\n               IS_CURR_IND = \\'Y\\')FIL\\n     on 1 = 1\\n     left join\\n     (select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          from\\n               PGMPDM.SRC_SYS_DIM\\n          where\\n               SRC_SYS_CD = \\'TMF\\')SYS\\n     on 1 = 1\\n',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> srcgeo",
						"source(output(",
						"          LKP_GEO_DIM_UID as integer,",
						"          LKP_IOT_RGN_CD as string,",
						"          LKP_IOT_RGN_NM as string,",
						"          LKP_IMT_RGN_CD as string,",
						"          LKP_IMT_SRGN_NM as string,",
						"          LKP_SRGN_CD as string,",
						"          LKP_SRGNCTRY_CD as string,",
						"          LKP_SRGNCTRY_NM as string,",
						"          LKP_IOTSORT_NUM as integer,",
						"          LKP_GEO_CD as string,",
						"          LKP_GEO_TYP_CD as string,",
						"          LKP_GMR_RGN_CD as string,",
						"          LKP_GMR_RGN_NM as string,",
						"          LKP_IOT_SHORT_NM as string,",
						"          LKP_IMT_SHORT_NM as string,",
						"          LKP_ETL_JOB_ID as integer,",
						"          LKP_SRC_SYS_DIM_UID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n      GEO_DIM_UID  LKP_GEO_DIM_UID ,\\n     IOT_RGN_CD AS LKP_IOT_RGN_CD  ,\\n    IOT_RGN_NM as  LKP_IOT_RGN_NM  ,\\n     IMT_RGN_CD as LKP_IMT_RGN_CD  ,\\n     IMT_SRGN_NM as LKP_IMT_SRGN_NM  ,\\n     SRGN_CD as LKP_SRGN_CD  ,\\n     SRGNCTRY_CD as LKP_SRGNCTRY_CD  ,\\n     SRGNCTRY_NM as LKP_SRGNCTRY_NM  ,\\n     IOTSORT_NUM as LKP_IOTSORT_NUM   ,\\n     GEO_CD as LKP_GEO_CD  ,\\n     GEO_TYP_CD as LKP_GEO_TYP_CD  ,\\n     GMR_RGN_CD as LKP_GMR_RGN_CD  ,\\n     GMR_RGN_NM as LKP_GMR_RGN_NM  ,\\n     IOT_SHORT_NM as LKP_IOT_SHORT_NM  ,\\n     IMT_SHORT_NM as LKP_IMT_SHORT_NM  ,\\n     ETL_JOB_ID as LKP_ETL_JOB_ID  ,\\n     SRC_SYS_DIM_UID as LKP_SRC_SYS_DIM_UID  \\nfrom\\n     PGMPDM.GEO_DIM',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> srtdtgtgeolkp",
						"srcgeo, srtdtgtgeodata join(GEO_DIM_UID == LKP_GEO_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergeddata",
						"srtdtgtgeolkp sort(asc(LKP_GEO_DIM_UID, true)) ~> srtdtgtgeodata",
						"mergeddata split(GEO_DIM_UID==LKP_GEO_DIM_UID,",
						"     disjoint: false) ~> split@(datatobeupdted, datatobeinserted)",
						"split@datatobeupdted derive(DM_CRETD_USER_ID = \"dsadm\",",
						"          DM_UPDTD_USER_ID = \"dsadm\") ~> rowstat",
						"rowstat select(mapColumn(",
						"          GEO_DIM_UID,",
						"          IOT_RGN_CD,",
						"          IOT_RGN_NM,",
						"          IMT_RGN_CD,",
						"          IMT_SRGN_NM,",
						"          SRGN_CD,",
						"          SRGNCTRY_CD,",
						"          SRGNCTRY_NM,",
						"          IOTSORT_NUM,",
						"          GEO_CD,",
						"          GEO_TYP_CD,",
						"          GMR_RGN_CD,",
						"          GMR_RGN_NM,",
						"          IOT_SHORT_NM,",
						"          IMT_SHORT_NM,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 alterRow(updateIf(true())) ~> alterRow1",
						"split@datatobeinserted derive(DM_CRETD_USER_ID = \"dsadm\",",
						"          DM_UPDTD_USER_ID = \"dsadm\") ~> rowstatIns",
						"rowstatIns select(mapColumn(",
						"          GEO_DIM_UID,",
						"          IOT_RGN_CD,",
						"          IOT_RGN_NM,",
						"          IMT_RGN_CD,",
						"          IMT_SRGN_NM,",
						"          SRGN_CD,",
						"          SRGNCTRY_CD,",
						"          SRGNCTRY_NM,",
						"          IOTSORT_NUM,",
						"          GEO_CD,",
						"          GEO_TYP_CD,",
						"          GMR_RGN_CD,",
						"          GMR_RGN_NM,",
						"          IOT_SHORT_NM,",
						"          IMT_SHORT_NM,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select2",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          GEO_DIM_UID as integer,",
						"          IOT_RGN_CD as string,",
						"          IOT_RGN_NM as string,",
						"          IMT_RGN_CD as string,",
						"          IMT_SRGN_NM as string,",
						"          SRGN_CD as string,",
						"          SRGNCTRY_CD as string,",
						"          SRGNCTRY_NM as string,",
						"          IOTSORT_NUM as integer,",
						"          GEO_CD as string,",
						"          GEO_TYP_CD as string,",
						"          GMR_RGN_CD as string,",
						"          GMR_RGN_NM as string,",
						"          IOT_SHORT_NM as string,",
						"          IMT_SHORT_NM as string,",
						"          ROW_STAT_CD as string,",
						"          SRC_SYS_DIM_UID as integer,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          DM_CRETD_TMS as timestamp,",
						"          DM_CRETD_USER_ID as string,",
						"          DM_UPDTD_TMS as timestamp,",
						"          DM_UPDTD_USER_ID as string,",
						"          KYNDRYL_GLBL_ORG as string,",
						"          KYNDRYL_ORG_CD as string,",
						"          KYNDRYL_RGN as string,",
						"          KYNDRYL_RGN_CD as string",
						"     ),",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'GEO_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['GEO_DIM_UID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          GEO_DIM_UID,",
						"          IOT_RGN_CD,",
						"          IOT_RGN_NM,",
						"          IMT_RGN_CD,",
						"          IMT_SRGN_NM,",
						"          SRGN_CD,",
						"          SRGNCTRY_CD,",
						"          SRGNCTRY_NM,",
						"          IOTSORT_NUM,",
						"          GEO_CD,",
						"          GEO_TYP_CD,",
						"          GMR_RGN_CD,",
						"          GMR_RGN_NM,",
						"          IOT_SHORT_NM,",
						"          IMT_SHORT_NM,",
						"          SRC_SYS_DIM_UID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID",
						"     )) ~> Tgtupdgeodimtable",
						"select2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'GEO_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          GEO_DIM_UID,",
						"          IOT_RGN_CD,",
						"          IOT_RGN_NM,",
						"          IMT_RGN_CD,",
						"          IMT_SRGN_NM,",
						"          SRGN_CD,",
						"          SRGNCTRY_CD,",
						"          SRGNCTRY_NM,",
						"          IOTSORT_NUM,",
						"          GEO_CD,",
						"          GEO_TYP_CD,",
						"          GMR_RGN_CD,",
						"          GMR_RGN_NM,",
						"          IOT_SHORT_NM,",
						"          IMT_SHORT_NM,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID",
						"     )) ~> InsTgtGeoDim"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0270_GOVRNNC_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 20-06-2022\nJob Name: DF_BALD0270_GOVRNNC_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						}
					],
					"scriptLines": [
						"parameters{",
						"     current_user as string",
						"}",
						"source(output(",
						"          GOVRNNC_DIM_UID as integer,",
						"          PRJCT_ID as string,",
						"          GOVRNNC_REM_TXT as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer,",
						"          ORIG_ORG as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     GVN.PROC_ID as GOVRNNC_DIM_UID,\\n     GVN.PROJECT_ID as PRJCT_ID,\\n     cast(GVN.REMARKS as varchar(4096)) as GOVRNNC_REM_TXT,\\n     GVN.CREATED_TS as SRC_CRETD_TMS,\\n     GVN.CREATED_USERID as SRC_CRETD_USER_ID,\\n     GVN.UPDATED_TS as SRC_UPDTD_TMS,\\n     GVN.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED,\\n     GVN.ORIG_ORG\\nFrom\\n     APPFUN.GOVERNANCE GVN\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on GVN.PROC_ID = ZDT.PROC_ID\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on GVN.PROC_ID = PDEL.PROC_ID\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD0280_GOVRNNC_DIM\\'\\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'\\n     ) SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          LKP_GOVRNNC_DIM_UID as integer,",
						"          LKP_PRJCT_ID as string,",
						"          LKP_GOVRNNC_REM_TXT as string,",
						"          LKP_SRC_CRETD_TMS as timestamp,",
						"          LKP_SRC_CRETD_USER_ID as string,",
						"          LKP_SRC_UPDTD_TMS as timestamp,",
						"          LKP_SRC_UPDTD_USER_ID as string,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_ETL_JOB_ID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     S.GOVRNNC_DIM_UID as LKP_GOVRNNC_DIM_UID,\\n     S.PRJCT_ID as LKP_PRJCT_ID,\\n     S.GOVRNNC_REM_TXT as LKP_GOVRNNC_REM_TXT,\\n     S.SRC_CRETD_TMS as LKP_SRC_CRETD_TMS,\\n     S.SRC_CRETD_USER_ID as LKP_SRC_CRETD_USER_ID,\\n     S.SRC_UPDTD_TMS as LKP_SRC_UPDTD_TMS,\\n     S.SRC_UPDTD_USER_ID as LKP_SRC_UPDTD_USER_ID,\\n     S.SRC_SYS_DIM_UID as LKP_SRC_SYS_DIM_UID,\\n     S.ETL_JOB_ID as LKP_ETL_JOB_ID\\nFrom\\n     PGMPDM.GOVRNNC_DIM S\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on S.GOVRNNC_DIM_UID = ZDT.PROC_ID     ',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"getLookupData sort(asc(LKP_GOVRNNC_DIM_UID, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"mergedData split(GOVRNNC_DIM_UID==LKP_GOVRNNC_DIM_UID,",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(GOVRNNC_DIM_UID == LKP_GOVRNNC_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          GOVRNNC_DIM_UID,",
						"          PRJCT_ID,",
						"          GOVRNNC_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ORIG_ORG,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'U'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'I'),",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          GOVRNNC_DIM_UID,",
						"          PRJCT_ID,",
						"          GOVRNNC_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ORIG_ORG,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'GOVRNNC_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['GOVRNNC_DIM_UID'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'GOVRNNC_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0280_IBMCOMMS_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job creation:29-06-2022\nJob name: DF_BALD0280_IBMCOMMS_DIM\nCreatedBy: Varaprasad\n\nThis job will pull all GEO_TMF and loading into PGMPDM.GEO_DIM table",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "srcappfunibmcomms"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "srtdtgtibmcommlkp"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "Tgtupdibmcommdimtable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "InsTgtibmcommsDim"
						}
					],
					"transformations": [
						{
							"name": "mergeddata"
						},
						{
							"name": "srtdtgtIbmcomm"
						},
						{
							"name": "split"
						},
						{
							"name": "rowstat"
						},
						{
							"name": "select1"
						},
						{
							"name": "alterRow1"
						},
						{
							"name": "rowstatIns"
						},
						{
							"name": "select2"
						}
					],
					"scriptLines": [
						"source(output(",
						"          IBMCOMMS_DIM_UID as integer,",
						"          PRJCT_ID as string,",
						"          IBMCOMMS_REM_TXT as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer,",
						"          ORIG_ORG as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n     ICM.PROC_ID as IBMCOMMS_DIM_UID,\\n     ICM.PROJECT_ID as PRJCT_ID,\\n     cast(ICM.REMARKS as varchar(1024)) as IBMCOMMS_REM_TXT,\\n     ICM.CREATED_TS as SRC_CRETD_TMS,\\n     ICM.CREATED_USERID as SRC_CRETD_USER_ID,\\n     ICM.UPDATED_TS as SRC_UPDTD_TMS,\\n     ICM.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED,ICM.ORIG_ORG\\nfrom\\n     APPFUN.IBMCOMMS ICM\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on ICM.PROC_ID = ZDT.PROC_ID\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on ICM.PROC_ID = PDEL.PROC_ID\\n     left join\\n     (select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          from\\n               PGMPDM.ZAUX_ETL_JOBS\\n          where\\n               ETL_JOB_NM = \\'#DSJobName#\\')JOB\\n     on 1 = 1\\n     left join\\n     (select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          where\\n               IS_CURR_IND = \\'Y\\')FIL\\n     on 1 = 1\\n     left join\\n(select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          from\\n               PGMPDM.SRC_SYS_DIM\\n          where\\n               SRC_SYS_CD = \\'PGMP\\')     SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> srcappfunibmcomms",
						"source(output(",
						"          LKP_IBMCOMMS_DIM_UID as integer,",
						"          LKP_PRJCT_ID as string,",
						"          LKP_IBMCOMMS_REM_TXT as string,",
						"          LKP_SRC_CRETD_TMS as timestamp,",
						"          LKP_SRC_CRETD_USER_ID as string,",
						"          LKP_SRC_UPDTD_TMS as timestamp,",
						"          LKP_SRC_UPDTD_USER_ID as string,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_ETL_JOB_ID as integer,",
						"          LKP_ORIG_ORG as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n     S.IBMCOMMS_DIM_UID  AS LKP_IBMCOMMS_DIM_UID ,\\n     S.PRJCT_ID AS LKP_PRJCT_ID ,\\n     S.IBMCOMMS_REM_TXT AS LKP_IBMCOMMS_REM_TXT ,\\n     S.SRC_CRETD_TMS AS LKP_SRC_CRETD_TMS ,\\n     S.SRC_CRETD_USER_ID AS LKP_SRC_CRETD_USER_ID ,\\n     S.SRC_UPDTD_TMS AS LKP_SRC_UPDTD_TMS ,\\n     S.SRC_UPDTD_USER_ID AS LKP_SRC_UPDTD_USER_ID ,\\n     S.SRC_SYS_DIM_UID AS LKP_SRC_SYS_DIM_UID ,\\n     S.ETL_JOB_ID AS LKP_ETL_JOB_ID ,\\n     S.ORIG_ORG AS LKP_ORIG_ORG\\nfrom\\n     PGMPDM.IBMCOMMS_DIM S\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on S.IBMCOMMS_DIM_UID = ZDT.PROC_ID     ',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> srtdtgtibmcommlkp",
						"srcappfunibmcomms, srtdtgtIbmcomm join(IBMCOMMS_DIM_UID == LKP_IBMCOMMS_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergeddata",
						"srtdtgtibmcommlkp sort(asc(LKP_IBMCOMMS_DIM_UID, true)) ~> srtdtgtIbmcomm",
						"mergeddata split(IBMCOMMS_DIM_UID==LKP_IBMCOMMS_DIM_UID,",
						"     disjoint: false) ~> split@(datatobeupdted, datatobeinserted)",
						"split@datatobeupdted derive(DM_CRETD_USER_ID = \"dsadm\",",
						"          DM_UPDTD_USER_ID = \"dsadm\",",
						"          ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'U'),",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> rowstat",
						"rowstat select(mapColumn(",
						"          IBMCOMMS_DIM_UID,",
						"          PRJCT_ID,",
						"          IBMCOMMS_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          IS_DELETED,",
						"          ORIG_ORG,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          ROW_STAT_CD,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 alterRow(updateIf(true())) ~> alterRow1",
						"split@datatobeinserted derive(DM_CRETD_USER_ID = \"dsadm\",",
						"          DM_UPDTD_USER_ID = \"dsadm\",",
						"          ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'I'),",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> rowstatIns",
						"rowstatIns select(mapColumn(",
						"          IBMCOMMS_DIM_UID,",
						"          PRJCT_ID,",
						"          IBMCOMMS_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          IS_DELETED,",
						"          ORIG_ORG,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select2",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          GEO_DIM_UID as integer,",
						"          IOT_RGN_CD as string,",
						"          IOT_RGN_NM as string,",
						"          IMT_RGN_CD as string,",
						"          IMT_SRGN_NM as string,",
						"          SRGN_CD as string,",
						"          SRGNCTRY_CD as string,",
						"          SRGNCTRY_NM as string,",
						"          IOTSORT_NUM as integer,",
						"          GEO_CD as string,",
						"          GEO_TYP_CD as string,",
						"          GMR_RGN_CD as string,",
						"          GMR_RGN_NM as string,",
						"          IOT_SHORT_NM as string,",
						"          IMT_SHORT_NM as string,",
						"          ROW_STAT_CD as string,",
						"          SRC_SYS_DIM_UID as integer,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          DM_CRETD_TMS as timestamp,",
						"          DM_CRETD_USER_ID as string,",
						"          DM_UPDTD_TMS as timestamp,",
						"          DM_UPDTD_USER_ID as string,",
						"          KYNDRYL_GLBL_ORG as string,",
						"          KYNDRYL_ORG_CD as string,",
						"          KYNDRYL_RGN as string,",
						"          KYNDRYL_RGN_CD as string",
						"     ),",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'IBMCOMMS_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['IBMCOMMS_DIM_UID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Tgtupdibmcommdimtable",
						"select2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'IBMCOMMS_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> InsTgtibmcommsDim"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0290_ISSUE_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job creation:29-06-2022\nJob name: DF_BALD0290_ISSUE_DIM\nCreatedBy: Varaprasad\n\n",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "srcissuedim"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "srtgtissuedimlkp"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "srcissuesmpl"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "Tgtupdibmcommdimtable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "InsTgtibmcommsDim"
						}
					],
					"transformations": [
						{
							"name": "mergeddata"
						},
						{
							"name": "srtdtgtIbmcomm"
						},
						{
							"name": "split"
						},
						{
							"name": "rowstat"
						},
						{
							"name": "select1"
						},
						{
							"name": "alterRow1"
						},
						{
							"name": "rowstatIns"
						},
						{
							"name": "select2"
						},
						{
							"name": "union2"
						}
					],
					"scriptLines": [
						"source(output(",
						"          ISSUE_DIM_UID as integer,",
						"          PRJCT_ID as string,",
						"          ISSUE_REM_TXT as string,",
						"          RESOLUTION_txt as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer,",
						"          ORIG_ORG as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n     ISS.PROC_ID as ISSUE_DIM_UID,\\n     ISS.PROJECT_ID as PRJCT_ID,\\n     cast(ISS.REMARKS as varchar(1024)) as ISSUE_REM_TXT,\\n     --SYSIBM.ENCRYPT(APPFUN.DECRYPTDATA(ISS.ISSUE_OWNER, PWD.MIS_REP_REF_CD), PWD.MIS_REP_REF_CD) as ISSUE_OWNR_TXT,\\n  \\n     CAST(ISS.RESOLUTION as varchar(7024)) as RESOLUTION_txt,\\n\\n     ISS.CREATED_TS as SRC_CRETD_TMS,\\n     ISS.CREATED_USERID as SRC_CRETD_USER_ID,\\n     ISS.UPDATED_TS as SRC_UPDTD_TMS,\\n     ISS.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED,\\ncast (ISS.ORIG_ORG as varchar(200)) as ORIG_ORG\\n\\nfrom\\n     APPFUN.ISSUE ISS\\n     cross join\\n     (select\\n               MIS_REP_REF_CD\\n          from\\n               PGMPDM.MISC_REP_REF\\n          where\\n               MIS_REP_REF_UID = 3     )PWD\\n                inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on ISS.PROC_ID = ZDT.PROC_ID\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on ISS.PROC_ID = PDEL.PROC_ID\\n     left join\\n     (select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          from\\n               PGMPDM.ZAUX_ETL_JOBS\\n          where\\n               ETL_JOB_NM = \\'#DSJobName#\\')JOB\\n     on 1 = 1\\n     left join\\n     (select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          where\\n               IS_CURR_IND = \\'Y\\')FIL\\n     on 1 = 1\\n     left join\\n     (select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          from\\n               PGMPDM.SRC_SYS_DIM\\n          where\\n               SRC_SYS_CD = \\'PGMP\\'     )SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> srcissuedim",
						"source(output(",
						"          LKP_ISSUE_DIM_UID as integer,",
						"          LKP_PRJCT_ID as string,",
						"          LKP_ISSUE_REM_TXT as string,",
						"          LKP_ISSUE_OWNR_TXT as binary,",
						"          LKP_RESOLUTION_TXT as string,",
						"          LKP_SRC_CRETD_TMS as timestamp,",
						"          LKP_SRC_CRETD_USER_ID as string,",
						"          LKP_SRC_UPDTD_TMS as timestamp,",
						"          LKP_SRC_UPDTD_USER_ID as string,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_ETL_JOB_ID as integer,",
						"          LKP_ORIG_ORG as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n     S.ISSUE_DIM_UID AS LKP_ISSUE_DIM_UID,\\n     S.PRJCT_ID AS LKP_PRJCT_ID,\\n     S.ISSUE_REM_TXT AS LKP_ISSUE_REM_TXT,\\n     S.ISSUE_OWNR_TXT AS LKP_ISSUE_OWNR_TXT,\\n     S.RESOLUTION_TXT AS LKP_RESOLUTION_TXT,\\n     S.SRC_CRETD_TMS AS LKP_SRC_CRETD_TMS,\\n     S.SRC_CRETD_USER_ID AS LKP_SRC_CRETD_USER_ID,\\n     S.SRC_UPDTD_TMS AS LKP_SRC_UPDTD_TMS,\\n     S.SRC_UPDTD_USER_ID AS LKP_SRC_UPDTD_USER_ID,\\n     S.SRC_SYS_DIM_UID AS LKP_SRC_SYS_DIM_UID,\\n     S.ETL_JOB_ID AS LKP_ETL_JOB_ID,\\n     S.ORIG_ORG  AS LKP_ORIG_ORG\\nfrom\\n     PGMPDM.ISSUE_DIM S\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on S.ISSUE_DIM_UID = ZDT.PROC_ID',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> srtgtissuedimlkp",
						"source(output(",
						"          ISSUE_DIM_UID as integer,",
						"          PRJCT_ID as string,",
						"          ISSUE_REM_TXT as string,",
						"          RESOLUTION_TXT as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer,",
						"          ORIG_ORG as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: '\\n\\nselect\\n     ISS.PROC_ID as ISSUE_DIM_UID,\\n     \\'\\' as PRJCT_ID,\\n     \\'\\'as ISSUE_REM_TXT,\\n     --SYSIBM.ENCRYPT(APPFUN.DECRYPTDATA(ISS.ISSUE_OWNER, PWD.MIS_REP_REF_CD), PWD.MIS_REP_REF_CD) as ISSUE_OWNR_TXT,\\n                      cast(ISS.RESOLUTION as varchar(7000)) as RESOLUTION_TXT,\\n     ISS.CREATED_TS as SRC_CRETD_TMS,\\n     ISS.CREATED_USERID as SRC_CRETD_USER_ID,\\n     ISS.UPDATED_TS as SRC_UPDTD_TMS,\\n     ISS.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED,\\n                    \\'NA\\' as ORIG_ORG\\n     from\\n     APPFUN.ISSUE_SMPL ISS\\n     cross join\\n     (select\\n               MIS_REP_REF_CD\\n          from\\n               PGMPDM.MISC_REP_REF\\n          where\\n               MIS_REP_REF_UID = 3     )PWD\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on ISS.PROC_ID = ZDT.PROC_ID\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on ISS.PROC_ID = PDEL.PROC_ID\\n     left join\\n     (select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          from\\n               PGMPDM.ZAUX_ETL_JOBS\\n          where\\n               ETL_JOB_NM = \\'#DSJobName#\\'     )JOB\\n     on 1 = 1\\n     left join\\n     (select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          where\\n               IS_CURR_IND = \\'Y\\')FIL\\n     on 1 = 1\\n     left join\\n     (select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          from\\n               PGMPDM.SRC_SYS_DIM\\n          where\\n               SRC_SYS_CD = \\'PGMP\\'          )SYS\\n     on 1 = 1\\n\\n',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> srcissuesmpl",
						"union2, srtdtgtIbmcomm join(ISSUE_DIM_UID == LKP_ISSUE_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergeddata",
						"srtgtissuedimlkp sort(asc(LKP_ISSUE_DIM_UID, true)) ~> srtdtgtIbmcomm",
						"mergeddata split(ISSUE_DIM_UID==LKP_ISSUE_DIM_UID,",
						"     disjoint: false) ~> split@(datatobeupdted, datatobeinserted)",
						"split@datatobeupdted derive(DM_CRETD_USER_ID = \"dsadm\",",
						"          DM_UPDTD_USER_ID = \"dsadm\",",
						"          ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'U'),",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> rowstat",
						"rowstat select(mapColumn(",
						"          ISSUE_DIM_UID,",
						"          PRJCT_ID,",
						"          ISSUE_REM_TXT,",
						"          RESOLUTION_txt = split@datatobeupdted@RESOLUTION_txt,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          IS_DELETED,",
						"          ORIG_ORG,",
						"          RESOLUTION_TXT = split@datatobeupdted@RESOLUTION_TXT,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          ROW_STAT_CD,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 alterRow(updateIf(true())) ~> alterRow1",
						"split@datatobeinserted derive(DM_CRETD_USER_ID = \"dsadm\",",
						"          DM_UPDTD_USER_ID = \"dsadm\",",
						"          ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'I'),",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> rowstatIns",
						"rowstatIns select(mapColumn(",
						"          ISSUE_DIM_UID,",
						"          PRJCT_ID,",
						"          ISSUE_REM_TXT,",
						"          RESOLUTION_txt = split@datatobeinserted@RESOLUTION_txt,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          IS_DELETED,",
						"          ORIG_ORG,",
						"          RESOLUTION_TXT = split@datatobeinserted@RESOLUTION_TXT,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select2",
						"srcissuedim, srcissuesmpl union(byName: true)~> union2",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          GEO_DIM_UID as integer,",
						"          IOT_RGN_CD as string,",
						"          IOT_RGN_NM as string,",
						"          IMT_RGN_CD as string,",
						"          IMT_SRGN_NM as string,",
						"          SRGN_CD as string,",
						"          SRGNCTRY_CD as string,",
						"          SRGNCTRY_NM as string,",
						"          IOTSORT_NUM as integer,",
						"          GEO_CD as string,",
						"          GEO_TYP_CD as string,",
						"          GMR_RGN_CD as string,",
						"          GMR_RGN_NM as string,",
						"          IOT_SHORT_NM as string,",
						"          IMT_SHORT_NM as string,",
						"          ROW_STAT_CD as string,",
						"          SRC_SYS_DIM_UID as integer,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          DM_CRETD_TMS as timestamp,",
						"          DM_CRETD_USER_ID as string,",
						"          DM_UPDTD_TMS as timestamp,",
						"          DM_UPDTD_USER_ID as string,",
						"          KYNDRYL_GLBL_ORG as string,",
						"          KYNDRYL_ORG_CD as string,",
						"          KYNDRYL_RGN as string,",
						"          KYNDRYL_RGN_CD as string",
						"     ),",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'ISSUE_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['ISSUE_DIM_UID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Tgtupdibmcommdimtable",
						"select2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'ISSUE_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> InsTgtibmcommsDim"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0291_PROC_CSTM_IBM_DATA_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job creation:29-06-2022\nJob name: DF_BALD0291_PROC_CSTM_IBM_DATA_DIM\nCreatedBy: Varaprasad\n\n",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "srcproccstmpublicdata"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "tgtproccstmibmdatalkp"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "Tgtupdibmcommdimtable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "InsTgtibmcommsDim"
						}
					],
					"transformations": [
						{
							"name": "mergeddata"
						},
						{
							"name": "srtdtgtIbmcomm"
						},
						{
							"name": "split"
						},
						{
							"name": "select1"
						},
						{
							"name": "alterRow1"
						},
						{
							"name": "select2"
						}
					],
					"scriptLines": [
						"source(output(",
						"          PROC_DIM_UID as integer,",
						"          IBM_ONLY_TEXT_1 as string,",
						"          IBM_ONLY_TEXT_2 as string,",
						"          IBM_ONLY_TEXT_3 as string,",
						"          IBM_ONLY_TEXT_4 as string,",
						"          IBM_ONLY_TEXT_5 as string,",
						"          IBM_ONLY_TEXT_6 as string,",
						"          IBM_ONLY_TEXT_7 as string,",
						"          IBM_ONLY_TEXT_8 as string,",
						"          IBM_ONLY_TEXT_9 as string,",
						"          IBM_ONLY_TEXT_10 as string,",
						"          IBM_ONLY_TEXT_11 as string,",
						"          IBM_ONLY_TEXT_12 as string,",
						"          IBM_ONLY_TEXT_13 as string,",
						"          IBM_ONLY_TEXT_14 as string,",
						"          IBM_ONLY_TEXT_15 as string,",
						"          IBM_ONLY_TEXT_16 as string,",
						"          IBM_ONLY_TEXT_17 as string,",
						"          IBM_ONLY_TEXT_18 as string,",
						"          IBM_ONLY_TEXT_19 as string,",
						"          IBM_ONLY_TEXT_20 as string,",
						"          IBM_ONLY_TEXT_21 as string,",
						"          IBM_ONLY_TEXT_22 as string,",
						"          IBM_ONLY_TEXT_23 as string,",
						"          IBM_ONLY_TEXT_24 as string,",
						"          IBM_ONLY_TEXT_25 as string,",
						"          IBM_ONLY_DATE_1 as date,",
						"          IBM_ONLY_DATE_2 as date,",
						"          IBM_ONLY_DATE_3 as date,",
						"          IBM_ONLY_DATE_4 as date,",
						"          IBM_ONLY_DATE_5 as date,",
						"          IBM_ONLY_DATE_6 as date,",
						"          IBM_ONLY_DATE_7 as date,",
						"          IBM_ONLY_DATE_8 as date,",
						"          IBM_ONLY_DATE_9 as date,",
						"          IBM_ONLY_DATE_10 as date,",
						"          IBM_ONLY_DATE_11 as date,",
						"          IBM_ONLY_DATE_12 as date,",
						"          IBM_ONLY_DATE_13 as date,",
						"          IBM_ONLY_DATE_14 as date,",
						"          IBM_ONLY_DATE_15 as date,",
						"          CLIENT_ONLY_TEXT_1 as string,",
						"          CLIENT_ONLY_TEXT_2 as string,",
						"          CLIENT_ONLY_TEXT_3 as string,",
						"          CLIENT_ONLY_TEXT_4 as string,",
						"          CLIENT_ONLY_TEXT_5 as string,",
						"          CLIENT_ONLY_TEXT_6 as string,",
						"          CLIENT_ONLY_DATE_1 as date,",
						"          CLIENT_ONLY_DATE_2 as date,",
						"          CLIENT_ONLY_DATE_3 as date,",
						"          CLIENT_ONLY_DATE_4 as date,",
						"          CLIENT_ONLY_DATE_5 as date,",
						"          CLIENT_ONLY_DATE_6 as date,",
						"          IBM_ONLY_TEXT_26 as string,",
						"          IBM_ONLY_TEXT_27 as string,",
						"          CLIENT_ONLY_TEXT_7 as string,",
						"          CLIENT_ONLY_TEXT_8 as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'SELECT \\nR.PROC_ID AS PROC_DIM_UID,\\nIBM_ONLY_TEXT_1,\\nIBM_ONLY_TEXT_2,\\nIBM_ONLY_TEXT_3,\\nIBM_ONLY_TEXT_4,\\nIBM_ONLY_TEXT_5,\\nIBM_ONLY_TEXT_6,\\nIBM_ONLY_TEXT_7,IBM_ONLY_TEXT_8,IBM_ONLY_TEXT_9,IBM_ONLY_TEXT_10,IBM_ONLY_TEXT_11,IBM_ONLY_TEXT_12,IBM_ONLY_TEXT_13,\\nIBM_ONLY_TEXT_14,IBM_ONLY_TEXT_15,IBM_ONLY_TEXT_16,IBM_ONLY_TEXT_17,IBM_ONLY_TEXT_18,IBM_ONLY_TEXT_19,IBM_ONLY_TEXT_20,\\nIBM_ONLY_TEXT_21,IBM_ONLY_TEXT_22,IBM_ONLY_TEXT_23,IBM_ONLY_TEXT_24,IBM_ONLY_TEXT_25,IBM_ONLY_DATE_1,IBM_ONLY_DATE_2,\\nIBM_ONLY_DATE_3,IBM_ONLY_DATE_4,IBM_ONLY_DATE_5,IBM_ONLY_DATE_6,IBM_ONLY_DATE_7,IBM_ONLY_DATE_8,IBM_ONLY_DATE_9,\\nIBM_ONLY_DATE_10,IBM_ONLY_DATE_11,IBM_ONLY_DATE_12,IBM_ONLY_DATE_13,IBM_ONLY_DATE_14,IBM_ONLY_DATE_15,CLIENT_ONLY_TEXT_1,\\nCLIENT_ONLY_TEXT_2,CLIENT_ONLY_TEXT_3,CLIENT_ONLY_TEXT_4,CLIENT_ONLY_TEXT_5,CLIENT_ONLY_TEXT_6,CLIENT_ONLY_DATE_1,CLIENT_ONLY_DATE_2,CLIENT_ONLY_DATE_3,CLIENT_ONLY_DATE_4,CLIENT_ONLY_DATE_5,CLIENT_ONLY_DATE_6,IBM_ONLY_TEXT_26,\\nIBM_ONLY_TEXT_27,\\nCLIENT_ONLY_TEXT_7,\\nCLIENT_ONLY_TEXT_8\\nFROM \\nAPPFUN.CSTM_DATA_IBM_CLIENT R\\ninner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS TGR\\n     on R.PROC_ID = TGR.PROC_ID',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> srcproccstmpublicdata",
						"source(output(",
						"          LKP_PROC_DIM_UID as integer,",
						"          LKP_IBM_ONLY_TEXT_1 as string,",
						"          LKP_IBM_ONLY_TEXT_2 as string,",
						"          LKP_IBM_ONLY_TEXT_3 as string,",
						"          LKP_IBM_ONLY_TEXT_4 as string,",
						"          LKP_IBM_ONLY_TEXT_5 as string,",
						"          LKP_IBM_ONLY_TEXT_6 as string,",
						"          LKP_IBM_ONLY_TEXT_7 as string,",
						"          LKP_IBM_ONLY_TEXT_8 as string,",
						"          LKP_IBM_ONLY_TEXT_9 as string,",
						"          LKP_IBM_ONLY_TEXT_10 as string,",
						"          LKP_IBM_ONLY_TEXT_11 as string,",
						"          LKP_IBM_ONLY_TEXT_12 as string,",
						"          LKP_IBM_ONLY_TEXT_13 as string,",
						"          LKP_IBM_ONLY_TEXT_14 as string,",
						"          LKP_IBM_ONLY_TEXT_15 as string,",
						"          LKP_IBM_ONLY_TEXT_16 as string,",
						"          LKP_IBM_ONLY_TEXT_17 as string,",
						"          LKP_IBM_ONLY_TEXT_18 as string,",
						"          LKP_IBM_ONLY_TEXT_19 as string,",
						"          LKP_IBM_ONLY_TEXT_20 as string,",
						"          LKP_IBM_ONLY_TEXT_21 as string,",
						"          LKP_IBM_ONLY_TEXT_22 as string,",
						"          LKP_IBM_ONLY_TEXT_23 as string,",
						"          LKP_IBM_ONLY_TEXT_24 as string,",
						"          LKP_IBM_ONLY_TEXT_25 as string,",
						"          LKP_IBM_ONLY_DATE_1 as date,",
						"          LKP_IBM_ONLY_DATE_2 as date,",
						"          LKP_IBM_ONLY_DATE_3 as date,",
						"          LKP_IBM_ONLY_DATE_4 as date,",
						"          LKP_IBM_ONLY_DATE_5 as date,",
						"          LKP_IBM_ONLY_DATE_6 as date,",
						"          LKP_IBM_ONLY_DATE_7 as date,",
						"          LKP_IBM_ONLY_DATE_8 as date,",
						"          LKP_IBM_ONLY_DATE_9 as date,",
						"          LKP_IBM_ONLY_DATE_10 as date,",
						"          LKP_IBM_ONLY_DATE_11 as date,",
						"          LKP_IBM_ONLY_DATE_12 as date,",
						"          LKP_IBM_ONLY_DATE_13 as date,",
						"          LKP_IBM_ONLY_DATE_14 as date,",
						"          LKP_IBM_ONLY_DATE_15 as date,",
						"          LKP_CLIENT_ONLY_TEXT_1 as string,",
						"          LKP_CLIENT_ONLY_TEXT_2 as string,",
						"          LKP_CLIENT_ONLY_TEXT_3 as string,",
						"          LKP_CLIENT_ONLY_TEXT_4 as string,",
						"          LKP_CLIENT_ONLY_TEXT_5 as string,",
						"          LKP_CLIENT_ONLY_TEXT_6 as string,",
						"          LKP_CLIENT_ONLY_DATE_1 as date,",
						"          LKP_CLIENT_ONLY_DATE_2 as date,",
						"          LKP_CLIENT_ONLY_DATE_3 as date,",
						"          LKP_CLIENT_ONLY_DATE_4 as date,",
						"          LKP_CLIENT_ONLY_DATE_5 as date,",
						"          LKP_CLIENT_ONLY_DATE_6 as date,",
						"          LKP_IBM_ONLY_TEXT_26 as string,",
						"          LKP_IBM_ONLY_TEXT_27 as string,",
						"          LKP_CLIENT_ONLY_TEXT_7 as string,",
						"          LKP_CLIENT_ONLY_TEXT_8 as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'SELECT \\nR.PROC_DIM_UID AS LKP_PROC_DIM_UID,\\nIBM_ONLY_TEXT_1    AS    LKP_IBM_ONLY_TEXT_1    ,\\nIBM_ONLY_TEXT_2    AS    LKP_IBM_ONLY_TEXT_2    ,\\nIBM_ONLY_TEXT_3    AS    LKP_IBM_ONLY_TEXT_3    ,\\nIBM_ONLY_TEXT_4    AS    LKP_IBM_ONLY_TEXT_4    ,\\nIBM_ONLY_TEXT_5    AS    LKP_IBM_ONLY_TEXT_5    ,\\nIBM_ONLY_TEXT_6    AS    LKP_IBM_ONLY_TEXT_6    ,\\nIBM_ONLY_TEXT_7    AS    LKP_IBM_ONLY_TEXT_7    ,\\nIBM_ONLY_TEXT_8    AS    LKP_IBM_ONLY_TEXT_8    ,\\nIBM_ONLY_TEXT_9    AS    LKP_IBM_ONLY_TEXT_9    ,\\nIBM_ONLY_TEXT_10   AS    LKP_IBM_ONLY_TEXT_10   ,\\nIBM_ONLY_TEXT_11   AS    LKP_IBM_ONLY_TEXT_11   ,\\nIBM_ONLY_TEXT_12   AS    LKP_IBM_ONLY_TEXT_12   ,\\nIBM_ONLY_TEXT_13   AS    LKP_IBM_ONLY_TEXT_13   ,\\nIBM_ONLY_TEXT_14   AS    LKP_IBM_ONLY_TEXT_14   ,\\nIBM_ONLY_TEXT_15   AS    LKP_IBM_ONLY_TEXT_15   ,\\nIBM_ONLY_TEXT_16   AS    LKP_IBM_ONLY_TEXT_16   ,\\nIBM_ONLY_TEXT_17   AS    LKP_IBM_ONLY_TEXT_17   ,\\nIBM_ONLY_TEXT_18   AS    LKP_IBM_ONLY_TEXT_18   ,\\nIBM_ONLY_TEXT_19   AS    LKP_IBM_ONLY_TEXT_19   ,\\nIBM_ONLY_TEXT_20   AS    LKP_IBM_ONLY_TEXT_20   ,\\nIBM_ONLY_TEXT_21   AS    LKP_IBM_ONLY_TEXT_21   ,\\nIBM_ONLY_TEXT_22   AS    LKP_IBM_ONLY_TEXT_22   ,\\nIBM_ONLY_TEXT_23   AS    LKP_IBM_ONLY_TEXT_23   ,\\nIBM_ONLY_TEXT_24   AS    LKP_IBM_ONLY_TEXT_24   ,\\nIBM_ONLY_TEXT_25   AS    LKP_IBM_ONLY_TEXT_25   ,\\nIBM_ONLY_DATE_1    AS    LKP_IBM_ONLY_DATE_1    ,\\nIBM_ONLY_DATE_2    AS    LKP_IBM_ONLY_DATE_2    ,\\nIBM_ONLY_DATE_3    AS    LKP_IBM_ONLY_DATE_3    ,\\nIBM_ONLY_DATE_4    AS    LKP_IBM_ONLY_DATE_4    ,\\nIBM_ONLY_DATE_5    AS    LKP_IBM_ONLY_DATE_5    ,\\nIBM_ONLY_DATE_6    AS    LKP_IBM_ONLY_DATE_6    ,\\nIBM_ONLY_DATE_7    AS    LKP_IBM_ONLY_DATE_7    ,\\nIBM_ONLY_DATE_8    AS    LKP_IBM_ONLY_DATE_8    ,\\nIBM_ONLY_DATE_9    AS    LKP_IBM_ONLY_DATE_9    ,\\nIBM_ONLY_DATE_10   AS    LKP_IBM_ONLY_DATE_10   ,\\nIBM_ONLY_DATE_11   AS    LKP_IBM_ONLY_DATE_11   ,\\nIBM_ONLY_DATE_12   AS    LKP_IBM_ONLY_DATE_12   ,\\nIBM_ONLY_DATE_13   AS    LKP_IBM_ONLY_DATE_13   ,\\nIBM_ONLY_DATE_14   AS    LKP_IBM_ONLY_DATE_14   ,\\nIBM_ONLY_DATE_15   AS    LKP_IBM_ONLY_DATE_15   ,\\nCLIENT_ONLY_TEXT_1 AS    LKP_CLIENT_ONLY_TEXT_1 ,\\nCLIENT_ONLY_TEXT_2 AS    LKP_CLIENT_ONLY_TEXT_2 ,\\nCLIENT_ONLY_TEXT_3 AS    LKP_CLIENT_ONLY_TEXT_3 ,\\nCLIENT_ONLY_TEXT_4 AS    LKP_CLIENT_ONLY_TEXT_4 ,\\nCLIENT_ONLY_TEXT_5 AS    LKP_CLIENT_ONLY_TEXT_5 ,\\nCLIENT_ONLY_TEXT_6 AS    LKP_CLIENT_ONLY_TEXT_6 ,\\nCLIENT_ONLY_DATE_1 AS    LKP_CLIENT_ONLY_DATE_1 ,\\nCLIENT_ONLY_DATE_2 AS    LKP_CLIENT_ONLY_DATE_2 ,\\nCLIENT_ONLY_DATE_3 AS    LKP_CLIENT_ONLY_DATE_3 ,\\nCLIENT_ONLY_DATE_4 AS    LKP_CLIENT_ONLY_DATE_4 ,\\nCLIENT_ONLY_DATE_5 AS    LKP_CLIENT_ONLY_DATE_5 ,\\nCLIENT_ONLY_DATE_6 AS    LKP_CLIENT_ONLY_DATE_6 ,\\nIBM_ONLY_TEXT_26   AS    LKP_IBM_ONLY_TEXT_26   ,\\nIBM_ONLY_TEXT_27   AS    LKP_IBM_ONLY_TEXT_27   ,\\nCLIENT_ONLY_TEXT_7 AS    LKP_CLIENT_ONLY_TEXT_7 ,\\nCLIENT_ONLY_TEXT_8 AS    LKP_CLIENT_ONLY_TEXT_8\\nFROM \\nPGMPDM.PROC_CSTM_DATA_IBM_CLIENT_DIM R\\ninner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS TGR\\n     on R.PROC_DIM_UID = TGR.PROC_ID',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> tgtproccstmibmdatalkp",
						"srcproccstmpublicdata, srtdtgtIbmcomm join(PROC_DIM_UID == LKP_PROC_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergeddata",
						"tgtproccstmibmdatalkp sort(asc(LKP_PROC_DIM_UID, true)) ~> srtdtgtIbmcomm",
						"mergeddata split(PROC_DIM_UID==LKP_PROC_DIM_UID,",
						"     disjoint: false) ~> split@(datatobeupdted, datatobeinserted)",
						"split@datatobeupdted select(mapColumn(",
						"          PROC_DIM_UID,",
						"          IBM_ONLY_TEXT_1,",
						"          IBM_ONLY_TEXT_2,",
						"          IBM_ONLY_TEXT_3,",
						"          IBM_ONLY_TEXT_4,",
						"          IBM_ONLY_TEXT_5,",
						"          IBM_ONLY_TEXT_6,",
						"          IBM_ONLY_TEXT_7,",
						"          IBM_ONLY_TEXT_8,",
						"          IBM_ONLY_TEXT_9,",
						"          IBM_ONLY_TEXT_10,",
						"          IBM_ONLY_TEXT_11,",
						"          IBM_ONLY_TEXT_12,",
						"          IBM_ONLY_TEXT_13,",
						"          IBM_ONLY_TEXT_14,",
						"          IBM_ONLY_TEXT_15,",
						"          IBM_ONLY_TEXT_16,",
						"          IBM_ONLY_TEXT_17,",
						"          IBM_ONLY_TEXT_18,",
						"          IBM_ONLY_TEXT_19,",
						"          IBM_ONLY_TEXT_20,",
						"          IBM_ONLY_TEXT_21,",
						"          IBM_ONLY_TEXT_22,",
						"          IBM_ONLY_TEXT_23,",
						"          IBM_ONLY_TEXT_24,",
						"          IBM_ONLY_TEXT_25,",
						"          IBM_ONLY_DATE_1,",
						"          IBM_ONLY_DATE_2,",
						"          IBM_ONLY_DATE_3,",
						"          IBM_ONLY_DATE_4,",
						"          IBM_ONLY_DATE_5,",
						"          IBM_ONLY_DATE_6,",
						"          IBM_ONLY_DATE_7,",
						"          IBM_ONLY_DATE_8,",
						"          IBM_ONLY_DATE_9,",
						"          IBM_ONLY_DATE_10,",
						"          IBM_ONLY_DATE_11,",
						"          IBM_ONLY_DATE_12,",
						"          IBM_ONLY_DATE_13,",
						"          IBM_ONLY_DATE_14,",
						"          IBM_ONLY_DATE_15,",
						"          CLIENT_ONLY_TEXT_1,",
						"          CLIENT_ONLY_TEXT_2,",
						"          CLIENT_ONLY_TEXT_3,",
						"          CLIENT_ONLY_TEXT_4,",
						"          CLIENT_ONLY_TEXT_5,",
						"          CLIENT_ONLY_TEXT_6,",
						"          CLIENT_ONLY_DATE_1,",
						"          CLIENT_ONLY_DATE_2,",
						"          CLIENT_ONLY_DATE_3,",
						"          CLIENT_ONLY_DATE_4,",
						"          CLIENT_ONLY_DATE_5,",
						"          CLIENT_ONLY_DATE_6,",
						"          IBM_ONLY_TEXT_26,",
						"          IBM_ONLY_TEXT_27,",
						"          CLIENT_ONLY_TEXT_7,",
						"          CLIENT_ONLY_TEXT_8",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 alterRow(updateIf(true())) ~> alterRow1",
						"split@datatobeinserted select(mapColumn(",
						"          PROC_DIM_UID,",
						"          IBM_ONLY_TEXT_1,",
						"          IBM_ONLY_TEXT_2,",
						"          IBM_ONLY_TEXT_3,",
						"          IBM_ONLY_TEXT_4,",
						"          IBM_ONLY_TEXT_5,",
						"          IBM_ONLY_TEXT_6,",
						"          IBM_ONLY_TEXT_7,",
						"          IBM_ONLY_TEXT_8,",
						"          IBM_ONLY_TEXT_9,",
						"          IBM_ONLY_TEXT_10,",
						"          IBM_ONLY_TEXT_11,",
						"          IBM_ONLY_TEXT_12,",
						"          IBM_ONLY_TEXT_13,",
						"          IBM_ONLY_TEXT_14,",
						"          IBM_ONLY_TEXT_15,",
						"          IBM_ONLY_TEXT_16,",
						"          IBM_ONLY_TEXT_17,",
						"          IBM_ONLY_TEXT_18,",
						"          IBM_ONLY_TEXT_19,",
						"          IBM_ONLY_TEXT_20,",
						"          IBM_ONLY_TEXT_21,",
						"          IBM_ONLY_TEXT_22,",
						"          IBM_ONLY_TEXT_23,",
						"          IBM_ONLY_TEXT_24,",
						"          IBM_ONLY_TEXT_25,",
						"          IBM_ONLY_DATE_1,",
						"          IBM_ONLY_DATE_2,",
						"          IBM_ONLY_DATE_3,",
						"          IBM_ONLY_DATE_4,",
						"          IBM_ONLY_DATE_5,",
						"          IBM_ONLY_DATE_6,",
						"          IBM_ONLY_DATE_7,",
						"          IBM_ONLY_DATE_8,",
						"          IBM_ONLY_DATE_9,",
						"          IBM_ONLY_DATE_10,",
						"          IBM_ONLY_DATE_11,",
						"          IBM_ONLY_DATE_12,",
						"          IBM_ONLY_DATE_13,",
						"          IBM_ONLY_DATE_14,",
						"          IBM_ONLY_DATE_15,",
						"          CLIENT_ONLY_TEXT_1,",
						"          CLIENT_ONLY_TEXT_2,",
						"          CLIENT_ONLY_TEXT_3,",
						"          CLIENT_ONLY_TEXT_4,",
						"          CLIENT_ONLY_TEXT_5,",
						"          CLIENT_ONLY_TEXT_6,",
						"          CLIENT_ONLY_DATE_1,",
						"          CLIENT_ONLY_DATE_2,",
						"          CLIENT_ONLY_DATE_3,",
						"          CLIENT_ONLY_DATE_4,",
						"          CLIENT_ONLY_DATE_5,",
						"          CLIENT_ONLY_DATE_6,",
						"          IBM_ONLY_TEXT_26,",
						"          IBM_ONLY_TEXT_27,",
						"          CLIENT_ONLY_TEXT_7,",
						"          CLIENT_ONLY_TEXT_8",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select2",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          GEO_DIM_UID as integer,",
						"          IOT_RGN_CD as string,",
						"          IOT_RGN_NM as string,",
						"          IMT_RGN_CD as string,",
						"          IMT_SRGN_NM as string,",
						"          SRGN_CD as string,",
						"          SRGNCTRY_CD as string,",
						"          SRGNCTRY_NM as string,",
						"          IOTSORT_NUM as integer,",
						"          GEO_CD as string,",
						"          GEO_TYP_CD as string,",
						"          GMR_RGN_CD as string,",
						"          GMR_RGN_NM as string,",
						"          IOT_SHORT_NM as string,",
						"          IMT_SHORT_NM as string,",
						"          ROW_STAT_CD as string,",
						"          SRC_SYS_DIM_UID as integer,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          DM_CRETD_TMS as timestamp,",
						"          DM_CRETD_USER_ID as string,",
						"          DM_UPDTD_TMS as timestamp,",
						"          DM_UPDTD_USER_ID as string,",
						"          KYNDRYL_GLBL_ORG as string,",
						"          KYNDRYL_ORG_CD as string,",
						"          KYNDRYL_RGN as string,",
						"          KYNDRYL_RGN_CD as string",
						"     ),",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'PROC_CSTM_DATA_IBM_CLIENT_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['PROC_DIM_UID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Tgtupdibmcommdimtable",
						"select2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'PROC_CSTM_DATA_IBM_CLIENT_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> InsTgtibmcommsDim"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0292_PROC_CSTM_DATA_PUBLIC_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job creation:29-06-2022\nJob name: DF_DF_BALD0292_PROC_CSTM_DATA_PUBLIC_DIM\nCreatedBy: Varaprasad\n\n",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "srcproccstmpublicdata"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "tgtproccstmpublicdatalkp"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "Tgtupdibmcommdimtable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "InsTgtibmcommsDim"
						}
					],
					"transformations": [
						{
							"name": "mergeddata"
						},
						{
							"name": "srtdtgtIbmcomm"
						},
						{
							"name": "split"
						},
						{
							"name": "select1"
						},
						{
							"name": "alterRow1"
						},
						{
							"name": "select2"
						}
					],
					"scriptLines": [
						"source(output(",
						"          PROC_DIM_UID as integer,",
						"          PUBLIC_TEXT_1 as string,",
						"          PUBLIC_TEXT_2 as string,",
						"          PUBLIC_TEXT_3 as string,",
						"          PUBLIC_TEXT_4 as string,",
						"          PUBLIC_TEXT_5 as string,",
						"          PUBLIC_TEXT_6 as string,",
						"          PUBLIC_TEXT_7 as string,",
						"          PUBLIC_TEXT_8 as string,",
						"          PUBLIC_TEXT_9 as string,",
						"          PUBLIC_TEXT_10 as string,",
						"          PUBLIC_TEXT_11 as string,",
						"          PUBLIC_TEXT_12 as string,",
						"          PUBLIC_TEXT_13 as string,",
						"          PUBLIC_TEXT_14 as string,",
						"          PUBLIC_TEXT_15 as string,",
						"          PUBLIC_TEXT_16 as string,",
						"          PUBLIC_TEXT_17 as string,",
						"          PUBLIC_TEXT_18 as string,",
						"          PUBLIC_TEXT_19 as string,",
						"          PUBLIC_TEXT_20 as string,",
						"          PUBLIC_TEXT_21 as string,",
						"          PUBLIC_TEXT_22 as string,",
						"          PUBLIC_TEXT_23 as string,",
						"          PUBLIC_TEXT_24 as string,",
						"          PUBLIC_TEXT_25 as string,",
						"          PUBLIC_TEXT_26 as string,",
						"          PUBLIC_TEXT_27 as string,",
						"          PUBLIC_TEXT_28 as string,",
						"          PUBLIC_TEXT_29 as string,",
						"          PUBLIC_TEXT_30 as string,",
						"          PUBLIC_TEXT_31 as string,",
						"          PUBLIC_TEXT_32 as string,",
						"          PUBLIC_TEXT_33 as string,",
						"          PUBLIC_TEXT_34 as string,",
						"          PUBLIC_TEXT_35 as string,",
						"          PUBLIC_DATE_1 as date,",
						"          PUBLIC_DATE_2 as date,",
						"          PUBLIC_DATE_3 as date,",
						"          PUBLIC_DATE_4 as date,",
						"          PUBLIC_DATE_5 as date,",
						"          PUBLIC_DATE_6 as date,",
						"          PUBLIC_DATE_7 as date,",
						"          PUBLIC_DATE_8 as date,",
						"          PUBLIC_DATE_9 as date,",
						"          PUBLIC_DATE_10 as date,",
						"          PUBLIC_DATE_11 as date,",
						"          PUBLIC_DATE_12 as date,",
						"          PUBLIC_DATE_13 as date,",
						"          PUBLIC_DATE_14 as date,",
						"          PUBLIC_DATE_15 as date,",
						"          PUBLIC_TEXT_36 as string,",
						"          PUBLIC_TEXT_37 as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'SELECT R.PROC_ID AS PROC_DIM_UID,\\nPUBLIC_TEXT_1,PUBLIC_TEXT_2,PUBLIC_TEXT_3,\\nPUBLIC_TEXT_4,PUBLIC_TEXT_5,PUBLIC_TEXT_6,\\nPUBLIC_TEXT_7,PUBLIC_TEXT_8,PUBLIC_TEXT_9,\\nPUBLIC_TEXT_10,PUBLIC_TEXT_11,PUBLIC_TEXT_12,\\nPUBLIC_TEXT_13,PUBLIC_TEXT_14,PUBLIC_TEXT_15,\\nPUBLIC_TEXT_16,PUBLIC_TEXT_17,PUBLIC_TEXT_18,\\nPUBLIC_TEXT_19,PUBLIC_TEXT_20,PUBLIC_TEXT_21,\\nPUBLIC_TEXT_22,PUBLIC_TEXT_23,PUBLIC_TEXT_24,\\nPUBLIC_TEXT_25,PUBLIC_TEXT_26,PUBLIC_TEXT_27,\\nPUBLIC_TEXT_28,PUBLIC_TEXT_29,PUBLIC_TEXT_30,\\nPUBLIC_TEXT_31,PUBLIC_TEXT_32,PUBLIC_TEXT_33,\\nPUBLIC_TEXT_34,PUBLIC_TEXT_35,PUBLIC_DATE_1,\\nPUBLIC_DATE_2,PUBLIC_DATE_3,PUBLIC_DATE_4,\\nPUBLIC_DATE_5,PUBLIC_DATE_6,PUBLIC_DATE_7,\\nPUBLIC_DATE_8,PUBLIC_DATE_9,PUBLIC_DATE_10,\\nPUBLIC_DATE_11,PUBLIC_DATE_12,PUBLIC_DATE_13,\\nPUBLIC_DATE_14,PUBLIC_DATE_15,PUBLIC_TEXT_36,\\nPUBLIC_TEXT_37\\nFROM\\n\\nAPPFUN.CSTM_DATA_PUBLIC R\\ninner join     \\nPGMPDM.ZAUX_DATE_TRIGGERS TGR\\n     on R.PROC_ID = TGR.PROC_ID',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> srcproccstmpublicdata",
						"source(output(",
						"          LKP_PROC_DIM_UID as integer,",
						"          LKP_PUBLIC_TEXT_1 as string,",
						"          LKP_PUBLIC_TEXT_2 as string,",
						"          LKP_PUBLIC_TEXT_3 as string,",
						"          LKP_PUBLIC_TEXT_4 as string,",
						"          LKP_PUBLIC_TEXT_5 as string,",
						"          LKP_PUBLIC_TEXT_6 as string,",
						"          LKP_PUBLIC_TEXT_7 as string,",
						"          LKP_PUBLIC_TEXT_8 as string,",
						"          LKP_PUBLIC_TEXT_9 as string,",
						"          LKP_PUBLIC_TEXT_10 as string,",
						"          LKP_PUBLIC_TEXT_11 as string,",
						"          LKP_PUBLIC_TEXT_12 as string,",
						"          LKP_PUBLIC_TEXT_13 as string,",
						"          LKP_PUBLIC_TEXT_14 as string,",
						"          LKP_PUBLIC_TEXT_15 as string,",
						"          LKP_PUBLIC_TEXT_16 as string,",
						"          LKP_PUBLIC_TEXT_17 as string,",
						"          LKP_PUBLIC_TEXT_18 as string,",
						"          LKP_PUBLIC_TEXT_19 as string,",
						"          LKP_PUBLIC_TEXT_20 as string,",
						"          LKP_PUBLIC_TEXT_21 as string,",
						"          LKP_PUBLIC_TEXT_22 as string,",
						"          LKP_PUBLIC_TEXT_23 as string,",
						"          LKP_PUBLIC_TEXT_24 as string,",
						"          LKP_PUBLIC_TEXT_25 as string,",
						"          LKP_PUBLIC_TEXT_26 as string,",
						"          LKP_PUBLIC_TEXT_27 as string,",
						"          LKP_PUBLIC_TEXT_28 as string,",
						"          LKP_PUBLIC_TEXT_29 as string,",
						"          LKP_PUBLIC_TEXT_30 as string,",
						"          LKP_PUBLIC_TEXT_31 as string,",
						"          LKP_PUBLIC_TEXT_32 as string,",
						"          LKP_PUBLIC_TEXT_33 as string,",
						"          LKP_PUBLIC_TEXT_34 as string,",
						"          LKP_PUBLIC_TEXT_35 as string,",
						"          LKP_PUBLIC_DATE_1 as date,",
						"          LKP_PUBLIC_DATE_2 as date,",
						"          LKP_PUBLIC_DATE_3 as date,",
						"          LKP_PUBLIC_DATE_4 as date,",
						"          LKP_PUBLIC_DATE_5 as date,",
						"          LKP_PUBLIC_DATE_6 as date,",
						"          LKP_PUBLIC_DATE_7 as date,",
						"          LKP_PUBLIC_DATE_8 as date,",
						"          LKP_PUBLIC_DATE_9 as date,",
						"          LKP_PUBLIC_DATE_10 as date,",
						"          LKP_PUBLIC_DATE_11 as date,",
						"          LKP_PUBLIC_DATE_12 as date,",
						"          LKP_PUBLIC_DATE_13 as date,",
						"          LKP_PUBLIC_DATE_14 as date,",
						"          LKP_PUBLIC_DATE_15 as date,",
						"          LKP_PUBLIC_TEXT_36 as string,",
						"          LKP_PUBLIC_TEXT_37 as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'SELECT\\nR.PROC_DIM_UID       AS LKP_PROC_DIM_UID,\\nPUBLIC_TEXT_1        AS LKP_PUBLIC_TEXT_1 ,\\nPUBLIC_TEXT_2        AS LKP_PUBLIC_TEXT_2 ,\\nPUBLIC_TEXT_3        AS LKP_PUBLIC_TEXT_3 ,\\nPUBLIC_TEXT_4        AS LKP_PUBLIC_TEXT_4 ,\\nPUBLIC_TEXT_5        AS LKP_PUBLIC_TEXT_5 ,\\nPUBLIC_TEXT_6        AS LKP_PUBLIC_TEXT_6 ,\\nPUBLIC_TEXT_7        AS LKP_PUBLIC_TEXT_7 ,\\nPUBLIC_TEXT_8        AS LKP_PUBLIC_TEXT_8 ,\\nPUBLIC_TEXT_9        AS LKP_PUBLIC_TEXT_9 ,\\nPUBLIC_TEXT_10       AS LKP_PUBLIC_TEXT_10,\\nPUBLIC_TEXT_11       AS LKP_PUBLIC_TEXT_11,\\nPUBLIC_TEXT_12       AS LKP_PUBLIC_TEXT_12,\\nPUBLIC_TEXT_13       AS LKP_PUBLIC_TEXT_13,\\nPUBLIC_TEXT_14       AS LKP_PUBLIC_TEXT_14,\\nPUBLIC_TEXT_15       AS LKP_PUBLIC_TEXT_15,\\nPUBLIC_TEXT_16       AS LKP_PUBLIC_TEXT_16,\\nPUBLIC_TEXT_17       AS LKP_PUBLIC_TEXT_17,\\nPUBLIC_TEXT_18       AS LKP_PUBLIC_TEXT_18,\\nPUBLIC_TEXT_19       AS LKP_PUBLIC_TEXT_19,\\nPUBLIC_TEXT_20       AS LKP_PUBLIC_TEXT_20,\\nPUBLIC_TEXT_21       AS LKP_PUBLIC_TEXT_21,\\nPUBLIC_TEXT_22       AS LKP_PUBLIC_TEXT_22,\\nPUBLIC_TEXT_23       AS LKP_PUBLIC_TEXT_23,\\nPUBLIC_TEXT_24       AS LKP_PUBLIC_TEXT_24,\\nPUBLIC_TEXT_25       AS LKP_PUBLIC_TEXT_25,\\nPUBLIC_TEXT_26       AS LKP_PUBLIC_TEXT_26,\\nPUBLIC_TEXT_27       AS LKP_PUBLIC_TEXT_27,\\nPUBLIC_TEXT_28       AS LKP_PUBLIC_TEXT_28,\\nPUBLIC_TEXT_29       AS LKP_PUBLIC_TEXT_29,\\nPUBLIC_TEXT_30       AS LKP_PUBLIC_TEXT_30,\\nPUBLIC_TEXT_31       AS LKP_PUBLIC_TEXT_31,\\nPUBLIC_TEXT_32       AS LKP_PUBLIC_TEXT_32,\\nPUBLIC_TEXT_33       AS LKP_PUBLIC_TEXT_33,\\nPUBLIC_TEXT_34       AS LKP_PUBLIC_TEXT_34,\\nPUBLIC_TEXT_35       AS LKP_PUBLIC_TEXT_35,\\nPUBLIC_DATE_1        AS LKP_PUBLIC_DATE_1 ,\\nPUBLIC_DATE_2        AS LKP_PUBLIC_DATE_2 ,\\nPUBLIC_DATE_3        AS LKP_PUBLIC_DATE_3 ,\\nPUBLIC_DATE_4        AS LKP_PUBLIC_DATE_4 ,\\nPUBLIC_DATE_5        AS LKP_PUBLIC_DATE_5 ,\\nPUBLIC_DATE_6        AS LKP_PUBLIC_DATE_6 ,\\nPUBLIC_DATE_7        AS LKP_PUBLIC_DATE_7 ,\\nPUBLIC_DATE_8        AS LKP_PUBLIC_DATE_8 ,\\nPUBLIC_DATE_9        AS LKP_PUBLIC_DATE_9 ,\\nPUBLIC_DATE_10       AS LKP_PUBLIC_DATE_10,\\nPUBLIC_DATE_11       AS LKP_PUBLIC_DATE_11,\\nPUBLIC_DATE_12       AS LKP_PUBLIC_DATE_12,\\nPUBLIC_DATE_13       AS LKP_PUBLIC_DATE_13,\\nPUBLIC_DATE_14       AS LKP_PUBLIC_DATE_14,\\nPUBLIC_DATE_15       AS LKP_PUBLIC_DATE_15,\\nPUBLIC_TEXT_36       AS LKP_PUBLIC_TEXT_36,\\nPUBLIC_TEXT_37       AS LKP_PUBLIC_TEXT_37\\nFROM\\n\\nPGMPDM.PROC_CSTM_DATA_PUBLIC_DIM R\\ninner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS TGR\\n     on R.PROC_DIM_UID = TGR.PROC_ID',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> tgtproccstmpublicdatalkp",
						"srcproccstmpublicdata, srtdtgtIbmcomm join(PROC_DIM_UID == LKP_PROC_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergeddata",
						"tgtproccstmpublicdatalkp sort(asc(LKP_PROC_DIM_UID, true)) ~> srtdtgtIbmcomm",
						"mergeddata split(PROC_DIM_UID==LKP_PROC_DIM_UID,",
						"     disjoint: false) ~> split@(datatobeupdted, datatobeinserted)",
						"split@datatobeupdted select(mapColumn(",
						"          PROC_DIM_UID,",
						"          PUBLIC_TEXT_1,",
						"          PUBLIC_TEXT_2,",
						"          PUBLIC_TEXT_3,",
						"          PUBLIC_TEXT_4,",
						"          PUBLIC_TEXT_5,",
						"          PUBLIC_TEXT_6,",
						"          PUBLIC_TEXT_7,",
						"          PUBLIC_TEXT_8,",
						"          PUBLIC_TEXT_9,",
						"          PUBLIC_TEXT_10,",
						"          PUBLIC_TEXT_11,",
						"          PUBLIC_TEXT_12,",
						"          PUBLIC_TEXT_13,",
						"          PUBLIC_TEXT_14,",
						"          PUBLIC_TEXT_15,",
						"          PUBLIC_TEXT_16,",
						"          PUBLIC_TEXT_17,",
						"          PUBLIC_TEXT_18,",
						"          PUBLIC_TEXT_19,",
						"          PUBLIC_TEXT_20,",
						"          PUBLIC_TEXT_21,",
						"          PUBLIC_TEXT_22,",
						"          PUBLIC_TEXT_23,",
						"          PUBLIC_TEXT_24,",
						"          PUBLIC_TEXT_25,",
						"          PUBLIC_TEXT_26,",
						"          PUBLIC_TEXT_27,",
						"          PUBLIC_TEXT_28,",
						"          PUBLIC_TEXT_29,",
						"          PUBLIC_TEXT_30,",
						"          PUBLIC_TEXT_31,",
						"          PUBLIC_TEXT_32,",
						"          PUBLIC_TEXT_33,",
						"          PUBLIC_TEXT_34,",
						"          PUBLIC_TEXT_35,",
						"          PUBLIC_DATE_1,",
						"          PUBLIC_DATE_2,",
						"          PUBLIC_DATE_3,",
						"          PUBLIC_DATE_4,",
						"          PUBLIC_DATE_5,",
						"          PUBLIC_DATE_6,",
						"          PUBLIC_DATE_7,",
						"          PUBLIC_DATE_8,",
						"          PUBLIC_DATE_9,",
						"          PUBLIC_DATE_10,",
						"          PUBLIC_DATE_11,",
						"          PUBLIC_DATE_12,",
						"          PUBLIC_DATE_13,",
						"          PUBLIC_DATE_14,",
						"          PUBLIC_DATE_15,",
						"          PUBLIC_TEXT_36,",
						"          PUBLIC_TEXT_37",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 alterRow(updateIf(true())) ~> alterRow1",
						"split@datatobeinserted select(mapColumn(",
						"          PROC_DIM_UID,",
						"          PUBLIC_TEXT_1,",
						"          PUBLIC_TEXT_2,",
						"          PUBLIC_TEXT_3,",
						"          PUBLIC_TEXT_4,",
						"          PUBLIC_TEXT_5,",
						"          PUBLIC_TEXT_6,",
						"          PUBLIC_TEXT_7,",
						"          PUBLIC_TEXT_8,",
						"          PUBLIC_TEXT_9,",
						"          PUBLIC_TEXT_10,",
						"          PUBLIC_TEXT_11,",
						"          PUBLIC_TEXT_12,",
						"          PUBLIC_TEXT_13,",
						"          PUBLIC_TEXT_14,",
						"          PUBLIC_TEXT_15,",
						"          PUBLIC_TEXT_16,",
						"          PUBLIC_TEXT_17,",
						"          PUBLIC_TEXT_18,",
						"          PUBLIC_TEXT_19,",
						"          PUBLIC_TEXT_20,",
						"          PUBLIC_TEXT_21,",
						"          PUBLIC_TEXT_22,",
						"          PUBLIC_TEXT_23,",
						"          PUBLIC_TEXT_24,",
						"          PUBLIC_TEXT_25,",
						"          PUBLIC_TEXT_26,",
						"          PUBLIC_TEXT_27,",
						"          PUBLIC_TEXT_28,",
						"          PUBLIC_TEXT_29,",
						"          PUBLIC_TEXT_30,",
						"          PUBLIC_TEXT_31,",
						"          PUBLIC_TEXT_32,",
						"          PUBLIC_TEXT_33,",
						"          PUBLIC_TEXT_34,",
						"          PUBLIC_TEXT_35,",
						"          PUBLIC_DATE_1,",
						"          PUBLIC_DATE_2,",
						"          PUBLIC_DATE_3,",
						"          PUBLIC_DATE_4,",
						"          PUBLIC_DATE_5,",
						"          PUBLIC_DATE_6,",
						"          PUBLIC_DATE_7,",
						"          PUBLIC_DATE_8,",
						"          PUBLIC_DATE_9,",
						"          PUBLIC_DATE_10,",
						"          PUBLIC_DATE_11,",
						"          PUBLIC_DATE_12,",
						"          PUBLIC_DATE_13,",
						"          PUBLIC_DATE_14,",
						"          PUBLIC_DATE_15,",
						"          PUBLIC_TEXT_36,",
						"          PUBLIC_TEXT_37",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select2",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          GEO_DIM_UID as integer,",
						"          IOT_RGN_CD as string,",
						"          IOT_RGN_NM as string,",
						"          IMT_RGN_CD as string,",
						"          IMT_SRGN_NM as string,",
						"          SRGN_CD as string,",
						"          SRGNCTRY_CD as string,",
						"          SRGNCTRY_NM as string,",
						"          IOTSORT_NUM as integer,",
						"          GEO_CD as string,",
						"          GEO_TYP_CD as string,",
						"          GMR_RGN_CD as string,",
						"          GMR_RGN_NM as string,",
						"          IOT_SHORT_NM as string,",
						"          IMT_SHORT_NM as string,",
						"          ROW_STAT_CD as string,",
						"          SRC_SYS_DIM_UID as integer,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          DM_CRETD_TMS as timestamp,",
						"          DM_CRETD_USER_ID as string,",
						"          DM_UPDTD_TMS as timestamp,",
						"          DM_UPDTD_USER_ID as string,",
						"          KYNDRYL_GLBL_ORG as string,",
						"          KYNDRYL_ORG_CD as string,",
						"          KYNDRYL_RGN as string,",
						"          KYNDRYL_RGN_CD as string",
						"     ),",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'PROC_CSTM_DATA_PUBLIC_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['PROC_DIM_UID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Tgtupdibmcommdimtable",
						"select2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'PROC_CSTM_DATA_PUBLIC_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> InsTgtibmcommsDim"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0295_L30_OFFRNG_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job creation:29-06-2022\nJob name: DF_BALD0295_L30_OFFRNG_DIM\nCreatedBy: Varaprasad\n\n",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "srcl30offeringdata"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "tgtl30offeringlkp"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "Tgtupdibmcommdimtable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "InsTgtibmcommsDim"
						}
					],
					"transformations": [
						{
							"name": "mergeddata"
						},
						{
							"name": "srtdtgtIbmcomm"
						},
						{
							"name": "split"
						},
						{
							"name": "select1"
						},
						{
							"name": "alterRow1"
						},
						{
							"name": "select2"
						}
					],
					"scriptLines": [
						"source(output(",
						"          MAX_ID as integer,",
						"          ROW_NUM as long,",
						"          OFFRNG_ID as string,",
						"          OFFRNG_DESC as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n     O.MAX_ID as MAX_ID,\\n     O.ROW_NUM as ROW_NUM,\\n     O.OFFRNG_ID,\\n     O.OFFRNG_DESC,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID\\nfrom\\n     (select\\n               (select coalesce(max(L30_OFFRNG_UID), 0) from PGMPDM.L30_OFFRNG_DIM) as MAX_ID,\\n               ROW_NUMBER() over (order by SELLIST.UI_SEL_LIST_ID,SELLIST.MULTILANG_SEL_TEXT_ID  ) as ROW_NUM,\\n               SELLIST.INTERNAL_VAL AS OFFRNG_ID,\\n            T.TEXT AS OFFRNG_DESC\\n        FROM \\n            APPFUN.UI_SEL_LIST_DEF SELLIST, APPFUN.MULTILANG_TEXT_DEF T\\n        WHERE \\n            UI_SEL_LIST_ID = 105000\\n/*Commented for filter the both active and Inactive */\\n       --AND SELLIST.ACTIVE_IND = \\'Y\\'\\n        AND T.MULTILANG_TEXT_ID = SELLIST.MULTILANG_SEL_TEXT_ID\\n        AND T.LANG_ID = 1) O\\n     left join\\n     (     select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          from\\n               PGMPDM.ZAUX_ETL_JOBS\\n          where\\n               ETL_JOB_NM = \\'#DSJobName#\\')JOB\\n     on 1 = 1\\n     left join\\n     (select\\n               ETL_EXCTN_ID\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          where\\n               IS_CURR_IND = \\'Y\\')FIL\\n     on 1 = 1\\n     left join\\n     (select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          from\\n               PGMPDM.SRC_SYS_DIM\\n          where\\n               SRC_SYS_CD = \\'PGMP\\')SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> srcl30offeringdata",
						"source(output(",
						"          LKP_OFFRNG_ID as string,",
						"          LKP_OFFRNG_DESC as string,",
						"          LKP_ETL_JOB_ID as integer,",
						"          LKP_SRC_SYS_DIM_UID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n     OFFRNG_ID  AS LKP_OFFRNG_ID,\\n     OFFRNG_DESC  AS LKP_OFFRNG_DESC,\\n     ETL_JOB_ID  AS LKP_ETL_JOB_ID,\\n     SRC_SYS_DIM_UID  AS LKP_SRC_SYS_DIM_UID\\nfrom\\n     PGMPDM.L30_OFFRNG_DIM',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> tgtl30offeringlkp",
						"srcl30offeringdata, srtdtgtIbmcomm join(OFFRNG_ID == LKP_OFFRNG_ID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergeddata",
						"tgtl30offeringlkp sort(asc(LKP_OFFRNG_ID, true)) ~> srtdtgtIbmcomm",
						"mergeddata split(OFFRNG_ID==LKP_OFFRNG_ID,",
						"     disjoint: false) ~> split@(datatobeupdted, datatobeinserted)",
						"split@datatobeupdted select(mapColumn(",
						"          MAX_ID,",
						"          ROW_NUM,",
						"          OFFRNG_ID,",
						"          OFFRNG_DESC,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 alterRow(updateIf(true())) ~> alterRow1",
						"split@datatobeinserted select(mapColumn(",
						"          MAX_ID,",
						"          ROW_NUM,",
						"          OFFRNG_ID,",
						"          OFFRNG_DESC,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select2",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          GEO_DIM_UID as integer,",
						"          IOT_RGN_CD as string,",
						"          IOT_RGN_NM as string,",
						"          IMT_RGN_CD as string,",
						"          IMT_SRGN_NM as string,",
						"          SRGN_CD as string,",
						"          SRGNCTRY_CD as string,",
						"          SRGNCTRY_NM as string,",
						"          IOTSORT_NUM as integer,",
						"          GEO_CD as string,",
						"          GEO_TYP_CD as string,",
						"          GMR_RGN_CD as string,",
						"          GMR_RGN_NM as string,",
						"          IOT_SHORT_NM as string,",
						"          IMT_SHORT_NM as string,",
						"          ROW_STAT_CD as string,",
						"          SRC_SYS_DIM_UID as integer,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          DM_CRETD_TMS as timestamp,",
						"          DM_CRETD_USER_ID as string,",
						"          DM_UPDTD_TMS as timestamp,",
						"          DM_UPDTD_USER_ID as string,",
						"          KYNDRYL_GLBL_ORG as string,",
						"          KYNDRYL_ORG_CD as string,",
						"          KYNDRYL_RGN as string,",
						"          KYNDRYL_RGN_CD as string",
						"     ),",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'L30_OFFRNG_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['OFFRNG_ID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Tgtupdibmcommdimtable",
						"select2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'L30_OFFRNG_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> InsTgtibmcommsDim"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0296_EVALN_ANSWR_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job creation:29-06-2022\nJob name: DF_BALD0296_EVALN_ANSWR_DIMCreatedBy: Varaprasad\n\n",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "srcEvalnansdim"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "tgtEvalnansdimlkp"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "Tgtupdibmcommdimtable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "InsTgtibmcommsDim"
						}
					],
					"transformations": [
						{
							"name": "mergeddata"
						},
						{
							"name": "srtdtgtIbmcomm"
						},
						{
							"name": "split"
						},
						{
							"name": "select1"
						},
						{
							"name": "alterRow1"
						},
						{
							"name": "select2"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "derivedColumn2"
						},
						{
							"name": "CdcVal"
						},
						{
							"name": "filter1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          MAX_ID as integer,",
						"          ROW_NUM as long,",
						"          EVALN_ANSWR_CD as string,",
						"          EVALN_ANSWR_TXT as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          HASHED_VAL as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n     EVL.MAX_ID as MAX_ID,\\n     EVL.ROW_NUM as ROW_NUM,\\n     EVL.EVALN_ANSWR_CD,\\n     EVL.EVALN_ANSWR_TXT,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n      HashBytes(\\'SHA2_256\\', coalesce(\\n          cast(EVL.MAX_ID as varchar) + \\n          Cast(EVL.ROW_NUM as varchar) + \\n          EVL.EVALN_ANSWR_CD+ \\n            EVL.EVALN_ANSWR_TXT+\\n            Cast(JOB.ETL_JOB_ID as Varchar)+\\n            Cast(FIL.ETL_EXCTN_ID as Varchar)+\\n            cast(SYS.SRC_SYS_DIM_UID as Varchar) , \\'\\')\\n          ) as HASHED_VAL\\nfrom\\n     (select\\n               (select coalesce(max(EVALN_ANSWR_DIM_UID), 0) from PGMPDM.EVALN_ANSWR_DIM) as MAX_ID,\\n               ROW_NUMBER() over (order by SELLIST.UI_SEL_LIST_ID,SELLIST.MULTILANG_SEL_TEXT_ID  ) as ROW_NUM,\\n               SELLIST.INTERNAL_VAL AS EVALN_ANSWR_CD,\\n            T.TEXT AS EVALN_ANSWR_TXT\\n        FROM \\n            APPFUN.UI_SEL_LIST_DEF SELLIST, APPFUN.MULTILANG_TEXT_DEF T\\n        WHERE \\n            UI_SEL_LIST_ID = 95515\\n/*Commented for filter the both active and Inactive */\\n       --AND SELLIST.ACTIVE_IND = \\'Y\\'\\n        AND T.MULTILANG_TEXT_ID = SELLIST.MULTILANG_SEL_TEXT_ID\\n        AND T.LANG_ID = 1) EVL\\n     left join\\n     (select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          from\\n               PGMPDM.ZAUX_ETL_JOBS\\n          where\\n               ETL_JOB_NM = \\'#DSJobName#\\')JOB\\n     on 1 = 1\\n     left join\\n     (select\\n               ETL_EXCTN_ID\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          where\\n               IS_CURR_IND = \\'Y\\')FIL\\n     on 1 = 1\\n     left join\\n     (select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          from\\n               PGMPDM.SRC_SYS_DIM\\n          where\\n               SRC_SYS_CD = \\'PGMP\\')SYS\\n     on 1 = 1\\n\\n',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> srcEvalnansdim",
						"source(output(",
						"          LKP_EVALN_ANSWR_CD as string,",
						"          LKP_EVALN_ANSWR_TXT as string,",
						"          LKP_ETL_JOB_ID as integer,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_HASHED_VAL as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n     EVALN_ANSWR_CD AS LKP_EVALN_ANSWR_CD,\\n     EVALN_ANSWR_TXT AS LKP_EVALN_ANSWR_TXT,\\n     ETL_JOB_ID AS LKP_ETL_JOB_ID,\\n     SRC_SYS_DIM_UID AS LKP_SRC_SYS_DIM_UID,\\n     HashBytes(\\'SHA2_256\\', coalesce(\\n                   EVALN_ANSWR_CD+ \\n            EVALN_ANSWR_TXT+\\n            Cast(ETL_JOB_ID as Varchar)+\\n            Cast(ETL_EXCTN_ID as Varchar)+\\n            cast(SRC_SYS_DIM_UID as Varchar) , \\'\\')\\n          ) as  LKP_HASHED_VAL\\nfrom\\n     PGMPDM.EVALN_ANSWR_DIM',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> tgtEvalnansdimlkp",
						"srcEvalnansdim, srtdtgtIbmcomm join(EVALN_ANSWR_CD == LKP_EVALN_ANSWR_CD,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergeddata",
						"tgtEvalnansdimlkp sort(asc(LKP_EVALN_ANSWR_CD, true)) ~> srtdtgtIbmcomm",
						"filter1 split(ROW_STATUS_CD!='I',",
						"     disjoint: false) ~> split@(datatobeupdted, datatobeinserted)",
						"derivedColumn1 select(mapColumn(",
						"          MAX_ID,",
						"          ROW_NUM,",
						"          EVALN_ANSWR_CD,",
						"          EVALN_ANSWR_TXT,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          DM_CRETD_TMS,",
						"          ROW_STAT_CD",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 alterRow(updateIf(true())) ~> alterRow1",
						"derivedColumn2 select(mapColumn(",
						"          MAX_ID,",
						"          ROW_NUM,",
						"          EVALN_ANSWR_CD,",
						"          EVALN_ANSWR_TXT,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          HASHED_VAL,",
						"          LKP_HASHED_VAL,",
						"          ROW_STATUS_CD,",
						"          EVALN_ANSWR_DIM_UID,",
						"          DM_CRETD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select2",
						"split@datatobeupdted derive(DM_CRETD_TMS = currentTimestamp(),",
						"          ROW_STAT_CD = 'U') ~> derivedColumn1",
						"split@datatobeinserted derive(EVALN_ANSWR_DIM_UID = MAX_ID+ROW_NUM,",
						"          DM_CRETD_TMS = currentTimestamp()) ~> derivedColumn2",
						"mergeddata derive(ROW_STATUS_CD = case(EVALN_ANSWR_CD == toString(null()) && LKP_EVALN_ANSWR_CD != toString(null()), 'D', \r",
						"      case(EVALN_ANSWR_CD != toString(null()) && LKP_EVALN_ANSWR_CD == toString(null()), 'I', \r",
						"      case(HASHED_VAL == LKP_HASHED_VAL, 'L', 'U')))) ~> CdcVal",
						"CdcVal filter(ROW_STATUS_CD!='L') ~> filter1",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          GEO_DIM_UID as integer,",
						"          IOT_RGN_CD as string,",
						"          IOT_RGN_NM as string,",
						"          IMT_RGN_CD as string,",
						"          IMT_SRGN_NM as string,",
						"          SRGN_CD as string,",
						"          SRGNCTRY_CD as string,",
						"          SRGNCTRY_NM as string,",
						"          IOTSORT_NUM as integer,",
						"          GEO_CD as string,",
						"          GEO_TYP_CD as string,",
						"          GMR_RGN_CD as string,",
						"          GMR_RGN_NM as string,",
						"          IOT_SHORT_NM as string,",
						"          IMT_SHORT_NM as string,",
						"          ROW_STAT_CD as string,",
						"          SRC_SYS_DIM_UID as integer,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          DM_CRETD_TMS as timestamp,",
						"          DM_CRETD_USER_ID as string,",
						"          DM_UPDTD_TMS as timestamp,",
						"          DM_UPDTD_USER_ID as string,",
						"          KYNDRYL_GLBL_ORG as string,",
						"          KYNDRYL_ORG_CD as string,",
						"          KYNDRYL_RGN as string,",
						"          KYNDRYL_RGN_CD as string",
						"     ),",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'EVALN_ANSWR_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['EVALN_ANSWR_CD'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Tgtupdibmcommdimtable",
						"select2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'EVALN_ANSWR_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> InsTgtibmcommsDim"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0297_BTT_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job creation:29-06-2022\nJob name: DF_BALD0297_BTT_DIM\nCreatedBy: Varaprasad\n\n",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "srcBttdim"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "tgtBttdimlkp"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "Tgtupdibmcommdimtable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "InsTgtibmcommsDim"
						}
					],
					"transformations": [
						{
							"name": "mergeddata"
						},
						{
							"name": "srtdtgtIbmcomm"
						},
						{
							"name": "split"
						},
						{
							"name": "select1"
						},
						{
							"name": "alterRow1"
						},
						{
							"name": "select2"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "derivedColumn2"
						},
						{
							"name": "CdcVal"
						},
						{
							"name": "filter1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          MAX_ID as integer,",
						"          ROW_NUM as long,",
						"          BTT_NUM as string,",
						"          BTT_DESC as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          HASHED_VAL as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: '\\nselect\\n     BT.MAX_ID as MAX_ID,\\n     BT.ROW_NUM as ROW_NUM,\\n     BT.BTT_NUM,\\n     BT.BTT_DESC,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     HashBytes(\\'SHA2_256\\', coalesce(\\n          cast(BT.MAX_ID as varchar) + \\n          cast(BT.ROW_NUM AS varchar) +\\n         cast(BT.BTT_NUM as varchar) +\\n          BT.BTT_DESC,         \\n            cast(JOB.ETL_JOB_ID as varchar) +\\n            cast(FIL.ETL_EXCTN_ID as varchar) +\\n            cast(SYS.SRC_SYS_DIM_UID as varchar), \\'\\')\\n          ) as HASHED_VAL\\nfrom\\n     (select\\n               (select coalesce(max(BTT_DIM_UID), 0) from PGMPDM.BTT_DIM) as MAX_ID,\\n               ROW_NUMBER() over (order by SELLIST.UI_SEL_LIST_ID,SELLIST.MULTILANG_SEL_TEXT_ID  ) as ROW_NUM,\\n               SELLIST.INTERNAL_VAL AS BTT_NUM,\\n            T.TEXT AS  BTT_DESC\\n        FROM \\n            APPFUN.UI_SEL_LIST_DEF SELLIST, APPFUN.MULTILANG_TEXT_DEF T\\n        WHERE \\n            UI_SEL_LIST_ID = 95512\\n/*Commented for filter the both active and Inactive */\\n       --AND SELLIST.ACTIVE_IND = \\'Y\\'\\n        AND T.MULTILANG_TEXT_ID = SELLIST.MULTILANG_SEL_TEXT_ID\\n        AND T.LANG_ID = 1) BT\\n     left join\\n     (select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          from\\n               PGMPDM.ZAUX_ETL_JOBS\\n          where\\n               ETL_JOB_NM = \\'#DSJobName#\\')JOB\\n     on 1 = 1\\n     left join\\n     (select\\n               ETL_EXCTN_ID\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          where\\n               IS_CURR_IND = \\'Y\\')FIL\\n     on 1 = 1\\n     left join\\n     (select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          from\\n               PGMPDM.SRC_SYS_DIM\\n          where\\n               SRC_SYS_CD = \\'PGMP\\')SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> srcBttdim",
						"source(output(",
						"          LKP_BTT_NUM as string,",
						"          LKP_BTT_DESC as string,",
						"          LKP_ETL_JOB_ID as integer,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_HASHED_VAL as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n     BTT_NUM as LKP_BTT_NUM,\\n     BTT_DESC as LKP_BTT_DESC,\\n     ETL_JOB_ID as LKP_ETL_JOB_ID,\\n     SRC_SYS_DIM_UID as LKP_SRC_SYS_DIM_UID,\\nHashBytes(\\'SHA2_256\\', coalesce(\\n        cast(BTT_NUM as varchar) +\\n        BTT_DESC +     \\n          cast(ETL_JOB_ID as varchar) +\\n           cast(SRC_SYS_DIM_UID as varchar), \\'\\')\\n          ) as LKP_HASHED_VAL\\n            \\n            from\\n     PGMPDM.BTT_DIM',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> tgtBttdimlkp",
						"srcBttdim, srtdtgtIbmcomm join(BTT_NUM == LKP_BTT_NUM,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergeddata",
						"tgtBttdimlkp sort(asc(LKP_BTT_NUM, true)) ~> srtdtgtIbmcomm",
						"filter1 split(ROW_STATUS_CD!='I',",
						"     disjoint: false) ~> split@(datatobeupdted, datatobeinserted)",
						"derivedColumn1 select(mapColumn(",
						"          BTT_NUM,",
						"          BTT_DESC,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STATUS_CD,",
						"          DM_CRETD_TMS,",
						"          ROW_STAT_CD",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 alterRow(updateIf(true())) ~> alterRow1",
						"derivedColumn2 select(mapColumn(",
						"          BTT_NUM,",
						"          BTT_DESC,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          HASHED_VAL,",
						"          ROW_STATUS_CD,",
						"          BTT_DIM_UID,",
						"          DM_CRETD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select2",
						"split@datatobeupdted derive(DM_CRETD_TMS = currentTimestamp(),",
						"          ROW_STAT_CD = 'U') ~> derivedColumn1",
						"split@datatobeinserted derive(BTT_DIM_UID = MAX_ID+ROW_NUM,",
						"          DM_CRETD_TMS = currentTimestamp()) ~> derivedColumn2",
						"mergeddata derive(ROW_STATUS_CD = case(isNull(BTT_NUM) && not(isNull(LKP_BTT_NUM)), 'D', case(not(isNull(BTT_NUM)) && isNull(LKP_BTT_NUM), 'I', case(HASHED_VAL == LKP_HASHED_VAL, 'L', 'U')))) ~> CdcVal",
						"CdcVal filter(ROW_STATUS_CD!='L') ~> filter1",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          GEO_DIM_UID as integer,",
						"          IOT_RGN_CD as string,",
						"          IOT_RGN_NM as string,",
						"          IMT_RGN_CD as string,",
						"          IMT_SRGN_NM as string,",
						"          SRGN_CD as string,",
						"          SRGNCTRY_CD as string,",
						"          SRGNCTRY_NM as string,",
						"          IOTSORT_NUM as integer,",
						"          GEO_CD as string,",
						"          GEO_TYP_CD as string,",
						"          GMR_RGN_CD as string,",
						"          GMR_RGN_NM as string,",
						"          IOT_SHORT_NM as string,",
						"          IMT_SHORT_NM as string,",
						"          ROW_STAT_CD as string,",
						"          SRC_SYS_DIM_UID as integer,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          DM_CRETD_TMS as timestamp,",
						"          DM_CRETD_USER_ID as string,",
						"          DM_UPDTD_TMS as timestamp,",
						"          DM_UPDTD_USER_ID as string,",
						"          KYNDRYL_GLBL_ORG as string,",
						"          KYNDRYL_ORG_CD as string,",
						"          KYNDRYL_RGN as string,",
						"          KYNDRYL_RGN_CD as string",
						"     ),",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'BTT_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['EVALN_ANSWR_CD'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Tgtupdibmcommdimtable",
						"select2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'BTT_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> InsTgtibmcommsDim"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0298_SRVC_PRVDR_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job creation:13-07-2022\nJob name: DF_BALD0298_SRVC_PRVDR_DIM\nCreatedBy: Varaprasad\n\n",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "srcBttdim"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "tgtBttdimlkp"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "Tgtupdibmcommdimtable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "InsTgtibmcommsDim"
						}
					],
					"transformations": [
						{
							"name": "mergeddata"
						},
						{
							"name": "srtdtgtIbmcomm"
						},
						{
							"name": "split"
						},
						{
							"name": "select1"
						},
						{
							"name": "alterRow1"
						},
						{
							"name": "select2"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "derivedColumn2"
						},
						{
							"name": "CdcVal"
						},
						{
							"name": "filter1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          MAX_ID as integer,",
						"          ROW_NUM as long,",
						"          SRVC_PRVDR_CD as string,",
						"          SRVC_PRVDR_DESC as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          HASHED_VAL as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n     O.MAX_ID as MAX_ID,\\n     O.ROW_NUM as ROW_NUM,\\n     O.SRVC_PRVDR_CD,\\n     O.SRVC_PRVDR_DESC,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     HashBytes(\\'SHA2_256\\', coalesce(\\n        cast(O.MAX_ID as varchar) +\\n        cast(O.MAX_ID as varchar) +\\n            SRVC_PRVDR_CD  +\\n            SRVC_PRVDR_DESC +\\n            cast(JOB.ETL_JOB_ID as varchar) +\\n             cast(FIL.ETL_EXCTN_ID as varchar) +\\n            cast(SYS.SRC_SYS_DIM_UID as varchar),\\'\\')\\n       ) as  HASHED_VAL\\nfrom\\n     (select\\n               (select coalesce(max(SRVC_PRVDR_DIM_UID), 0) from PGMPDM.SRVC_PRVDR_DIM) as MAX_ID,\\n               ROW_NUMBER() over (order by SELLIST.UI_SEL_LIST_ID,SELLIST.MULTILANG_SEL_TEXT_ID  ) as ROW_NUM,\\n               SELLIST.INTERNAL_VAL AS SRVC_PRVDR_CD,\\n            T.TEXT AS SRVC_PRVDR_DESC\\n        FROM \\n            APPFUN.UI_SEL_LIST_DEF SELLIST, APPFUN.MULTILANG_TEXT_DEF T\\n        WHERE \\n            UI_SEL_LIST_ID = 95511\\n/*Commented for filter the both active and Inactive */\\n       --AND SELLIST.ACTIVE_IND = \\'Y\\'\\n        AND T.MULTILANG_TEXT_ID = SELLIST.MULTILANG_SEL_TEXT_ID\\n        AND T.LANG_ID = 1\\n          ) O\\n     left join\\n     (select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          from\\n               PGMPDM.ZAUX_ETL_JOBS\\n          where\\n               ETL_JOB_NM = \\'#DSJobName#\\')JOB\\n     on 1 = 1\\n     left join\\n     (select\\n               ETL_EXCTN_ID\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          where\\n               IS_CURR_IND = \\'Y\\')FIL\\n     on 1 = 1\\n     left join\\n     (select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          from\\n               PGMPDM.SRC_SYS_DIM\\n          where\\n               SRC_SYS_CD = \\'PGMP\\')SYS\\n     on 1 = 1\\n\\n',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> srcBttdim",
						"source(output(",
						"          LKP_SRVC_PRVDR_CD as string,",
						"          LKP_SRVC_PRVDR_DESC as string,",
						"          LKP_ETL_JOB_ID as integer,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_HASHED_VAL as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n     SRVC_PRVDR_CD as LKP_SRVC_PRVDR_CD,\\n     SRVC_PRVDR_DESC as LKP_SRVC_PRVDR_DESC,\\n     ETL_JOB_ID as LKP_ETL_JOB_ID,\\n     SRC_SYS_DIM_UID as LKP_SRC_SYS_DIM_UID,\\n     HashBytes(\\'SHA2_256\\', coalesce(\\n             SRVC_PRVDR_CD  +\\n            SRVC_PRVDR_DESC +\\n            cast(ETL_JOB_ID as varchar) +\\n            cast(SRC_SYS_DIM_UID as varchar),\\'\\')\\n       ) as  LKP_HASHED_VAL\\nfrom\\n     PGMPDM.SRVC_PRVDR_DIM',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> tgtBttdimlkp",
						"srcBttdim, srtdtgtIbmcomm join(SRVC_PRVDR_CD == LKP_SRVC_PRVDR_CD,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergeddata",
						"tgtBttdimlkp sort(asc(LKP_SRVC_PRVDR_CD, true)) ~> srtdtgtIbmcomm",
						"filter1 split(ROW_STATUS_CD!='I',",
						"     disjoint: false) ~> split@(datatobeupdted, datatobeinserted)",
						"derivedColumn1 select(mapColumn(",
						"          SRVC_PRVDR_CD,",
						"          SRVC_PRVDR_DESC,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STATUS_CD,",
						"          DM_CRETD_TMS,",
						"          ROW_STAT_CD",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 alterRow(updateIf(true())) ~> alterRow1",
						"derivedColumn2 select(mapColumn(",
						"          SRVC_PRVDR_CD,",
						"          SRVC_PRVDR_DESC,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          HASHED_VAL,",
						"          ROW_STATUS_CD,",
						"          SRVC_PRVDR_DIM_UID,",
						"          DM_CRETD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select2",
						"split@datatobeupdted derive(DM_CRETD_TMS = currentTimestamp(),",
						"          ROW_STAT_CD = 'U') ~> derivedColumn1",
						"split@datatobeinserted derive(SRVC_PRVDR_DIM_UID = MAX_ID+ROW_NUM,",
						"          DM_CRETD_TMS = currentTimestamp()) ~> derivedColumn2",
						"mergeddata derive(ROW_STATUS_CD = case(SRVC_PRVDR_CD == toString(null()) && LKP_SRVC_PRVDR_CD != toString(null()), 'D', \r",
						"      case(SRVC_PRVDR_CD != toString(null()) && LKP_SRVC_PRVDR_CD == toString(null()), 'I', \r",
						"      case(HASHED_VAL == LKP_HASHED_VAL, 'L', 'U')))) ~> CdcVal",
						"CdcVal filter(ROW_STATUS_CD!='L') ~> filter1",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          GEO_DIM_UID as integer,",
						"          IOT_RGN_CD as string,",
						"          IOT_RGN_NM as string,",
						"          IMT_RGN_CD as string,",
						"          IMT_SRGN_NM as string,",
						"          SRGN_CD as string,",
						"          SRGNCTRY_CD as string,",
						"          SRGNCTRY_NM as string,",
						"          IOTSORT_NUM as integer,",
						"          GEO_CD as string,",
						"          GEO_TYP_CD as string,",
						"          GMR_RGN_CD as string,",
						"          GMR_RGN_NM as string,",
						"          IOT_SHORT_NM as string,",
						"          IMT_SHORT_NM as string,",
						"          ROW_STAT_CD as string,",
						"          SRC_SYS_DIM_UID as integer,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          DM_CRETD_TMS as timestamp,",
						"          DM_CRETD_USER_ID as string,",
						"          DM_UPDTD_TMS as timestamp,",
						"          DM_UPDTD_USER_ID as string,",
						"          KYNDRYL_GLBL_ORG as string,",
						"          KYNDRYL_ORG_CD as string,",
						"          KYNDRYL_RGN as string,",
						"          KYNDRYL_RGN_CD as string",
						"     ),",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'SRVC_PRVDR_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['EVALN_ANSWR_CD'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Tgtupdibmcommdimtable",
						"select2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'SRVC_PRVDR_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> InsTgtibmcommsDim"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0320_ORG_CNTRCT_MAP_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 18-07-2022\nJob Name: DF_BALD0320_ORG_CNTRCT_MAP_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						},
						{
							"name": "CDCval"
						},
						{
							"name": "dropUnchangedRows"
						}
					],
					"scriptLines": [
						"parameters{",
						"     current_user as string",
						"}",
						"source(output(",
						"          MAX_ID as integer,",
						"          ROW_NUM as integer,",
						"          PRICE_TYPE_CD as string,",
						"          PRICE_TYPE_DESC as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          HASHED_VAL as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select \\n     (Select coalesce(max(PRICE_TYPE_CD_DIM_UID), 0) from PGMPDM.PRICE_TYPE_CD_DIM) as MAX_ID,\\n     cast(ROW_NUMBER() over (order by S.INTERNAL_VAL) as Integer) as ROW_NUM,\\n     cast(S.INTERNAL_VAL as char(1)) as PRICE_TYPE_CD,\\n     cast(S.TEXT as varchar(100)) as PRICE_TYPE_DESC,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     HashBytes(\\'SHA2_256\\',\\n          coalesce(cast(S.INTERNAL_VAL as char(1)), \\'\\') +\\n          coalesce(cast(S.TEXT as varchar(100)), \\'\\') +\\n          coalesce(cast(JOB.ETL_JOB_ID as varchar), \\'\\') +\\n          coalesce(cast(SYS.SRC_SYS_DIM_UID as varchar), \\'\\')\\n     ) as HASHED_VAL\\nFrom\\n     APPFUN.SEL_LIST_REF_V S\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD0345_PRICE_TYPE_CD_DIM\\'\\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'\\n     ) SYS\\n     on 1 = 1\\nWhere\\n     S.UI_SEL_LIST_ID = 95566\\n     and S.LANG_ID = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          LKP_PRICE_TYPE_CD as string,",
						"          LKP_PRICE_TYPE_DESC as string,",
						"          LKP_ETL_JOB_ID as integer,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_HASHED_VAL as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     PRICE_TYPE_CD as LKP_PRICE_TYPE_CD,\\n     PRICE_TYPE_DESC as LKP_PRICE_TYPE_DESC,\\n     ETL_JOB_ID as LKP_ETL_JOB_ID,\\n     SRC_SYS_DIM_UID as LKP_SRC_SYS_DIM_UID,\\n     HashBytes(\\'SHA2_256\\', \\n          coalesce(cast(PRICE_TYPE_CD as varchar), \\'\\') +\\n          coalesce(cast(PRICE_TYPE_DESC as varchar), \\'\\') + \\n          coalesce(cast(ETL_JOB_ID as varchar), \\'\\') + \\n          coalesce(cast(SRC_SYS_DIM_UID as varchar), \\'\\') \\n     ) as LKP_HASHED_VAL\\nFrom\\n     PGMPDM.PRICE_TYPE_CD_DIM',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"getLookupData sort(asc(LKP_PRICE_TYPE_CD, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"dropUnchangedRows split(ROW_STATUS!='I',",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(PRICE_TYPE_CD == LKP_PRICE_TYPE_CD,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          PRICE_TYPE_CD_DIM_UID,",
						"          PRICE_TYPE_CD,",
						"          PRICE_TYPE_DESC,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD = ROW_STATUS,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(DM_CRETD_USER_ID = $current_user,",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(PRICE_TYPE_CD_DIM_UID = MAX_ID + ROW_NUM,",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          PRICE_TYPE_CD,",
						"          PRICE_TYPE_DESC,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD = ROW_STATUS,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"mergedData derive(ROW_STATUS = case(PRICE_TYPE_CD == toString(null()) && LKP_PRICE_TYPE_CD != toString(null()), 'D', case(PRICE_TYPE_CD != toString(null()) && LKP_PRICE_TYPE_CD == toString(null()), 'I', case(HASHED_VAL == LKP_HASHED_VAL, 'L', 'U')))) ~> CDCval",
						"CDCval filter(ROW_STATUS != 'L') ~> dropUnchangedRows",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'PRICE_TYPE_CD_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['PRICE_TYPE_CD'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'PRICE_TYPE_CD_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0330_PCR_CRC_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 14-07-2022\nJob Name: DF_BALD0330_PCR_CRC_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourcePCRData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceCRCData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "clientProposalRqstDate"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "clientRevisedProposalRqstDate"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "proposalSentToClientDate"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "clientRqstImplementCompletionDate"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "clientRevisedRqstImplementCompletionDate"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "implementationReadyForCADate"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						},
						{
							"name": "CDCval"
						},
						{
							"name": "dropUnchangedRows"
						},
						{
							"name": "renamedColumns"
						},
						{
							"name": "sortedLookupData"
						},
						{
							"name": "sourcePCRCRCData"
						},
						{
							"name": "joinCPRD"
						},
						{
							"name": "joinCRPRD"
						},
						{
							"name": "joinPSTCD"
						},
						{
							"name": "joinCRICD"
						},
						{
							"name": "joinCRRICD"
						},
						{
							"name": "joinIRFCAD"
						},
						{
							"name": "dropUnwantedKeyColumns"
						},
						{
							"name": "hashColumn"
						},
						{
							"name": "lookupHashColumn"
						}
					],
					"scriptLines": [
						"parameters{",
						"     current_user as string,",
						"     hash_columns as string ('PCR_CRC_DIM_UID, PRPSL_PRICE_TERM_DIM_UID, CMPLXTY_DIM_UID, RQST_STAT_DIM_UID, RQST_TYPE_DIM_UID, RQST_RCVD_DT, PRPSL_RQST_DT, CLNT_RVSD_PRPSL_RQSTD_DT, PRPSL_SENT_TO_CLNT_DT, PRPSL_SENT_TO_CLNT_DT_WD, PRPSL_ACCPTD_DT, PRPSL_ACCPTD_DT_WD, PRPSL_REJCT_DT, RQST_IMPLMNT_CMPLTD_DT, CLNT_RVSD_RQST_IMPL_CMPL_DT, IMPLMNT_READY_CUST_ACCEPT_DT, IMPLMNT_READY_CUST_ACCEPT_DT_WD, PRPSL_EXP_DT, PRPSL_DSPSN_DT_WD, ON_HOLD_DT, WITHDRWN_DT, DELD_DT, LATEST_PRPSL_RQSTD_DT, LATEST_IMPLMNT_RQSTD_DT, PRPSL_OR_REJCT_DT, MERGED_DT, CLNT_ACCNTG_CD, CLNT_CNTCT_NM, CLNT_AFFCT_AREA_TXT, AGRMT_REF_DTL_TXT, SRVC_TYPE_TXT, CLNT_APPRVL_NUM, RFS_GROUP_NM, SRVC_PRVDR_ORGNZN_TXT, PREDEFND_SRVCS_DESC, SIEBEL_OPPRTNTY_NUM, BTT_NUM, SLTN_OWNR_TXT, DLVR_CRDNTR_TXT, SLTN_DSGNR_TXT, CLAIM_ACCNT_CD, CLAIM_WORK_ITEM_CD, RSRC_SUMRY_PRSNL_INVOLV_IND, RSRC_SUMRY_SERVER_COM_IND, RSRC_SUMRY_STRG_COM_IND, RSRC_SUMRY_HW_COM_IND, RSRC_SUMRY_SOFTWR_COM_IND, RJCT_RSN_CD, RJCT_SRC_CD, RJCT_EXPLNN_TXT, PRICE_TYPE_CD, RSN_CD, PURCHS_ORDR_NM, UNSLCTD_RQST_IND, CNTRCT_CHNG_RQRD_IND, OATS_RCD_RRQRD_IND, NON_STD_T_AND_C_IND, TRVL_INVC_IND, TERMNTN_CHRGS_IND, RNWL_MNTH_CD, BYPSS_SLTN_DSGN_IND, PRPSL_OWNR_TXT, ETL_JOB_ID, SRC_SYS_DIM_UID, HAS_PRNT_RFS_IND, PRNT_PROC_DIM_UID, PRJCT_DIM_UID, L30_OFFRNG_ID_1,  L30_OFFRNG_ID_2,  L30_OFFRNG_ID_3, CLNT_PRPSL_RQSTD_FIRST_TM_DT,  CLNT_RVSD_PRPSL_RQSTD_FIRST_TM_DT,  PRPSL_SENT_TO_CLNT_FIRST_TM_DT,  CLNT_RQSTD_IMPLMTN_CMPLT_FIRST_TM_DT,  CLNT_RVSD_RQSTD_IMPLMTN_CMPLT_FIRST_TM_DT,  IMPLMTN_READY_FOR_CUST_ACCPTNC_FIRST_TM_DT, GEO_DIM_UID, ORGNTG_ORG, PRICER_NM, SOLTN_TEAM, ERO_CHCKD_DT,  EA_CHCKD_DT, CE_CHCKD_DT, EXPORT_REG, ENVMNTL_AFF, CMPLNCE_ENGG, GROSS_TCP_CTRY_CD, GROSS_TCP_GEO_DIM_UID')",
						"}",
						"source(output(",
						"          PCR_CRC_DIM_UID as integer,",
						"          PRPSL_PRICE_TERM_DIM_UID as integer,",
						"          CMPLXTY_DIM_UID as integer,",
						"          RQST_STAT_DIM_UID as integer,",
						"          RQST_TYPE_DIM_UID as integer,",
						"          RQST_RCVD_DT as date,",
						"          PRPSL_RQST_DT as date,",
						"          CLNT_RVSD_PRPSL_RQSTD_DT as date,",
						"          PRPSL_SENT_TO_CLNT_DT as date,",
						"          PRPSL_SENT_TO_CLNT_DT_WD as date,",
						"          PRPSL_ACCPTD_DT as date,",
						"          PRPSL_ACCPTD_DT_WD as date,",
						"          PRPSL_REJCT_DT as date,",
						"          RQST_IMPLMNT_CMPLTD_DT as date,",
						"          CLNT_RVSD_RQST_IMPL_CMPL_DT as date,",
						"          IMPLMNT_READY_CUST_ACCEPT_DT as date,",
						"          IMPLMNT_READY_CUST_ACCEPT_DT_WD as date,",
						"          PRPSL_EXP_DT as date,",
						"          PRPSL_DSPSN_DT_WD as date,",
						"          ON_HOLD_DT as date,",
						"          WITHDRWN_DT as date,",
						"          DELD_DT as date,",
						"          LATEST_PRPSL_RQSTD_DT as date,",
						"          LATEST_IMPLMNT_RQSTD_DT as date,",
						"          PRPSL_OR_REJCT_DT as date,",
						"          MERGED_DT as date,",
						"          CLNT_ACCNTG_CD as string,",
						"          CLIENT_CNTCT_NAME as string,",
						"          CLNT_AFFCT_AREA_TXT as string,",
						"          AGRMT_REF_DTL_TXT as string,",
						"          SRVC_TYPE_TXT as string,",
						"          CLNT_APPRVL_NUM as integer,",
						"          RFS_GROUP_NM as string,",
						"          SRVC_PRVDR_ORGNZN_TXT as string,",
						"          PREDEFND_SRVCS_DESC as string,",
						"          SIEBEL_OPPRTNTY_NUM as string,",
						"          BTT_NUM as integer,",
						"          SOLTN_OWNER as string,",
						"          DLVR_COORDTR as string,",
						"          SOLTN_DESIGNER as string,",
						"          CLAIM_ACCNT_CD as string,",
						"          CLAIM_WORK_ITEM_CD as string,",
						"          RSRC_SUMRY_PRSNL_INVOLV_IND as string,",
						"          RSRC_SUMRY_SERVER_COM_IND as string,",
						"          RSRC_SUMRY_STRG_COM_IND as string,",
						"          RSRC_SUMRY_HW_COM_IND as string,",
						"          RSRC_SUMRY_SOFTWR_COM_IND as string,",
						"          RJCT_RSN_CD as string,",
						"          RJCT_SRC_CD as string,",
						"          RJCT_EXPLNN_TXT as string,",
						"          PRICE_TYPE_CD as string,",
						"          RSN_CD as string,",
						"          PURCHS_ORDR_NM as string,",
						"          UNSLCTD_RQST_IND as string,",
						"          CNTRCT_CHNG_RQRD_IND as string,",
						"          OATS_RCD_RRQRD_IND as string,",
						"          NON_STD_T_AND_C_IND as string,",
						"          TRVL_INVC_IND as string,",
						"          TERMNTN_CHRGS_IND as string,",
						"          RNWL_MNTH_CD as string,",
						"          BYPSS_SLTN_DSGN_IND as string,",
						"          PRPSL_OWNER as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer,",
						"          HAS_PRNT_RFS_IND as string,",
						"          PRNT_PROC_DIM_UID as integer,",
						"          PRJCT_DIM_UID as integer,",
						"          L30_OFFRNG_ID_1 as string,",
						"          L30_OFFRNG_ID_2 as string,",
						"          L30_OFFRNG_ID_3 as string,",
						"          GEO_DIM_UID as integer,",
						"          ORGNTG_ORG as string,",
						"          PRICER_NM as string,",
						"          SOLTN_TEAM as string,",
						"          ERO_CHCKD_DT as date,",
						"          EA_CHCKD_DT as date,",
						"          CE_CHCKD_DT as date,",
						"          EXPORT_REG as string,",
						"          ENVMNTL_AFF as string,",
						"          CMPLNCE_ENGG as string,",
						"          GROSS_TCP_CTRY_CD as string,",
						"          GROSS_TCP_GEO_DIM_UID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     C.PROC_ID as PCR_CRC_DIM_UID,\\n     coalesce(PPT.PRPSL_PRICE_TERM_DIM_UID, -1) as PRPSL_PRICE_TERM_DIM_UID,\\n     coalesce(C.COMPLEXITY, -1) as CMPLXTY_DIM_UID,\\n     coalesce(RSTD.RQST_STAT_DIM_UID, -1) as RQST_STAT_DIM_UID,\\n     coalesce(RT.RQST_TYPE_DIM_UID, -1) as RQST_TYPE_DIM_UID,\\n     C.RQST_RECEIV_DATE as RQST_RCVD_DT,\\n     C.PRPSL_RQST_DATE as PRPSL_RQST_DT,\\n     C.CLIENT_REVISD_PRPSL_RQSTD_DT as CLNT_RVSD_PRPSL_RQSTD_DT,\\n     C.PRPSL_SENT_TO_CLIENT_DATE as PRPSL_SENT_TO_CLNT_DT,\\n     TGR.PRPSL_SENT_TO_CLNT_DT as PRPSL_SENT_TO_CLNT_DT_WD,\\n     C.PRPSL_ACCEPTED_DATE as PRPSL_ACCPTD_DT,\\n     TGR.PRPSL_ACCPTD_DT as PRPSL_ACCPTD_DT_WD,\\n     C.PRPSL_REJCT_DATE as PRPSL_REJCT_DT,\\n     C.RQST_IMPLMNT_COMPLT_DATE as RQST_IMPLMNT_CMPLTD_DT,\\n     C.CLIENT_RVSD_RQST_IMPL_CMPL_DT as CLNT_RVSD_RQST_IMPL_CMPL_DT,\\n     R.IMPLMNT_RDY_CUST_ACCEPT_DT as IMPLMNT_READY_CUST_ACCEPT_DT,\\n     TGR.IMPLMTN_READY_DT as IMPLMNT_READY_CUST_ACCEPT_DT_WD,\\n     C.PRPSL_EXP_DATE as PRPSL_EXP_DT,\\n     TGR.PRPSL_DSPSN_DT as PRPSL_DSPSN_DT_WD,\\n     P.ON_HOLD_DATE as ON_HOLD_DT,\\n     P.WITHDRAWN_DATE as WITHDRWN_DT,\\n     P.DELETED_DATE as DELD_DT,\\n     coalesce(R.CLIENT_REVISD_PRPSL_RQSTD_DT, R.PRPSL_RQST_DATE) as LATEST_PRPSL_RQSTD_DT,\\n     coalesce(R.CLIENT_RVSD_RQST_IMPL_CMPL_DT, R.RQST_IMPLMNT_COMPLT_DATE) as LATEST_IMPLMNT_RQSTD_DT,\\n     case\\n          when (TGR.PRPSL_ACCPTD_DT is null and R.PRPSL_REJCT_DATE is not null) then R.PRPSL_REJCT_DATE\\n          when (TGR.PRPSL_ACCPTD_DT is not null and R.PRPSL_REJCT_DATE is null) then TGR.PRPSL_ACCPTD_DT\\n          when (TGR.PRPSL_ACCPTD_DT < R.PRPSL_REJCT_DATE) then TGR.PRPSL_ACCPTD_DT\\n          else R.PRPSL_REJCT_DATE\\n     end as PRPSL_OR_REJCT_DT,\\n     case\\n          when WLZ.WINLOSS_CD =\\'WIN\\' then C.PRPSL_ACCEPTED_DATE\\n          when WLZ.WINLOSS_CD =\\'LOSS\\' then\\n               case\\n               when R.PRPSL_REJCT_DATE is null then P.WITHDRAWN_DATE\\n               else R.PRPSL_REJCT_DATE\\n               end\\n          when WLZ.WINLOSS_CD =\\'UNK\\' then NULL\\n     end as MERGED_DT,\\n     C.CLIENT_ACCNTG_CD as CLNT_ACCNTG_CD,\\n     C.CLIENT_CNTCT_NAME, --SYSIBM.ENCRYPT(APPFUN.DECRYPTDATA(C.CLIENT_CNTCT_NAME, PWD.MIS_REP_REF_CD), PWD.MIS_REP_REF_CD) as CLNT_CNTCT_NM,\\n     C.CLIENT_AFFECT_AREA as CLNT_AFFCT_AREA_TXT,\\n     C.AGREEMENT_REF_DETAILS as AGRMT_REF_DTL_TXT,\\n     C.SRVC_TYPE as SRVC_TYPE_TXT,\\n     C.CLIENT_APPRV as CLNT_APPRVL_NUM,\\n     C.RFS_GROUP_NAME as RFS_GROUP_NM,\\n     C.SRVC_PRVDR_ORG as SRVC_PRVDR_ORGNZN_TXT,\\n     cast(C.PREDEFINED_SRVCS_DESCR as varchar(4096)) as PREDEFND_SRVCS_DESC,\\n     C.SIEBEL_OPPORTUNITY_NUMBER as SIEBEL_OPPRTNTY_NUM,\\n     C.BTT as BTT_NUM,\\n     C.SOLTN_OWNER, --SYSIBM.ENCRYPT(APPFUN.DECRYPTDATA(C.SOLTN_OWNER, PWD.MIS_REP_REF_CD), PWD.MIS_REP_REF_CD) as SLTN_OWNR_TXT,\\n     C.DLVR_COORDTR, --SYSIBM.ENCRYPT(APPFUN.DECRYPTDATA(C.DLVR_COORDTR, PWD.MIS_REP_REF_CD), PWD.MIS_REP_REF_CD) as DLVR_CRDNTR_TXT,\\n     C.SOLTN_DESIGNER, --SYSIBM.ENCRYPT(APPFUN.DECRYPTDATA(C.SOLTN_DESIGNER, PWD.MIS_REP_REF_CD), PWD.MIS_REP_REF_CD) as SLTN_DSGNR_TXT,\\n     C.CLAIM_ACCNT_CD as CLAIM_ACCNT_CD,\\n     cast(C.CLAIM_WORK_ITEM_CDS as varchar(50)) as CLAIM_WORK_ITEM_CD,\\n     C.RESRC_SUMRY_PERSNL_INVOLV_IND as RSRC_SUMRY_PRSNL_INVOLV_IND,\\n     C.RESRC_SUMRY_SERVER_COM_IND as RSRC_SUMRY_SERVER_COM_IND,\\n     C.RESRC_SUMRY_STORAGE_COM_IND as RSRC_SUMRY_STRG_COM_IND,\\n     C.RESRC_SUMRY_HARDWARE_COM_IND as RSRC_SUMRY_HW_COM_IND,\\n     C.RESRC_SUMRY_SFTWR_COM_IND as RSRC_SUMRY_SOFTWR_COM_IND,\\n     C.REJECT_REASON_CD as RJCT_RSN_CD,\\n     C.REJECT_SRC_CD as RJCT_SRC_CD,\\n     C.REJECT_EXPLNTN as RJCT_EXPLNN_TXT,\\n     C.PRICE_TYPE_CDS as PRICE_TYPE_CD,\\n     C.REASON_CDS as RSN_CD,\\n     C.PURCHS_ORDR_NM as PURCHS_ORDR_NM,\\n     C.UNSOLICITED_RQST_IND as UNSLCTD_RQST_IND,\\n     C.CNTRCT_CHNG_REQD_IND as CNTRCT_CHNG_RQRD_IND,\\n     C.OATS_RCD_REQD_IND as OATS_RCD_RRQRD_IND,\\n     C.NON_STD_T_AND_C_IND as NON_STD_T_AND_C_IND,\\n     C.TRAVEL_INVOICED_IND as TRVL_INVC_IND,\\n     C.TERMINATN_CHRGS_IND as TERMNTN_CHRGS_IND,\\n     C.RENEWAL_MNTH as RNWL_MNTH_CD,\\n     C.BYPASS_SOLN_DSGN_IND as BYPSS_SLTN_DSGN_IND,\\n     C.PRPSL_OWNER, --SYSIBM.ENCRYPT(APPFUN.DECRYPTDATA(C.PRPSL_OWNER, PWD.MIS_REP_REF_CD), PWD.MIS_REP_REF_CD) as PRPSL_OWNR_TXT,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when D.PROC_ID is null then 0 else 1 end as IS_DELETED,\\n     cast(PCR_FLAGS.HAS_PRNT_RFS_IND as char(1)) as HAS_PRNT_RFS_IND,\\n     PCR_FLAGS.PRNT_PROC_DIM_UID,\\n     PCR_FLAGS.PRJCT_DIM_UID,\\n     C.L30_OFFRNG_1_CD as L30_OFFRNG_ID_1,\\n     C.L30_OFFRNG_2_CD as L30_OFFRNG_ID_2,\\n     C.L30_OFFRNG_3_CD as L30_OFFRNG_ID_3,\\n     S.GEO_DIM_UID,\\n     S.ORIGINATING_ORG AS ORGNTG_ORG,\\n     S.PRICER AS PRICER_NM, --SYSIBM.ENCRYPT(APPFUN.DECRYPTDATA(S.PRICER, PWD.MIS_REP_REF_CD), PWD.MIS_REP_REF_CD) as PRICER_NM,\\n     C.SOLTN_TEAM,\\n     C.ERO_CHECKED AS ERO_CHCKD_DT, \\n     C.EA_CHECKED as EA_CHCKD_DT,\\n     C.CE_CHECKED as CE_CHCKD_DT,\\n     C.EXPORT_REGULATION as EXPORT_REG,\\n     C.ENVIORMENTAL_AFFAIRS AS ENVMNTL_AFF,\\n     C.COMPLIANCE_ENGG AS CMPLNCE_ENGG,\\n     CAST(C.CNTRY AS CHAR(2)) AS GROSS_TCP_CTRY_CD,\\n     GEO.GEO_DIM_UID AS GROSS_TCP_GEO_DIM_UID\\nFrom\\n     APPFUN.PCR C\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS TGR\\n     on C.PROC_ID = TGR.PROC_ID\\n     inner join\\n     APPFUN.\"PROC\" P\\n     on C.PROC_ID = P.PROC_ID\\n     left join\\n     APPFUN.RQST R\\n     on C.PROC_ID = R.PROC_ID\\n     left join\\n     PGMPDM.RQST_TYPE_DIM RT\\n     -- As the Catalog Only records were defined at the PROC level, an exception should be made to\\n     -- ensure that this category is taken into consideration\\n     on case when P.PROC_TYPE_ID = \\'CTLGONLY\\' then \\'C\\' else C.RQST_TYPE end = RT.RQST_TYPE_DIM_CD\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID D\\n     on C.PROC_ID = D.PROC_ID\\n     left join\\n     PGMPDM.PRPSL_PRICE_TERM_DIM PPT\\n     on R.PRPSL_PRICING_TERMS = PPT.PRPSL_PRICE_TERM_DIM_CD\\n     left join (\\n          Select\\n               P.PROC_ID,\\n               P.STATE,\\n               R.PRPSL_REJCT_DATE,\\n               WSD.WKFLW_STEP_SEQ,\\n               case\\n                    when (P.STATE in (\\'A\\', \\'C\\') and WSD.WKFLW_STEP_SEQ between 1160 and 1250) then \\'WON\\'\\n                    when ((P.STATE = \\'W\\' and WSD.WKFLW_STEP_SEQ between 1135 and 1150) or (P.STATE in (\\'C\\', \\'Q\\', \\'R\\') and R.PRPSL_REJCT_DATE is not null)) then \\'LOST\\'\\n                    when (WSD.WKFLW_STEP_SEQ between 1135 and 1145) then \\'WIP-CR\\'\\n                    when ((WSD.WKFLW_STEP_SEQ between 1010 and 1131) or WSD.WKFLW_STEP_SEQ = 1150) then \\'WIP-NWC\\'\\n                    else \\'UNK\\'\\n               end as PROC_STATUS\\n          From\\n               PGMPDM.ZAUX_DATE_TRIGGERS TGR\\n               inner join\\n               APPFUN.\"PROC\" P\\n               on TGR.PROC_ID = P.PROC_ID\\n               inner join\\n               APPFUN.PCR R\\n               on P.PROC_ID = R.PROC_ID\\n               inner join\\n               APPFUN.PROC_STEP_DATA PSD\\n               on\\n                    P.PROC_ID = PSD.PROC_ID and\\n                    P.CUR_PROC_STEP_DATA_ID = PSD.PROC_STEP_DATA_ID\\n               inner join\\n               APPFUN.WKFLW_STEP_DEF WSD\\n               on\\n                    PSD.WKFLW_STEP_DEF_ID = WSD.WKFLW_STEP_DEF_ID and\\n                    PSD.WKFLW_DEF_ID = WSD.WKFLW_DEF_ID\\n     ) RST\\n     on P.PROC_ID = RST.PROC_ID\\n     left join\\n     PGMPDM.RQST_STAT_DIM RSTD\\n     on RST.PROC_STATUS = RSTD.RQST_STAT_DIM_CD\\n     left join \\n     PGMPDM.GEO_DIM GEO\\n     on C.CNTRY = GEO.SRGN_CD\\n     left join (\\n          Select\\n               S.PROC_ID,\\n               case\\n                    when S.PLV0_PROC_TYPE_ID = \\'REQUEST\\' and S.PLV1_PROC_ID = 0 then \\'Y\\'\\n                    when S.PLV1_PROC_TYPE_ID = \\'REQUEST\\' and S.PLV2_PROC_ID = 0 then \\'Y\\'\\n                    when S.PLV2_PROC_TYPE_ID = \\'REQUEST\\' and S.PLV3_PROC_ID = 0 then \\'Y\\'\\n                    when S.PLV3_PROC_TYPE_ID = \\'REQUEST\\' and S.PLV4_PROC_ID = 0 then \\'Y\\'\\n                    when S.PLV4_PROC_TYPE_ID = \\'REQUEST\\' and S.PLV5_PROC_ID = 0 then \\'Y\\'\\n                    when S.PLV5_PROC_TYPE_ID = \\'REQUEST\\' then \\'Y\\'\\n                    else \\'N\\'\\n               end as HAS_PRNT_RFS_IND,\\n               case\\n                    when S.PLV0_PROC_TYPE_ID = \\'REQUEST\\' and S.PLV1_PROC_ID = 0 then S.PLV0_PROC_ID\\n                    when S.PLV1_PROC_TYPE_ID = \\'REQUEST\\' and S.PLV2_PROC_ID = 0 then S.PLV1_PROC_ID\\n                    when S.PLV2_PROC_TYPE_ID = \\'REQUEST\\' and S.PLV3_PROC_ID = 0 then S.PLV2_PROC_ID\\n                    when S.PLV3_PROC_TYPE_ID = \\'REQUEST\\' and S.PLV4_PROC_ID = 0 then S.PLV3_PROC_ID\\n                    when S.PLV4_PROC_TYPE_ID = \\'REQUEST\\' and S.PLV5_PROC_ID = 0 then S.PLV4_PROC_ID\\n                    when S.PLV5_PROC_TYPE_ID = \\'REQUEST\\' then S.PLV5_PROC_ID\\n                    else -1\\n               end as PRNT_PROC_DIM_UID,\\n               case\\n                    when S.PLV0_PROC_TYPE_ID = \\'PROJECT\\' and S.PLV1_PROC_TYPE_ID = \\'REQUEST\\' and S.PLV2_PROC_ID = 0 then S.PLV0_PROC_ID\\n                    when S.PLV1_PROC_TYPE_ID = \\'PROJECT\\' and S.PLV2_PROC_TYPE_ID = \\'REQUEST\\' and S.PLV3_PROC_ID = 0 then S.PLV1_PROC_ID\\n                    when S.PLV2_PROC_TYPE_ID = \\'PROJECT\\' and S.PLV3_PROC_TYPE_ID = \\'REQUEST\\' and S.PLV4_PROC_ID = 0 then S.PLV2_PROC_ID\\n                    when S.PLV3_PROC_TYPE_ID = \\'PROJECT\\' and S.PLV4_PROC_TYPE_ID = \\'REQUEST\\' and S.PLV5_PROC_ID = 0 then S.PLV3_PROC_ID\\n                    when S.PLV4_PROC_TYPE_ID = \\'PROJECT\\' and S.PLV5_PROC_TYPE_ID = \\'REQUEST\\' then S.PLV4_PROC_ID\\n                    -- There is not need to level 5, since it won\\'t have a valid parent\\n                    else -1\\n               end as PRJCT_DIM_UID\\n          From (\\n               Select\\n                    B.PROC_ID,\\n                    B.PROC_TYPE_ID,\\n                    coalesce(B.PLV0_PROC_ID, 0) as PLV0_PROC_ID,\\n                    B.PLV0_PROC_TYPE_ID,\\n                    coalesce(B.PLV1_PROC_ID, 0) as PLV1_PROC_ID,\\n                    B.PLV1_PROC_TYPE_ID,\\n                    coalesce(B.PLV2_PROC_ID, 0) as PLV2_PROC_ID,\\n                    B.PLV2_PROC_TYPE_ID,\\n                    coalesce(B.PLV3_PROC_ID, 0) as PLV3_PROC_ID,\\n                    B.PLV3_PROC_TYPE_ID,\\n                    coalesce(B.PLV4_PROC_ID, 0) as PLV4_PROC_ID,\\n                    B.PLV4_PROC_TYPE_ID,\\n                    coalesce(B.PLV5_PROC_ID, 0) as PLV5_PROC_ID,\\n                    P.PROC_TYPE_ID as PLV5_PROC_TYPE_ID\\n               From (\\n                    Select\\n                         B.PROC_ID,\\n                         B.PROC_TYPE_ID,\\n                         B.PLV0_PROC_ID,\\n                         B.PLV0_PROC_TYPE_ID,\\n                         B.PLV1_PROC_ID,\\n                         B.PLV1_PROC_TYPE_ID,\\n                         B.PLV2_PROC_ID,\\n                         B.PLV2_PROC_TYPE_ID,\\n                         B.PLV3_PROC_ID,\\n                         B.PLV3_PROC_TYPE_ID,\\n                         B.PLV4_PROC_ID,\\n                         P.PROC_TYPE_ID as PLV4_PROC_TYPE_ID,\\n                         P.PARENT_PROC_ID as PLV5_PROC_ID\\n                    From (\\n                         Select\\n                              B.PROC_ID,\\n                              B.PROC_TYPE_ID,\\n                              B.PLV0_PROC_ID,\\n                              B.PLV0_PROC_TYPE_ID,\\n                              B.PLV1_PROC_ID,\\n                              B.PLV1_PROC_TYPE_ID,\\n                              B.PLV2_PROC_ID,\\n                              B.PLV2_PROC_TYPE_ID,\\n                              B.PLV3_PROC_ID,\\n                              P.PROC_TYPE_ID as PLV3_PROC_TYPE_ID,\\n                              P.PARENT_PROC_ID as PLV4_PROC_ID\\n                         From (\\n                              Select\\n                                   B.PROC_ID,\\n                                   B.PROC_TYPE_ID,\\n                                   B.PLV0_PROC_ID,\\n                                   B.PLV0_PROC_TYPE_ID,\\n                                   B.PLV1_PROC_ID,\\n                                   B.PLV1_PROC_TYPE_ID,\\n                                   B.PLV2_PROC_ID,\\n                                   P.PROC_TYPE_ID as PLV2_PROC_TYPE_ID,\\n                                   P.PARENT_PROC_ID as PLV3_PROC_ID\\n                              From (\\n                                   Select\\n                                        B.PROC_ID,\\n                                        B.PROC_TYPE_ID,\\n                                        B.PLV0_PROC_ID,\\n                                        B.PLV0_PROC_TYPE_ID,\\n                                        B.PLV1_PROC_ID,\\n                                        P.PROC_TYPE_ID as PLV1_PROC_TYPE_ID,\\n                                        P.PARENT_PROC_ID as PLV2_PROC_ID\\n                                   From (\\n                                        Select\\n                                             B.PROC_ID,\\n                                             B.PROC_TYPE_ID,\\n                                             B.PLV0_PROC_ID,\\n                                             P.PROC_TYPE_ID as PLV0_PROC_TYPE_ID,\\n                                             P.PARENT_PROC_ID as PLV1_PROC_ID\\n                                        From (\\n                                             Select\\n                                                  PCR.PROC_ID,\\n                                                  P.PROC_TYPE_ID,\\n                                                  P.PARENT_PROC_ID as PLV0_PROC_ID\\n                                             From\\n                                                  APPFUN.PCR PCR\\n                                                  inner join\\n                                                  PGMPDM.ZAUX_DATE_TRIGGERS TGR\\n                                                  on PCR.PROC_ID = TGR.PROC_ID\\n                                                  inner join\\n                                                  APPFUN.\"PROC\" P\\n                                                  on PCR.PROC_ID = P.PROC_ID\\n                                        ) B\\n                                        left join\\n                                        APPFUN.\"PROC\" P\\n                                        on B.PLV0_PROC_ID = P.PROC_ID\\n                                   ) B\\n                                   left join\\n                                   APPFUN.\"PROC\" P\\n                                   on B.PLV1_PROC_ID = P.PROC_ID\\n                              ) B\\n                              left join\\n                              APPFUN.\"PROC\" P\\n                              on B.PLV2_PROC_ID = P.PROC_ID\\n                         ) B\\n                         left join\\n                         APPFUN.\"PROC\" P\\n                         on B.PLV3_PROC_ID = P.PROC_ID\\n                    ) B\\n                    left join\\n                    APPFUN.\"PROC\" P\\n                    on B.PLV4_PROC_ID = P.PROC_ID\\n               ) B\\n               left join\\n               APPFUN.\"PROC\" P\\n               on B.PLV5_PROC_ID = P.PROC_ID \\n          ) S\\n     ) PCR_FLAGS\\n     on C.PROC_ID = PCR_FLAGS.PROC_ID\\n     left join (\\n          Select\\n               PCR.PROC_ID,\\n               GEO.GEO_DIM_UID,\\n               PCR.ORIGINATING_ORG,\\n               PCR.PRICER\\n          From\\n               APPFUN.PCR PCR\\n               inner join\\n               APPFUN.\"PROC\" P\\n               on PCR.PROC_ID = P.PROC_ID\\n               left outer join\\n               PGMPDM.GEO_DIM GEO\\n               on PCR.CNTRY=GEO.SRGN_CD\\n     ) S\\n     on C.PROC_ID = S.PROC_ID\\n     left join\\n     PGMPDM.ZAUX_WIN_LOSS WLZ\\n     on C.PROC_ID = WLZ.PROC_DIM_UID\\n     left join (\\n          Select\\n            coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n        From\\n            PGMPDM.ZAUX_ETL_JOBS\\n        Where\\n            ETL_JOB_NM = \\'BALD0330_PCR_CRC_DIM\\'\\n    ) JOB\\n    on 1 = 1\\n    left join (\\n        Select\\n            ETL_EXCTN_ID,\\n            ETL_PARAM_START_TMS,\\n            ETL_PARAM_END_TMS\\n        From\\n            PGMPDM.ZAUX_ETL_EXCTN\\n        Where\\n            IS_CURR_IND = \\'Y\\'\\n    ) FIL\\n    on 1 = 1\\n    left join (\\n        Select\\n            coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n        From\\n            PGMPDM.SRC_SYS_DIM\\n        Where\\n            SRC_SYS_CD = \\'PGMP\\'\\n    ) SYS\\n    on 1 = 1\\n    left join (\\n        Select\\n            MIS_REP_REF_UID,\\n            MIS_REP_REF_CD\\n        From\\n            PGMPDM.MISC_REP_REF               \\n    ) PWD\\n    on PWD.MIS_REP_REF_UID = 3',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourcePCRData",
						"source(output(",
						"          PCR_CRC_DIM_UID as integer,",
						"          PRPSL_PRICE_TERM_DIM_UID as integer,",
						"          CMPLXTY_DIM_UID as integer,",
						"          RQST_STAT_DIM_UID as integer,",
						"          RQST_TYPE_DIM_UID as integer,",
						"          RQST_RCVD_DT as date,",
						"          PRPSL_RQST_DT as date,",
						"          CLNT_RVSD_PRPSL_RQSTD_DT as date,",
						"          PRPSL_SENT_TO_CLNT_DT as date,",
						"          PRPSL_SENT_TO_CLNT_DT_WD as date,",
						"          PRPSL_ACCPTD_DT as date,",
						"          PRPSL_ACCPTD_DT_WD as date,",
						"          PRPSL_REJCT_DT as date,",
						"          RQST_IMPLMNT_CMPLTD_DT as date,",
						"          CLNT_RVSD_RQST_IMPL_CMPL_DT as date,",
						"          IMPLMNT_READY_CUST_ACCEPT_DT as date,",
						"          IMPLMNT_READY_CUST_ACCEPT_DT_WD as date,",
						"          PRPSL_EXP_DT as date,",
						"          PRPSL_DSPSN_DT_WD as date,",
						"          ON_HOLD_DT as date,",
						"          WITHDRWN_DT as date,",
						"          DELD_DT as date,",
						"          LATEST_PRPSL_RQSTD_DT as date,",
						"          LATEST_IMPLMNT_RQSTD_DT as date,",
						"          PRPSL_OR_REJCT_DT as date,",
						"          MERGED_DT as date,",
						"          CLNT_ACCNTG_CD as string,",
						"          CLIENT_CNTCT_NAME as string,",
						"          CLNT_AFFCT_AREA_TXT as string,",
						"          AGRMT_REF_DTL_TXT as string,",
						"          SRVC_TYPE_TXT as string,",
						"          CLNT_APPRVL_NUM as integer,",
						"          RFS_GROUP_NM as string,",
						"          SRVC_PRVDR_ORGNZN_TXT as string,",
						"          PREDEFND_SRVCS_DESC as string,",
						"          SIEBEL_OPPRTNTY_NUM as string,",
						"          BTT_NUM as integer,",
						"          SOLTN_OWNER as string,",
						"          DLVR_COORDTR as string,",
						"          SOLTN_DESIGNER as string,",
						"          CLAIM_ACCNT_CD as string,",
						"          CLAIM_WORK_ITEM_CD as string,",
						"          RSRC_SUMRY_PRSNL_INVOLV_IND as string,",
						"          RSRC_SUMRY_SERVER_COM_IND as string,",
						"          RSRC_SUMRY_STRG_COM_IND as string,",
						"          RSRC_SUMRY_HW_COM_IND as string,",
						"          RSRC_SUMRY_SOFTWR_COM_IND as string,",
						"          RJCT_RSN_CD as string,",
						"          RJCT_SRC_CD as string,",
						"          RJCT_EXPLNN_TXT as string,",
						"          PRICE_TYPE_CD as string,",
						"          RSN_CD as string,",
						"          PURCHS_ORDR_NM as string,",
						"          UNSLCTD_RQST_IND as string,",
						"          CNTRCT_CHNG_RQRD_IND as string,",
						"          OATS_RCD_RRQRD_IND as string,",
						"          NON_STD_T_AND_C_IND as string,",
						"          TRVL_INVC_IND as string,",
						"          TERMNTN_CHRGS_IND as string,",
						"          RNWL_MNTH_CD as string,",
						"          BYPSS_SLTN_DSGN_IND as string,",
						"          PRPSL_OWNER as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer,",
						"          HAS_PRNT_RFS_IND as string,",
						"          PRNT_PROC_DIM_UID as integer,",
						"          PRJCT_DIM_UID as integer,",
						"          L30_OFFRNG_ID_1 as string,",
						"          L30_OFFRNG_ID_2 as string,",
						"          L30_OFFRNG_ID_3 as string,",
						"          GEO_DIM_UID as integer,",
						"          ORGNTG_ORG as string,",
						"          PRICER_NM as string,",
						"          SOLTN_TEAM as string,",
						"          ERO_CHCKD_DT as date,",
						"          EA_CHCKD_DT as date,",
						"          CE_CHCKD_DT as date,",
						"          EXPORT_REG as string,",
						"          ENVMNTL_AFF as string,",
						"          CMPLNCE_ENGG as string,",
						"          GROSS_TCP_CTRY_CD as string,",
						"          GROSS_TCP_GEO_DIM_UID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     C.PROC_ID as PCR_CRC_DIM_UID,\\n     coalesce(PPT.PRPSL_PRICE_TERM_DIM_UID, -1) as PRPSL_PRICE_TERM_DIM_UID,\\n     coalesce(R.COMPLEXITY, -1) as CMPLXTY_DIM_UID,\\n     coalesce(RSTD.RQST_STAT_DIM_UID, -1) as RQST_STAT_DIM_UID,\\n     coalesce(RT.RQST_TYPE_DIM_UID, -1) as RQST_TYPE_DIM_UID,\\n     R.RQST_RECEIV_DATE as RQST_RCVD_DT,\\n     R.PRPSL_RQST_DATE as PRPSL_RQST_DT,\\n     R.CLIENT_REVISD_PRPSL_RQSTD_DT as CLNT_RVSD_PRPSL_RQSTD_DT,\\n     C.SENT_TO_CLIENT_DATE as PRPSL_SENT_TO_CLNT_DT,\\n     TGR.PRPSL_SENT_TO_CLNT_DT as PRPSL_SENT_TO_CLNT_DT_WD,\\n     R.PRPSL_ACCEPTED_DATE as PRPSL_ACCPTD_DT,\\n     TGR.PRPSL_ACCPTD_DT as PRPSL_ACCPTD_DT_WD,\\n     R.PRPSL_REJCT_DATE as PRPSL_REJCT_DT,\\n     R.RQST_IMPLMNT_COMPLT_DATE as RQST_IMPLMNT_CMPLTD_DT,\\n     R.CLIENT_RVSD_RQST_IMPL_CMPL_DT as CLNT_RVSD_RQST_IMPL_CMPL_DT,\\n     R.IMPLMNT_RDY_CUST_ACCEPT_DT as IMPLMNT_READY_CUST_ACCEPT_DT,\\n     TGR.IMPLMTN_READY_DT as IMPLMNT_READY_CUST_ACCEPT_DT_WD,\\n     R.PRPSL_EXP_DATE as PRPSL_EXP_DT,\\n     TGR.PRPSL_DSPSN_DT as PRPSL_DSPSN_DT_WD,\\n     P.ON_HOLD_DATE as ON_HOLD_DT,\\n     P.WITHDRAWN_DATE as WITHDRWN_DT,\\n     P.DELETED_DATE as DELD_DT,\\n     coalesce(R.CLIENT_REVISD_PRPSL_RQSTD_DT, R.PRPSL_RQST_DATE) as LATEST_PRPSL_RQSTD_DT,\\n     coalesce(R.CLIENT_RVSD_RQST_IMPL_CMPL_DT, R.RQST_IMPLMNT_COMPLT_DATE) as LATEST_IMPLMNT_RQSTD_DT,\\n     case\\n          when (TGR.PRPSL_ACCPTD_DT is null and R.PRPSL_REJCT_DATE is not null) then R.PRPSL_REJCT_DATE\\n          when (TGR.PRPSL_ACCPTD_DT is not null and R.PRPSL_REJCT_DATE is null) then TGR.PRPSL_ACCPTD_DT\\n          when (TGR.PRPSL_ACCPTD_DT < R.PRPSL_REJCT_DATE) then TGR.PRPSL_ACCPTD_DT\\n          else R.PRPSL_REJCT_DATE\\n     end as PRPSL_OR_REJCT_DT,\\n     case\\n          when P.WITHDRAWN_DATE is null then\\n               case\\n               when (TGR.PRPSL_ACCPTD_DT is null and R.PRPSL_REJCT_DATE is not null) then R.PRPSL_REJCT_DATE\\n               when (TGR.PRPSL_ACCPTD_DT is not null and R.PRPSL_REJCT_DATE is null) then TGR.PRPSL_ACCPTD_DT\\n               when (TGR.PRPSL_ACCPTD_DT > R.PRPSL_REJCT_DATE) then TGR.PRPSL_ACCPTD_DT\\n               else R.PRPSL_REJCT_DATE\\n               end\\n          else P.WITHDRAWN_DATE\\n     end as MERGED_DT,\\n     C.CLIENT_ACCNTG_CD as CLNT_ACCNTG_CD,\\n     C.CLIENT_CNTCT_NAME, --SYSIBM.ENCRYPT(APPFUN.DECRYPTDATA(C.CLIENT_CNTCT_NAME, PWD.MIS_REP_REF_CD), PWD.MIS_REP_REF_CD) as CLNT_CNTCT_NM,\\n     R.CLIENT_AFFECT_AREA as CLNT_AFFCT_AREA_TXT,\\n     R.AGREEMENT_REF_DETAILS as AGRMT_REF_DTL_TXT,\\n     R.SRVC_TYPE as SRVC_TYPE_TXT,\\n     R.CLIENT_APPRV as CLNT_APPRVL_NUM,\\n     R.RFS_GROUP_NAME as RFS_GROUP_NM,\\n     R.SRVC_PRVDR_ORG as SRVC_PRVDR_ORGNZN_TXT,\\n     cast(R.PREDEFINED_SRVCS_DESCR as varchar(4096)) as PREDEFND_SRVCS_DESC,\\n     R.SIEBEL_OPPORTUNITY_NUMBER as SIEBEL_OPPRTNTY_NUM,\\n     R.BTT as BTT_NUM,\\n     R.SOLTN_OWNER, --SYSIBM.ENCRYPT(APPFUN.DECRYPTDATA(R.SOLTN_OWNER, PWD.MIS_REP_REF_CD), PWD.MIS_REP_REF_CD) as SLTN_OWNR_TXT,\\n     R.DLVR_COORDTR, --SYSIBM.ENCRYPT(APPFUN.DECRYPTDATA(R.DLVR_COORDTR, PWD.MIS_REP_REF_CD), PWD.MIS_REP_REF_CD) as DLVR_CRDNTR_TXT,\\n     R.SOLTN_DESIGNER, --SYSIBM.ENCRYPT(APPFUN.DECRYPTDATA(R.SOLTN_DESIGNER, PWD.MIS_REP_REF_CD), PWD.MIS_REP_REF_CD) as SLTN_DSGNR_TXT,\\n     R.CLAIM_ACCNT_CD as CLAIM_ACCNT_CD,\\n     cast(\\'\\' as varchar(50)) as CLAIM_WORK_ITEM_CD,\\n     R.RESRC_SUMRY_PERSNL_INVOLV_IND as RSRC_SUMRY_PRSNL_INVOLV_IND,\\n     R.RESRC_SUMRY_SERVER_COM_IND as RSRC_SUMRY_SERVER_COM_IND,\\n     R.RESRC_SUMRY_STORAGE_COM_IND as RSRC_SUMRY_STRG_COM_IND,\\n     R.RESRC_SUMRY_HARDWARE_COM_IND as RSRC_SUMRY_HW_COM_IND,\\n     R.RESRC_SUMRY_SFTWR_COM_IND as RSRC_SUMRY_SOFTWR_COM_IND,\\n     R.REJECT_REASON_CD as RJCT_RSN_CD,\\n     R.REJECT_SRC_CD as RJCT_SRC_CD,\\n     R.REJECT_EXPLNTN as RJCT_EXPLNN_TXT,\\n     R.PRICE_TYPE_CDS as PRICE_TYPE_CD,\\n     \\'\\' as RSN_CD,\\n     R.PURCHS_ORDR_NM as PURCHS_ORDR_NM,\\n     R.UNSOLICITED_RQST_IND as UNSLCTD_RQST_IND,\\n     R.CNTRCT_CHNG_REQD_IND as CNTRCT_CHNG_RQRD_IND,\\n     R.OATS_RCD_REQD_IND as OATS_RCD_RRQRD_IND,\\n     R.NON_STD_T_AND_C_IND as NON_STD_T_AND_C_IND,\\n     R.TRAVEL_INVOICED_IND as TRVL_INVC_IND,\\n     R.TERMINATN_CHRGS_IND as TERMNTN_CHRGS_IND,\\n     R.RENEWAL_MNTH as RNWL_MNTH_CD,\\n     R.BYPASS_SOLN_DSGN_IND as BYPSS_SLTN_DSGN_IND,\\n     R.PRPSL_OWNER, --SYSIBM.ENCRYPT(APPFUN.DECRYPTDATA(R.PRPSL_OWNER, PWD.MIS_REP_REF_CD), PWD.MIS_REP_REF_CD) as PRPSL_OWNR_TXT,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when D.PROC_ID is null then 0 else 1 end as IS_DELETED,\\n     cast(\\'Y\\' as char(1)) as HAS_PRNT_RFS_IND,\\n     P.PARENT_PROC_ID as PRNT_PROC_DIM_UID,\\n     -1 as PRJCT_DIM_UID,\\n     \\'\\' as L30_OFFRNG_ID_1,\\n     \\'\\' as L30_OFFRNG_ID_2,\\n     \\'\\' as L30_OFFRNG_ID_3,\\n     0 as GEO_DIM_UID,\\n     CAST(NULL as varchar) as ORGNTG_ORG,\\n     CAST(NULL as varchar) as PRICER_NM,\\n     R.SOLTN_TEAM,\\n     R.ERO_CHECKED AS ERO_CHCKD_DT, \\n     R.EA_CHECKED as EA_CHCKD_DT,\\n     R.CE_CHECKED as CE_CHCKD_DT,\\n     R.EXPORT_REGULATION as EXPORT_REG,\\n     R.ENVIORMENTAL_AFFAIRS AS ENVMNTL_AFF,\\n     R.COMPLIANCE_ENGG AS CMPLNCE_ENGG,\\n     CAST(R.CNTRY AS CHAR(2)) AS GROSS_TCP_CTRY_CD,\\n     GEO.GEO_DIM_UID AS GROSS_TCP_GEO_DIM_UID\\nFrom\\n     PGMPDM.ZAUX_DATE_TRIGGERS TGR\\n     inner join\\n     APPFUN.CTLG_RQ_CHG C\\n     on TGR.PROC_ID = C.PROC_ID\\n     inner join\\n     APPFUN.\"PROC\" P\\n     on P.PROC_ID = C.PROC_ID\\n     left join\\n     APPFUN.RQST R\\n     on P.PARENT_PROC_ID = R.PROC_ID\\n     left join\\n     PGMPDM.RQST_TYPE_DIM RT\\n     -- As the Catalog Only records were defined at the PROC level, an exception should be made to\\n     -- ensure that this category is taken into consideration\\n     on case when P.PROC_TYPE_ID = \\'CTLGONLY\\' then \\'C\\' else R.RQST_TYPE end = RT.RQST_TYPE_DIM_CD\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID D\\n     on C.PROC_ID = D.PROC_ID\\n     left join\\n     PGMPDM.PRPSL_PRICE_TERM_DIM PPT\\n     on R.PRPSL_PRICING_TERMS = PPT.PRPSL_PRICE_TERM_DIM_CD\\n     left join (\\n          Select\\n               P.PROC_ID,\\n               P.STATE,\\n               R.REJECTED_DATE,\\n               WSD.WKFLW_STEP_SEQ,\\n               case\\n                    when (P.STATE in (\\'A\\', \\'C\\') and WSD.WKFLW_STEP_SEQ between 1160 and 1250) then \\'WON\\'\\n                    when ((P.STATE = \\'W\\' and WSD.WKFLW_STEP_SEQ between 1135 and 1150) or (P.STATE in (\\'C\\', \\'Q\\', \\'R\\') and R.REJECTED_DATE is not null)) then \\'LOST\\'\\n                    when (WSD.WKFLW_STEP_SEQ between 1135 and 1145) then \\'WIP-CR\\'\\n                    when ((WSD.WKFLW_STEP_SEQ between 1010 and 1131) or WSD.WKFLW_STEP_SEQ = 1150) then \\'WIP-NWC\\'\\n                    else \\'UNK\\'\\n               end as PROC_STATUS\\n          From\\n               PGMPDM.ZAUX_DATE_TRIGGERS TGR\\n               inner join\\n               APPFUN.\"PROC\" P\\n               on TGR.PROC_ID = P.PROC_ID\\n               inner join\\n               APPFUN.CTLG_RQ_CHG R\\n               on P.PROC_ID = R.PROC_ID\\n               inner join\\n               APPFUN.PROC_STEP_DATA PSD\\n               on\\n                    P.PROC_ID = PSD.PROC_ID and\\n                    P.CUR_PROC_STEP_DATA_ID = PSD.PROC_STEP_DATA_ID\\n               inner join\\n               APPFUN.WKFLW_STEP_DEF WSD\\n               on\\n                    PSD.WKFLW_STEP_DEF_ID = WSD.WKFLW_STEP_DEF_ID and\\n                    PSD.WKFLW_DEF_ID = WSD.WKFLW_DEF_ID\\n      ) RST\\n     on P.PROC_ID = RST.PROC_ID\\n     left join\\n     PGMPDM.RQST_STAT_DIM RSTD\\n     on RST.PROC_STATUS = RSTD.RQST_STAT_DIM_CD\\n     left join \\n     PGMPDM.GEO_DIM GEO\\n     on R.CNTRY = GEO.SRGN_CD\\n     left join (\\n          Select\\n            coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n        From\\n            PGMPDM.ZAUX_ETL_JOBS\\n        Where\\n            ETL_JOB_NM = \\'BALD0330_PCR_CRC_DIM\\'\\n    ) JOB\\n    on 1 = 1\\n    left join (\\n        Select\\n            ETL_EXCTN_ID,\\n            ETL_PARAM_START_TMS,\\n            ETL_PARAM_END_TMS\\n        From\\n            PGMPDM.ZAUX_ETL_EXCTN\\n        Where\\n            IS_CURR_IND = \\'Y\\'\\n    ) FIL\\n    on 1 = 1\\n    left join (\\n        Select\\n            coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n        From\\n            PGMPDM.SRC_SYS_DIM\\n        Where\\n            SRC_SYS_CD = \\'PGMP\\'\\n    ) SYS\\n    on 1 = 1\\n    left join (\\n        Select\\n            MIS_REP_REF_UID,\\n            MIS_REP_REF_CD\\n        From\\n            PGMPDM.MISC_REP_REF               \\n    ) PWD\\n    on PWD.MIS_REP_REF_UID = 3',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceCRCData",
						"source(output(",
						"          PCR_CRC_DIM_UID as integer,",
						"          CLNT_PRPSL_RQSTD_FIRST_TM_DT as date",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select \\n    S.PCR_CRC_DIM_UID,\\n    S.PRPSL_RQST_DATE as CLNT_PRPSL_RQSTD_FIRST_TM_DT\\nFrom\\n    (\\n        Select\\n            H.PROC_ID as PCR_CRC_DIM_UID,\\n            H.PRPSL_RQST_DATE,\\n            ROW_NUMBER() over (partition by H.PROC_ID order by H.UPDATED_TS asc) as POS\\n        From \\n            APPFUN.PCR_H as H\\n            inner join \\n            PGMPDM.ZAUX_DATE_TRIGGERS as TGR\\n            on \\n                H.PROC_ID = TGR.PROC_ID\\n                and H.UPDATED_TS >= TGR.PRPSL_SENT_TO_CLNT_DT\\n        Where \\n            H.PRPSL_RQST_DATE is not Null\\n    ) as S\\nWhere\\n    S.POS = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> clientProposalRqstDate",
						"source(output(",
						"          PCR_CRC_DIM_UID as integer,",
						"          CLNT_RVSD_PRPSL_RQSTD_FIRST_TM_DT as date",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     S.PCR_CRC_DIM_UID,\\n     S.CLIENT_REVISD_PRPSL_RQSTD_DT as CLNT_RVSD_PRPSL_RQSTD_FIRST_TM_DT\\nFrom\\n     (\\n          Select\\n               H.PROC_ID as PCR_CRC_DIM_UID,\\n               H.CLIENT_REVISD_PRPSL_RQSTD_DT,\\n               ROW_NUMBER() over (partition by H.PROC_ID order by H.UPDATED_TS asc) as POS\\n          From\\n               APPFUN.PCR_H as H\\n               inner join\\n               PGMPDM.ZAUX_DATE_TRIGGERS as TGR\\n               on \\n                    H.PROC_ID = TGR.PROC_ID\\n                    and H.UPDATED_TS >= TGR.PRPSL_SENT_TO_CLNT_DT\\n          Where\\n               H.CLIENT_REVISD_PRPSL_RQSTD_DT is not null\\n     ) as S\\nWhere\\n     S.POS = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> clientRevisedProposalRqstDate",
						"source(output(",
						"          PCR_CRC_DIM_UID as integer,",
						"          PRPSL_SENT_TO_CLNT_FIRST_TM_DT as date",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     S.PCR_CRC_DIM_UID,\\n     S.PRPSL_SENT_TO_CLIENT_DATE as PRPSL_SENT_TO_CLNT_FIRST_TM_DT\\nFrom\\n     (\\n          Select\\n               H.PROC_ID as PCR_CRC_DIM_UID,\\n               H.PRPSL_SENT_TO_CLIENT_DATE,\\n               ROW_NUMBER() over (partition by H.PROC_ID order by H.UPDATED_TS asc) as POS\\n          From\\n               APPFUN.PCR_H as H\\n               inner join\\n               PGMPDM.ZAUX_DATE_TRIGGERS as TGR\\n               on H.PROC_ID = TGR.PROC_ID\\n          Where\\n               H.PRPSL_SENT_TO_CLIENT_DATE is not null\\n     ) as S\\nWhere\\n     S.POS = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> proposalSentToClientDate",
						"source(output(",
						"          PCR_CRC_DIM_UID as integer,",
						"          CLNT_RQSTD_IMPLMTN_CMPLT_FIRST_TM_DT as date",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     S.PCR_CRC_DIM_UID,\\n     S.RQST_IMPLMNT_COMPLT_DATE as CLNT_RQSTD_IMPLMTN_CMPLT_FIRST_TM_DT\\nFrom\\n     (\\n          Select\\n               H.PROC_ID as PCR_CRC_DIM_UID,\\n               H.RQST_IMPLMNT_COMPLT_DATE,\\n               ROW_NUMBER() over (partition by H.PROC_ID order by H.UPDATED_TS asc) as POS\\n          From\\n               APPFUN.PCR_H as H\\n               inner join\\n               PGMPDM.ZAUX_DATE_TRIGGERS as TGR\\n               on \\n                    H.PROC_ID = TGR.PROC_ID\\n                    and H.UPDATED_TS >= TGR.IMPLMTN_READY_DT\\n          Where\\n               H.RQST_IMPLMNT_COMPLT_DATE is not null\\n     ) as S\\nWhere\\n     S.POS = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> clientRqstImplementCompletionDate",
						"source(output(",
						"          PCR_CRC_DIM_UID as integer,",
						"          CLNT_RVSD_RQSTD_IMPLMTN_CMPLT_FIRST_TM_DT as date",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     S.PCR_CRC_DIM_UID,\\n     S.CLIENT_RVSD_RQST_IMPL_CMPL_DT as CLNT_RVSD_RQSTD_IMPLMTN_CMPLT_FIRST_TM_DT\\nFrom\\n     (\\n          Select\\n               H.PROC_ID as PCR_CRC_DIM_UID,\\n               H.CLIENT_RVSD_RQST_IMPL_CMPL_DT,\\n               ROW_NUMBER() over (partition by H.PROC_ID order by H.UPDATED_TS asc) as POS\\n          From\\n               APPFUN.PCR_H as H\\n               inner join\\n               PGMPDM.ZAUX_DATE_TRIGGERS as TGR\\n               on\\n                    H.PROC_ID = TGR.PROC_ID\\n                    and H.UPDATED_TS >= TGR.IMPLMTN_READY_DT               \\n          Where\\n               H.CLIENT_RVSD_RQST_IMPL_CMPL_DT is not null\\n     ) as S\\nWhere\\n     S.POS = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> clientRevisedRqstImplementCompletionDate",
						"source(output(",
						"          PCR_CRC_DIM_UID as integer,",
						"          IMPLMTN_READY_FOR_CUST_ACCPTNC_FIRST_TM_DT as date",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     S.PCR_CRC_DIM_UID,\\n     S.IMPLMNT_RDY_CUST_ACCEPT_DT as IMPLMTN_READY_FOR_CUST_ACCPTNC_FIRST_TM_DT\\nFrom\\n     (\\n          Select\\n               H.PROC_ID as PCR_CRC_DIM_UID,\\n               cast(null as Date) as IMPLMNT_RDY_CUST_ACCEPT_DT,\\n               ROW_NUMBER() over (partition by H.PROC_ID order by H.UPDATED_TS asc) as POS\\n          From\\n               APPFUN.PCR_H as H\\n               inner join\\n               PGMPDM.ZAUX_DATE_TRIGGERS as TGR\\n               on H.PROC_ID = TGR.PROC_ID\\n     ) as S\\nWhere\\n     S.POS = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> implementationReadyForCADate",
						"source(output(",
						"          PCR_CRC_DIM_UID as integer,",
						"          PRPSL_PRICE_TERM_DIM_UID as integer,",
						"          CMPLXTY_DIM_UID as integer,",
						"          RQST_STAT_DIM_UID as integer,",
						"          RQST_TYPE_DIM_UID as integer,",
						"          RQST_RCVD_DT as date,",
						"          PRPSL_RQST_DT as date,",
						"          CLNT_RVSD_PRPSL_RQSTD_DT as date,",
						"          PRPSL_SENT_TO_CLNT_DT as date,",
						"          PRPSL_SENT_TO_CLNT_DT_WD as date,",
						"          PRPSL_ACCPTD_DT as date,",
						"          PRPSL_ACCPTD_DT_WD as date,",
						"          PRPSL_REJCT_DT as date,",
						"          RQST_IMPLMNT_CMPLTD_DT as date,",
						"          CLNT_RVSD_RQST_IMPL_CMPL_DT as date,",
						"          IMPLMNT_READY_CUST_ACCEPT_DT as date,",
						"          IMPLMNT_READY_CUST_ACCEPT_DT_WD as date,",
						"          PRPSL_EXP_DT as date,",
						"          PRPSL_DSPSN_DT_WD as date,",
						"          ON_HOLD_DT as date,",
						"          WITHDRWN_DT as date,",
						"          DELD_DT as date,",
						"          LATEST_PRPSL_RQSTD_DT as date,",
						"          LATEST_IMPLMNT_RQSTD_DT as date,",
						"          PRPSL_OR_REJCT_DT as date,",
						"          MERGED_DT as date,",
						"          CLNT_ACCNTG_CD as string,",
						"          CLNT_CNTCT_NM as binary,",
						"          CLNT_AFFCT_AREA_TXT as string,",
						"          AGRMT_REF_DTL_TXT as string,",
						"          SRVC_TYPE_TXT as string,",
						"          CLNT_APPRVL_NUM as integer,",
						"          RFS_GROUP_NM as string,",
						"          SRVC_PRVDR_ORGNZN_TXT as string,",
						"          PREDEFND_SRVCS_DESC as string,",
						"          SIEBEL_OPPRTNTY_NUM as string,",
						"          BTT_NUM as integer,",
						"          SLTN_OWNR_TXT as binary,",
						"          DLVR_CRDNTR_TXT as binary,",
						"          SLTN_DSGNR_TXT as binary,",
						"          CLAIM_ACCNT_CD as string,",
						"          CLAIM_WORK_ITEM_CD as string,",
						"          RSRC_SUMRY_PRSNL_INVOLV_IND as string,",
						"          RSRC_SUMRY_SERVER_COM_IND as string,",
						"          RSRC_SUMRY_STRG_COM_IND as string,",
						"          RSRC_SUMRY_HW_COM_IND as string,",
						"          RSRC_SUMRY_SOFTWR_COM_IND as string,",
						"          RJCT_RSN_CD as string,",
						"          RJCT_SRC_CD as string,",
						"          RJCT_EXPLNN_TXT as string,",
						"          PRICE_TYPE_CD as string,",
						"          RSN_CD as string,",
						"          PURCHS_ORDR_NM as string,",
						"          UNSLCTD_RQST_IND as string,",
						"          CNTRCT_CHNG_RQRD_IND as string,",
						"          OATS_RCD_RRQRD_IND as string,",
						"          NON_STD_T_AND_C_IND as string,",
						"          TRVL_INVC_IND as string,",
						"          TERMNTN_CHRGS_IND as string,",
						"          RNWL_MNTH_CD as string,",
						"          BYPSS_SLTN_DSGN_IND as string,",
						"          PRPSL_OWNR_TXT as binary,",
						"          ETL_JOB_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          HAS_PRNT_RFS_IND as string,",
						"          PRNT_PROC_DIM_UID as integer,",
						"          PRJCT_DIM_UID as integer,",
						"          L30_OFFRNG_ID_1 as string,",
						"          L30_OFFRNG_ID_2 as string,",
						"          L30_OFFRNG_ID_3 as string,",
						"          CLNT_PRPSL_RQSTD_FIRST_TM_DT as date,",
						"          CLNT_RVSD_PRPSL_RQSTD_FIRST_TM_DT as date,",
						"          PRPSL_SENT_TO_CLNT_FIRST_TM_DT as date,",
						"          CLNT_RQSTD_IMPLMTN_CMPLT_FIRST_TM_DT as date,",
						"          CLNT_RVSD_RQSTD_IMPLMTN_CMPLT_FIRST_TM_DT as date,",
						"          IMPLMTN_READY_FOR_CUST_ACCPTNC_FIRST_TM_DT as date,",
						"          GEO_DIM_UID as integer,",
						"          ORGNTG_ORG as string,",
						"          PRICER_NM as binary,",
						"          SOLTN_TEAM as string,",
						"          ERO_CHCKD_DT as date,",
						"          EA_CHCKD_DT as date,",
						"          CE_CHCKD_DT as date,",
						"          EXPORT_REG as string,",
						"          ENVMNTL_AFF as string,",
						"          CMPLNCE_ENGG as string,",
						"          GROSS_TCP_CTRY_CD as string,",
						"          GROSS_TCP_GEO_DIM_UID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     S.PCR_CRC_DIM_UID,\\n     S.PRPSL_PRICE_TERM_DIM_UID,\\n     S.CMPLXTY_DIM_UID,\\n     S.RQST_STAT_DIM_UID,\\n     S.RQST_TYPE_DIM_UID,\\n     S.RQST_RCVD_DT,\\n     S.PRPSL_RQST_DT,\\n     S.CLNT_RVSD_PRPSL_RQSTD_DT,\\n     S.PRPSL_SENT_TO_CLNT_DT,\\n     S.PRPSL_SENT_TO_CLNT_DT_WD,\\n     S.PRPSL_ACCPTD_DT,\\n     S.PRPSL_ACCPTD_DT_WD,\\n     S.PRPSL_REJCT_DT,\\n     S.RQST_IMPLMNT_CMPLTD_DT,\\n     S.CLNT_RVSD_RQST_IMPL_CMPL_DT,\\n     S.IMPLMNT_READY_CUST_ACCEPT_DT,\\n     S.IMPLMNT_READY_CUST_ACCEPT_DT_WD,\\n     S.PRPSL_EXP_DT, PRPSL_DSPSN_DT_WD,\\n     S.ON_HOLD_DT,\\n     S.WITHDRWN_DT,\\n     S.DELD_DT,\\n     S.LATEST_PRPSL_RQSTD_DT,\\n     S.LATEST_IMPLMNT_RQSTD_DT,\\n     S.PRPSL_OR_REJCT_DT,\\n     S.MERGED_DT,\\n     S.CLNT_ACCNTG_CD,\\n     S.CLNT_CNTCT_NM,\\n     S.CLNT_AFFCT_AREA_TXT,\\n     S.AGRMT_REF_DTL_TXT,\\n     S.SRVC_TYPE_TXT,\\n     S.CLNT_APPRVL_NUM,\\n     S.RFS_GROUP_NM,\\n     S.SRVC_PRVDR_ORGNZN_TXT,\\n     S.PREDEFND_SRVCS_DESC,\\n     S.SIEBEL_OPPRTNTY_NUM,\\n     S.BTT_NUM,\\n     S.SLTN_OWNR_TXT,\\n     S.DLVR_CRDNTR_TXT,\\n     S.SLTN_DSGNR_TXT,\\n     S.CLAIM_ACCNT_CD,\\n     S.CLAIM_WORK_ITEM_CD,\\n     S.RSRC_SUMRY_PRSNL_INVOLV_IND,\\n     S.RSRC_SUMRY_SERVER_COM_IND,\\n     S.RSRC_SUMRY_STRG_COM_IND,\\n     S.RSRC_SUMRY_HW_COM_IND,\\n     S.RSRC_SUMRY_SOFTWR_COM_IND,\\n     S.RJCT_RSN_CD,\\n     S.RJCT_SRC_CD,\\n     S.RJCT_EXPLNN_TXT,\\n     S.PRICE_TYPE_CD,\\n     S.RSN_CD,\\n     S.PURCHS_ORDR_NM,\\n     S.UNSLCTD_RQST_IND,\\n     S.CNTRCT_CHNG_RQRD_IND,\\n     S.OATS_RCD_RRQRD_IND,\\n     S.NON_STD_T_AND_C_IND,\\n     S.TRVL_INVC_IND,\\n     S.TERMNTN_CHRGS_IND,\\n     S.RNWL_MNTH_CD,\\n     S.BYPSS_SLTN_DSGN_IND,\\n     S.PRPSL_OWNR_TXT,\\n     S.ETL_JOB_ID,\\n     S.SRC_SYS_DIM_UID,\\n     S.HAS_PRNT_RFS_IND,\\n     S.PRNT_PROC_DIM_UID,\\n     S.PRJCT_DIM_UID,\\n     S.L30_OFFRNG_ID_1, \\n     S.L30_OFFRNG_ID_2, \\n     S.L30_OFFRNG_ID_3,\\n     S.CLNT_PRPSL_RQSTD_FIRST_TM_DT, \\n     S.CLNT_RVSD_PRPSL_RQSTD_FIRST_TM_DT, \\n     S.PRPSL_SENT_TO_CLNT_FIRST_TM_DT, \\n     S.CLNT_RQSTD_IMPLMTN_CMPLT_FIRST_TM_DT, \\n     S.CLNT_RVSD_RQSTD_IMPLMTN_CMPLT_FIRST_TM_DT, \\n     S.IMPLMTN_READY_FOR_CUST_ACCPTNC_FIRST_TM_DT,\\n    S.GEO_DIM_UID,\\n    S.ORGNTG_ORG,\\n    S.PRICER_NM,\\n    S.SOLTN_TEAM,\\n    S.ERO_CHCKD_DT, \\n    S.EA_CHCKD_DT,\\n    S.CE_CHCKD_DT,\\n    S.EXPORT_REG,\\n    S.ENVMNTL_AFF,\\n    S.CMPLNCE_ENGG,\\n    S.GROSS_TCP_CTRY_CD,\\n    S.GROSS_TCP_GEO_DIM_UID     \\nFrom\\n     PGMPDM.PCR_CRC_DIM as S\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS as ZDT\\n     on S.PCR_CRC_DIM_UID = ZDT.PROC_ID',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"dropUnchangedRows split(ROW_STATUS!='I',",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"joinIRFCAD, sortedLookupData join(sourcePCRCRCData@PCR_CRC_DIM_UID == LKP_PCR_CRC_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          PCR_CRC_DIM_UID,",
						"          PRPSL_PRICE_TERM_DIM_UID,",
						"          CMPLXTY_DIM_UID,",
						"          RQST_STAT_DIM_UID,",
						"          RQST_TYPE_DIM_UID,",
						"          RQST_RCVD_DT,",
						"          PRPSL_RQST_DT,",
						"          CLNT_RVSD_PRPSL_RQSTD_DT,",
						"          PRPSL_SENT_TO_CLNT_DT,",
						"          PRPSL_SENT_TO_CLNT_DT_WD,",
						"          PRPSL_ACCPTD_DT,",
						"          PRPSL_ACCPTD_DT_WD,",
						"          PRPSL_REJCT_DT,",
						"          RQST_IMPLMNT_CMPLTD_DT,",
						"          CLNT_RVSD_RQST_IMPL_CMPL_DT,",
						"          IMPLMNT_READY_CUST_ACCEPT_DT,",
						"          IMPLMNT_READY_CUST_ACCEPT_DT_WD,",
						"          PRPSL_EXP_DT,",
						"          PRPSL_DSPSN_DT_WD,",
						"          ON_HOLD_DT,",
						"          WITHDRWN_DT,",
						"          DELD_DT,",
						"          LATEST_PRPSL_RQSTD_DT,",
						"          LATEST_IMPLMNT_RQSTD_DT,",
						"          PRPSL_OR_REJCT_DT,",
						"          MERGED_DT,",
						"          CLNT_ACCNTG_CD,",
						"          CLIENT_CNTCT_NAME,",
						"          CLNT_AFFCT_AREA_TXT,",
						"          AGRMT_REF_DTL_TXT,",
						"          SRVC_TYPE_TXT,",
						"          CLNT_APPRVL_NUM,",
						"          RFS_GROUP_NM,",
						"          SRVC_PRVDR_ORGNZN_TXT,",
						"          PREDEFND_SRVCS_DESC,",
						"          SIEBEL_OPPRTNTY_NUM,",
						"          BTT_NUM,",
						"          SOLTN_OWNER,",
						"          DLVR_COORDTR,",
						"          SOLTN_DESIGNER,",
						"          CLAIM_ACCNT_CD,",
						"          CLAIM_WORK_ITEM_CD,",
						"          RSRC_SUMRY_PRSNL_INVOLV_IND,",
						"          RSRC_SUMRY_SERVER_COM_IND,",
						"          RSRC_SUMRY_STRG_COM_IND,",
						"          RSRC_SUMRY_HW_COM_IND,",
						"          RSRC_SUMRY_SOFTWR_COM_IND,",
						"          RJCT_RSN_CD,",
						"          RJCT_SRC_CD,",
						"          RJCT_EXPLNN_TXT,",
						"          PRICE_TYPE_CD,",
						"          RSN_CD,",
						"          PURCHS_ORDR_NM,",
						"          UNSLCTD_RQST_IND,",
						"          CNTRCT_CHNG_RQRD_IND,",
						"          OATS_RCD_RRQRD_IND,",
						"          NON_STD_T_AND_C_IND,",
						"          TRVL_INVC_IND,",
						"          TERMNTN_CHRGS_IND,",
						"          RNWL_MNTH_CD,",
						"          BYPSS_SLTN_DSGN_IND,",
						"          PRPSL_OWNER,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          IS_DELETED,",
						"          HAS_PRNT_RFS_IND,",
						"          PRNT_PROC_DIM_UID,",
						"          PRJCT_DIM_UID,",
						"          L30_OFFRNG_ID_1,",
						"          L30_OFFRNG_ID_2,",
						"          L30_OFFRNG_ID_3,",
						"          GEO_DIM_UID,",
						"          ORGNTG_ORG,",
						"          PRICER_NM,",
						"          SOLTN_TEAM,",
						"          ERO_CHCKD_DT,",
						"          EA_CHCKD_DT,",
						"          CE_CHCKD_DT,",
						"          EXPORT_REG,",
						"          ENVMNTL_AFF,",
						"          CMPLNCE_ENGG,",
						"          GROSS_TCP_CTRY_CD,",
						"          GROSS_TCP_GEO_DIM_UID,",
						"          CLNT_PRPSL_RQSTD_FIRST_TM_DT,",
						"          CLNT_RVSD_PRPSL_RQSTD_FIRST_TM_DT,",
						"          PRPSL_SENT_TO_CLNT_FIRST_TM_DT,",
						"          CLNT_RQSTD_IMPLMTN_CMPLT_FIRST_TM_DT,",
						"          CLNT_RVSD_RQSTD_IMPLMTN_CMPLT_FIRST_TM_DT,",
						"          IMPLMTN_READY_FOR_CUST_ACCPTNC_FIRST_TM_DT,",
						"          ROW_STAT_CD = ROW_STATUS,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(DM_CRETD_USER_ID = $current_user,",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(DM_CRETD_USER_ID = $current_user,",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          PCR_CRC_DIM_UID,",
						"          PRPSL_PRICE_TERM_DIM_UID,",
						"          CMPLXTY_DIM_UID,",
						"          RQST_STAT_DIM_UID,",
						"          RQST_TYPE_DIM_UID,",
						"          RQST_RCVD_DT,",
						"          PRPSL_RQST_DT,",
						"          CLNT_RVSD_PRPSL_RQSTD_DT,",
						"          PRPSL_SENT_TO_CLNT_DT,",
						"          PRPSL_SENT_TO_CLNT_DT_WD,",
						"          PRPSL_ACCPTD_DT,",
						"          PRPSL_ACCPTD_DT_WD,",
						"          PRPSL_REJCT_DT,",
						"          RQST_IMPLMNT_CMPLTD_DT,",
						"          CLNT_RVSD_RQST_IMPL_CMPL_DT,",
						"          IMPLMNT_READY_CUST_ACCEPT_DT,",
						"          IMPLMNT_READY_CUST_ACCEPT_DT_WD,",
						"          PRPSL_EXP_DT,",
						"          PRPSL_DSPSN_DT_WD,",
						"          ON_HOLD_DT,",
						"          WITHDRWN_DT,",
						"          DELD_DT,",
						"          LATEST_PRPSL_RQSTD_DT,",
						"          LATEST_IMPLMNT_RQSTD_DT,",
						"          PRPSL_OR_REJCT_DT,",
						"          MERGED_DT,",
						"          CLNT_ACCNTG_CD,",
						"          CLIENT_CNTCT_NAME,",
						"          CLNT_AFFCT_AREA_TXT,",
						"          AGRMT_REF_DTL_TXT,",
						"          SRVC_TYPE_TXT,",
						"          CLNT_APPRVL_NUM,",
						"          RFS_GROUP_NM,",
						"          SRVC_PRVDR_ORGNZN_TXT,",
						"          PREDEFND_SRVCS_DESC,",
						"          SIEBEL_OPPRTNTY_NUM,",
						"          BTT_NUM,",
						"          SOLTN_OWNER,",
						"          DLVR_COORDTR,",
						"          SOLTN_DESIGNER,",
						"          CLAIM_ACCNT_CD,",
						"          CLAIM_WORK_ITEM_CD,",
						"          RSRC_SUMRY_PRSNL_INVOLV_IND,",
						"          RSRC_SUMRY_SERVER_COM_IND,",
						"          RSRC_SUMRY_STRG_COM_IND,",
						"          RSRC_SUMRY_HW_COM_IND,",
						"          RSRC_SUMRY_SOFTWR_COM_IND,",
						"          RJCT_RSN_CD,",
						"          RJCT_SRC_CD,",
						"          RJCT_EXPLNN_TXT,",
						"          PRICE_TYPE_CD,",
						"          RSN_CD,",
						"          PURCHS_ORDR_NM,",
						"          UNSLCTD_RQST_IND,",
						"          CNTRCT_CHNG_RQRD_IND,",
						"          OATS_RCD_RRQRD_IND,",
						"          NON_STD_T_AND_C_IND,",
						"          TRVL_INVC_IND,",
						"          TERMNTN_CHRGS_IND,",
						"          RNWL_MNTH_CD,",
						"          BYPSS_SLTN_DSGN_IND,",
						"          PRPSL_OWNER,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          IS_DELETED,",
						"          HAS_PRNT_RFS_IND,",
						"          PRNT_PROC_DIM_UID,",
						"          PRJCT_DIM_UID,",
						"          L30_OFFRNG_ID_1,",
						"          L30_OFFRNG_ID_2,",
						"          L30_OFFRNG_ID_3,",
						"          GEO_DIM_UID,",
						"          ORGNTG_ORG,",
						"          PRICER_NM,",
						"          SOLTN_TEAM,",
						"          ERO_CHCKD_DT,",
						"          EA_CHCKD_DT,",
						"          CE_CHCKD_DT,",
						"          EXPORT_REG,",
						"          ENVMNTL_AFF,",
						"          CMPLNCE_ENGG,",
						"          GROSS_TCP_CTRY_CD,",
						"          GROSS_TCP_GEO_DIM_UID,",
						"          CLNT_PRPSL_RQSTD_FIRST_TM_DT,",
						"          CLNT_RVSD_PRPSL_RQSTD_FIRST_TM_DT,",
						"          PRPSL_SENT_TO_CLNT_FIRST_TM_DT,",
						"          CLNT_RQSTD_IMPLMTN_CMPLT_FIRST_TM_DT,",
						"          CLNT_RVSD_RQSTD_IMPLMTN_CMPLT_FIRST_TM_DT,",
						"          IMPLMTN_READY_FOR_CUST_ACCPTNC_FIRST_TM_DT,",
						"          ROW_STAT_CD = ROW_STATUS,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"dropUnwantedKeyColumns derive(ROW_STATUS = case(isNull(PCR_CRC_DIM_UID) && not(isNull(LKP_PCR_CRC_DIM_UID)), 'D', case(not(isNull(PCR_CRC_DIM_UID)) && isNull(LKP_PCR_CRC_DIM_UID), 'I', case(HASHED_VAL == LKP_HASHED_VAL, 'L', 'U')))) ~> CDCval",
						"CDCval filter(ROW_STATUS != 'L') ~> dropUnchangedRows",
						"lookupHashColumn select(mapColumn(",
						"          each(match(true()),",
						"               'LKP_'+$$ = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> renamedColumns",
						"renamedColumns sort(asc(LKP_PCR_CRC_DIM_UID, false)) ~> sortedLookupData",
						"getSourcePCRData, getSourceCRCData union(byName: true)~> sourcePCRCRCData",
						"hashColumn, clientProposalRqstDate join(sourcePCRCRCData@PCR_CRC_DIM_UID == clientProposalRqstDate@PCR_CRC_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> joinCPRD",
						"joinCPRD, clientRevisedProposalRqstDate join(sourcePCRCRCData@PCR_CRC_DIM_UID == clientRevisedProposalRqstDate@PCR_CRC_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> joinCRPRD",
						"joinCRPRD, proposalSentToClientDate join(sourcePCRCRCData@PCR_CRC_DIM_UID == proposalSentToClientDate@PCR_CRC_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> joinPSTCD",
						"joinPSTCD, clientRqstImplementCompletionDate join(sourcePCRCRCData@PCR_CRC_DIM_UID == clientRqstImplementCompletionDate@PCR_CRC_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> joinCRICD",
						"joinCRICD, clientRevisedRqstImplementCompletionDate join(sourcePCRCRCData@PCR_CRC_DIM_UID == clientRevisedRqstImplementCompletionDate@PCR_CRC_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> joinCRRICD",
						"joinCRRICD, implementationReadyForCADate join(sourcePCRCRCData@PCR_CRC_DIM_UID == implementationReadyForCADate@PCR_CRC_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> joinIRFCAD",
						"mergedData select(mapColumn(",
						"          PCR_CRC_DIM_UID = sourcePCRCRCData@PCR_CRC_DIM_UID,",
						"          PRPSL_PRICE_TERM_DIM_UID,",
						"          CMPLXTY_DIM_UID,",
						"          RQST_STAT_DIM_UID,",
						"          RQST_TYPE_DIM_UID,",
						"          RQST_RCVD_DT,",
						"          PRPSL_RQST_DT,",
						"          CLNT_RVSD_PRPSL_RQSTD_DT,",
						"          PRPSL_SENT_TO_CLNT_DT,",
						"          PRPSL_SENT_TO_CLNT_DT_WD,",
						"          PRPSL_ACCPTD_DT,",
						"          PRPSL_ACCPTD_DT_WD,",
						"          PRPSL_REJCT_DT,",
						"          RQST_IMPLMNT_CMPLTD_DT,",
						"          CLNT_RVSD_RQST_IMPL_CMPL_DT,",
						"          IMPLMNT_READY_CUST_ACCEPT_DT,",
						"          IMPLMNT_READY_CUST_ACCEPT_DT_WD,",
						"          PRPSL_EXP_DT,",
						"          PRPSL_DSPSN_DT_WD,",
						"          ON_HOLD_DT,",
						"          WITHDRWN_DT,",
						"          DELD_DT,",
						"          LATEST_PRPSL_RQSTD_DT,",
						"          LATEST_IMPLMNT_RQSTD_DT,",
						"          PRPSL_OR_REJCT_DT,",
						"          MERGED_DT,",
						"          CLNT_ACCNTG_CD,",
						"          CLIENT_CNTCT_NAME,",
						"          CLNT_AFFCT_AREA_TXT,",
						"          AGRMT_REF_DTL_TXT,",
						"          SRVC_TYPE_TXT,",
						"          CLNT_APPRVL_NUM,",
						"          RFS_GROUP_NM,",
						"          SRVC_PRVDR_ORGNZN_TXT,",
						"          PREDEFND_SRVCS_DESC,",
						"          SIEBEL_OPPRTNTY_NUM,",
						"          BTT_NUM,",
						"          SOLTN_OWNER,",
						"          DLVR_COORDTR,",
						"          SOLTN_DESIGNER,",
						"          CLAIM_ACCNT_CD,",
						"          CLAIM_WORK_ITEM_CD,",
						"          RSRC_SUMRY_PRSNL_INVOLV_IND,",
						"          RSRC_SUMRY_SERVER_COM_IND,",
						"          RSRC_SUMRY_STRG_COM_IND,",
						"          RSRC_SUMRY_HW_COM_IND,",
						"          RSRC_SUMRY_SOFTWR_COM_IND,",
						"          RJCT_RSN_CD,",
						"          RJCT_SRC_CD,",
						"          RJCT_EXPLNN_TXT,",
						"          PRICE_TYPE_CD,",
						"          RSN_CD,",
						"          PURCHS_ORDR_NM,",
						"          UNSLCTD_RQST_IND,",
						"          CNTRCT_CHNG_RQRD_IND,",
						"          OATS_RCD_RRQRD_IND,",
						"          NON_STD_T_AND_C_IND,",
						"          TRVL_INVC_IND,",
						"          TERMNTN_CHRGS_IND,",
						"          RNWL_MNTH_CD,",
						"          BYPSS_SLTN_DSGN_IND,",
						"          PRPSL_OWNER,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          IS_DELETED,",
						"          HAS_PRNT_RFS_IND,",
						"          PRNT_PROC_DIM_UID,",
						"          PRJCT_DIM_UID,",
						"          L30_OFFRNG_ID_1,",
						"          L30_OFFRNG_ID_2,",
						"          L30_OFFRNG_ID_3,",
						"          GEO_DIM_UID,",
						"          ORGNTG_ORG,",
						"          PRICER_NM,",
						"          SOLTN_TEAM,",
						"          ERO_CHCKD_DT,",
						"          EA_CHCKD_DT,",
						"          CE_CHCKD_DT,",
						"          EXPORT_REG,",
						"          ENVMNTL_AFF,",
						"          CMPLNCE_ENGG,",
						"          GROSS_TCP_CTRY_CD,",
						"          GROSS_TCP_GEO_DIM_UID,",
						"          CLNT_PRPSL_RQSTD_FIRST_TM_DT,",
						"          CLNT_RVSD_PRPSL_RQSTD_FIRST_TM_DT,",
						"          PRPSL_SENT_TO_CLNT_FIRST_TM_DT,",
						"          CLNT_RQSTD_IMPLMTN_CMPLT_FIRST_TM_DT,",
						"          CLNT_RVSD_RQSTD_IMPLMTN_CMPLT_FIRST_TM_DT,",
						"          IMPLMTN_READY_FOR_CUST_ACCPTNC_FIRST_TM_DT,",
						"          HASHED_VAL,",
						"          LKP_PCR_CRC_DIM_UID,",
						"          LKP_PRPSL_PRICE_TERM_DIM_UID,",
						"          LKP_CMPLXTY_DIM_UID,",
						"          LKP_RQST_STAT_DIM_UID,",
						"          LKP_RQST_TYPE_DIM_UID,",
						"          LKP_RQST_RCVD_DT,",
						"          LKP_PRPSL_RQST_DT,",
						"          LKP_CLNT_RVSD_PRPSL_RQSTD_DT,",
						"          LKP_PRPSL_SENT_TO_CLNT_DT,",
						"          LKP_PRPSL_SENT_TO_CLNT_DT_WD,",
						"          LKP_PRPSL_ACCPTD_DT,",
						"          LKP_PRPSL_ACCPTD_DT_WD,",
						"          LKP_PRPSL_REJCT_DT,",
						"          LKP_RQST_IMPLMNT_CMPLTD_DT,",
						"          LKP_CLNT_RVSD_RQST_IMPL_CMPL_DT,",
						"          LKP_IMPLMNT_READY_CUST_ACCEPT_DT,",
						"          LKP_IMPLMNT_READY_CUST_ACCEPT_DT_WD,",
						"          LKP_PRPSL_EXP_DT,",
						"          LKP_PRPSL_DSPSN_DT_WD,",
						"          LKP_ON_HOLD_DT,",
						"          LKP_WITHDRWN_DT,",
						"          LKP_DELD_DT,",
						"          LKP_LATEST_PRPSL_RQSTD_DT,",
						"          LKP_LATEST_IMPLMNT_RQSTD_DT,",
						"          LKP_PRPSL_OR_REJCT_DT,",
						"          LKP_MERGED_DT,",
						"          LKP_CLNT_ACCNTG_CD,",
						"          LKP_CLNT_CNTCT_NM,",
						"          LKP_CLNT_AFFCT_AREA_TXT,",
						"          LKP_AGRMT_REF_DTL_TXT,",
						"          LKP_SRVC_TYPE_TXT,",
						"          LKP_CLNT_APPRVL_NUM,",
						"          LKP_RFS_GROUP_NM,",
						"          LKP_SRVC_PRVDR_ORGNZN_TXT,",
						"          LKP_PREDEFND_SRVCS_DESC,",
						"          LKP_SIEBEL_OPPRTNTY_NUM,",
						"          LKP_BTT_NUM,",
						"          LKP_SLTN_OWNR_TXT,",
						"          LKP_DLVR_CRDNTR_TXT,",
						"          LKP_SLTN_DSGNR_TXT,",
						"          LKP_CLAIM_ACCNT_CD,",
						"          LKP_CLAIM_WORK_ITEM_CD,",
						"          LKP_RSRC_SUMRY_PRSNL_INVOLV_IND,",
						"          LKP_RSRC_SUMRY_SERVER_COM_IND,",
						"          LKP_RSRC_SUMRY_STRG_COM_IND,",
						"          LKP_RSRC_SUMRY_HW_COM_IND,",
						"          LKP_RSRC_SUMRY_SOFTWR_COM_IND,",
						"          LKP_RJCT_RSN_CD,",
						"          LKP_RJCT_SRC_CD,",
						"          LKP_RJCT_EXPLNN_TXT,",
						"          LKP_PRICE_TYPE_CD,",
						"          LKP_RSN_CD,",
						"          LKP_PURCHS_ORDR_NM,",
						"          LKP_UNSLCTD_RQST_IND,",
						"          LKP_CNTRCT_CHNG_RQRD_IND,",
						"          LKP_OATS_RCD_RRQRD_IND,",
						"          LKP_NON_STD_T_AND_C_IND,",
						"          LKP_TRVL_INVC_IND,",
						"          LKP_TERMNTN_CHRGS_IND,",
						"          LKP_RNWL_MNTH_CD,",
						"          LKP_BYPSS_SLTN_DSGN_IND,",
						"          LKP_PRPSL_OWNR_TXT,",
						"          LKP_ETL_JOB_ID,",
						"          LKP_SRC_SYS_DIM_UID,",
						"          LKP_HAS_PRNT_RFS_IND,",
						"          LKP_PRNT_PROC_DIM_UID,",
						"          LKP_PRJCT_DIM_UID,",
						"          LKP_L30_OFFRNG_ID_1,",
						"          LKP_L30_OFFRNG_ID_2,",
						"          LKP_L30_OFFRNG_ID_3,",
						"          LKP_CLNT_PRPSL_RQSTD_FIRST_TM_DT,",
						"          LKP_CLNT_RVSD_PRPSL_RQSTD_FIRST_TM_DT,",
						"          LKP_PRPSL_SENT_TO_CLNT_FIRST_TM_DT,",
						"          LKP_CLNT_RQSTD_IMPLMTN_CMPLT_FIRST_TM_DT,",
						"          LKP_CLNT_RVSD_RQSTD_IMPLMTN_CMPLT_FIRST_TM_DT,",
						"          LKP_IMPLMTN_READY_FOR_CUST_ACCPTNC_FIRST_TM_DT,",
						"          LKP_GEO_DIM_UID,",
						"          LKP_ORGNTG_ORG,",
						"          LKP_PRICER_NM,",
						"          LKP_SOLTN_TEAM,",
						"          LKP_ERO_CHCKD_DT,",
						"          LKP_EA_CHCKD_DT,",
						"          LKP_CE_CHCKD_DT,",
						"          LKP_EXPORT_REG,",
						"          LKP_ENVMNTL_AFF,",
						"          LKP_CMPLNCE_ENGG,",
						"          LKP_GROSS_TCP_CTRY_CD,",
						"          LKP_GROSS_TCP_GEO_DIM_UID,",
						"          LKP_HASHED_VAL",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> dropUnwantedKeyColumns",
						"sourcePCRCRCData derive(HASHED_VAL = sha2(256, byNames(split($hash_columns, ',')))) ~> hashColumn",
						"getLookupData derive(HASHED_VAL = sha2(256, byNames(split($hash_columns, ',')))) ~> lookupHashColumn",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'PCR_CRC_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['PCR_CRC_DIM_UID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'PCR_CRC_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0340_PRFRMNC_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 13-07-2022\nJob Name: DF_BALD0340_PRFRMNC_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						},
						{
							"name": "CDCval"
						},
						{
							"name": "dropUnchangedRows"
						}
					],
					"scriptLines": [
						"parameters{",
						"     current_user as string",
						"}",
						"source(output(",
						"          PRFRMNC_DIM_UID as integer,",
						"          PRJCT_ID as string,",
						"          PRFRMNC_REM_TXT as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer,",
						"          ORIG_ORG as string,",
						"          HASHED_VAL as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     PRF.PROC_ID as PRFRMNC_DIM_UID,\\n     PRF.PROJECT_ID as PRJCT_ID,\\n     cast(PRF.REMARKS as varchar(1024)) as PRFRMNC_REM_TXT,\\n     PRF.CREATED_TS as SRC_CRETD_TMS,\\n     PRF.CREATED_USERID as SRC_CRETD_USER_ID,\\n     PRF.UPDATED_TS as SRC_UPDTD_TMS,\\n     PRF.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED,\\n     PRF.ORIG_ORG,\\n     HashBytes(\\'SHA2_256\\',\\n          coalesce(cast(PRF.PROJECT_ID as varchar), \\'\\') +\\n          coalesce(cast(PRF.REMARKS as varchar(1024)), \\'\\') +\\n          coalesce(cast(JOB.ETL_JOB_ID as varchar), \\'\\') +\\n          coalesce(cast(SYS.SRC_SYS_DIM_UID as varchar), \\'\\') +\\n          coalesce(PRF.ORIG_ORG, \\'\\') \\n     ) as HASHED_VAL\\nFrom\\n     APPFUN.PERFORMANCE PRF\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on PRF.PROC_ID = ZDT.PROC_ID\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on PRF.PROC_ID = PDEL.PROC_ID\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD0340_PRFRMNC_DIM\\'\\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'\\n     ) SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          LKP_PRFRMNC_DIM_UID as integer,",
						"          LKP_PRJCT_ID as string,",
						"          LKP_PRFRMNC_REM_TXT as string,",
						"          LKP_SRC_CRETD_TMS as timestamp,",
						"          LKP_SRC_CRETD_USER_ID as string,",
						"          LKP_SRC_UPDTD_TMS as timestamp,",
						"          LKP_SRC_UPDTD_USER_ID as string,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_ETL_JOB_ID as integer,",
						"          LKP_ORIG_ORG as string,",
						"          LKP_HASHED_VAL as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     P.PRFRMNC_DIM_UID as LKP_PRFRMNC_DIM_UID,\\n     P.PRJCT_ID as LKP_PRJCT_ID,\\n     P.PRFRMNC_REM_TXT as LKP_PRFRMNC_REM_TXT,\\n     P.SRC_CRETD_TMS as LKP_SRC_CRETD_TMS,\\n     P.SRC_CRETD_USER_ID as LKP_SRC_CRETD_USER_ID,\\n     P.SRC_UPDTD_TMS as LKP_SRC_UPDTD_TMS,\\n     P.SRC_UPDTD_USER_ID as LKP_SRC_UPDTD_USER_ID,\\n     P.SRC_SYS_DIM_UID as LKP_SRC_SYS_DIM_UID,\\n     P.ETL_JOB_ID as LKP_ETL_JOB_ID,\\n    P.ORIG_ORG as LKP_ORIG_ORG,\\n     HashBytes(\\'SHA2_256\\',\\n          coalesce(cast(PRJCT_ID as varchar), \\'\\') +\\n          coalesce(PRFRMNC_REM_TXT, \\'\\') +\\n          coalesce(cast(ETL_JOB_ID as varchar), \\'\\') +\\n          coalesce(cast(SRC_SYS_DIM_UID as varchar), \\'\\') +\\n          coalesce(ORIG_ORG, \\'\\') \\n     ) as LKP_HASHED_VAL\\nFrom\\n     PGMPDM.PRFRMNC_DIM P\\n    inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on P.PRFRMNC_DIM_UID = ZDT.PROC_ID',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"getLookupData sort(asc(LKP_PRFRMNC_DIM_UID, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"dropUnchangedRows split(ROW_STATUS!='I',",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(PRFRMNC_DIM_UID == LKP_PRFRMNC_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          PRFRMNC_DIM_UID,",
						"          PRJCT_ID,",
						"          PRFRMNC_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ORIG_ORG,",
						"          ROW_STAT_CD = ROW_STATUS,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(DM_CRETD_USER_ID = $current_user,",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(DM_CRETD_USER_ID = $current_user,",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          PRFRMNC_DIM_UID,",
						"          PRJCT_ID,",
						"          PRFRMNC_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD = ROW_STATUS,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"mergedData derive(ROW_STATUS = case(PRFRMNC_DIM_UID == toInteger(null()) && LKP_PRFRMNC_DIM_UID != toInteger(null()), 'D', case(PRFRMNC_DIM_UID != toInteger(null()) && LKP_PRFRMNC_DIM_UID == toInteger(null()), 'I', case(HASHED_VAL == LKP_HASHED_VAL, 'L', 'U')))) ~> CDCval",
						"CDCval filter(ROW_STATUS != 'L') ~> dropUnchangedRows",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'PRFRMNC_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['PRFRMNC_DIM_UID'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'PRFRMNC_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0345_PRICE_TYPE_CD_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 12-07-2022\nJob Name: DF_BALD0345_PRICE_TYPE_CD_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						},
						{
							"name": "CDCval"
						},
						{
							"name": "dropUnchangedRows"
						}
					],
					"scriptLines": [
						"parameters{",
						"     current_user as string",
						"}",
						"source(output(",
						"          MAX_ID as integer,",
						"          ROW_NUM as integer,",
						"          PRICE_TYPE_CD as string,",
						"          PRICE_TYPE_DESC as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          HASHED_VAL as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select \\n     (Select coalesce(max(PRICE_TYPE_CD_DIM_UID), 0) from PGMPDM.PRICE_TYPE_CD_DIM) as MAX_ID,\\n     cast(ROW_NUMBER() over (order by S.INTERNAL_VAL) as Integer) as ROW_NUM,\\n     cast(S.INTERNAL_VAL as char(1)) as PRICE_TYPE_CD,\\n     cast(S.TEXT as varchar(100)) as PRICE_TYPE_DESC,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     HashBytes(\\'SHA2_256\\',\\n          coalesce(cast(S.INTERNAL_VAL as char(1)), \\'\\') +\\n          coalesce(cast(S.TEXT as varchar(100)), \\'\\') +\\n          coalesce(cast(JOB.ETL_JOB_ID as varchar), \\'\\') +\\n          coalesce(cast(SYS.SRC_SYS_DIM_UID as varchar), \\'\\')\\n     ) as HASHED_VAL\\nFrom\\n     APPFUN.SEL_LIST_REF_V S\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD0345_PRICE_TYPE_CD_DIM\\'\\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'\\n     ) SYS\\n     on 1 = 1\\nWhere\\n     S.UI_SEL_LIST_ID = 95566\\n     and S.LANG_ID = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          LKP_PRICE_TYPE_CD as string,",
						"          LKP_PRICE_TYPE_DESC as string,",
						"          LKP_ETL_JOB_ID as integer,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_HASHED_VAL as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     PRICE_TYPE_CD as LKP_PRICE_TYPE_CD,\\n     PRICE_TYPE_DESC as LKP_PRICE_TYPE_DESC,\\n     ETL_JOB_ID as LKP_ETL_JOB_ID,\\n     SRC_SYS_DIM_UID as LKP_SRC_SYS_DIM_UID,\\n     HashBytes(\\'SHA2_256\\', \\n          coalesce(cast(PRICE_TYPE_CD as varchar), \\'\\') +\\n          coalesce(cast(PRICE_TYPE_DESC as varchar), \\'\\') + \\n          coalesce(cast(ETL_JOB_ID as varchar), \\'\\') + \\n          coalesce(cast(SRC_SYS_DIM_UID as varchar), \\'\\') \\n     ) as LKP_HASHED_VAL\\nFrom\\n     PGMPDM.PRICE_TYPE_CD_DIM',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"getLookupData sort(asc(LKP_PRICE_TYPE_CD, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"dropUnchangedRows split(ROW_STATUS!='I',",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(PRICE_TYPE_CD == LKP_PRICE_TYPE_CD,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          PRICE_TYPE_CD_DIM_UID,",
						"          PRICE_TYPE_CD,",
						"          PRICE_TYPE_DESC,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD = ROW_STATUS,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(DM_CRETD_USER_ID = $current_user,",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(PRICE_TYPE_CD_DIM_UID = MAX_ID + ROW_NUM,",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          PRICE_TYPE_CD,",
						"          PRICE_TYPE_DESC,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD = ROW_STATUS,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"mergedData derive(ROW_STATUS = case(PRICE_TYPE_CD == toString(null()) && LKP_PRICE_TYPE_CD != toString(null()), 'D', case(PRICE_TYPE_CD != toString(null()) && LKP_PRICE_TYPE_CD == toString(null()), 'I', case(HASHED_VAL == LKP_HASHED_VAL, 'L', 'U')))) ~> CDCval",
						"CDCval filter(ROW_STATUS != 'L') ~> dropUnchangedRows",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'PRICE_TYPE_CD_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['PRICE_TYPE_CD'],",
						"     skipKeyWrites:true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'PRICE_TYPE_CD_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD0600_BIZDEVMT_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 07-06-2022\nJob Name: DF_BALD0600_BIZDEVMT_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						}
					],
					"scriptLines": [
						"source(output(",
						"          BIZDEVMT_DIM_UID as integer,",
						"          PRJCT_ID as string,",
						"          BIZDEVMT_REM_TXT as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     P.PROC_ID as BIZDEVMT_DIM_UID,\\n     P.PROJECT_ID as PRJCT_ID,\\n     cast(P.REMARKS as varchar(2048)) as BIZDEVMT_REM_TXT,\\n     P.CREATED_TS as SRC_CRETD_TMS,\\n     P.CREATED_USERID as SRC_CRETD_USER_ID,\\n     P.UPDATED_TS as SRC_UPDTD_TMS,\\n     P.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED\\nFrom\\n     APPFUN.BIZDEVMT P\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on P.PROC_ID = ZDT.PROC_ID\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on P.PROC_ID = PDEL.PROC_ID\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD0600_BIZDEVMT_DIM\\'\\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'\\n     ) SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          LKP_BIZDEVMT_DIM_UID as integer,",
						"          LKP_PRJCT_ID as string,",
						"          LKP_BIZDEVMT_REM_TXT as string,",
						"          LKP_SRC_CRETD_TMS as timestamp,",
						"          LKP_SRC_CRETD_USER_ID as string,",
						"          LKP_SRC_UPDTD_TMS as timestamp,",
						"          LKP_SRC_UPDTD_USER_ID as string,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_ETL_JOB_ID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     P.BIZDEVMT_DIM_UID as LKP_BIZDEVMT_DIM_UID,\\n     P.PRJCT_ID as LKP_PRJCT_ID,\\n     P.BIZDEVMT_REM_TXT as LKP_BIZDEVMT_REM_TXT,\\n     P.SRC_CRETD_TMS as LKP_SRC_CRETD_TMS,\\n     P.SRC_CRETD_USER_ID as LKP_SRC_CRETD_USER_ID,\\n     P.SRC_UPDTD_TMS as LKP_SRC_UPDTD_TMS,\\n     P.SRC_UPDTD_USER_ID as LKP_SRC_UPDTD_USER_ID,\\n     P.SRC_SYS_DIM_UID as LKP_SRC_SYS_DIM_UID,\\n     P.ETL_JOB_ID as LKP_ETL_JOB_ID\\nFrom\\n     PGMPDM.BIZDEVMT_DIM P\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on P.BIZDEVMT_DIM_UID = ZDT.PROC_ID     ',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"getLookupData sort(asc(LKP_BIZDEVMT_DIM_UID, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"mergedData split(BIZDEVMT_DIM_UID==LKP_BIZDEVMT_DIM_UID,",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(BIZDEVMT_DIM_UID == LKP_BIZDEVMT_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          BIZDEVMT_DIM_UID,",
						"          PRJCT_ID,",
						"          BIZDEVMT_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'U'),",
						"          DM_CRETD_USER_ID = \"dsdam\",",
						"          DM_UPDTD_USER_ID = \"dsdam\",",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(ROW_STAT_CD = iif(IS_DELETED == 1, 'D', 'I'),",
						"          DM_CRETD_USER_ID = \"dsdam\",",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = \"dsdam\",",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          BIZDEVMT_DIM_UID,",
						"          PRJCT_ID,",
						"          BIZDEVMT_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          ROW_STAT_CD,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'BIZDEVMT_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['BIZDEVMT_DIM_UID'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'BIZDEVMT_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_BALD1090_CNTRCT_DIM')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Job Creation: 22-06-2022\nJob Name: DF_BALD1090_CNTRCT_DIM\nCreated By: Subramanian R",
				"folder": {
					"name": "PGMP/Dimension"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "updateTable"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "insertTable"
						}
					],
					"transformations": [
						{
							"name": "sortedLookupData"
						},
						{
							"name": "updateRows"
						},
						{
							"name": "CDC"
						},
						{
							"name": "mergedData"
						},
						{
							"name": "selectInsertData"
						},
						{
							"name": "addColumns1"
						},
						{
							"name": "addColumns2"
						},
						{
							"name": "selectUpdateData"
						},
						{
							"name": "CDCval"
						},
						{
							"name": "dropUnchangedRows"
						}
					],
					"scriptLines": [
						"parameters{",
						"     current_user as string",
						"}",
						"source(output(",
						"          MAX_ID as integer,",
						"          ROW_NUM as long,",
						"          CNTRCT_ID as integer,",
						"          CLNT_DIM_UID as integer,",
						"          SO_NONSO_DIM_UID as integer,",
						"          CNTRCT_STATE_CD as string,",
						"          CNTCT_TXT as string,",
						"          CNTRCT_NUM as string,",
						"          CNTRCT_NM as string,",
						"          CNTRCT_ATTCHMT_LCTN_TXT as string,",
						"          START_DT as date,",
						"          END_DT as date,",
						"          LINE_OF_BUS_TYPE_CD as string,",
						"          INDSTRY_TXT as string,",
						"          INDSTRY_SRC_TXT as string,",
						"          VRFYD_DT as date,",
						"          INDSTRY_OVERRDEN_IND as string,",
						"          GEO_DIM_UID as integer,",
						"          SCTR_DIM_UID as integer,",
						"          CNTRCT_TYPE_DIM_UID as integer,",
						"          PRVT_IND as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          CLIENT_UNIT as string,",
						"          HASHED_VAL as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     (Select \\n          coalesce(max(CNTRCT_DIM_UID), 0) \\n      From PGMPDM.CNTRCT_DIM\\n     ) as MAX_ID,\\n     ROW_NUMBER() over (order by C.CNTRCT_ID) as ROW_NUM,\\n     C.CNTRCT_ID as CNTRCT_ID,\\n     C.CLIENT_ID as CLNT_DIM_UID, -- The CLIENT_ID is reused as CLNT_DIM_UID, not look up required\\n     SND.SO_NONSO_DIM_UID as SO_NONSO_DIM_UID,\\n     coalesce(C.STATUS, PC.STATUS) as CNTRCT_STATE_CD,\\n     coalesce(C.CNTCT, PC.CNTCT) as CNTCT_TXT,\\n     C.CNTRCT_NUMBER as CNTRCT_NUM,\\n     C.NAME as CNTRCT_NM,\\n     C.CNTRCT_ATTCHMNT_LOC as CNTRCT_ATTCHMT_LCTN_TXT,\\n     C.CNTRCT_START_DATE as START_DT,\\n     C.CNTRCT_END_DATE as END_DT,\\n     PC.LINE_OF_BUSINESS_TYPE as LINE_OF_BUS_TYPE_CD,\\n     PC.INDUSTRY_TXT as INDSTRY_TXT,\\n     PC.INDUSTRY_SOURCE_TXT as INDSTRY_SRC_TXT,\\n     PC.VERIFIED_DATE as VRFYD_DT,\\n     PC.INDUSTRY_OVERRIDDEN_FLAG as INDSTRY_OVERRDEN_IND,\\n     G.GEO_DIM_UID as GEO_DIM_UID,\\n     coalesce(SD.SCTR_DIM_UID, -1) as SCTR_DIM_UID,\\n     coalesce(CT.CNTRCT_TYPE_DIM_UID, -1) as CNTRCT_TYPE_DIM_UID,\\n     case \\n          when PRIVATE_CONTRACTS.CNTRCT_ID is null then cast(\\'N\\' as char(1)) \\n          else cast(\\'Y\\' as char(1)) \\n     end as PRVT_IND,\\n     C.CREATED_TS as SRC_CRETD_TMS,\\n     C.CREATED_USERID as SRC_CRETD_USER_ID,\\n     C.UPDATED_TS as SRC_UPDTD_TMS,\\n     C.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     C.CLIENT_UNIT as CLIENT_UNIT,\\n     HashBytes(\\'SHA2_256\\', \\n          coalesce(cast(C.CLIENT_ID as varchar), \\'\\') +\\n          coalesce(cast(SND.SO_NONSO_DIM_UID as varchar), \\'\\') +\\n          coalesce(C.STATUS, PC.STATUS, \\'\\') +\\n          coalesce(C.CNTCT, PC.CNTCT, \\'\\') +\\n          coalesce(C.CNTRCT_NUMBER, \\'\\') + \\n          coalesce(C.NAME, \\'\\') +\\n          coalesce(C.CNTRCT_ATTCHMNT_LOC, \\'\\') +\\n          coalesce(cast(C.CNTRCT_START_DATE as varchar), \\'\\') +\\n          coalesce(cast(C.CNTRCT_END_DATE as varchar), \\'\\') +\\n          coalesce(PC.LINE_OF_BUSINESS_TYPE, \\'\\') +\\n          coalesce(PC.INDUSTRY_TXT, \\'\\') +\\n          coalesce(PC.INDUSTRY_SOURCE_TXT, \\'\\') +\\n          coalesce(cast(PC.VERIFIED_DATE as varchar), \\'\\') +\\n          coalesce(PC.INDUSTRY_OVERRIDDEN_FLAG, \\'\\') +\\n          coalesce(cast(G.GEO_DIM_UID as varchar), \\'\\') +\\n          cast(coalesce(SD.SCTR_DIM_UID, -1) as varchar) +\\n          cast(coalesce(CT.CNTRCT_TYPE_DIM_UID, -1)  as varchar) +\\n          case \\n               when PRIVATE_CONTRACTS.CNTRCT_ID is null then cast(\\'N\\' as char(1)) \\n               else cast(\\'Y\\' as char(1)) \\n          end +\\n          cast(C.CREATED_TS as varchar) +\\n          cast(C.CREATED_USERID as varchar) +\\n          cast(C.UPDATED_TS as varchar) +\\n          cast(C.UPDATED_USERID as varchar) +\\n          cast(JOB.ETL_JOB_ID as varchar) +\\n          cast(SYS.SRC_SYS_DIM_UID as varchar) +\\n          coalesce(cast(C.CLIENT_UNIT as varchar), \\'\\')\\n     ) as HASHED_VAL\\nFrom\\n     APPFUN.CNTRCT C\\n     left join\\n     APPFUN.PST_CNTRCT PC\\n     on C.CNTRCT_ID = PC.PST_CNTRCT_ID\\n     -- SO_NONSO_DIM_UID: Find out if the contract is defined as Non-SO\\n     left join\\n     APPFUN.NON_SO_CNTRCT_TMF SOTMF\\n     on C.CNTRCT_ID = SOTMF.CNTRCT_ID\\n     left join\\n     PGMPDM.SO_NONSO_DIM SND\\n     -- The case validates if the TMF record for the contract is null\\n     -- If the value is null, the contract is NOT considered Non-SO, which means \"SO\"\\n     -- Otherwise, if the contract id is found in the tmf table, \\n     -- it implies that the record should be treated as \"NON_SO\" \\n     on case when SOTMF.CNTRCT_ID is null then \\'SO\\' else \\'NON_SO\\' end = SND.SO_NONSO_CD\\n     -- GEO_DIM_UID: find out the corresponding GEO record\\n     left join\\n     APPFUN.ORG_CNTRCT_MAP OCM \\n     on C.CNTRCT_ID = OCM.CNTRCT_ID\\n     left join\\n     APPFUN.PST_ORG O \\n     on OCM.ORG_ID = O.PST_ORG_ID\\n     left join\\n     PGMPDM.GEO_DIM G\\n     on O.CNTRY = G.SRGN_CD     \\n     -- SCTR_DIM_UID: find out the corresponding sector\\n     left join\\n     PGMPDM.SCTR_DIM SD\\n     on PC.INDUSTRY_TXT = SD.ISU_INDSTRY_CD         \\n     -- CNTRCT_TYPE_DIM_UID: Logic to find out the corresponding contract type\\n     left join \\n     PGMPDM.CNTRCT_TYPE_DIM CT\\n     on \\n          case\\n            when O.TST_ACCNT_IND = \\'Y\\' AND C.CNTRCT_ID = 1203950002 then \\'Production Contract\\'  -- Jira PGMP-931\\n               when /*O.PST_ORG_STATUS = \\'O\\' and */ O.ORG_ACTV_FLAG = \\'N\\' then \\'Inactive Contract\\'   ---  795819\\n               when O.TST_ACCNT_IND =\\'Y\\' then \\'Test Contract\\'\\n            when O.TST_ACCNT_IND =\\'I\\' then \\'Inactive Contract\\'    ------ added as part of 553286\\n               when C.STATUS = \\'A\\' and lower(C.NAME) like \\'test%\\' then \\'Test Contract\\'          \\n               else \\'Production Contract\\'\\n        end = CT.CNTRCT_TYPE_DESC\\n     -- Determination of the private contracts  \\n     left join (\\n          Select \\n               OCM.CNTRCT_ID\\n          From \\n               (\\n                    Select\\n                         ORG_ID\\n                    From\\n                         APPFUN.ORG R\\n                    Where\\n                         R.PARENT_ORG_ID is null\\n                         and upper(R.TITLE) like \\'%PRIVATE%\\'                               \\n               ) R\\n               left join\\n               APPFUN.ORG C1\\n               on R.ORG_ID = C1.PARENT_ORG_ID\\n               left join\\n               APPFUN.ORG C2\\n               on C1.ORG_ID = C2.PARENT_ORG_ID\\n               left join\\n               APPFUN.ORG C3\\n               on C2.ORG_ID = C3.PARENT_ORG_ID\\n               left join\\n               APPFUN.ORG_CNTRCT_MAP OCM\\n               on \\n                    C1.ORG_ID = OCM.ORG_ID\\n                    or C2.ORG_ID = OCM.ORG_ID\\n                    or C3.ORG_ID = OCM.ORG_ID\\n          Where\\n               OCM.CNTRCT_ID is not null     \\n     ) PRIVATE_CONTRACTS\\n     on C.CNTRCT_ID = PRIVATE_CONTRACTS.CNTRCT_ID\\n     left join (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          From\\n               PGMPDM.ZAUX_ETL_JOBS\\n          Where\\n               ETL_JOB_NM = \\'BALD1090_CNTRCT_DIM\\'          \\n     ) JOB\\n     on 1 = 1\\n     left join (\\n          Select\\n               ETL_EXCTN_ID\\n          From\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where\\n               IS_CURR_IND = \\'Y\\'\\n     ) FIL\\n     on 1 = 1\\n     left join (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          From\\n               PGMPDM.SRC_SYS_DIM\\n          Where\\n               SRC_SYS_CD = \\'PGMP\\'               \\n     ) SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          LKP_CNTRCT_ID as integer,",
						"          LKP_CLNT_DIM_UID as integer,",
						"          LKP_SO_NONSO_DIM_UID as integer,",
						"          LKP_CNTRCT_STATE_CD as string,",
						"          LKP_CNTCT_TXT as string,",
						"          LKP_CNTRCT_NUM as string,",
						"          LKP_CNTRCT_NM as string,",
						"          LKP_CNTRCT_ATTCHMT_LCTN_TXT as string,",
						"          LKP_START_DT as date,",
						"          LKP_END_DT as date,",
						"          LKP_LINE_OF_BUS_TYPE_CD as string,",
						"          LKP_INDSTRY_TXT as string,",
						"          LKP_INDSTRY_SRC_TXT as string,",
						"          LKP_VRFYD_DT as date,",
						"          LKP_INDSTRY_OVERRDEN_IND as string,",
						"          LKP_GEO_DIM_UID as integer,",
						"          LKP_SCTR_DIM_UID as integer,",
						"          LKP_CNTRCT_TYPE_DIM_UID as integer,",
						"          LKP_PRVT_IND as string,",
						"          LKP_SRC_CRETD_TMS as timestamp,",
						"          LKP_SRC_CRETD_USER_ID as string,",
						"          LKP_SRC_UPDTD_TMS as timestamp,",
						"          LKP_SRC_UPDTD_USER_ID as string,",
						"          LKP_ETL_JOB_ID as integer,",
						"          LKP_SRC_SYS_DIM_UID as integer,",
						"          LKP_CLIENT_UNIT as string,",
						"          LKP_HASHED_VAL as binary",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     CNTRCT_ID as LKP_CNTRCT_ID,\\n     CLNT_DIM_UID as LKP_CLNT_DIM_UID,\\n     SO_NONSO_DIM_UID as LKP_SO_NONSO_DIM_UID,\\n     CNTRCT_STATE_CD as LKP_CNTRCT_STATE_CD,\\n     CNTCT_TXT as LKP_CNTCT_TXT,\\n     CNTRCT_NUM as LKP_CNTRCT_NUM,\\n     CNTRCT_NM as LKP_CNTRCT_NM,\\n     CNTRCT_ATTCHMT_LCTN_TXT as LKP_CNTRCT_ATTCHMT_LCTN_TXT,\\n     START_DT as LKP_START_DT,\\n     END_DT as LKP_END_DT,\\n     LINE_OF_BUS_TYPE_CD as LKP_LINE_OF_BUS_TYPE_CD,\\n     INDSTRY_TXT as LKP_INDSTRY_TXT,\\n     INDSTRY_SRC_TXT as LKP_INDSTRY_SRC_TXT,\\n     VRFYD_DT as LKP_VRFYD_DT,\\n     INDSTRY_OVERRDEN_IND as LKP_INDSTRY_OVERRDEN_IND,\\n     GEO_DIM_UID as LKP_GEO_DIM_UID,\\n     SCTR_DIM_UID as LKP_SCTR_DIM_UID,\\n     CNTRCT_TYPE_DIM_UID as LKP_CNTRCT_TYPE_DIM_UID,\\n     PRVT_IND as LKP_PRVT_IND,\\n     SRC_CRETD_TMS as LKP_SRC_CRETD_TMS,\\n     SRC_CRETD_USER_ID as LKP_SRC_CRETD_USER_ID,\\n     SRC_UPDTD_TMS as LKP_SRC_UPDTD_TMS,\\n     SRC_UPDTD_USER_ID as LKP_SRC_UPDTD_USER_ID,\\n     ETL_JOB_ID as LKP_ETL_JOB_ID,\\n     SRC_SYS_DIM_UID as LKP_SRC_SYS_DIM_UID,\\n     \\'\\' as LKP_CLIENT_UNIT, -- CLIENT_UNIT as LKP_CLIENT_UNIT\\n     HashBytes(\\'SHA2_256\\', \\n          coalesce(cast(CLNT_DIM_UID as varchar), \\'\\') +\\n          coalesce(cast(SO_NONSO_DIM_UID as varchar), \\'\\') +\\n          coalesce(CNTRCT_STATE_CD, \\'\\') +\\n          coalesce(CNTCT_TXT, \\'\\') +\\n          coalesce(CNTRCT_NUM, \\'\\') +\\n          coalesce(CNTRCT_NM, \\'\\') +\\n          coalesce(CNTRCT_ATTCHMT_LCTN_TXT, \\'\\') +\\n          coalesce(cast(START_DT as varchar), \\'\\') +\\n          coalesce(cast(END_DT as varchar), \\'\\') +\\n          coalesce(LINE_OF_BUS_TYPE_CD, \\'\\') +\\n          coalesce(INDSTRY_TXT, \\'\\') +\\n          coalesce(INDSTRY_SRC_TXT, \\'\\') +\\n          coalesce(cast(VRFYD_DT  as varchar), \\'\\') +\\n          coalesce(INDSTRY_OVERRDEN_IND, \\'\\') +\\n          coalesce(cast(GEO_DIM_UID as varchar), \\'\\') +\\n          cast(SCTR_DIM_UID as varchar) +\\n          cast(CNTRCT_TYPE_DIM_UID as varchar) +\\n          PRVT_IND +\\n          cast(SRC_CRETD_TMS  as varchar) +\\n          cast(SRC_CRETD_USER_ID as varchar) +\\n          cast(SRC_UPDTD_TMS as varchar) +\\n          cast(SRC_UPDTD_USER_ID as varchar) +\\n          cast(ETL_JOB_ID as varchar) +\\n          cast(SRC_SYS_DIM_UID as varchar) +\\n          cast(\\'\\' as varchar)\\n     ) as LKP_HASHED_VAL\\nFrom\\n     PGMPDM.CNTRCT_DIM',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"getLookupData sort(asc(LKP_CNTRCT_ID, false)) ~> sortedLookupData",
						"selectUpdateData alterRow(updateIf(true())) ~> updateRows",
						"dropUnchangedRows split(ROW_STATUS!='I',",
						"     disjoint: false) ~> CDC@(dataToBeUpdated, dataToBeInserted)",
						"getSourceData, sortedLookupData join(CNTRCT_ID == LKP_CNTRCT_ID,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> mergedData",
						"addColumns2 select(mapColumn(",
						"          CNTRCT_ID,",
						"          CLNT_DIM_UID,",
						"          SO_NONSO_DIM_UID,",
						"          CNTRCT_STATE_CD,",
						"          CNTCT_TXT,",
						"          CNTRCT_NUM,",
						"          CNTRCT_NM,",
						"          CNTRCT_ATTCHMT_LCTN_TXT,",
						"          START_DT,",
						"          END_DT,",
						"          LINE_OF_BUS_TYPE_CD,",
						"          INDSTRY_TXT,",
						"          INDSTRY_SRC_TXT,",
						"          VRFYD_DT,",
						"          INDSTRY_OVERRDEN_IND,",
						"          GEO_DIM_UID,",
						"          SCTR_DIM_UID,",
						"          CNTRCT_TYPE_DIM_UID,",
						"          PRVT_IND,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          CLIENT_UNIT,",
						"          ROW_STAT_CD = ROW_STATUS,",
						"          DM_CRETD_USER_ID,",
						"          DM_CRETD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertData",
						"CDC@dataToBeUpdated derive(DM_CRETD_USER_ID = $current_user,",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns1",
						"CDC@dataToBeInserted derive(CNTRCT_ID = MAX_ID+ROW_NUM,",
						"          DM_CRETD_USER_ID = $current_user,",
						"          DM_CRETD_TMS = currentTimestamp(),",
						"          DM_UPDTD_USER_ID = $current_user,",
						"          DM_UPDTD_TMS = currentTimestamp()) ~> addColumns2",
						"addColumns1 select(mapColumn(",
						"          CNTRCT_ID,",
						"          CLNT_DIM_UID,",
						"          SO_NONSO_DIM_UID,",
						"          CNTRCT_STATE_CD,",
						"          CNTCT_TXT,",
						"          CNTRCT_NUM,",
						"          CNTRCT_NM,",
						"          CNTRCT_ATTCHMT_LCTN_TXT,",
						"          START_DT,",
						"          END_DT,",
						"          LINE_OF_BUS_TYPE_CD,",
						"          INDSTRY_TXT,",
						"          INDSTRY_SRC_TXT,",
						"          VRFYD_DT,",
						"          INDSTRY_OVERRDEN_IND,",
						"          GEO_DIM_UID,",
						"          SCTR_DIM_UID,",
						"          CNTRCT_TYPE_DIM_UID,",
						"          PRVT_IND,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID,",
						"          CLIENT_UNIT,",
						"          ROW_STAT_CD = ROW_STATUS,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_USER_ID,",
						"          DM_UPDTD_TMS",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectUpdateData",
						"mergedData derive(ROW_STATUS = case(CNTRCT_ID == toInteger(null()) && LKP_CNTRCT_ID != toInteger(null()), 'D', case(CNTRCT_ID != toInteger(null()) && LKP_CNTRCT_ID == toInteger(null()), 'I', case(HASHED_VAL == LKP_HASHED_VAL, 'L', 'U')))) ~> CDCval",
						"CDCval filter(ROW_STATUS != 'L') ~> dropUnchangedRows",
						"updateRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'CNTRCT_DIM',",
						"     insertable: false,",
						"     updateable: true,",
						"     deletable: false,",
						"     upsertable: false,",
						"     keys:['CNTRCT_ID'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     preCommands: [],",
						"     postCommands: []) ~> updateTable",
						"selectInsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'CNTRCT_DIM',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> insertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_TMF_GEO')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "PGMP/Dummies"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ln_rfs_pgmp",
								"type": "LinkedServiceReference"
							},
							"name": "TMFGEO"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "dl_tgt_geo",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(allowSchemaDrift: true,",
						"     validateSchema: true,",
						"     limit: 100,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'with\\n     -- This sub query gets the limits of the data that will be processed\\n     FIL as (\\n          select\\n               ETL_EXCTN_ID\\n          from\\n               PGMPDM.[ZAUX_ETL_EXCTN]\\n          where\\n               IS_CURR_IND = \\'Y\\'\\n     ),\\n     \\n     -- This subquery gets the JOB ID. Returns -1 is the value is unknown\\n     JOB as (\\n          select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          from\\n               PGMPDM.ZAUX_ETL_JOBS\\n          where\\n               ETL_JOB_NM = \\'#DSJobName#\\'          \\n     ),\\n     \\n     -- This subquery gets the SRC_SYS_ID. Returns -1 is the value is unknown\\n     SYS as (\\n          select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          from\\n               PGMPDM.SRC_SYS_DIM\\n          where\\n               SRC_SYS_CD = \\'TMF\\'               \\n     )\\n     \\n     \\n\\n\\nselect\\n     G.GEO_DIM_UID,\\n     G.IOT_RGN_CD/*,\\n     G.IOT_RGN_NM,\\n     G.IMT_RGN_CD,\\n     G.IMT_SRGN_NM,\\n     G.SRGN_CD,\\n     G.SRGNCTRY_CD,\\n     G.SRGNCTRY_NM,\\n     G.IOTSORT_NUM,\\n     G.GEO_CD,\\n     G.GEO_TYP_CD,\\n     G.GMR_RGN_CD,\\n     G.GMR_RGN_NM,\\n     G.IOT_SHORT_NM,\\n     G.IMT_SHORT_NM,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID*/\\nfrom\\n     PGMPDM.GEO_DIM_TMF G\\n     left join\\n     JOB\\n     on 1 = 1\\n     left join\\n     FIL\\n     on 1 = 1\\n     left join\\n     SYS\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> TMFGEO",
						"TMFGEO sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ln_rfs_pgmp')]",
				"[concat(variables('workspaceId'), '/datasets/dl_tgt_geo')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_TRIRIGA_CAMPUS_SFTP_PULL')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "IWTRIRIGACampusQuery",
								"type": "DatasetReference"
							},
							"name": "TririgaRemoteServerSite"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "IWTRIRIGACampusQueryCSV",
								"type": "DatasetReference"
							},
							"name": "AzureDataLakeStorage",
							"rejectedDataLinkedService": {
								"referenceName": "AzureDataLakeStorage2",
								"type": "LinkedServiceReference"
							}
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(output(",
						"          CAMPUS_ID as string,",
						"          CAMPUS_NAME as string,",
						"          CAMPUS_STATUS as string,",
						"          CAMPUS_GROUP as string,",
						"          SITE_ID as string,",
						"          PHYSICAL_GEO as string,",
						"          WORLD_REGION_CODE as string,",
						"          WORLD_REGION_NAME as string,",
						"          MARKET_TEAM_REGION_CODE as string,",
						"          MARKET_TEAM_REGION_NAME as string,",
						"          WORK_LOCATION_CODE as string,",
						"          ADDRESS as string,",
						"          CITY as string,",
						"          STATE_PROVINCE_ID as string,",
						"          POSTAL_CODE as string,",
						"          COUNTRY_CODE as string,",
						"          LATITUDE as string,",
						"          LONGITUDE as string,",
						"          UTC_OFFSET as string,",
						"          ICU_TIME_ZONE as string,",
						"          PEOPLE_HOUSED_FLAG as string,",
						"          REMOTE_SUPPORT_FLAG as string,",
						"          PRIMARY_CAMPUS_USE_ID as string,",
						"          PRIMARY_CAMPUS_USE_NAME as string,",
						"          PRIMARY_CAMPUS_USE_DESCRIPTION as string,",
						"          CAMPUS_OWNERSHIP as string,",
						"          CAMPUS_ACTIVATION_YEAR as string,",
						"          CAMPUS_INACTIVATION_YEAR as string,",
						"          CAMPUS_MODIFIED_DATE_AND_TIME as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     inferDriftedColumnTypes: true,",
						"     ignoreNoFilesFound: false) ~> TririgaRemoteServerSite",
						"TririgaRemoteServerSite sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     umask: 0777,",
						"     preCommands: ['rm -r /extract/Tririga/campus/'],",
						"     postCommands: ['mv /extract/Tririga/campus/*.csv /extract/Tririga/campus/IWTRIRIGASCampusQuery.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          CAMPUS_ID,",
						"          CAMPUS_NAME,",
						"          CAMPUS_STATUS,",
						"          CAMPUS_GROUP,",
						"          SITE_ID,",
						"          PHYSICAL_GEO,",
						"          WORLD_REGION_CODE,",
						"          WORLD_REGION_NAME,",
						"          MARKET_TEAM_REGION_CODE,",
						"          MARKET_TEAM_REGION_NAME,",
						"          WORK_LOCATION_CODE,",
						"          ADDRESS,",
						"          CITY,",
						"          STATE_PROVINCE_ID,",
						"          POSTAL_CODE,",
						"          COUNTRY_CODE,",
						"          LATITUDE,",
						"          LONGITUDE,",
						"          UTC_OFFSET,",
						"          ICU_TIME_ZONE,",
						"          PEOPLE_HOUSED_FLAG,",
						"          REMOTE_SUPPORT_FLAG,",
						"          PRIMARY_CAMPUS_USE_ID,",
						"          PRIMARY_CAMPUS_USE_NAME,",
						"          PRIMARY_CAMPUS_USE_DESCRIPTION,",
						"          CAMPUS_OWNERSHIP,",
						"          CAMPUS_ACTIVATION_YEAR,",
						"          CAMPUS_INACTIVATION_YEAR,",
						"          CAMPUS_MODIFIED_DATE_AND_TIME",
						"     )) ~> AzureDataLakeStorage"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/IWTRIRIGACampusQuery')]",
				"[concat(variables('workspaceId'), '/datasets/IWTRIRIGACampusQueryCSV')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_TRIRIGA_LOCATION_SFTP_PULL')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "IWTRIRIGABuildingQuery",
								"type": "DatasetReference"
							},
							"name": "TririgaRemoteServer"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "IWTRIRIGABuildingQueryCSV",
								"type": "DatasetReference"
							},
							"name": "AzureDataLakeStorage"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(output(",
						"          CAMPUS_ID as string,",
						"          BUILDING_ID as string,",
						"          BUILDING_NAME as string,",
						"          BUILDING_STATUS as string,",
						"          BUILDING_OWNERSHIP_ID as string,",
						"          BUILDING_OWNERSHIP_NAME as string,",
						"          BUILDING_OWNERSHIP_DESCRIPTION as string,",
						"          BUILDING_PRIMARY_USE_ID as string,",
						"          BUILDING_PRIMARY_USE_NAME as string,",
						"          BUILDING_PRIMARY_USE_DESCRIPTION as string,",
						"          BUILDING_SECONDARY_USE_ID as string,",
						"          BUILDING_SECONDARY_USE_NAME as string,",
						"          BUILDING_SECONDARY_USE_DESCRIPTION as string,",
						"          RENTABLE_AREA as string,",
						"          ACTIVE_YEAR as string,",
						"          ACTUAL_RETIREMENT_INACTIVATION as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> TririgaRemoteServer",
						"TririgaRemoteServer sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     umask: 0022,",
						"     preCommands: ['rm -r /extract/Tririga/building/'],",
						"     postCommands: ['mv /extract/Tririga/building/*.csv /extract/Tririga/building/IWTRIRIGABuildingQuery.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          CAMPUS_ID,",
						"          BUILDING_ID,",
						"          BUILDING_NAME,",
						"          BUILDING_STATUS,",
						"          BUILDING_OWNERSHIP_ID,",
						"          BUILDING_OWNERSHIP_NAME,",
						"          BUILDING_OWNERSHIP_DESCRIPTION,",
						"          BUILDING_PRIMARY_USE_ID,",
						"          BUILDING_PRIMARY_USE_NAME,",
						"          BUILDING_PRIMARY_USE_DESCRIPTION,",
						"          BUILDING_SECONDARY_USE_ID,",
						"          BUILDING_SECONDARY_USE_NAME,",
						"          BUILDING_SECONDARY_USE_DESCRIPTION,",
						"          RENTABLE_AREA,",
						"          ACTIVE_YEAR,",
						"          ACTUAL_RETIREMENT_INACTIVATION",
						"     ),",
						"     partitionBy('hash', 1)) ~> AzureDataLakeStorage"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/IWTRIRIGABuildingQuery')]",
				"[concat(variables('workspaceId'), '/datasets/IWTRIRIGABuildingQueryCSV')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_TRIRIGA_OfficeSpace_SFTP_PULL')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "resodbspacetririgastaging",
								"type": "DatasetReference"
							},
							"name": "TririgaRemoteServerSite"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "ResodbOfficeSpaceTririgaStaging",
								"type": "DatasetReference"
							},
							"name": "AzureDataLakeStorage",
							"rejectedDataLinkedService": {
								"referenceName": "AzureDataLakeStorage2",
								"type": "LinkedServiceReference"
							}
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(output(",
						"          CAMPUS_ID as string,",
						"          BLDG_ID as string,",
						"          FLOOR_ID as string,",
						"          SPACE_ID as string,",
						"          SPACE_DESCR as string,",
						"          AREA as string,",
						"          CAPACITY as string,",
						"          SPACE_CLASS_CODE1 as string,",
						"          SPACE_CLASS_CODE2 as string,",
						"          OSCRE_CODE as string,",
						"          WORKPOINT as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     inferDriftedColumnTypes: true,",
						"     ignoreNoFilesFound: false,",
						"     partitionBy('hash', 1)) ~> TririgaRemoteServerSite",
						"TririgaRemoteServerSite sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          CAMPUS_ID as string,",
						"          BLDG_ID as string,",
						"          FLOOR_ID as string,",
						"          SPACE_ID as string,",
						"          SPACE_DESCR as string,",
						"          AREA as string,",
						"          CAPACITY as string,",
						"          SPACE_CLASS_CODE1 as string,",
						"          SPACE_CLASS_CODE2 as string,",
						"          OSCRE_CODE as string,",
						"          WORKPOINT as string",
						"     ),",
						"     umask: 0777,",
						"     preCommands: ['rm -r /extract/Tririga/OfficeSpace/'],",
						"     postCommands: ['mv /extract/Tririga/OfficeSpace/*.csv /extract/Tririga/OfficeSpace/ResodbOfficeSpaceTririgaStaging.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          CAMPUS_ID,",
						"          BLDG_ID,",
						"          FLOOR_ID,",
						"          SPACE_ID,",
						"          SPACE_DESCR,",
						"          AREA,",
						"          CAPACITY,",
						"          SPACE_CLASS_CODE1,",
						"          SPACE_CLASS_CODE2,",
						"          OSCRE_CODE,",
						"          WORKPOINT",
						"     )) ~> AzureDataLakeStorage"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/resodbspacetririgastaging')]",
				"[concat(variables('workspaceId'), '/datasets/ResodbOfficeSpaceTririgaStaging')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DF_TRIRIGA_SITE_SFTP_PULL')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "IWTRIRIGASiteQuery",
								"type": "DatasetReference"
							},
							"name": "TririgaRemoteServerSite"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "IWTRIRIGASiteQueryCSV",
								"type": "DatasetReference"
							},
							"name": "AzureDataLakeStorage"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(output(",
						"          SITE_ID as string,",
						"          SITE_NAME as string,",
						"          SITE_STATUS as string,",
						"          GEOGRAPHY_ID as string,",
						"          GEOGRAPHY_NAME as string,",
						"          REGION_ID as string,",
						"          REGION_NAME as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> TririgaRemoteServerSite",
						"TririgaRemoteServerSite sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     umask: 0777,",
						"     preCommands: ['rm -r /extract/Tririga/Site/'],",
						"     postCommands: ['mv /extract/Tririga/Site/*.csv /extract/Tririga/Site/IWTRIRIGASiteQuery.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          SITE_ID,",
						"          SITE_NAME,",
						"          SITE_STATUS,",
						"          GEOGRAPHY_ID,",
						"          GEOGRAPHY_NAME,",
						"          REGION_ID,",
						"          REGION_NAME",
						"     ),",
						"     partitionBy('hash', 1)) ~> AzureDataLakeStorage"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/IWTRIRIGASiteQuery')]",
				"[concat(variables('workspaceId'), '/datasets/IWTRIRIGASiteQueryCSV')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dataflow')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Dataset",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DelimitedText2",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "select1"
						}
					],
					"scriptLines": [
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 select(skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          SIEBEL_SALES_STAGE_CODE as string,",
						"          SIEBEL_SALES_STAGE_NAME as string,",
						"          SSM_STEP_NO as string,",
						"          SSM_STEP_NAME as string",
						"     ),",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Dataset')]",
				"[concat(variables('workspaceId'), '/datasets/DelimitedText2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dataflow1')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DBXDH_DHT_PROJECT_SIV",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DelimitedText2",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "select1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          PROJECT_KEY as integer,",
						"          PROJECT_VERSION as integer,",
						"          PROJECT_ID as string,",
						"          FINANCIAL_COUNTRY_CD as string,",
						"          LEDGER_CD as string,",
						"          OFFERING_COMPONENT_CD as string,",
						"          OPPORTUNITY_NUM as string,",
						"          PROJECT_DESC as string,",
						"          SIGNINGS_CD as string,",
						"          SIGNINGS_DESC as string,",
						"          BUSINESS_TYPE_CD as string,",
						"          BUSINESS_TYPE_DESC as string,",
						"          PROJECT_STATUS_CD as string,",
						"          PROJECT_STATUS_DESC as string,",
						"          PROJECT_CUSTOMER_NO as string,",
						"          PROJECT_CREATION_DATE as date,",
						"          ACCOUTNING_DIVISION as string,",
						"          RESPONSIBLE_SERV_OFFICE as string,",
						"          CURRENT_IND as string,",
						"          EXTRACT_DT as timestamp,",
						"          REC_START_DT as timestamp,",
						"          REC_END_DT as timestamp,",
						"          SOURCE_SYSTEM as string,",
						"          REC_CHECKSUM as string,",
						"          REC_STATUS as string,",
						"          IMG_LST_UPD_DT as timestamp,",
						"          IMG_CREATED_DT as timestamp,",
						"          DATA_IND as string,",
						"          ACTIVE_IN_SOURCE_IND as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_COMMITTED',",
						"     query: 'select * from DBXDH.DHT_PROJECT_SIV where PROJECT_ID = \\'EEI0001\\'',",
						"     format: 'query') ~> source1",
						"source1 select(mapColumn(",
						"          PROJECT_KEY,",
						"          PROJECT_VERSION,",
						"          PROJECT_ID,",
						"          FINANCIAL_COUNTRY_CD,",
						"          LEDGER_CD,",
						"          OFFERING_COMPONENT_CD,",
						"          OPPORTUNITY_NUM,",
						"          PROJECT_DESC,",
						"          SIGNINGS_CD,",
						"          SIGNINGS_DESC,",
						"          BUSINESS_TYPE_CD,",
						"          BUSINESS_TYPE_DESC,",
						"          PROJECT_STATUS_CD,",
						"          PROJECT_STATUS_DESC,",
						"          PROJECT_CUSTOMER_NO,",
						"          PROJECT_CREATION_DATE,",
						"          ACCOUTNING_DIVISION,",
						"          RESPONSIBLE_SERV_OFFICE,",
						"          CURRENT_IND,",
						"          EXTRACT_DT,",
						"          REC_START_DT,",
						"          REC_END_DT,",
						"          SOURCE_SYSTEM,",
						"          REC_CHECKSUM,",
						"          REC_STATUS,",
						"          IMG_LST_UPD_DT,",
						"          IMG_CREATED_DT,",
						"          DATA_IND,",
						"          ACTIVE_IN_SOURCE_IND",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          SIEBEL_SALES_STAGE_CODE as string,",
						"          SIEBEL_SALES_STAGE_NAME as string,",
						"          SSM_STEP_NO as string,",
						"          SSM_STEP_NAME as string",
						"     ),",
						"     partitionFileNames:['proj_tgt_file_with_filter.csv'],",
						"     umask: 0777,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DBXDH_DHT_PROJECT_SIV')]",
				"[concat(variables('workspaceId'), '/datasets/DelimitedText2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dataflow_copy1')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Dataset",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DelimitedText2",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "select1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          SIEBEL_SALES_STAGE_CODE as string,",
						"          SIEBEL_SALES_STAGE_NAME as string,",
						"          SSM_STEP_NO as string,",
						"          SSM_STEP_NAME as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 select(skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          SIEBEL_SALES_STAGE_CODE as string,",
						"          SIEBEL_SALES_STAGE_NAME as string,",
						"          SSM_STEP_NO as string,",
						"          SSM_STEP_NAME as string",
						"     ),",
						"     partitionFileNames:['sell_cycle_tgt_file.csv'],",
						"     umask: 0777,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Dataset')]",
				"[concat(variables('workspaceId'), '/datasets/DelimitedText2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Get_Main_Dates')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "PGMP/Utility"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getDRAandPRT"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DelimitedText4",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "selectPRTData"
						},
						{
							"name": "selectDRAData"
						},
						{
							"name": "getMainDates"
						}
					],
					"scriptLines": [
						"source(output(",
						"          PRT_PROC_ID as integer,",
						"          PRT_PROC_TYPE_ID as string,",
						"          PRT_CRETD_DT as date,",
						"          PRT_RCVD_DT as date,",
						"          PRT_ETL_EXCTN_ID as integer,",
						"          DRA_PROC_ID as integer,",
						"          DRA_PROC_TYPE_ID as string,",
						"          DRA_CRETD_DT as date,",
						"          DRA_RCVD_DT as date,",
						"          DRA_ETL_EXCTN_ID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     PRNT.PROC_ID as PRT_PROC_ID,\\n     PRNT.PROC_TYPE_ID as PRT_PROC_TYPE_ID,\\n     cast(PRNT.CREATED_TS as Date) as PRT_CRETD_DT,\\n     coalesce(R.RQST_RECEIV_DATE, PCR.RQST_RECEIV_DATE) as PRT_RCVD_DT,\\n     F.ETL_EXCTN_ID as PRT_ETL_EXCTN_ID,\\n     DRA.*\\nFrom \\n     (\\n     Select\\n          P.PROC_ID as DRA_PROC_ID,\\n          P.PROC_TYPE_ID as DRA_PROC_TYPE_ID,\\n          cast(P.CREATED_TS as Date) as DRA_CRETD_DT,\\n          coalesce(R.RQST_RECEIV_DATE, PCR.RQST_RECEIV_DATE) as DRA_RCVD_DT,\\n          F.ETL_EXCTN_ID as DRA_ETL_EXCTN_ID\\n     From \\n          APPFUN.\"PROC\" P\\n          left join \\n          APPFUN.RQST R\\n          on P.PROC_ID = R.PROC_ID\\n          left join \\n          APPFUN.PCR PCR\\n          on P.PROC_ID = PCR.PROC_ID\\n          inner join \\n          (\\n               Select\\n                    ETL_EXCTN_ID,\\n                    ETL_PARAM_START_TMS,\\n                    ETL_PARAM_END_TMS\\n               From \\n                    PGMPDM.ZAUX_ETL_EXCTN\\n               Where \\n                    IS_CURR_IND = \\'Y\\'\\n          ) as F\\n          on\\n          -- For new records, it is required to validate whether the CREATED_TS is in range \\n          P.CREATED_TS between F.ETL_PARAM_START_TMS and F.ETL_PARAM_END_TMS or\\n          -- For modified records, it is required to validate whether the UPDATED_TS is in range\\n          P.UPDATED_TS between F.ETL_PARAM_START_TMS and F.ETL_PARAM_END_TMS or\\n          -- Request\\n          -- Proposals Requested date in range\\n          DATEADD(dd, 1, coalesce(R.CLIENT_REVISD_PRPSL_RQSTD_DT, R.PRPSL_RQST_DATE)) between F.ETL_PARAM_START_TMS and F.ETL_PARAM_END_TMS or\\n          -- Expiration date in range\\n          DATEADD(dd, 1, R.PRPSL_EXP_DATE) between F.ETL_PARAM_START_TMS and F.ETL_PARAM_END_TMS or\\n          -- Implementation Requested date in range\\n          DATEADD(dd, 1, coalesce(R.CLIENT_RVSD_RQST_IMPL_CMPL_DT, R.RQST_IMPLMNT_COMPLT_DATE)) between F.ETL_PARAM_START_TMS and F.ETL_PARAM_END_TMS or\\n          -- PCR\\n          -- Proposals Requested date in range\\n          DATEADD(dd, 1, coalesce(PCR.CLIENT_REVISD_PRPSL_RQSTD_DT, PCR.PRPSL_RQST_DATE)) between F.ETL_PARAM_START_TMS and F.ETL_PARAM_END_TMS or\\n          -- Expiration date in range\\n          DATEADD(dd, 1, PCR.PRPSL_EXP_DATE) between F.ETL_PARAM_START_TMS and F.ETL_PARAM_END_TMS or\\n          -- Implementation Requested date in range\\n          DATEADD(dd, 1, coalesce(PCR.CLIENT_RVSD_RQST_IMPL_CMPL_DT, PCR.RQST_IMPLMNT_COMPLT_DATE)) between F.ETL_PARAM_START_TMS and F.ETL_PARAM_END_TMS\\n     ) DRA\\n     inner join \\n     APPFUN.\"PROC\" P\\n     on DRA.DRA_PROC_ID = P.PROC_ID\\n     inner join \\n     APPFUN.\"PROC\" PRNT\\n     on P.PARENT_PROC_ID = PRNT.PROC_ID\\n     left join \\n     APPFUN.RQST R\\n     on PRNT.PROC_ID = R.PROC_ID\\n     left join \\n     APPFUN.PCR PCR\\n     on PRNT.PROC_ID = PCR.PROC_ID\\n     inner join (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          From \\n               PGMPDM.ZAUX_ETL_EXCTN\\n          Where \\n               IS_CURR_IND = \\'Y\\'\\n     ) as F\\n     on 1 = 1',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getDRAandPRT",
						"getDRAandPRT select(mapColumn(",
						"          PROC_ID = PRT_PROC_ID,",
						"          PROC_TYPE_ID = PRT_PROC_TYPE_ID,",
						"          CRETD_DT = PRT_CRETD_DT,",
						"          RCVD_DT = PRT_RCVD_DT,",
						"          ETL_EXCTN_ID = PRT_ETL_EXCTN_ID",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectPRTData",
						"getDRAandPRT select(mapColumn(",
						"          PROC_ID = DRA_PROC_ID,",
						"          PROC_TYPE_ID = DRA_PROC_TYPE_ID,",
						"          CRETD_DT = DRA_CRETD_DT,",
						"          RCVD_DT = DRA_RCVD_DT,",
						"          ETL_EXCTN_ID = DRA_ETL_EXCTN_ID",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectDRAData",
						"selectDRAData, selectPRTData union(byName: true)~> getMainDates",
						"getMainDates sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]",
				"[concat(variables('workspaceId'), '/datasets/DelimitedText4')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Src_tgt_Geo_CDC')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "PGMP/Dummies"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "GetSrGeoTmf"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "TgtGeoDim"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "TgtUpsertGeoData"
						}
					],
					"transformations": [
						{
							"name": "changecapture"
						},
						{
							"name": "sortedlkpdata"
						},
						{
							"name": "srclkpgeodata"
						},
						{
							"name": "UpsertGeo"
						}
					],
					"scriptLines": [
						"source(output(",
						"          GEO_DIM_UID as integer,",
						"          IOT_RGN_CD as string,",
						"          IOT_RGN_NM as string,",
						"          IMT_RGN_CD as string,",
						"          IMT_SRGN_NM as string,",
						"          SRGN_CD as string,",
						"          SRGNCTRY_CD as string,",
						"          SRGNCTRY_NM as string,",
						"          IOTSORT_NUM as integer,",
						"          GEO_CD as string,",
						"          GEO_TYP_CD as string,",
						"          GMR_RGN_CD as string,",
						"          GMR_RGN_NM as string,",
						"          IOT_SHORT_NM as string,",
						"          IMT_SHORT_NM as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n     G.GEO_DIM_UID,\\n     G.IOT_RGN_CD,\\n     G.IOT_RGN_NM,\\n     G.IMT_RGN_CD,\\n     G.IMT_SRGN_NM,\\n     G.SRGN_CD,\\n     G.SRGNCTRY_CD,\\n     G.SRGNCTRY_NM,\\n     G.IOTSORT_NUM,\\n     G.GEO_CD,\\n     G.GEO_TYP_CD,\\n     G.GMR_RGN_CD,\\n     G.GMR_RGN_NM,\\n     G.IOT_SHORT_NM,\\n     G.IMT_SHORT_NM,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID\\nfrom\\n     PGMPDM.GEO_DIM_TMF G\\n     left join\\n     (select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          from\\n               PGMPDM.ZAUX_ETL_JOBS\\n          where\\n               ETL_JOB_NM = \\'#DSJobName#\\'     )JOB\\n     on 1 = 1\\n     left join\\n     (select\\n               ETL_EXCTN_ID\\n          from\\n               PGMPDM.[ZAUX_ETL_EXCTN]\\n          where\\n               IS_CURR_IND = \\'Y\\')FIL\\n     on 1 = 1\\n     left join\\n     (select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          from\\n               PGMPDM.SRC_SYS_DIM\\n          where\\n               SRC_SYS_CD = \\'TMF\\')SYS\\n     on 1 = 1\\n',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> GetSrGeoTmf",
						"source(output(",
						"          GEO_DIM_UID as integer,",
						"          IOT_RGN_CD as string,",
						"          IOT_RGN_NM as string,",
						"          IMT_RGN_CD as string,",
						"          IMT_SRGN_NM as string,",
						"          SRGN_CD as string,",
						"          SRGNCTRY_CD as string,",
						"          SRGNCTRY_NM as string,",
						"          IOTSORT_NUM as integer,",
						"          GEO_CD as string,",
						"          GEO_TYP_CD as string,",
						"          GMR_RGN_CD as string,",
						"          GMR_RGN_NM as string,",
						"          IOT_SHORT_NM as string,",
						"          IMT_SHORT_NM as string,",
						"          ETL_JOB_ID as integer,",
						"          SRC_SYS_DIM_UID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'select\\n     GEO_DIM_UID,\\n     IOT_RGN_CD,\\n     IOT_RGN_NM,\\n     IMT_RGN_CD,\\n     IMT_SRGN_NM,\\n     SRGN_CD,\\n     SRGNCTRY_CD,\\n     SRGNCTRY_NM,\\n     IOTSORT_NUM,\\n     GEO_CD,\\n     GEO_TYP_CD,\\n     GMR_RGN_CD,\\n     GMR_RGN_NM,\\n     IOT_SHORT_NM,\\n     IMT_SHORT_NM,\\n     ETL_JOB_ID,\\n     SRC_SYS_DIM_UID\\nfrom\\n     PGMPDM.GEO_DIM\\n',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> TgtGeoDim",
						"sortedlkpdata, TgtGeoDim join(GetSrGeoTmf@GEO_DIM_UID == TgtGeoDim@GEO_DIM_UID,",
						"     joinType:'inner',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> changecapture",
						"GetSrGeoTmf sort(asc(GEO_DIM_UID, true)) ~> sortedlkpdata",
						"TgtGeoDim sort(asc(GEO_DIM_UID, true)) ~> srclkpgeodata",
						"changecapture alterRow(upsertIf(GetSrGeoTmf@GEO_DIM_UID!=toInteger(null()))) ~> UpsertGeo",
						"UpsertGeo sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'sqlserver',",
						"     schemaName: 'PGMPDM',",
						"     tableName: 'GEO_DIM',",
						"     insertable: false,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: true,",
						"     keys:['GEO_DIM_UID'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> TgtUpsertGeoData"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/account_reports_job')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "PGMP/Dummies"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getSourceData"
						},
						{
							"linkedService": {
								"referenceName": "ls_pgmp_rfs_db",
								"type": "LinkedServiceReference"
							},
							"name": "getLookupData"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "output_ar",
								"type": "DatasetReference"
							},
							"name": "upsertTable"
						}
					],
					"transformations": [
						{
							"name": "CDC"
						},
						{
							"name": "sortedLookupData"
						},
						{
							"name": "selectNewData"
						},
						{
							"name": "upsertData"
						}
					],
					"scriptLines": [
						"source(output(",
						"          ACCTRPTS_DIM_UID as integer,",
						"          PRJCT_ID as string,",
						"          ACCTRPTS_REM_TXT as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ETL_JOB_ID as integer,",
						"          ETL_EXCTN_ID as integer,",
						"          SRC_SYS_DIM_UID as integer,",
						"          IS_DELETED as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     ACC.PROC_ID as ACCTRPTS_DIM_UID,\\n     ACC.PROJECT_ID as PRJCT_ID,\\n     cast(ACC.REMARKS as varchar(1024)) as ACCTRPTS_REM_TXT,\\n     ACC.CREATED_TS as SRC_CRETD_TMS,\\n     ACC.CREATED_USERID as SRC_CRETD_USER_ID,\\n     ACC.UPDATED_TS as SRC_UPDTD_TMS,\\n     ACC.UPDATED_USERID as SRC_UPDTD_USER_ID,\\n     JOB.ETL_JOB_ID,\\n     FIL.ETL_EXCTN_ID,\\n     SYS.SRC_SYS_DIM_UID,\\n     case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED\\nfrom\\n     APPFUN.ACCTRPTS ACC\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on ACC.PROC_ID = ZDT.PROC_ID\\n     left join\\n     PGMPDM.ZAUX_DELD_PROC_ID PDEL\\n     on ACC.PROC_ID = PDEL.PROC_ID\\n     left join\\n     (\\n          Select\\n               coalesce(max(ETL_JOB_ID), -1) as ETL_JOB_ID\\n          from\\n               PGMPDM.ZAUX_ETL_JOBS\\n          where\\n               ETL_JOB_NM = \\'BALD0010_ACCTRPTS_DIM\\'          \\n     ) as JOB\\n     on \\'1\\' = \\'1\\'\\n     left join\\n     (\\n          Select\\n               ETL_EXCTN_ID,\\n               ETL_PARAM_START_TMS,\\n               ETL_PARAM_END_TMS\\n          from\\n               PGMPDM.ZAUX_ETL_EXCTN\\n          where\\n               IS_CURR_IND = \\'Y\\'\\n     ) as FIL\\n     on \\'1\\' = \\'1\\'\\n     left join\\n     (\\n          Select\\n               coalesce(max(SRC_SYS_DIM_UID), -1) as SRC_SYS_DIM_UID\\n          from\\n               PGMPDM.SRC_SYS_DIM\\n          where\\n               SRC_SYS_CD = \\'PGMP\\'               \\n     ) as SYS\\n     on \\'1\\' = \\'1\\'\\n',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getSourceData",
						"source(output(",
						"          ACCTRPTS_DIM_UID as integer,",
						"          PRJCT_ID as string,",
						"          ACCTRPTS_REM_TXT as string,",
						"          SRC_CRETD_TMS as timestamp,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as timestamp,",
						"          SRC_UPDTD_USER_ID as string,",
						"          SRC_SYS_DIM_UID as integer,",
						"          ETL_JOB_ID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'query',",
						"     store: 'sqlserver',",
						"     query: 'Select\\n     S.ACCTRPTS_DIM_UID,\\n     S.PRJCT_ID,\\n     S.ACCTRPTS_REM_TXT,\\n     S.SRC_CRETD_TMS,\\n     S.SRC_CRETD_USER_ID,\\n     S.SRC_UPDTD_TMS,\\n     S.SRC_UPDTD_USER_ID,\\n     S.SRC_SYS_DIM_UID,\\n     S.ETL_JOB_ID\\nfrom\\n     PGMPDM.ACCTRPTS_DIM S\\n     inner join\\n     PGMPDM.ZAUX_DATE_TRIGGERS ZDT\\n     on S.ACCTRPTS_DIM_UID = ZDT.PROC_ID     \\n     ',",
						"     isolationLevel: 'READ_UNCOMMITTED') ~> getLookupData",
						"getSourceData, sortedLookupData join(getSourceData@ACCTRPTS_DIM_UID == getLookupData@ACCTRPTS_DIM_UID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> CDC",
						"getLookupData sort(asc(ACCTRPTS_DIM_UID, false)) ~> sortedLookupData",
						"CDC select(mapColumn(",
						"          ACCTRPTS_DIM_UID = getSourceData@ACCTRPTS_DIM_UID,",
						"          PRJCT_ID = getSourceData@PRJCT_ID,",
						"          ACCTRPTS_REM_TXT = getSourceData@ACCTRPTS_REM_TXT,",
						"          SRC_CRETD_TMS = getSourceData@SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID = getSourceData@SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS = getSourceData@SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID = getSourceData@SRC_UPDTD_USER_ID,",
						"          ETL_JOB_ID = getSourceData@ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID = getSourceData@SRC_SYS_DIM_UID,",
						"          IS_DELETED,",
						"          ACCTRPTS_DIM_UID = getLookupData@ACCTRPTS_DIM_UID,",
						"          PRJCT_ID = getLookupData@PRJCT_ID,",
						"          ACCTRPTS_REM_TXT = getLookupData@ACCTRPTS_REM_TXT,",
						"          SRC_CRETD_TMS = getLookupData@SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID = getLookupData@SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS = getLookupData@SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID = getLookupData@SRC_UPDTD_USER_ID,",
						"          SRC_SYS_DIM_UID = getLookupData@SRC_SYS_DIM_UID,",
						"          ETL_JOB_ID = getLookupData@ETL_JOB_ID",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectNewData",
						"selectNewData alterRow(upsertIf(ACCTRPTS_DIM_UID!=toInteger(null()))) ~> upsertData",
						"upsertData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 0,",
						"     partitionBy('hash', 1)) ~> upsertTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ls_pgmp_rfs_db')]",
				"[concat(variables('workspaceId'), '/datasets/output_ar')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/account_reports_lookup_df')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "PGMP/Dummies"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "pgmpdm_acctrpts_dim",
								"type": "DatasetReference"
							},
							"name": "pgmpdmAcctrptsDim"
						},
						{
							"dataset": {
								"referenceName": "pgmpdm_zaux_date_triggers",
								"type": "DatasetReference"
							},
							"name": "pgmpdmZauxDateTriggers"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "account_reports_lookup",
								"type": "DatasetReference"
							},
							"name": "lookupAcctrpts"
						}
					],
					"transformations": [
						{
							"name": "accIJzdt"
						},
						{
							"name": "selectLookupCols"
						}
					],
					"scriptLines": [
						"source(output(",
						"          ACCTRPTS_DIM_UID as string,",
						"          PRJCT_ID as string,",
						"          ACCTRPTS_REM_TXT as string,",
						"          SRC_CRETD_TMS as string,",
						"          SRC_CRETD_USER_ID as string,",
						"          SRC_UPDTD_TMS as string,",
						"          SRC_UPDTD_USER_ID as string,",
						"          ROW_STAT_CD as string,",
						"          SRC_SYS_DIM_UID as string,",
						"          ETL_JOB_ID as string,",
						"          ETL_EXCTN_ID as string,",
						"          DM_CRETD_TMS as string,",
						"          DM_CRETD_USER_ID as string,",
						"          DM_UPDTD_TMS as string,",
						"          DM_UPDTD_USER_ID as string,",
						"          ORIG_ORG as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> pgmpdmAcctrptsDim",
						"source(output(",
						"          PROC_ID as string,",
						"          CRETD_DT as string,",
						"          RCVD_DT as string,",
						"          PRPSL_SENT_TO_CLNT_DT as string,",
						"          PRPSL_DSPSN_DT as string,",
						"          PRPSL_ACCPTD_DT as string,",
						"          IMPLMTN_READY_DT as string,",
						"          IMPLMTN_CLOSE_OUT_DT as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> pgmpdmZauxDateTriggers",
						"pgmpdmAcctrptsDim, pgmpdmZauxDateTriggers join(ACCTRPTS_DIM_UID == PROC_ID,",
						"     joinType:'inner',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> accIJzdt",
						"accIJzdt select(mapColumn(",
						"          ACCTRPTS_DIM_UID,",
						"          PRJCT_ID,",
						"          ACCTRPTS_REM_TXT,",
						"          SRC_CRETD_TMS,",
						"          SRC_CRETD_USER_ID,",
						"          SRC_UPDTD_TMS,",
						"          SRC_UPDTD_USER_ID,",
						"          ROW_STAT_CD,",
						"          SRC_SYS_DIM_UID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          DM_CRETD_TMS,",
						"          DM_CRETD_USER_ID,",
						"          DM_UPDTD_TMS,",
						"          DM_UPDTD_USER_ID,",
						"          ORIG_ORG",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectLookupCols",
						"selectLookupCols sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['lookupData.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> lookupAcctrpts"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/pgmpdm_acctrpts_dim')]",
				"[concat(variables('workspaceId'), '/datasets/pgmpdm_zaux_date_triggers')]",
				"[concat(variables('workspaceId'), '/datasets/account_reports_lookup')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/account_reports_source_df')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "PGMP/Dummies"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "appfun_acctrpts",
								"type": "DatasetReference"
							},
							"name": "appfunAcctrpts"
						},
						{
							"dataset": {
								"referenceName": "pgmpdm_zaux_date_triggers_src",
								"type": "DatasetReference"
							},
							"name": "pgmpdmZauxDateTriggers"
						},
						{
							"dataset": {
								"referenceName": "pgmpdm_zaux_deld_proc",
								"type": "DatasetReference"
							},
							"name": "pgmpdmZauxDeldProc"
						},
						{
							"dataset": {
								"referenceName": "pgmpdm_zaux_etl_exctn",
								"type": "DatasetReference"
							},
							"name": "pgmpdmZauxEtlExctn"
						},
						{
							"dataset": {
								"referenceName": "pgmpdm_zaux_etl_jobs",
								"type": "DatasetReference"
							},
							"name": "pgmpdmZauxEtlJobs"
						},
						{
							"dataset": {
								"referenceName": "pgmpdm_src_sys_dim",
								"type": "DatasetReference"
							},
							"name": "pgmpdmSrcSysDim"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "account_reports_source",
								"type": "DatasetReference"
							},
							"name": "sourceAcctrpts"
						}
					],
					"transformations": [
						{
							"name": "accIJzdt"
						},
						{
							"name": "accLJpdel"
						},
						{
							"name": "accLJjob"
						},
						{
							"name": "accLJfil"
						},
						{
							"name": "accLJsys"
						},
						{
							"name": "selectSourceCols"
						}
					],
					"scriptLines": [
						"source(output(",
						"          PROC_ID as string,",
						"          CNTRY as string,",
						"          PROJECT_ID as string,",
						"          DUE_DATE as string,",
						"          REVISD_DUE_DATE as string,",
						"          REMARKS as string,",
						"          ORIG_ORG as string,",
						"          CREATED_TS as string,",
						"          CREATED_USERID as string,",
						"          UPDATED_TS as string,",
						"          UPDATED_USERID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> appfunAcctrpts",
						"source(output(",
						"          PROC_ID as string,",
						"          CRETD_DT as string,",
						"          RCVD_DT as string,",
						"          PRPSL_SENT_TO_CLNT_DT as string,",
						"          PRPSL_DSPSN_DT as string,",
						"          PRPSL_ACCPTD_DT as string,",
						"          IMPLMTN_READY_DT as string,",
						"          IMPLMTN_CLOSE_OUT_DT as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> pgmpdmZauxDateTriggers",
						"source(output(",
						"          PROC_ID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> pgmpdmZauxDeldProc",
						"source(output(",
						"          ETL_EXCTN_ID as string,",
						"          ETL_PARAM_START_TMS as string,",
						"          ETL_PARAM_END_TMS as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> pgmpdmZauxEtlExctn",
						"source(output(",
						"          ETL_JOB_ID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> pgmpdmZauxEtlJobs",
						"source(output(",
						"          SRC_SYS_DIM_UID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> pgmpdmSrcSysDim",
						"appfunAcctrpts, pgmpdmZauxDateTriggers join(appfunAcctrpts@PROC_ID == pgmpdmZauxDateTriggers@PROC_ID,",
						"     joinType:'inner',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> accIJzdt",
						"accIJzdt, pgmpdmZauxDeldProc join(appfunAcctrpts@PROC_ID == pgmpdmZauxDeldProc@PROC_ID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> accLJpdel",
						"accLJpdel, pgmpdmZauxEtlJobs join(abs(1) == abs(1),",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'right')~> accLJjob",
						"accLJjob, pgmpdmZauxEtlExctn join(abs(1) == abs(1),",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'right')~> accLJfil",
						"accLJfil, pgmpdmSrcSysDim join(abs(1) == abs(1),",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'right')~> accLJsys",
						"accLJsys select(mapColumn(",
						"          PROC_ID = appfunAcctrpts@PROC_ID,",
						"          PROJECT_ID,",
						"          REMARKS,",
						"          CREATED_TS,",
						"          CREATED_USERID,",
						"          UPDATED_TS,",
						"          UPDATED_USERID,",
						"          ETL_JOB_ID,",
						"          ETL_EXCTN_ID,",
						"          SRC_SYS_DIM_UID",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectSourceCols",
						"selectSourceCols sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sourceAcctrpts"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/appfun_acctrpts')]",
				"[concat(variables('workspaceId'), '/datasets/pgmpdm_zaux_date_triggers_src')]",
				"[concat(variables('workspaceId'), '/datasets/pgmpdm_zaux_deld_proc')]",
				"[concat(variables('workspaceId'), '/datasets/pgmpdm_zaux_etl_exctn')]",
				"[concat(variables('workspaceId'), '/datasets/pgmpdm_zaux_etl_jobs')]",
				"[concat(variables('workspaceId'), '/datasets/pgmpdm_src_sys_dim')]",
				"[concat(variables('workspaceId'), '/datasets/account_reports_source')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bkp_scdType2_Project_Dimension')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "backup"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "dsProjectDimensionRaw",
								"type": "DatasetReference"
							},
							"name": "genericInput"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "genericDimensionTable"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "dsProjectDimensionRaw",
								"type": "DatasetReference"
							},
							"name": "DimensionTable"
						}
					],
					"transformations": [
						{
							"name": "derivedAddHashInput"
						},
						{
							"name": "derivedAddHashExisting"
						},
						{
							"name": "fullOuterJoin"
						},
						{
							"name": "NoChangeRecords"
						},
						{
							"name": "select1"
						},
						{
							"name": "select2"
						},
						{
							"name": "select3"
						},
						{
							"name": "select4"
						},
						{
							"name": "select5"
						}
					],
					"scriptLines": [
						"parameters{",
						"     NaturalKey as string ('PROJECT_ID'),",
						"     NonkeyColumns as string ('FINANCIAL_COUNTRY_CD,LEDGER_CD,OFFERING_COMPONENT_CD,OPPORTUNITY_NUM,PROJECT_DESC,SIGNINGS_CD,SIGNINGS_DESC,BUSINESS_TYPE_CD,BUSINESS_TYPE_DESC,PROJECT_STATUS_CD,PROJECT_STATUS_DESC,PROJECT_CUSTOMER_NO,PROJECT_CREATION_DATE,ACCOUTNING_DIVISION,RESPONSIBLE_SERV_OFFICE')",
						"}",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> genericInput",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> genericDimensionTable",
						"genericInput derive(nk_hash = md5(byNames(split($NaturalKey,','))),",
						"          columns_hash = md5(byNames(split($NonkeyColumns,',')))) ~> derivedAddHashInput",
						"genericDimensionTable derive(nk_hash_existing = md5(byName($NaturalKey)),",
						"          columns_hash_existing = md5(byNames(split($NonkeyColumns,',')))) ~> derivedAddHashExisting",
						"derivedAddHashInput, derivedAddHashExisting join(nk_hash == nk_hash_existing,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> fullOuterJoin",
						"fullOuterJoin split(equals(nk_hash_existing,nk_hash) && equals(columns_hash_existing,columns_hash),",
						"     equals(nk_hash_existing,nk_hash) && iifNull((columns_hash_existing),'NULL',(columns_hash_existing)) != iifNull((columns_hash),'NULL',(columns_hash)),",
						"     isNull(nk_hash_existing),",
						"     isNull(nk_hash),",
						"     disjoint: false) ~> NoChangeRecords@(NoChangeRecords, ChangedRecordsForUpdate, NewrecodsForInsert, NotActiveInSource, RestAll)",
						"derivedAddHashExisting select(mapColumn(",
						"          nk_hash = nk_hash_existing,",
						"          columns_hash = columns_hash_existing",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"NoChangeRecords@ChangedRecordsForUpdate select(mapColumn(",
						"          nk_hash,",
						"          columns_hash,",
						"          nk_hash_existing,",
						"          columns_hash_existing",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select2",
						"NoChangeRecords@NewrecodsForInsert select(mapColumn(",
						"          nk_hash,",
						"          columns_hash,",
						"          nk_hash_existing,",
						"          columns_hash_existing",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select3",
						"NoChangeRecords@NotActiveInSource select(mapColumn(",
						"          nk_hash,",
						"          columns_hash,",
						"          nk_hash_existing,",
						"          columns_hash_existing",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select4",
						"NoChangeRecords@RestAll select(mapColumn(",
						"          nk_hash,",
						"          columns_hash,",
						"          nk_hash_existing,",
						"          columns_hash_existing",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select5",
						"NoChangeRecords@NoChangeRecords sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> DimensionTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/dsProjectDimensionRaw')]",
				"[concat(variables('workspaceId'), '/datasets/dsAzureSqlDBEtlhubGenericDimension')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bkp_scdType2_Project_Dimension_28thApril')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "backup"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "dsProjectDimensionRaw",
								"type": "DatasetReference"
							},
							"name": "genericInput"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "genericDimensionTable"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DBXDH_DHT_PROJECT_SIV",
								"type": "DatasetReference"
							},
							"name": "Update4ChangedRecords"
						},
						{
							"dataset": {
								"referenceName": "DBXDH_DHT_PROJECT_SIV",
								"type": "DatasetReference"
							},
							"name": "Insert4ChangedRows"
						},
						{
							"dataset": {
								"referenceName": "DBXDH_DHT_PROJECT_SIV",
								"type": "DatasetReference"
							},
							"name": "SinkInsert4NewRows"
						}
					],
					"transformations": [
						{
							"name": "derivedAddHashInput"
						},
						{
							"name": "derivedAddHashExisting"
						},
						{
							"name": "fullOuterJoin"
						},
						{
							"name": "NoChangeRecords"
						},
						{
							"name": "selectExisting"
						},
						{
							"name": "select5"
						},
						{
							"name": "derivedChangedRows4Update"
						},
						{
							"name": "ExistingRowsUpdate"
						},
						{
							"name": "select6"
						},
						{
							"name": "ExistingRowsInsert"
						},
						{
							"name": "AlterRowInsertsNewRows"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "derivedColumn2"
						},
						{
							"name": "derivedInserts4ChangedRows"
						},
						{
							"name": "getMaxKey"
						},
						{
							"name": "joinGetMaxKey"
						},
						{
							"name": "selectInsertNewRows"
						},
						{
							"name": "surrogateKey1"
						},
						{
							"name": "SelectInserts4ChangedRows"
						},
						{
							"name": "select7"
						}
					],
					"scriptLines": [
						"parameters{",
						"     NaturalKey as string ('PROJECT_ID'),",
						"     NonkeyColumns as string ('FINANCIAL_COUNTRY_CD,LEDGER_CD,OFFERING_COMPONENT_CD,OPPORTUNITY_NUM,PROJECT_DESC,SIGNINGS_CD,SIGNINGS_DESC,BUSINESS_TYPE_CD,BUSINESS_TYPE_DESC,PROJECT_STATUS_CD,PROJECT_STATUS_DESC,PROJECT_CUSTOMER_NO,PROJECT_CREATION_DATE,ACCOUTNING_DIVISION,RESPONSIBLE_SERV_OFFICE'),",
						"     EXTRACT_DT as timestamp (currentTimestamp()),",
						"     REC_START_DT as timestamp (currentTimestamp()),",
						"     UpdateKey4Changes as string ('PROJECT_KEY,REC_START_DT'),",
						"     SurrogateKey as string ('PROJECT_KEY'),",
						"     Updt_Key as string ('REC_START_DT')",
						"}",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> genericInput",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     query: 'SELECT A.*, MAX(A.PROJECT_KEY) OVER () AS MAX_KEY\\n FROM DBXDH.DHT_PROJECT_SIV A',",
						"     format: 'query') ~> genericDimensionTable",
						"genericInput derive(nk_hash = md5(byNames(split($NaturalKey,','))),",
						"          columns_hash = md5(byNames(split($NonkeyColumns,',')))) ~> derivedAddHashInput",
						"genericDimensionTable derive(nk_hash = md5(byNames(split($NaturalKey,','))),",
						"          columns_hash = md5(byNames(split($NonkeyColumns,',')))) ~> derivedAddHashExisting",
						"derivedAddHashInput, selectExisting join(nk_hash == existing_nk_hash,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> fullOuterJoin",
						"fullOuterJoin split(equals(existing_nk_hash,nk_hash) && equals(existing_columns_hash,columns_hash),",
						"     equals(existing_nk_hash,nk_hash) && iifNull((existing_columns_hash),'NULL',(existing_columns_hash)) != iifNull((columns_hash),'NULL',(columns_hash)),",
						"     isNull(existing_nk_hash),",
						"     isNull(nk_hash),",
						"     disjoint: false) ~> NoChangeRecords@(NoChangeRecords, ChangedRecordsForUpdate, NewrecodsForInsert, NotActiveInSource, RestAll)",
						"derivedAddHashExisting select(mapColumn(",
						"          each(match(true()),",
						"               'existing_'+$$ = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectExisting",
						"NoChangeRecords@RestAll select(skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select5",
						"NoChangeRecords@ChangedRecordsForUpdate derive(CURRENT_IND = 'N',",
						"          REC_END_DT = currentTimestamp(),",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          REC_STATUS = 'U') ~> derivedChangedRows4Update",
						"select7 alterRow(updateIf(true())) ~> ExistingRowsUpdate",
						"NoChangeRecords@NoChangeRecords select(skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select6",
						"SelectInserts4ChangedRows alterRow(insertIf(true())) ~> ExistingRowsInsert",
						"selectInsertNewRows alterRow(insertIf(true())) ~> AlterRowInsertsNewRows",
						"NoChangeRecords@NotActiveInSource derive(CURRENT_IND = 'Y') ~> derivedColumn1",
						"surrogateKey1 derive(CURRENT_IND = 'Y',",
						"          EXTRACT_DT = currentTimestamp(),",
						"          REC_START_DT = currentDate(),",
						"          REC_END_DT = '9999-12-31',",
						"          REC_STATUS = 'I',",
						"          SOURCE_SYSTEM = 'BMSIW',",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          IMG_CREATED_DT = currentTimestamp(),",
						"          DATA_IND = 'LG',",
						"          ACTIVE_IN_SOURCE_IND = 'Y',",
						"          Key2 = add(Key1,NewKey)) ~> derivedColumn2",
						"NoChangeRecords@ChangedRecordsForUpdate derive(CURRENT_IND = 'Y',",
						"          EXTRACT_DT = currentTimestamp(),",
						"          REC_START_DT = currentDate(),",
						"          REC_END_DT = '9999-12-31',",
						"          SOURCE_SYSTEM = 'BMSIW',",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          DATA_IND = 'LG',",
						"          REC_STATUS = 'U',",
						"          ACTIVE_IN_SOURCE_IND = 'Y') ~> derivedInserts4ChangedRows",
						"selectExisting aggregate(Key1 = max(toInteger(byName('existing_' + $SurrogateKey)))) ~> getMaxKey",
						"NoChangeRecords@NewrecodsForInsert, getMaxKey join(1==1,",
						"     joinType:'cross',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> joinGetMaxKey",
						"derivedColumn2 select(mapColumn(",
						"          nk_hash,",
						"          REC_CHECKSUM = columns_hash,",
						"          each(match(name=='Key2'),",
						"               $SurrogateKey = $$),",
						"          each(match(left(name,8)!='existing'))",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertNewRows",
						"joinGetMaxKey keyGenerate(output(NewKey as long),",
						"     startAt: 1L,",
						"     stepValue: 1L) ~> surrogateKey1",
						"derivedInserts4ChangedRows select(mapColumn(",
						"          nk_hash,",
						"          columns_hash,",
						"          existing_nk_hash,",
						"          existing_columns_hash,",
						"          CURRENT_IND,",
						"          EXTRACT_DT,",
						"          REC_START_DT,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$),",
						"          each(match(name=='existing_IMG_CREATED_DT'),",
						"               'IMG_CREATED_DT' = $$),",
						"          each(match(left(name,8)!='existing'))",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> SelectInserts4ChangedRows",
						"derivedChangedRows4Update select(mapColumn(",
						"          nk_hash,",
						"          columns_hash,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$),",
						"          each(match(name=='existing_REC_START_DT'),",
						"               'REC_START_DT' = $$),",
						"          each(match(true()))",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select7",
						"ExistingRowsUpdate sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          PROJECT_KEY as integer,",
						"          PROJECT_VERSION as integer,",
						"          PROJECT_ID as string,",
						"          FINANCIAL_COUNTRY_CD as string,",
						"          LEDGER_CD as string,",
						"          OFFERING_COMPONENT_CD as string,",
						"          OPPORTUNITY_NUM as string,",
						"          PROJECT_DESC as string,",
						"          SIGNINGS_CD as string,",
						"          SIGNINGS_DESC as string,",
						"          BUSINESS_TYPE_CD as string,",
						"          BUSINESS_TYPE_DESC as string,",
						"          PROJECT_STATUS_CD as string,",
						"          PROJECT_STATUS_DESC as string,",
						"          PROJECT_CUSTOMER_NO as string,",
						"          PROJECT_CREATION_DATE as date,",
						"          ACCOUTNING_DIVISION as string,",
						"          RESPONSIBLE_SERV_OFFICE as string,",
						"          CURRENT_IND as string,",
						"          EXTRACT_DT as timestamp,",
						"          REC_START_DT as timestamp,",
						"          REC_END_DT as timestamp,",
						"          SOURCE_SYSTEM as string,",
						"          REC_CHECKSUM as string,",
						"          REC_STATUS as string,",
						"          IMG_LST_UPD_DT as timestamp,",
						"          IMG_CREATED_DT as timestamp,",
						"          DATA_IND as string,",
						"          ACTIVE_IN_SOURCE_IND as string",
						"     ),",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:[($SurrogateKey),('REC_START_DT')],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Update4ChangedRecords",
						"ExistingRowsInsert sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          PROJECT_KEY as integer,",
						"          PROJECT_VERSION as integer,",
						"          PROJECT_ID as string,",
						"          FINANCIAL_COUNTRY_CD as string,",
						"          LEDGER_CD as string,",
						"          OFFERING_COMPONENT_CD as string,",
						"          OPPORTUNITY_NUM as string,",
						"          PROJECT_DESC as string,",
						"          SIGNINGS_CD as string,",
						"          SIGNINGS_DESC as string,",
						"          BUSINESS_TYPE_CD as string,",
						"          BUSINESS_TYPE_DESC as string,",
						"          PROJECT_STATUS_CD as string,",
						"          PROJECT_STATUS_DESC as string,",
						"          PROJECT_CUSTOMER_NO as string,",
						"          PROJECT_CREATION_DATE as date,",
						"          ACCOUTNING_DIVISION as string,",
						"          RESPONSIBLE_SERV_OFFICE as string,",
						"          CURRENT_IND as string,",
						"          EXTRACT_DT as timestamp,",
						"          REC_START_DT as timestamp,",
						"          REC_END_DT as timestamp,",
						"          SOURCE_SYSTEM as string,",
						"          REC_CHECKSUM as string,",
						"          REC_STATUS as string,",
						"          IMG_LST_UPD_DT as timestamp,",
						"          IMG_CREATED_DT as timestamp,",
						"          DATA_IND as string,",
						"          ACTIVE_IN_SOURCE_IND as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Insert4ChangedRows",
						"AlterRowInsertsNewRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          PROJECT_KEY as integer,",
						"          PROJECT_VERSION as integer,",
						"          PROJECT_ID as string,",
						"          FINANCIAL_COUNTRY_CD as string,",
						"          LEDGER_CD as string,",
						"          OFFERING_COMPONENT_CD as string,",
						"          OPPORTUNITY_NUM as string,",
						"          PROJECT_DESC as string,",
						"          SIGNINGS_CD as string,",
						"          SIGNINGS_DESC as string,",
						"          BUSINESS_TYPE_CD as string,",
						"          BUSINESS_TYPE_DESC as string,",
						"          PROJECT_STATUS_CD as string,",
						"          PROJECT_STATUS_DESC as string,",
						"          PROJECT_CUSTOMER_NO as string,",
						"          PROJECT_CREATION_DATE as date,",
						"          ACCOUTNING_DIVISION as string,",
						"          RESPONSIBLE_SERV_OFFICE as string,",
						"          CURRENT_IND as string,",
						"          EXTRACT_DT as timestamp,",
						"          REC_START_DT as timestamp,",
						"          REC_END_DT as timestamp,",
						"          SOURCE_SYSTEM as string,",
						"          REC_CHECKSUM as string,",
						"          REC_STATUS as string,",
						"          IMG_LST_UPD_DT as timestamp,",
						"          IMG_CREATED_DT as timestamp,",
						"          DATA_IND as string,",
						"          ACTIVE_IN_SOURCE_IND as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> SinkInsert4NewRows"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/dsProjectDimensionRaw')]",
				"[concat(variables('workspaceId'), '/datasets/dsAzureSqlDBEtlhubGenericDimension')]",
				"[concat(variables('workspaceId'), '/datasets/DBXDH_DHT_PROJECT_SIV')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bkp_scdType2_Project_Dimension_Latest')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "backup"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "dsProjectDimensionRaw",
								"type": "DatasetReference"
							},
							"name": "genericInput"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "genericDimensionTable"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "Update4ChangedRecords"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "Insert4ChangedRows"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "SinkInsert4NewRows"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "derivedAddHashInput"
						},
						{
							"name": "derivedAddHashExisting"
						},
						{
							"name": "fullOuterJoin"
						},
						{
							"name": "NoChangeRecords"
						},
						{
							"name": "selectExisting"
						},
						{
							"name": "select5"
						},
						{
							"name": "derivedChangedRows4Update"
						},
						{
							"name": "ExistingRowsUpdate"
						},
						{
							"name": "select6"
						},
						{
							"name": "ExistingRowsInsert"
						},
						{
							"name": "AlterRowInsertsNewRows"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "derivedColumn2"
						},
						{
							"name": "derivedInserts4ChangedRows"
						},
						{
							"name": "getMaxKey"
						},
						{
							"name": "joinGetMaxKey"
						},
						{
							"name": "selectInsertNewRows"
						},
						{
							"name": "surrogateKey1"
						},
						{
							"name": "SelectInserts4ChangedRows"
						},
						{
							"name": "selectChangedRecords4Update"
						},
						{
							"name": "select7"
						},
						{
							"name": "alterRow1"
						}
					],
					"scriptLines": [
						"parameters{",
						"     NaturalKey as string ('PROJECT_ID'),",
						"     NonkeyColumns as string ('FINANCIAL_COUNTRY_CD,LEDGER_CD,OFFERING_COMPONENT_CD,OPPORTUNITY_NUM,PROJECT_DESC,SIGNINGS_CD,SIGNINGS_DESC,BUSINESS_TYPE_CD,BUSINESS_TYPE_DESC,PROJECT_STATUS_CD,PROJECT_STATUS_DESC,PROJECT_CUSTOMER_NO,PROJECT_CREATION_DATE,ACCOUTNING_DIVISION,RESPONSIBLE_SERV_OFFICE'),",
						"     EXTRACT_DT as timestamp (currentTimestamp()),",
						"     REC_START_DT as timestamp (currentTimestamp()),",
						"     SurrogateKey as string ('PROJECT_KEY'),",
						"     DimTableName as string ('DHT_PROJECT_SIV')",
						"}",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> genericInput",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     query: ('SELECT * FROM DBXDH.' + $DimTableName + ' WHERE CURRENT_IND=\\'' + 'Y' + '\\''),",
						"     format: 'query') ~> genericDimensionTable",
						"genericInput derive(nk_hash = md5(byNames(split($NaturalKey,','))),",
						"          columns_hash = md5(byNames(split($NonkeyColumns,',')))) ~> derivedAddHashInput",
						"genericDimensionTable derive(nk_hash = md5(byNames(split($NaturalKey,','))),",
						"          columns_hash = toString(byName('REC_CHECKSUM'))) ~> derivedAddHashExisting",
						"derivedAddHashInput, selectExisting join(nk_hash == existing_nk_hash,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> fullOuterJoin",
						"fullOuterJoin split(equals(existing_nk_hash,nk_hash) && equals(existing_columns_hash,columns_hash),",
						"     equals(existing_nk_hash,nk_hash) && iifNull((existing_columns_hash),'NULL',(existing_columns_hash)) != iifNull((columns_hash),'NULL',(columns_hash)),",
						"     isNull(existing_nk_hash),",
						"     isNull(nk_hash),",
						"     disjoint: false) ~> NoChangeRecords@(NoChangeRecords, ChangedRecordsForUpdate, NewrecodsForInsert, NotActiveInSource, RestAll)",
						"derivedAddHashExisting select(mapColumn(",
						"          each(match(true()),",
						"               'existing_'+$$ = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectExisting",
						"NoChangeRecords@RestAll select(mapColumn(",
						"          REC_CHECKSUM = existing_columns_hash",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select5",
						"NoChangeRecords@ChangedRecordsForUpdate derive(CURRENT_IND = 'N',",
						"          REC_END_DT = currentTimestamp(),",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          REC_STATUS = 'U',",
						"          VERSION = toInteger(byName('existing_' + left($SurrogateKey,instr($SurrogateKey,'_KEY')) +'VERSION')) +1) ~> derivedChangedRows4Update",
						"selectChangedRecords4Update alterRow(updateIf(true())) ~> ExistingRowsUpdate",
						"NoChangeRecords@NoChangeRecords select(skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select6",
						"SelectInserts4ChangedRows alterRow(insertIf(true())) ~> ExistingRowsInsert",
						"selectInsertNewRows alterRow(insertIf(true())) ~> AlterRowInsertsNewRows",
						"NoChangeRecords@NotActiveInSource derive(ACTIVE_IN_SOURCE_IND = 'Y',",
						"          IMG_LST_UPD_DT = currentTimestamp()) ~> derivedColumn1",
						"surrogateKey1 derive(CURRENT_IND = 'Y',",
						"          EXTRACT_DT = $EXTRACT_DT,",
						"          REC_START_DT = $REC_START_DT,",
						"          REC_END_DT = '9999-12-31 00:00:00.000',",
						"          REC_STATUS = 'I',",
						"          SOURCE_SYSTEM = 'BMSIW',",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          IMG_CREATED_DT = currentTimestamp(),",
						"          DATA_IND = 'LG',",
						"          ACTIVE_IN_SOURCE_IND = 'Y',",
						"          Key2 = add(iifNull(Key1,0),NewKey),",
						"          VERSION = 1) ~> derivedColumn2",
						"NoChangeRecords@ChangedRecordsForUpdate derive(CURRENT_IND = 'Y',",
						"          EXTRACT_DT = $EXTRACT_DT,",
						"          REC_START_DT = $REC_START_DT,",
						"          REC_END_DT = '9999-12-31 00:00:00.000',",
						"          SOURCE_SYSTEM = 'BMSIW',",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          DATA_IND = 'LG',",
						"          REC_STATUS = 'I',",
						"          ACTIVE_IN_SOURCE_IND = 'Y',",
						"          VERSION = toInteger(byName('existing_' + left($SurrogateKey,instr($SurrogateKey,'_KEY'))+'VERSION'))+1) ~> derivedInserts4ChangedRows",
						"selectExisting aggregate(Key1 = max(toInteger(byName('existing_' + $SurrogateKey)))) ~> getMaxKey",
						"NoChangeRecords@NewrecodsForInsert, getMaxKey join(1==1,",
						"     joinType:'cross',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> joinGetMaxKey",
						"derivedColumn2 select(mapColumn(",
						"          REC_CHECKSUM = columns_hash,",
						"          each(match(name=='VERSION'),",
						"               left($SurrogateKey,instr($SurrogateKey,'_KEY'))+'VERSION' = $$),",
						"          each(match(name=='Key2'),",
						"               $SurrogateKey = $$),",
						"          each(match(left(name,8)!='existing'))",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertNewRows",
						"joinGetMaxKey keyGenerate(output(NewKey as long),",
						"     startAt: 1L,",
						"     stepValue: 1L) ~> surrogateKey1",
						"derivedInserts4ChangedRows select(mapColumn(",
						"          REC_CHECKSUM = columns_hash,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$),",
						"          each(match(name=='existing_IMG_CREATED_DT'),",
						"               'IMG_CREATED_DT' = $$),",
						"          each(match(name=='VERSION'),",
						"               left($SurrogateKey,instr($SurrogateKey,'_KEY'))+'VERSION' = $$),",
						"          each(match(left(name,8)!='existing'))",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> SelectInserts4ChangedRows",
						"derivedChangedRows4Update select(mapColumn(",
						"          REC_CHECKSUM = existing_columns_hash,",
						"          CURRENT_IND,",
						"          REC_END_DT,",
						"          IMG_LST_UPD_DT,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectChangedRecords4Update",
						"derivedColumn1 select(mapColumn(",
						"          REC_CHECKSUM = existing_columns_hash,",
						"          ACTIVE_IN_SOURCE_IND,",
						"          IMG_LST_UPD_DT,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select7",
						"select7 alterRow(updateIf(true())) ~> alterRow1",
						"ExistingRowsUpdate sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:([$SurrogateKey,'REC_CHECKSUM']),",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 1,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Update4ChangedRecords",
						"ExistingRowsInsert sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 2,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Insert4ChangedRows",
						"AlterRowInsertsNewRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 3,",
						"     errorHandlingOption: 'stopOnFirstError') ~> SinkInsert4NewRows",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:([$SurrogateKey,'REC_CHECKSUM']),",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 4,",
						"     errorHandlingOption: 'stopOnFirstError') ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/dsProjectDimensionRaw')]",
				"[concat(variables('workspaceId'), '/datasets/dsAzureSqlDBEtlhubGenericDimension')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bkp_scdType2_Project_Dimension_Latest_2ndMay')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "backup"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "dsProjectDimensionRaw",
								"type": "DatasetReference"
							},
							"name": "genericInput"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "genericDimensionTable"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "Update4ChangedRecords"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "Insert4ChangedRows"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "SinkInsert4NewRows"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "derivedAddHashInput"
						},
						{
							"name": "derivedAddHashExisting"
						},
						{
							"name": "fullOuterJoin"
						},
						{
							"name": "NoChangeRecords"
						},
						{
							"name": "selectExisting"
						},
						{
							"name": "select5"
						},
						{
							"name": "derivedChangedRows4Update"
						},
						{
							"name": "ExistingRowsUpdate"
						},
						{
							"name": "select6"
						},
						{
							"name": "ExistingRowsInsert"
						},
						{
							"name": "AlterRowInsertsNewRows"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "derivedColumn2"
						},
						{
							"name": "derivedInserts4ChangedRows"
						},
						{
							"name": "getMaxKey"
						},
						{
							"name": "joinGetMaxKey"
						},
						{
							"name": "selectInsertNewRows"
						},
						{
							"name": "surrogateKey1"
						},
						{
							"name": "SelectInserts4ChangedRows"
						},
						{
							"name": "selectChangedRecords4Update"
						},
						{
							"name": "select7"
						},
						{
							"name": "alterRow1"
						}
					],
					"scriptLines": [
						"parameters{",
						"     NaturalKey as string ('PROJECT_ID'),",
						"     NonkeyColumns as string ('FINANCIAL_COUNTRY_CD,LEDGER_CD,OFFERING_COMPONENT_CD,OPPORTUNITY_NUM,PROJECT_DESC,SIGNINGS_CD,SIGNINGS_DESC,BUSINESS_TYPE_CD,BUSINESS_TYPE_DESC,PROJECT_STATUS_CD,PROJECT_STATUS_DESC,PROJECT_CUSTOMER_NO,PROJECT_CREATION_DATE,ACCOUTNING_DIVISION,RESPONSIBLE_SERV_OFFICE'),",
						"     EXTRACT_DT as timestamp (currentTimestamp()),",
						"     REC_START_DT as timestamp (currentTimestamp()),",
						"     SurrogateKey as string ('PROJECT_KEY'),",
						"     DimTableName as string ('DHT_PROJECT_SIV')",
						"}",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> genericInput",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     query: ('SELECT * FROM DBXDH.' + $DimTableName + ' WHERE CURRENT_IND=\\'' + 'Y' + '\\''),",
						"     format: 'query') ~> genericDimensionTable",
						"genericInput derive(nk_hash = md5(byNames(split($NaturalKey,','))),",
						"          columns_hash = md5(byNames(split($NonkeyColumns,',')))) ~> derivedAddHashInput",
						"genericDimensionTable derive(nk_hash = md5(byNames(split($NaturalKey,','))),",
						"          columns_hash = toString(byName('REC_CHECKSUM'))) ~> derivedAddHashExisting",
						"derivedAddHashInput, selectExisting join(nk_hash == existing_nk_hash,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> fullOuterJoin",
						"fullOuterJoin split(equals(existing_nk_hash,nk_hash) && equals(existing_columns_hash,columns_hash),",
						"     equals(existing_nk_hash,nk_hash) && iifNull((existing_columns_hash),'NULL',(existing_columns_hash)) != iifNull((columns_hash),'NULL',(columns_hash)),",
						"     isNull(existing_nk_hash),",
						"     isNull(nk_hash),",
						"     disjoint: false) ~> NoChangeRecords@(NoChangeRecords, ChangedRecordsForUpdate, NewrecodsForInsert, NotActiveInSource, RestAll)",
						"derivedAddHashExisting select(mapColumn(",
						"          each(match(true()),",
						"               'existing_'+$$ = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectExisting",
						"NoChangeRecords@RestAll select(mapColumn(",
						"          REC_CHECKSUM = existing_columns_hash",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select5",
						"NoChangeRecords@ChangedRecordsForUpdate derive(CURRENT_IND = 'N',",
						"          REC_END_DT = currentTimestamp(),",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          REC_STATUS = 'U',",
						"          VERSION = toInteger(byName('existing_' + left($SurrogateKey,instr($SurrogateKey,'_KEY')) +'VERSION')) +1) ~> derivedChangedRows4Update",
						"selectChangedRecords4Update alterRow(updateIf(true())) ~> ExistingRowsUpdate",
						"NoChangeRecords@NoChangeRecords select(skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select6",
						"SelectInserts4ChangedRows alterRow(insertIf(true())) ~> ExistingRowsInsert",
						"selectInsertNewRows alterRow(insertIf(true())) ~> AlterRowInsertsNewRows",
						"NoChangeRecords@NotActiveInSource derive(ACTIVE_IN_SOURCE_IND = 'Y',",
						"          IMG_LST_UPD_DT = currentTimestamp()) ~> derivedColumn1",
						"surrogateKey1 derive(CURRENT_IND = 'Y',",
						"          EXTRACT_DT = $EXTRACT_DT,",
						"          REC_START_DT = $REC_START_DT,",
						"          REC_END_DT = '9999-12-31 00:00:00.000',",
						"          REC_STATUS = 'I',",
						"          SOURCE_SYSTEM = 'BMSIW',",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          IMG_CREATED_DT = currentTimestamp(),",
						"          DATA_IND = 'LG',",
						"          ACTIVE_IN_SOURCE_IND = 'Y',",
						"          Key2 = add(iifNull(Key1,0),NewKey),",
						"          VERSION = 1) ~> derivedColumn2",
						"NoChangeRecords@ChangedRecordsForUpdate derive(CURRENT_IND = 'Y',",
						"          EXTRACT_DT = $EXTRACT_DT,",
						"          REC_START_DT = $REC_START_DT,",
						"          REC_END_DT = '9999-12-31 00:00:00.000',",
						"          SOURCE_SYSTEM = 'BMSIW',",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          DATA_IND = 'LG',",
						"          REC_STATUS = 'I',",
						"          ACTIVE_IN_SOURCE_IND = 'Y',",
						"          VERSION = toInteger(byName('existing_' + left($SurrogateKey,instr($SurrogateKey,'_KEY'))+'VERSION'))+1) ~> derivedInserts4ChangedRows",
						"selectExisting aggregate(Key1 = max(toInteger(byName('existing_' + $SurrogateKey)))) ~> getMaxKey",
						"NoChangeRecords@NewrecodsForInsert, getMaxKey join(1==1,",
						"     joinType:'cross',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> joinGetMaxKey",
						"derivedColumn2 select(mapColumn(",
						"          REC_CHECKSUM = columns_hash,",
						"          each(match(name=='VERSION'),",
						"               left($SurrogateKey,instr($SurrogateKey,'_KEY'))+'VERSION' = $$),",
						"          each(match(name=='Key2'),",
						"               $SurrogateKey = $$),",
						"          each(match(left(name,8)!='existing'))",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertNewRows",
						"joinGetMaxKey keyGenerate(output(NewKey as long),",
						"     startAt: 1L,",
						"     stepValue: 1L) ~> surrogateKey1",
						"derivedInserts4ChangedRows select(mapColumn(",
						"          REC_CHECKSUM = columns_hash,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$),",
						"          each(match(name=='existing_IMG_CREATED_DT'),",
						"               'IMG_CREATED_DT' = $$),",
						"          each(match(name=='VERSION'),",
						"               left($SurrogateKey,instr($SurrogateKey,'_KEY'))+'VERSION' = $$),",
						"          each(match(left(name,8)!='existing'))",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> SelectInserts4ChangedRows",
						"derivedChangedRows4Update select(mapColumn(",
						"          REC_CHECKSUM = existing_columns_hash,",
						"          CURRENT_IND,",
						"          REC_END_DT,",
						"          IMG_LST_UPD_DT,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectChangedRecords4Update",
						"derivedColumn1 select(mapColumn(",
						"          REC_CHECKSUM = existing_columns_hash,",
						"          ACTIVE_IN_SOURCE_IND,",
						"          IMG_LST_UPD_DT,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select7",
						"select7 alterRow(updateIf(true())) ~> alterRow1",
						"ExistingRowsUpdate sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:([$SurrogateKey,'REC_CHECKSUM']),",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 1,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Update4ChangedRecords",
						"ExistingRowsInsert sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 2,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Insert4ChangedRows",
						"AlterRowInsertsNewRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 3,",
						"     errorHandlingOption: 'stopOnFirstError') ~> SinkInsert4NewRows",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:([$SurrogateKey,'REC_CHECKSUM']),",
						"     format: 'table',",
						"     preSQLs: ([$SurrogateKey,'REC_CHECKSUM']),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 4,",
						"     errorHandlingOption: 'stopOnFirstError') ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/dsProjectDimensionRaw')]",
				"[concat(variables('workspaceId'), '/datasets/dsAzureSqlDBEtlhubGenericDimension')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/df_SELL_CYCLE')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Dataset",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DBXDH_DHTS_SELL_CYCLE",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "select1"
						},
						{
							"name": "alterRow1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          SIEBEL_SALES_STAGE_CODE as string,",
						"          SIEBEL_SALES_STAGE_NAME as string,",
						"          SSM_STEP_NO as string,",
						"          SSM_STEP_NAME as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     partitionBy('hash', 1)) ~> source1",
						"source1 select(skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 alterRow(insertIf(true())) ~> alterRow1",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Dataset')]",
				"[concat(variables('workspaceId'), '/datasets/DBXDH_DHTS_SELL_CYCLE')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/scdType2_Generict_Dimension_Load')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "dsProjectDimensionRaw",
								"type": "DatasetReference"
							},
							"name": "genericInput"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "genericDimensionTable"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "Update4ChangedRecords"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "Insert4ChangedRows"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "SinkInsert4NewRows"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "Update4SoftDeletedRows"
						}
					],
					"transformations": [
						{
							"name": "derivedAddHashInput"
						},
						{
							"name": "derivedAddHashExisting"
						},
						{
							"name": "fullOuterJoin"
						},
						{
							"name": "NoChangeRecords"
						},
						{
							"name": "selectExisting"
						},
						{
							"name": "select5"
						},
						{
							"name": "derivedChangedRows4Update"
						},
						{
							"name": "ExistingRowsUpdate"
						},
						{
							"name": "select6"
						},
						{
							"name": "ExistingRowsInsert"
						},
						{
							"name": "AlterRowInsertsNewRows"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "derivedColumn2"
						},
						{
							"name": "derivedInserts4ChangedRows"
						},
						{
							"name": "getMaxKey"
						},
						{
							"name": "joinGetMaxKey"
						},
						{
							"name": "selectInsertNewRows"
						},
						{
							"name": "surrogateKey1"
						},
						{
							"name": "SelectInserts4ChangedRows"
						},
						{
							"name": "selectChangedRecords4Update"
						},
						{
							"name": "select7"
						},
						{
							"name": "alterRow1"
						}
					],
					"scriptLines": [
						"parameters{",
						"     NaturalKey as string ('PROJECT_ID1'),",
						"     NonkeyColumns as string ('FINANCIAL_COUNTRY_CD,LEDGER_CD,OFFERING_COMPONENT_CD,OPPORTUNITY_NUM,PROJECT_DESC,SIGNINGS_CD,SIGNINGS_DESC,BUSINESS_TYPE_CD,BUSINESS_TYPE_DESC,PROJECT_STATUS_CD,PROJECT_STATUS_DESC,PROJECT_CUSTOMER_NO,PROJECT_CREATION_DATE,ACCOUTNING_DIVISION,RESPONSIBLE_SERV_OFFICE'),",
						"     EXTRACT_DT as timestamp (currentTimestamp()),",
						"     REC_START_DT as timestamp (currentTimestamp()),",
						"     SurrogateKey as string ('CUSTOMER'),",
						"     DimTableName as string ('TableName1')",
						"}",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> genericInput",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     query: ('SELECT * FROM DBXDH.' + $DimTableName + ' WHERE CURRENT_IND=\\'' + 'Y' + '\\''),",
						"     format: 'query') ~> genericDimensionTable",
						"genericInput derive(nk_hash = md5(byNames(split($NaturalKey,','))),",
						"          columns_hash = md5(byNames(split($NonkeyColumns,',')))) ~> derivedAddHashInput",
						"genericDimensionTable derive(nk_hash = md5(byNames(split($NaturalKey,','))),",
						"          columns_hash = toString(byName('REC_CHECKSUM'))) ~> derivedAddHashExisting",
						"derivedAddHashInput, selectExisting join(nk_hash == existing_nk_hash,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> fullOuterJoin",
						"fullOuterJoin split(equals(existing_nk_hash,nk_hash) && equals(existing_columns_hash,columns_hash),",
						"     equals(existing_nk_hash,nk_hash) && iifNull((existing_columns_hash),'NULL',(existing_columns_hash)) != iifNull((columns_hash),'NULL',(columns_hash)),",
						"     isNull(existing_nk_hash),",
						"     isNull(nk_hash),",
						"     disjoint: false) ~> NoChangeRecords@(NoChangeRecords, ChangedRecordsForUpdate, NewrecodsForInsert, NotActiveInSource, RestAll)",
						"derivedAddHashExisting select(mapColumn(",
						"          each(match(true()),",
						"               'existing_'+$$ = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectExisting",
						"NoChangeRecords@RestAll select(mapColumn(",
						"          REC_CHECKSUM = existing_columns_hash",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select5",
						"NoChangeRecords@ChangedRecordsForUpdate derive(CURRENT_IND = 'N',",
						"          REC_END_DT = currentTimestamp(),",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          REC_STATUS = 'U',",
						"          VERSION = toInteger(byName('existing_' + left($SurrogateKey,instr($SurrogateKey,'_KEY')) +'VERSION')) +1) ~> derivedChangedRows4Update",
						"selectChangedRecords4Update alterRow(updateIf(true())) ~> ExistingRowsUpdate",
						"NoChangeRecords@NoChangeRecords select(skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select6",
						"SelectInserts4ChangedRows alterRow(insertIf(true())) ~> ExistingRowsInsert",
						"selectInsertNewRows alterRow(insertIf(true())) ~> AlterRowInsertsNewRows",
						"NoChangeRecords@NotActiveInSource derive(ACTIVE_IN_SOURCE_IND = 'N',",
						"          IMG_LST_UPD_DT = currentTimestamp()) ~> derivedColumn1",
						"surrogateKey1 derive(CURRENT_IND = 'Y',",
						"          EXTRACT_DT = $EXTRACT_DT,",
						"          REC_START_DT = $REC_START_DT,",
						"          REC_END_DT = '9999-12-31 00:00:00.000',",
						"          REC_STATUS = 'I',",
						"          SOURCE_SYSTEM = 'BMSIW',",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          IMG_CREATED_DT = currentTimestamp(),",
						"          DATA_IND = 'LG',",
						"          ACTIVE_IN_SOURCE_IND = 'Y',",
						"          Key2 = add(iifNull(Key1,0),NewKey),",
						"          VERSION = 1) ~> derivedColumn2",
						"NoChangeRecords@ChangedRecordsForUpdate derive(CURRENT_IND = 'Y',",
						"          EXTRACT_DT = $EXTRACT_DT,",
						"          REC_START_DT = $REC_START_DT,",
						"          REC_END_DT = '9999-12-31 00:00:00.000',",
						"          SOURCE_SYSTEM = 'BMSIW',",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          DATA_IND = 'LG',",
						"          REC_STATUS = 'I',",
						"          ACTIVE_IN_SOURCE_IND = 'Y',",
						"          VERSION = toInteger(byName('existing_' + left($SurrogateKey,instr($SurrogateKey,'_KEY'))+'VERSION'))+1) ~> derivedInserts4ChangedRows",
						"selectExisting aggregate(Key1 = max(toInteger(byName('existing_' + $SurrogateKey)))) ~> getMaxKey",
						"NoChangeRecords@NewrecodsForInsert, getMaxKey join(1==1,",
						"     joinType:'cross',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> joinGetMaxKey",
						"derivedColumn2 select(mapColumn(",
						"          REC_CHECKSUM = columns_hash,",
						"          each(match(name=='VERSION'),",
						"               left($SurrogateKey,instr($SurrogateKey,'_KEY'))+'VERSION' = $$),",
						"          each(match(name=='Key2'),",
						"               $SurrogateKey = $$),",
						"          each(match(left(name,8)!='existing'))",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertNewRows",
						"joinGetMaxKey keyGenerate(output(NewKey as long),",
						"     startAt: 1L,",
						"     stepValue: 1L) ~> surrogateKey1",
						"derivedInserts4ChangedRows select(mapColumn(",
						"          REC_CHECKSUM = columns_hash,",
						"          {'U'} = REC_STATUS,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$),",
						"          each(match(name=='existing_IMG_CREATED_DT'),",
						"               'IMG_CREATED_DT' = $$),",
						"          each(match(name=='VERSION'),",
						"               left($SurrogateKey,instr($SurrogateKey,'_KEY'))+'VERSION' = $$),",
						"          each(match(left(name,8)!='existing'))",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> SelectInserts4ChangedRows",
						"derivedChangedRows4Update select(mapColumn(",
						"          REC_CHECKSUM = existing_columns_hash,",
						"          CURRENT_IND,",
						"          REC_END_DT,",
						"          IMG_LST_UPD_DT,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$),",
						"          each(match(name=='existing_'+$NaturalKey),",
						"               $NaturalKey = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectChangedRecords4Update",
						"derivedColumn1 select(mapColumn(",
						"          REC_CHECKSUM = existing_columns_hash,",
						"          ACTIVE_IN_SOURCE_IND,",
						"          IMG_LST_UPD_DT,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select7",
						"select7 alterRow(updateIf(true())) ~> alterRow1",
						"ExistingRowsUpdate sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:([$SurrogateKey,'REC_CHECKSUM']),",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 1,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Update4ChangedRecords",
						"ExistingRowsInsert sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 2,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Insert4ChangedRows",
						"AlterRowInsertsNewRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 3,",
						"     errorHandlingOption: 'stopOnFirstError') ~> SinkInsert4NewRows",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:([$SurrogateKey,'REC_CHECKSUM']),",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 4,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Update4SoftDeletedRows"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/dsProjectDimensionRaw')]",
				"[concat(variables('workspaceId'), '/datasets/dsAzureSqlDBEtlhubGenericDimension')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/scdType2_Generict_Dimension_Load_WOVersion')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "dsProjectDimensionRaw",
								"type": "DatasetReference"
							},
							"name": "genericInput"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "genericDimensionTable"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "Update4ChangedRecords"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "Insert4ChangedRows"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "SinkInsert4NewRows"
						},
						{
							"dataset": {
								"referenceName": "dsAzureSqlDBEtlhubGenericDimension",
								"type": "DatasetReference"
							},
							"name": "Update4SoftDeletedRows"
						}
					],
					"transformations": [
						{
							"name": "derivedAddHashInput"
						},
						{
							"name": "derivedAddHashExisting"
						},
						{
							"name": "fullOuterJoin"
						},
						{
							"name": "NoChangeRecords"
						},
						{
							"name": "selectExisting"
						},
						{
							"name": "select5"
						},
						{
							"name": "derivedChangedRows4Update"
						},
						{
							"name": "ExistingRowsUpdate"
						},
						{
							"name": "select6"
						},
						{
							"name": "ExistingRowsInsert"
						},
						{
							"name": "AlterRowInsertsNewRows"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "derivedColumn2"
						},
						{
							"name": "derivedInserts4ChangedRows"
						},
						{
							"name": "getMaxKey"
						},
						{
							"name": "joinGetMaxKey"
						},
						{
							"name": "selectInsertNewRows"
						},
						{
							"name": "surrogateKey1"
						},
						{
							"name": "SelectInserts4ChangedRows"
						},
						{
							"name": "selectChangedRecords4Update"
						},
						{
							"name": "select7"
						},
						{
							"name": "alterRow1"
						}
					],
					"scriptLines": [
						"parameters{",
						"     NaturalKey as string ('PROJECT_ID1'),",
						"     NonkeyColumns as string ('FINANCIAL_COUNTRY_CD,LEDGER_CD,OFFERING_COMPONENT_CD,OPPORTUNITY_NUM,PROJECT_DESC,SIGNINGS_CD,SIGNINGS_DESC,BUSINESS_TYPE_CD,BUSINESS_TYPE_DESC,PROJECT_STATUS_CD,PROJECT_STATUS_DESC,PROJECT_CUSTOMER_NO,PROJECT_CREATION_DATE,ACCOUTNING_DIVISION,RESPONSIBLE_SERV_OFFICE'),",
						"     EXTRACT_DT as timestamp (currentTimestamp()),",
						"     REC_START_DT as timestamp (currentTimestamp()),",
						"     DimTableName as string ('TableName1'),",
						"     UpdateKey as string ('Key'),",
						"     SurrogateKey as string ('CUSTOMER_KEY')",
						"}",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> genericInput",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     query: ('SELECT * FROM DBXDH.' + $DimTableName + ' WHERE CURRENT_IND=\\'' + 'Y' + '\\''),",
						"     format: 'query') ~> genericDimensionTable",
						"genericInput derive(nk_hash = md5(byNames(split($NaturalKey,','))),",
						"          columns_hash = md5(byNames(split($NonkeyColumns,',')))) ~> derivedAddHashInput",
						"genericDimensionTable derive(nk_hash = md5(byNames(split($NaturalKey,','))),",
						"          columns_hash = toString(byName('REC_CHECKSUM'))) ~> derivedAddHashExisting",
						"derivedAddHashInput, selectExisting join(nk_hash == existing_nk_hash,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> fullOuterJoin",
						"fullOuterJoin split(equals(existing_nk_hash,nk_hash) && equals(existing_columns_hash,columns_hash),",
						"     equals(existing_nk_hash,nk_hash) && iifNull((existing_columns_hash),'NULL',(existing_columns_hash)) != iifNull((columns_hash),'NULL',(columns_hash)),",
						"     isNull(existing_nk_hash),",
						"     isNull(nk_hash),",
						"     disjoint: false) ~> NoChangeRecords@(NoChangeRecords, ChangedRecordsForUpdate, NewrecodsForInsert, NotActiveInSource, RestAll)",
						"derivedAddHashExisting select(mapColumn(",
						"          each(match(true()),",
						"               'existing_'+$$ = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectExisting",
						"NoChangeRecords@RestAll select(mapColumn(",
						"          REC_CHECKSUM = existing_columns_hash",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select5",
						"NoChangeRecords@ChangedRecordsForUpdate derive(CURRENT_IND = 'N',",
						"          REC_END_DT = currentTimestamp(),",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          REC_STATUS = 'U') ~> derivedChangedRows4Update",
						"selectChangedRecords4Update alterRow(updateIf(true())) ~> ExistingRowsUpdate",
						"NoChangeRecords@NoChangeRecords select(skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select6",
						"SelectInserts4ChangedRows alterRow(insertIf(true())) ~> ExistingRowsInsert",
						"selectInsertNewRows alterRow(insertIf(true())) ~> AlterRowInsertsNewRows",
						"NoChangeRecords@NotActiveInSource derive(ACTIVE_IN_SOURCE_IND = 'N',",
						"          IMG_LST_UPD_DT = currentTimestamp()) ~> derivedColumn1",
						"surrogateKey1 derive(CURRENT_IND = 'Y',",
						"          EXTRACT_DT = $EXTRACT_DT,",
						"          REC_START_DT = $REC_START_DT,",
						"          REC_END_DT = '9999-12-31 00:00:00.000',",
						"          REC_STATUS = 'I',",
						"          SOURCE_SYSTEM = 'BMSIW',",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          IMG_CREATED_DT = currentTimestamp(),",
						"          DATA_IND = 'LG',",
						"          ACTIVE_IN_SOURCE_IND = 'Y',",
						"          Key2 = add(iifNull(Key1,0),NewKey),",
						"          VERSION = 1) ~> derivedColumn2",
						"NoChangeRecords@ChangedRecordsForUpdate derive(CURRENT_IND = 'Y',",
						"          EXTRACT_DT = $EXTRACT_DT,",
						"          REC_START_DT = $REC_START_DT,",
						"          REC_END_DT = '9999-12-31 00:00:00.000',",
						"          SOURCE_SYSTEM = 'BMSIW',",
						"          IMG_LST_UPD_DT = currentTimestamp(),",
						"          DATA_IND = 'LG',",
						"          REC_STATUS = 'I',",
						"          ACTIVE_IN_SOURCE_IND = 'Y',",
						"          VERSION = toInteger(byName('existing_VERSION'))+1) ~> derivedInserts4ChangedRows",
						"selectExisting aggregate(Key1 = max(toInteger(byName('existing_' + $SurrogateKey)))) ~> getMaxKey",
						"NoChangeRecords@NewrecodsForInsert, getMaxKey join(1==1,",
						"     joinType:'cross',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> joinGetMaxKey",
						"derivedColumn2 select(mapColumn(",
						"          REC_CHECKSUM = columns_hash,",
						"          VERSION,",
						"          each(match(name=='Key2'),",
						"               $SurrogateKey = $$),",
						"          each(match(left(name,8)!='existing'))",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectInsertNewRows",
						"joinGetMaxKey keyGenerate(output(NewKey as long),",
						"     startAt: 1L,",
						"     stepValue: 1L) ~> surrogateKey1",
						"derivedInserts4ChangedRows select(mapColumn(",
						"          REC_CHECKSUM = columns_hash,",
						"          {'U'} = REC_STATUS,",
						"          VERSION,",
						"          each(match(and(instr(name,'existing')!=0,instr(name,'KEY')!=0)),",
						"               substring($$,10) = $$),",
						"          each(match(name=='existing_IMG_CREATED_DT'),",
						"               'IMG_CREATED_DT' = $$),",
						"          each(match(left(name,8)!='existing'))",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> SelectInserts4ChangedRows",
						"derivedChangedRows4Update select(mapColumn(",
						"          REC_CHECKSUM = existing_columns_hash,",
						"          CURRENT_IND,",
						"          REC_END_DT,",
						"          IMG_LST_UPD_DT,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$),",
						"          each(match(name=='existing_'+$NaturalKey),",
						"               $NaturalKey = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectChangedRecords4Update",
						"derivedColumn1 select(mapColumn(",
						"          REC_CHECKSUM = existing_columns_hash,",
						"          ACTIVE_IN_SOURCE_IND,",
						"          IMG_LST_UPD_DT,",
						"          each(match(name=='existing_'+$SurrogateKey),",
						"               $SurrogateKey = $$)",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select7",
						"select7 alterRow(updateIf(true())) ~> alterRow1",
						"ExistingRowsUpdate sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:[($UpdateKey),'REC_CHECKSUM'],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 1,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Update4ChangedRecords",
						"ExistingRowsInsert sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 2,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Insert4ChangedRows",
						"AlterRowInsertsNewRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 3,",
						"     errorHandlingOption: 'stopOnFirstError') ~> SinkInsert4NewRows",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:([$UpdateKey,'REC_CHECKSUM']),",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 4,",
						"     errorHandlingOption: 'stopOnFirstError') ~> Update4SoftDeletedRows"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/dsProjectDimensionRaw')]",
				"[concat(variables('workspaceId'), '/datasets/dsAzureSqlDBEtlhubGenericDimension')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CreateExternalDeltaTable')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseDeltaFormatCustomer') \n\tCREATE EXTERNAL FILE FORMAT [SynapseDeltaFormatCustomer] \n\tWITH ( FORMAT_TYPE = DELTA)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'deltalake_adls4fsoetlhubdevuseast_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [deltalake_adls4fsoetlhubdevuseast_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE customer_V3_dimension (\n\t[CUSTOMER_KEY] nvarchar(4000),\n\t[VERSION] nvarchar(4000),\n\t[CUSTOMER_NO] nvarchar(4000),\n\t[FINANCIAL_COUNTRY_CD] nvarchar(4000),\n\t[GBG_ID] nvarchar(4000),\n\t[CUSTOMER_NAME] nvarchar(4000),\n\t[CURRENT_IND] nvarchar(4000),\n\t[EXTRACT_DT] nvarchar(4000),\n\t[REC_START_DT] nvarchar(4000),\n\t[REC_END_DT] nvarchar(4000),\n\t[SOURCE_SYSTEM] nvarchar(4000),\n\t[REC_CHECKSUM] nvarchar(4000),\n\t[REC_STATUS] nvarchar(4000),\n\t[IMG_LST_UPD_DT] nvarchar(4000),\n\t[IMG_CREATED_DT] nvarchar(4000),\n\t[DATA_IND] nvarchar(4000),\n\t[ACTIVE_IN_SOURCE_IND] nvarchar(4000)\n\t)\n\tWITH (\n\tLOCATION = 'data/customer/customer_dimension2/',\n\tDATA_SOURCE = [deltalake_adls4fsoetlhubdevuseast_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseDeltaFormatCustomer]\n\t)\nGO\n\n\nSELECT count(*) FROM dbo.customer_V3_dimension\nGO\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "kyn",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Query data with SQL')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "\nSELECT * FROM DBXDH.DHT_CUSTOMER\n\nINSERT INTO DBXDH.DHT_CUSTOMER\n(CUSTOMER_KEY, VERSION, CUSTOMER_NO, FINANCIAL_COUNTRY_CD, GBG_ID, CUSTOMER_NAME, CURRENT_IND, EXTRACT_DT, REC_START_DT, REC_END_DT, SOURCE_SYSTEM, REC_CHECKSUM, REC_STATUS, IMG_LST_UPD_DT, IMG_CREATED_DT, DATA_IND, ACTIVE_IN_SOURCE_IND)\nVALUES(2, 1, '4514136', '897', 'GB302S62', 'SETERUS INC', 'Y', '2022-05-16 10:03:26.927' , '2022-05-16 10:03:26.927', '9999-12-31 00:00:00.000', 'BMSIW', '55cccdEDS643aaac65deb1388ada8643f5', 'I', '2022-05-16 10:03:26.927', '2022-05-16 10:03:26.927', 'LG', 'Y');\n\n\n--select CURRENT_TIMESTAMP\n--update\nupdate DBXDH.DHT_CUSTOMER\nset FINANCIAL_COUNTRY_CD='902'\nWHERE CUSTOMER_NO='9549530'\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "dsqlpoolKyn001494DevEtlHubEUS001",
						"poolName": "dsqlpoolKyn001494DevEtlHubEUS001"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://azureopendatastorage.blob.core.windows.net/isdweatherdatacontainer/ISDWeather/year=2018/month=*/*.parquet',\n        FORMAT='PARQUET'\n    ) AS [weather];\n\nhttps://adls4fsoetlhubdevuseast.blob.core.windows.net/project/project_siv_data.csv\n\nselect top 10 *\nfrom openrowset(\n    bulk 'https://adls4fsoetlhubdevuseast.blob.core.windows.net/project/project_data.csv',\n    format = 'csv',\n    parser_version = '2.0',\n    firstrow = 2 ) as rows\n;\n\nSELECT * FROM DBXDH.DHT_PROJECT_SIV\n\n\n\n\nSELECT\n\nTOP 100 *\n\nFROM\n\nOPENROWSET(\n\nBULK 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/project/proj_tgt_file.csv',\n\nFORMAT = 'CSV',\n\nPARSER_VERSION = '2.0'\n\n) AS [result]\n\n\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 10')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "update etlhubConfirmed.dbo.sell_cycle set SIEBEL_SALES_STAGE_NAME = \"1200 newly added\"\nwhere SIEBEL_SALES_STAGE_CODE = \"1000\"",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "etlhubconfirmed",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 11')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": " update [etlhubconfirmed].[dbo].[dht_sell_cycle]\n set [SIEBEL_SALES_STAGE_NAME] = \"\"\n where [SIEBEL_SALES_STAGE_CODE] = \"1000\"",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "etlhubconfirmed",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 12')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "\n\nIF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'deltalake_adls4fsoetlhubdevuseast_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [deltalake_adls4fsoetlhubdevuseast_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE customer_dim_temp (\n\t[CUSTOMER_KEY] nvarchar(4000),\n\t[VERSION] nvarchar(4000),\n\t[CUSTOMER_NO] nvarchar(4000),\n\t[FINANCIAL_COUNTRY_CD] nvarchar(4000),\n\t[GBG_ID] nvarchar(4000),\n\t[CUSTOMER_NAME] nvarchar(4000),\n\t[CURRENT_IND] nvarchar(4000),\n\t[EXTRACT_DT] nvarchar(4000),\n\t[REC_START_DT] nvarchar(4000),\n\t[REC_END_DT] nvarchar(4000),\n\t[SOURCE_SYSTEM] nvarchar(4000),\n\t[REC_CHECKSUM] nvarchar(4000),\n\t[REC_STATUS] nvarchar(4000),\n\t[IMG_LST_UPD_DT] nvarchar(4000),\n\t[IMG_CREATED_DT] nvarchar(4000),\n\t[DATA_IND] nvarchar(4000),\n\t[ACTIVE_IN_SOURCE_IND] nvarchar(4000)\n\t)\n\tWITH (\n\tLOCATION = 'data/customer/customer_dimension2/**',\n\tDATA_SOURCE = [deltalake_adls4fsoetlhubdevuseast_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.customer_dim_temp\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "kyn",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 13')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "etlhubconfirmed",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 14')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "etlhubconfirmed",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 15')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT count(*)\n FROM [etlhubconfirmed].dbo.[dht_building]\n WHERE CURRENT_IND='Y'",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "etlhubconfirmed",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 16')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "delete from\n  [etlhubconfirmed].[dbo].[dht_building]\n WHERE CURRENT_IND='Y'",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "etlhubconfirmed",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "select top 10 * from MASTER.DBXDH_DHT_PROJECT_SIV\n\n\nselect top 10 *\nfrom openrowset(\n    bulk 'https://adls4fsoetlhubdevuseast.blob.core.windows.net/project/project_data.csv',\n    format = 'csv',\n    parser_version = '2.0',\n    firstrow = 2 ) as rows",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 3')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://adlskyn001494deveus001.dfs.core.windows.net/kyn/DH_SF_SELLCYCE_DATA.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 4')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/project/sell_cycle_tgt.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 5')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/project/sell_cycle.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 6')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://contosolake.dfs.core.windows.net/users/NYCTripSmall.parquet',\n        FORMAT='PARQUET'\n    ) AS [result] ",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "dsqlpoolKyn001494DevEtlHubEUS001",
						"poolName": "dsqlpoolKyn001494DevEtlHubEUS001"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 7')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.objects O JOIN sys.schemas S ON O.schema_id = S.schema_id WHERE O.NAME = 'NYCTaxiTripSmall' AND O.TYPE = 'U' AND S.NAME = 'dbo')\nCREATE TABLE dbo.NYCTaxiTripSmall\n    (\n     [DateID] int,\n     [MedallionID] int,\n     [HackneyLicenseID] int,\n     [PickupTimeID] int,\n     [DropoffTimeID] int,\n     [PickupGeographyID] int,\n     [DropoffGeographyID] int,\n     [PickupLatitude] float,\n     [PickupLongitude] float,\n     [PickupLatLong] nvarchar(4000),\n     [DropoffLatitude] float,\n     [DropoffLongitude] float,\n     [DropoffLatLong] nvarchar(4000),\n     [PassengerCount] int,\n     [TripDurationSeconds] int,\n     [TripDistanceMiles] float,\n     [PaymentType] nvarchar(4000),\n     [FareAmount] numeric(19,4),\n     [SurchargeAmount] numeric(19,4),\n     [TaxAmount] numeric(19,4),\n     [TipAmount] numeric(19,4),\n     [TollsAmount] numeric(19,4),\n     [TotalAmount] numeric(19,4)\n    )\nWITH\n    (\n    DISTRIBUTION = ROUND_ROBIN,\n     CLUSTERED COLUMNSTORE INDEX\n     -- HEAP\n    )\nGO\n\nCOPY INTO dbo.NYCTaxiTripSmall\n(DateID 1, MedallionID 2, HackneyLicenseID 3, PickupTimeID 4, DropoffTimeID 5,\nPickupGeographyID 6, DropoffGeographyID 7, PickupLatitude 8, PickupLongitude 9, \nPickupLatLong 10, DropoffLatitude 11, DropoffLongitude 12, DropoffLatLong 13, \nPassengerCount 14, TripDurationSeconds 15, TripDistanceMiles 16, PaymentType 17, \nFareAmount 18, SurchargeAmount 19, TaxAmount 20, TipAmount 21, TollsAmount 22, \nTotalAmount 23)\nFROM 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/data/NYCTripSmall.parquet'\nWITH\n(\n    FILE_TYPE = 'PARQUET'\n    ,MAXERRORS = 0\n    ,IDENTITY_INSERT = 'OFF'\n)",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "dsqlpoolKyn001494DevEtlHubEUS001",
						"poolName": "dsqlpoolKyn001494DevEtlHubEUS001"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 8')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/data/customer/customer_dimension1/part-00000-75122f32-1c34-4457-a5e8-c099f452d769-c000.snappy.parquet',\n        FORMAT = 'PARQUET'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 9')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) [SIEBEL_SALES_STAGE_CODE]\n,[SIEBEL_SALES_STAGE_NAME]\n,[SSM_STEP_NO]\n,[SSM_STEP_NAME]\n FROM [etlhubconfirmed].[dbo].[dht_sell_cycle]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "etlhubconfirmed",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pgmp_sample_sql')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE TABLE ACCTRPTS_DIM (\n    ACCTRPTS_DIM_UID as BIGINT,\n    REMARKS as CHAR(255)\n);\n\nSELECT name, is_cdc_enabled FROM sys.databases;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/30thMay_deltaLake_Customer_SCD_Type2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Siva"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "82d36037-50df-42bf-a42d-a3a840d768cf"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"SCD Type2 using adls as source and delta lake as target"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#import necessary python libraries\n",
							"\n",
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"\n",
							"#Read data from adls csv file extracted from source at https://adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/data/customer/\n",
							"\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'customer' # fill in your container name \n",
							"relative_path = '' # fill in your relative folder path \n",
							"\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path = adls_path + 'customer_data.csv' \n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\n",
							"\n",
							"natural_key=\"CUSTOMER_NO\"\n",
							"#columns1 = [\"FINANCIAL_COUNTRY_CD\",\"CUSTOMER_DESC\",\"GBG_ID\"]\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"    #print (col_list)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\n",
							"\n",
							"#Create a deltalake table with necessary columns\n",
							"\n",
							"#incrementalData_DF2.show()\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT tgt.* from etlhubConfirmed.customer_dimension tgt WHERE CURRENT_IND='Y'\")\n",
							"#existingDF.createOrReplaceTempView('existingDF')\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX(CUSTOMER_KEY) existing_MAX_KEY from etlhubConfirmed.customer_dimension tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"#existingDataDF1.printSchema()\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_NO, \"fullouter\") \n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry= \"\"\"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"AND CURRENT_IND='Y'\n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND B.REC_CHECKSUM <> A.column_hash\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry1= \"\"\"\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"AND CURRENT_IND='Y'\n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND B.REC_CHECKSUM <> A.column_hash\n",
							";\n",
							"\"\"\"\n",
							"#df3=spark.sql(qry1).count()\n",
							"\n",
							"#a=spark.sql(qry)\n",
							"\n",
							"#print( df3 || ' rows affected ')\n",
							"\n",
							"#a.num_affected_rows\n",
							"#print(numOutputRows)\n",
							"\n",
							"\n",
							"#deltaTable1 = DeltaTable.forPath(spark, 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2')\n",
							"\n",
							"#deltaTable = DeltaTable.forName(spark, 'etlhubConfirmed.customer_dimension')\n",
							"\n",
							"\n",
							"#fullHistoryDF = deltaTable.history()    # get the full history of the table\n",
							"\n",
							"#lastOperationDF = deltaTable.history(1) # get the last operation\n",
							"\n",
							"#print(lastOperationDF.operationMetrics)\n",
							"\n",
							"#lastOperationDF.show()\n",
							"\n",
							"#fullHistoryDF.show()\n",
							"\n",
							"#print(num_affected_rows)\n",
							"\n",
							"#print(num_inserted_rows)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 78
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"/*\n",
							"\n",
							"select * from incrementalData_DF2;\n",
							"\n",
							"select * from existingDataDF1;\n",
							"\n",
							"select * from fullJoin;\n",
							"*/\n",
							"\n",
							"--select * from incrementalData_DF2;\n",
							"\n",
							"--select * from etlhubconfirmed.customer_dimension;\n",
							"\n",
							"SELECT 'All Rows' as Title, a.* FROM fullJoin a;\n",
							"\n",
							"\n",
							"--No change records, ignore --7ROWS\n",
							"select 'No Change Rows' as Title, a.*  from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.customer_dimension B\n",
							"WHERE A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert2' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert2' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--select * from etlhubConfirmed.customer_dimension where rec_checksum='c475b27f1b384e1d2289948edad59d84'\n",
							"/*\n",
							"\n",
							"SELECT * FROM etlhubconfirmed.customer_dimension;\n",
							"\n",
							"UPDATE etlhubConfirmed.customer_dimension\n",
							"SET GBG_ID='GB302S66'\n",
							"    ,REC_CHECKSUM='c475b27f1b384e1d2289948edad59d86'\n",
							"    WHERE CUSTOMER_KEY=5\n",
							"    ;\n",
							"\n",
							"\n",
							"SELECT * FROM etlhubconfirmed.customer_dimension;    \n",
							"*/\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"\n",
							"--select * from fullJoin WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"/*\n",
							"UPDATE etlhubconfirmed.customer_dimension\n",
							"set CURRENT_IND='N'\n",
							"    ,REC_END_DT=current_timestamp \n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"WHERE (CUSTOMER_NO ) =  (select CUSTOMER_NO from fullJoin WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum) )\n",
							"AND CURRENT_IND='Y'\n",
							";\n",
							"\n",
							"*/\n",
							"\n",
							"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select existing_CUSTOMER_KEY,1+existing_VERSION as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'U' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.customer_dimension B\n",
							"WHERE A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert after insert' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert after insert' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"MERGE INTO etlhubconfirmed.customer_dimension A\n",
							"USING fullJoin B\n",
							"ON A.CUSTOMER_NO = B.CUSTOMER_NO\n",
							"AND LOWER(B.CUSTOMER_NO) = LOWER(B.existing_CUSTOMER_NO) and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert After Update' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert After Update' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"\n",
							"select 'Final rows in SCD Type2' as Title,a.* from etlhubconfirmed.customer_dimension a;\n",
							"    "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 79
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"select * from etlhubconfirmed.customer_dimension ;\n",
							"--CUSTOMER_NO='C0000216';\n",
							"--group by VERSION having count(*)>1;\n",
							"/*\n",
							"Update etlhubconfirmed.customer_dimension\n",
							"set CURRENT_IND='Y'\n",
							"    WHERE CURRENT_IND='N' and VERSION=2.0;\n",
							"--DELETE FROM etlhubconfirmed.customer_dimension WHERE customer_key in (3.0,4.0);\n",
							"\n",
							"\n",
							"Update etlhubconfirmed.customer_dimension\n",
							"set FINANCIAL_COUNTRY_CD='896'\n",
							"    ,REC_CHECKSUM='3145dfee7cc94e4483b4b0c7244a9949'\n",
							"    WHERE customer_key=7.0;\n",
							"Update etlhubconfirmed.customer_dimension\n",
							"set GBG_ID='GB302S60'\n",
							"    ,customer_name='WALPOLE CO-OPERATIVE BANK1'\n",
							"    ,REC_CHECKSUM='0520612ce8718d5df3b8bb4b165a6548'\n",
							"    WHERE customer_key=8.0;\n",
							"UPDATE etlhubconfirmed.customer_dimension\n",
							"SET CURRENT_IND='N'\n",
							"    WHERE CUSTOMER_KEY=11.0;\n",
							"*/\n",
							"--select * from etlhubconfirmed.customer_dimension;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 90
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"\n",
							"select * from etlhubconfirmed.customer_dimension;\n",
							"\n",
							"\n",
							"\n",
							"select * from fullJoin;\n",
							"\n",
							"--No change records, ignore\n",
							"select * from fullJoin WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							"\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"select * from fullJoin WHERE WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--Soft deletes or rows no longer active in source\n",
							"select * from fullJoin WHERE WHERE nk_hash is null;\n",
							"\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1 \n",
							"select * from fullJoin where existing_existing_nk_hash is null;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"select customer_key,count(*) from etlhubconfirmed.customer_dimension where current_ind='Y' group by customer_key having count(*)>1;\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"etlhubConfirmed.customer_dimension.toDF('abc')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"UPDATE etlhubconfirmed.customer_dimension\n",
							"SET fiNANCIAL_COUNTRY_CD='907'\n",
							"    WHERE CURRENT_IND='Y' AND CUSTOMER_NO='0074657';\n",
							"select * from etlhubconfirmed.customer_dimension;\n",
							"\n",
							"DELETe from etlhubconfirmed.customer_dimension where fiNANCIAL_COUNTRY_CD='905';\n",
							"select * from etlhubconfirmed.customer_dimension;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"MERGE INTO default.people10m\n",
							"USING default.people10m_upload\n",
							"ON default.people10m.id = default.people10m_upload.id\n",
							"WHEN MATCHED THEN UPDATE SET *\n",
							"WHEN NOT MATCHED THEN INSERT *"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create a deltalake table with necessary columns\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--CREATE DATABASE etlhubConfirmed;\n",
							"\n",
							"--drop table etlhubConfirmed.customer_dimension;\n",
							"\n",
							"-- Create Delta Lake table, define schema and location\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.customer_dimension (\n",
							"    CUSTOMER_KEY INT NOT NULL,\n",
							"\tVERSION INT ,\n",
							"\tCUSTOMER_NO STRING ,\n",
							"\tFINANCIAL_COUNTRY_CD varchar(10) ,\n",
							"\tGBG_ID varchar(10)  ,\n",
							"\tCUSTOMER_NAME varchar(30)  ,\n",
							"\tCURRENT_IND varchar(1)  ,\n",
							"\tEXTRACT_DT TIMESTAMP ,\n",
							"\tREC_START_DT TIMESTAMP ,\n",
							"\tREC_END_DT TIMESTAMP ,\n",
							"\tSOURCE_SYSTEM varchar(50)  ,\n",
							"\tREC_CHECKSUM varchar(32)  ,\n",
							"\tREC_STATUS varchar(1)  ,\n",
							"\tIMG_LST_UPD_DT TIMESTAMP NOT NULL,\n",
							"\tIMG_CREATED_DT TIMESTAMP NOT NULL,\n",
							"\tDATA_IND varchar(10)  ,\n",
							"\tACTIVE_IN_SOURCE_IND char(1)  \n",
							")\n",
							"USING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"-- specify data lake folder location\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer'\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"source_data = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"source_format = 'csv'\n",
							"\n",
							"spark.sql(\"COPY INTO \" + table_name + \\\n",
							"  \" FROM '\" + source_data + \"'\" + \\\n",
							"  \" FILEFORMAT = \" + source_format + ';'\n",
							")\n",
							"\n",
							"customer_dim_data = spark.sql(\"SELECT * FROM \" + table_name)\n",
							"\n",
							"display(customer_dim_data)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"read_format = 'csv'\n",
							"write_format = 'delta'\n",
							"load_path = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"save_path = 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2'\n",
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"\n",
							"\n",
							"account_name1 = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name1 = 'customer' # fill in your container name \n",
							"relative_path1 = '' # fill in your relative folder path \n",
							"\n",
							"adls_path1 = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name1, account_name1, relative_path1) \n",
							"#print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path1 = adls_path + 'DHT_CUSTOMER_202205191833.csv' \n",
							"custData_DF1 = spark.read.csv(csv_path1, header = 'true')\n",
							"\n",
							"#custData_DF1.show()\n",
							"\n",
							"# Write the data to its target.\n",
							"\n",
							"custData_DF1.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .save(save_path)\n",
							"# Create the table.\n",
							"#spark.sql(\"DROP TABLE \" + table_name)\n",
							"spark.sql(\"CREATE TABLE IF NOT EXISTS \" + table_name + \" USING DELTA LOCATION '\" + save_path + \"'\" )\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--drop table etlhubConfirmed.customer_dimension;\n",
							"select * from etlhubConfirmed.customer_dimension"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"end"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"source_data = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"source_format = 'CSV'\n",
							"\n",
							"spark.sql(\"DROP TABLE IF EXISTS \" + table_name)\n",
							"\n",
							"spark.sql(\"CREATE TABLE \" + table_name + \" (\" \\\n",
							"  \"loan_id BIGINT, \" + \\\n",
							"  \"funded_amnt INT, \" + \\\n",
							"  \"paid_amnt DOUBLE, \" + \\\n",
							"  \"addr_state STRING)\"\n",
							")\n",
							"\n",
							"spark.sql(\"COPY INTO \" + table_name + \\\n",
							"  \" FROM '\" + source_data + \"'\" + \\\n",
							"  \" FILEFORMAT = \" + source_format\n",
							")\n",
							"\n",
							"loan_risks_upload_data = spark.sql(\"SELECT * FROM \" + table_name)\n",
							"\n",
							"display(loan_risks_upload_data)\n",
							"Load data to datalake table"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"delta_table_path = \"abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer\" \n",
							"data = spark.range(5,10) \n",
							"data.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
							"\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"old code\n",
							"\n",
							"\n",
							"\n",
							"# Create table in the metastore\n",
							"\n",
							"DeltaTable.createIfNotExists(spark) \\\n",
							"  .tableName(\"default.customer_dimension\") \\\n",
							"  .addColumn(\"CUSTOMER_KEY\", \"INT\") \\\n",
							"  .addColumn(\"VERSION\", \"INT\") \\\n",
							"  .addColumn(\"CUSTOMER_NO\", \"STRING\") \\\n",
							"  .addColumn(\"FINANCIAL_COUNTRY_CD\")\\\n",
							"  .addColumn(\"GBG_ID\", \"STRING\") \\\n",
							"  .addColumn(\"CUSTOMER_NAME\", \"STRING\") \\\n",
							"  .addColumn(\"CURRENT_IND\", \"STRING\") \\\n",
							"  .addColumn(\"EXTRACT_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"REC_START_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"REC_END_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"SOURCE_SYSTEM\", \"STRING\") \\\n",
							"  .addColumn(\"REC_STATUS\", \"STRING\") \\\n",
							"  .addColumn(\"IMG_LST_UPD_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"IMG_CREATED_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"DATA_IND\", \"STRING\") \\\n",
							"  .addColumn(\"ACTIVE_IN_SOURCE_IND\", \"STRING\") \\\n",
							"  .execute()\n",
							"\n",
							"######################\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"#jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;database=dsqlpoolKyn001494DevEtlHubEUS001;user=undefined@asa-kyn-001494-dev-eus-001;password={your_password_here};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\n",
							"existingDataDF = spark.read.format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"    .option(\"query\", \"SELECT MAX(CUSTOMER_KEY) OVER (ORDER BY CUSTOMER_KEY  ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) AS MAX_KEY, LOWER(CONVERT(VARCHAR(32),HashBytes('MD5', CUSTOMER_NO),2)) as existing_nk_hash,tgt.* FROM DBXDH.DHT_CUSTOMER as tgt WHERE CURRENT_IND='Y'\") \\\n",
							"    .option(\"user\", \"sqladminuser\") \\\n",
							"    .option(\"password\", \"try2find$5\") \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .load()\n",
							"#existingDataDF1 = existingDataDF.select([F.col(c).alias(\"`\"'existing_'+c+\"`\") for c in existingDataDF.columns])\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"#existingDataDF1.printSchema()\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_KEY, \"fullouter\") \n",
							"fullJoin1.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"#Insert for New rows\n",
							"\n",
							"fullJoin2=sqlContext.sql(\"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \\\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null\")\n",
							"fullJoin2.createOrReplaceTempView('fullJoin2')\n",
							"\n",
							"#insert new rows into database\n",
							"\n",
							"fullJoin2.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()\n",
							"fullJoin2.show()\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--select * from existingDataDF\n",
							"\n",
							"select * from fullJoin2\n",
							"\n",
							"#insert new rows into database\n",
							"/*\n",
							"fullJoin2.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()\n",
							"        */"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--ALL\n",
							"SELECT * FROM FULLJOIN;\n",
							"\n",
							"--No change records, ignore\n",
							"select * from fullJoin WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							"\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"select * from fullJoin WHERE WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--Soft deletes or rows no longer active in source\n",
							"select * from fullJoin WHERE WHERE nk_hash is null;\n",
							"\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1 \n",
							"select * from fullJoin where existing_existing_nk_hash is null;\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--INSERTS OR NEW ROWS\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION ,\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT,\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM,\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND,\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#test referance\n",
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"query\", \"INSERT INTO DBXDH.DHT_CUSTOMER1 \\\n",
							"        select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION , \\\n",
							"        CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"        CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"        'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"        'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"        from fullJoin A WHERE existing_existing_nk_hash is null\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"overwrite\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"df.write.mode(\"overwrite\") \\\n",
							"    .format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://localhost:1433;databaseName={database};\") \\\n",
							"    .option(\"dbtable\", table) \\\n",
							"    .option(\"user\", user) \\\n",
							"    .option(\"password\", password) \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_table_scripts')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ad8ff950-e363-42ca-8d9a-22667831768b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.opportunity_business_partner_daily_ft (\r\n",
							"    BUSINESS_PARTNER_FACT_KEY INT NOT NULL,\t\r\n",
							"    BUSINESS_PARTNER_FACT_VERSION INT ,\t\r\n",
							"    OPPORTUNITY_KEY INT,\r\n",
							"    BUSINESS_PARTNER_KEY INT,\r\n",
							"\tINFLUENCER_ROLE_CODE varchar(10) ,\r\n",
							"\tCURRENT_IND varchar(1)  ,\r\n",
							"\tEXTRACT_DT TIMESTAMP ,\r\n",
							"\tREC_START_DT TIMESTAMP ,\r\n",
							"\tREC_END_DT TIMESTAMP ,\r\n",
							"\tSOURCE_SYSTEM varchar(50)  ,\r\n",
							"\tREC_CHECKSUM varchar(32)  ,\r\n",
							"\tREC_STATUS varchar(1)  ,\r\n",
							"\tIMG_LST_UPD_DT TIMESTAMP NOT NULL,\r\n",
							"\tIMG_CREATED_DT TIMESTAMP NOT NULL,\r\n",
							"\tDATA_IND varchar(10)  ,\r\n",
							"\tACTIVE_IN_SOURCE_IND char(1)  \r\n",
							")\r\n",
							"USING DELTA\r\n",
							"PARTITIONED BY (CURRENT_IND)\r\n",
							"-- specify data lake folder location\r\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/BUSINESS_PARTNER_DAILY_FT'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.BUSINESS_PARTNER  (\r\n",
							"BUSINESS_PARTNER_KEY INT NOT NULL , \r\n",
							"BUSINESS_PARTNER_VERSION INT NOT NULL , \r\n",
							"BUSINESS_PARTNER_ID VARCHAR(30), \r\n",
							"BUS_PARTNER_NM VARCHAR(50), \r\n",
							"AGR_BUS_PARTNER_ID VARCHAR(30), \r\n",
							"AGR_BUS_PARTNER_NM VARCHAR(50), \r\n",
							"CURRENT_IND VARCHAR(1) NOT NULL , \r\n",
							"EXTRACT_DT TIMESTAMP, \r\n",
							"REC_START_DT TIMESTAMP, \r\n",
							"REC_END_DT TIMESTAMP, \r\n",
							"SOURCE_SYSTEM VARCHAR(50), \r\n",
							"REC_CHECKSUM VARCHAR(32), \r\n",
							"REC_STATUS VARCHAR(1), \r\n",
							"IMG_LST_UPD_DT TIMESTAMP , \r\n",
							"IMG_CREATED_DT TIMESTAMP )  \r\n",
							"USING DELTA\r\n",
							"PARTITIONED BY (CURRENT_IND)\r\n",
							"-- specify data lake folder location\r\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/Business_Partner'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"DELETE etlhubConfirmed.BUSINESS_PARTNER\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.opportunity(\r\n",
							"OPPORTUNITY_KEY INTEGER NOT NULL , \r\n",
							"OPPORTUNITY_VERSION INT NOT NULL, \r\n",
							"OPPORTUNITY_NUM varchar(50), \r\n",
							"CURRENT_IND varchar(1), \r\n",
							"EXTRACT_DT TIMESTAMP, \r\n",
							"REC_START_DT TIMESTAMP, \r\n",
							"REC_END_DT TIMESTAMP, \r\n",
							"SOURCE_SYSTEM varchar(50), \r\n",
							"REC_CHECKSUM varchar(32), \r\n",
							"REC_STATUS varchar(1), \r\n",
							"IMG_LST_UPD_DT TIMESTAMP, \r\n",
							"IMG_CREATED_DT TIMESTAMP, \r\n",
							"ISA_CODE varchar(10), \r\n",
							"ARCH_REASON_CD varchar(20), \r\n",
							"BUSINESS_TRANSACTION_TYPE varchar(1), \r\n",
							"CLIENT_REPRESENTATIVE_NAME varchar(100), \r\n",
							"COMPETITOR_LIST varchar(100), \r\n",
							"IDENTIFIER_USER_NAME varchar(100), \r\n",
							"OPPORTUNITY_NAME varchar(100), \r\n",
							"OPPORTUNITY_SOURCE_CD varchar(20), \r\n",
							"SBS_SOL_VALID_IND varchar(1) , \r\n",
							"OPPORTUNITY_IDENTIFIER varchar(100) , \r\n",
							"ROGUE_IND varchar(1), \r\n",
							"REASON_TO_ACT varchar(255) , \r\n",
							"SOLUTION_CATG varchar(1024) , \r\n",
							"BRAND_SPONSOR_LIST varchar(500) , \r\n",
							"GBS_GEOGRAPHY_NAME varchar(60), \r\n",
							"GBS_BUSINESS_UNIT_GROUP_NAME varchar(60) , \r\n",
							"GBS_BUSINESS_UNIT_NAME varchar(60) , \r\n",
							"TAG_LIST varchar(1000) , \r\n",
							"OPPORTUNITY_LEGACY_NO varchar(100) )\r\n",
							"USING DELTA\r\n",
							"PARTITIONED BY (CURRENT_IND)\r\n",
							"-- specify data lake folder location\r\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/OPPORTUNITY'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"ALTER TABLE etlhubConfirmed.opportunity ADD COLUMN ACTIVE_IN_SOURCE_IND char(1)\r\n",
							"\t--ACTIVE_IN_SOURCE_IND char(1)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DHTR_COUNTRY_ddl')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/CreateTableScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPool001494New",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "472g",
					"driverCores": 80,
					"executorMemory": "472g",
					"executorCores": 80,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a254821e-ff53-4296-b4db-1b077b1a747f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPool001494New",
						"name": "sPool001494New",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPool001494New",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 80,
						"memory": 472,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--DHTR_COUNTRY\n",
							"\n",
							"CREATE TABLE IF NOT EXISTS etlhubconfirmed.dhtr_country  (\n",
							"CODE VARCHAR(10),\n",
							"LONGDESCRIPTION VARCHAR(250),\n",
							"MEDIUMDESCRIPTION VARCHAR(250),\n",
							"SHORTDESCRIPTION VARCHAR(50),\n",
							"COMMENTS VARCHAR(250),\n",
							"RECORDSTATUS VARCHAR(10),\n",
							"ROWCREATETS TIMESTAMP,\n",
							"ROWUPDATETS TIMESTAMP,\n",
							"COUNTRY_ALPHA3_COMPLIANCE_CD VARCHAR(10),\n",
							"COUNTRY_NUM3_COMPLIANCE_CD VARCHAR(10)\n",
							")\n",
							"\n",
							"USING DELTA\n",
							"LOCATION 'abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/reference/DHTR_country'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from etlhubconfirmed.dhtr_country"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DROP TABLE etlhubconfirmed.dhtr_country"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DELETE FROM etlhubconfirmed.dhtr_country"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DHTR_SPACE_CLASS_ddl')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/CreateTableScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPool001494New",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "472g",
					"driverCores": 80,
					"executorMemory": "472g",
					"executorCores": 80,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ba4ef7a8-2400-4a2f-832a-f030cf569dfd"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPool001494New",
						"name": "sPool001494New",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPool001494New",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 80,
						"memory": 472,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--DHTR_SPACE_CLASS\n",
							"\n",
							"CREATE TABLE IF NOT EXISTS etlhubconfirmed.dhtr_space_class  (\n",
							"GROUP VARCHAR(10),\n",
							"OCCUPANCY_TYPE VARCHAR(20),\n",
							"SPACE_CLASS_CODE1 VARCHAR(20),\n",
							"SPACE_CLASS_CODE2 VARCHAR(50),\n",
							"SPACE_CLASS VARCHAR(50),\n",
							"SPACE_CLASS_DESCR VARCHAR(100)\n",
							")\n",
							"\n",
							"USING DELTA\n",
							"LOCATION 'abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/reference/DHTR_SPACE_CLASS'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from etlhubconfirmed.dhtr_space_class"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DROP TABLE etlhubconfirmed.dhtr_space_class"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DELETE FROM etlhubconfirmed.dhtr_country"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DHTR_SPACE_FLOOR_LEVEL_ddl')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/CreateTableScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPool001494New",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "472g",
					"driverCores": 80,
					"executorMemory": "472g",
					"executorCores": 80,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a995b56f-c3ff-4acf-934c-fe0f66ed49c9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPool001494New",
						"name": "sPool001494New",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPool001494New",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 80,
						"memory": 472,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--DHTR_SPACE_FLOOR_LEVEL DEFINITION\n",
							"\n",
							"CREATE TABLE IF NOT EXISTS etlhubconfirmed.DHTR_SPACE_FLOOR_LEVEL  (\n",
							"FLOOR_ID VARCHAR(9),\n",
							"FLOOR_DESCR VARCHAR(150),\n",
							"USAGE_NOTES VARCHAR(150) )\n",
							"\n",
							"USING DELTA\n",
							"\n",
							"LOCATION 'abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/reference/DHTR_SPACE_FLOOR_LEVEL'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\r\n",
							"select * from etlhubconfirmed.DHTR_SPACE_FLOOR_LEVEL"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DROP TABLE etlhubconfirmed.DHTR_SPACE_FLOOR_LEVEL"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DELETE FROM etlhubconfirmed.DHTR_SPACE_FLOOR_LEVEL"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DHTR_STTE_PROV_ddl')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/CreateTableScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPool001494New",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "472g",
					"driverCores": 80,
					"executorMemory": "472g",
					"executorCores": 80,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "725e78ca-ccb3-41db-ae51-866cefe5120d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPool001494New",
						"name": "sPool001494New",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPool001494New",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 80,
						"memory": 472,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--DHTR_STTE_PROV\n",
							"\n",
							"CREATE TABLE IF NOT EXISTS etlhubconfirmed.dhtr_stte_prov  (\n",
							"CNTRY_CD VARCHAR(10),\n",
							"STTE_PROV_CD VARCHAR(10),\n",
							"STTE_PROV_DESC VARCHAR(100),\n",
							"ACTIVE_DATE DATE,\n",
							"STATUS_CD VARCHAR(10)\n",
							")\n",
							"\n",
							"USING DELTA\n",
							"LOCATION 'abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/reference/DHTR_STTE_PROV'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from etlhubconfirmed.dhtr_stte_prov"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DROP TABLE etlhubconfirmed.dhtr_stte_prov"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DELETE FROM etlhubconfirmed.dhtr_country"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DHT_BUILDING')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/CreateTableScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPool001494New",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "472g",
					"driverCores": 80,
					"executorMemory": "472g",
					"executorCores": 80,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "85c6c5ef-1a21-4f9e-bbb6-82f1c3a01a2d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPool001494New",
						"name": "sPool001494New",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPool001494New",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 80,
						"memory": 472,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"-- DBXDH.DHT_BUILDING definition\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.DHT_BUILDING  (\t\n",
							"\tBUILDING_KEY INTEGER NOT NULL, -- Surrogate Key one to one relation with natural Key\n",
							"\tVERSION INT NOT NULL, -- Inremental version number. Record with max version number is the latest/active row  \n",
							"\tBUSINESS_ID STRING,\n",
							"\tCAMPUS_ID STRING NOT NULL,\n",
							"\tBUILDING_ID STRING NOT NULL,\n",
							"\tBUILDING_NAME STRING,\n",
							"\tBUILDING_STATUS STRING NOT NULL,\n",
							"\t--BUILDING_OWNERSHIP_ID STRING, Not needed\n",
							"\tBUILDING_OWNERSHIP_NAME STRING,\n",
							"\tBUILDING_OWNERSHIP_DESCR STRING,\n",
							"\t--PRIMARY_BUILDING_USE_ID STRING, Not needed\n",
							"\tPRIMARY_BUILDING_USE_NAME STRING,\n",
							"\tPRIMARY_BUILDING_USE_DESCR STRING,\n",
							"\t--SECONDARY_BUILDING_USE_ID STRING, Not needed\n",
							"\tSECONDARY_BUILDING_USE_NAME STRING,\n",
							"\tSECONDARY_BUILDING_USE_DESCR STRING,\n",
							"\tRENTABLE_AREA INTEGER,\n",
							"\tBUILDING_ACTIVATION_YEAR INTEGER,\n",
							"\tBUILDING_INACTIVATION_YEAR INTEGER,\n",
							"\t--PROCESSED_DT TIMESTAMP NOT NULL, Not needed\n",
							"\tCURRENT_IND\tSTRING -- Y for active row and N for all rows expired\n",
							",\tEXTRACT_DT TIMESTAMP -- Load Cycle date, contains same value for all the records inserted in a single batch\n",
							",\tREC_START_DT TIMESTAMP --SCD TYpe 2 start date\n",
							",\tREC_END_DT TIMESTAMP -- SCD Type 2 End date, will have high date (9999-12-31) for active dimensional row\n",
							",\tSOURCE_SYSTEM STRING -- Name of the source system\n",
							",\tREC_CHECKSUM STRING -- Hash value of all dimensional columns. Used to identify changed records \n",
							"--,\tREC_STATUS STRING\t--I or U depending on the DML operation\n",
							",\tIMG_LST_UPD_DT TIMESTAMP --Timestamp of when the record got updated in table\n",
							",\tIMG_CREATED_DT TIMESTAMP --Timestamp of when the record got inserted in table\n",
							")\n",
							"USING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"LOCATION 'abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/dimensions/DHT_BUILDING'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DROP TABLE etlhubConfirmed.DHT_BUILDING"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DELETE FROM etlhubConfirmed.DHT_BUILDING"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"-- RESOWW.BUILDING definition\n",
							"\n",
							"-- Drop table\n",
							"\n",
							"-- DROP TABLE RESOWW.BUILDING;\n",
							"\n",
							"CREATE TABLE RESOWW.BUILDING (\n",
							"\tBUILDING_KEY INTEGER NOT NULL,\n",
							"\tCAMPUS_ID CHAR(8) NOT NULL,\n",
							"\tBUILDING_ID CHAR(6) NOT NULL,\n",
							"\tBUILDING_NAME VARCHAR(150),\n",
							"\tBUILDING_STATUS VARCHAR(32) NOT NULL,\n",
							"\tBUILDING_OWNERSHIP_ID CHAR(2),\n",
							"\tBUILDING_OWNERSHIP_NAME VARCHAR(32),\n",
							"\tBUILDING_OWNERSHIP_DESCR VARCHAR(256),\n",
							"\tPRIMARY_BUILDING_USE_ID CHAR(2),\n",
							"\tPRIMARY_BUILDING_USE_NAME VARCHAR(64),\n",
							"\tPRIMARY_BUILDING_USE_DESCR VARCHAR(512),\n",
							"\tSECONDARY_BUILDING_USE_ID CHAR(3),\n",
							"\tSECONDARY_BUILDING_USE_NAME VARCHAR(64),\n",
							"\tSECONDARY_BUILDING_USE_DESCR VARCHAR(512),\n",
							"\tRENTABLE_AREA INTEGER,\n",
							"\tBUILDING_ACTIVATION_YEAR SMALLINT,\n",
							"\tBUILDING_INACTIVATION_YEAR SMALLINT,\n",
							"\tPROCESSED_DT TIMESTAMP NOT NULL,\n",
							"\tCONSTRAINT SQL200724134512910 PRIMARY KEY (BUILDING_KEY)\n",
							");\n",
							"CREATE UNIQUE INDEX SQL200724134512900 ON RESOWW.BUILDING (BUILDING_KEY);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DHT_CAMPUS')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/CreateTableScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e3eba01c-d437-4f18-9dda-1d51b5e03665"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"-- DBXDH.DHT_CAMPUS definition\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.DHT_CAMPUS  (\t\n",
							"\tCAMPUS_KEY INTEGER NOT NULL,\n",
							"\tCAMPUS_VERSION\tINT NOT NULL,\n",
							"\tCAMPUS_ID CHAR(8) NOT NULL,\n",
							"\tCAMPUS_NAME VARCHAR(50) NOT NULL,\n",
							"\tCAMPUS_STATUS VARCHAR(32) NOT NULL,\n",
							"\tSITE_ID CHAR(8) NOT NULL,\n",
							"\tCAMPUS_GROUP CHAR(11),\n",
							"\tPHYSICAL_GEO CHAR(5),\n",
							"\tWORLD_REGION_CODE CHAR(2),\n",
							"\tWORLD_REGION_NAME VARCHAR(60),\n",
							"\tMARKET_TEAM_REGION_CODE CHAR(3),\n",
							"\tMARKET_TEAM_REGION_NAME VARCHAR(60),\n",
							"\tWORK_LOCATION_CODE CHAR(3),\n",
							"\tADDRESS VARCHAR(200),\n",
							"\tCITY VARCHAR(53),\n",
							"\tSTATE_PROVINCE_ID CHAR(3),\n",
							"\tPOSTAL_CODE VARCHAR(15),\n",
							"\tCOUNTRY_CODE CHAR(2),\n",
							"\tLATITUDE DECIMAL(11,8),\n",
							"\tLONGITUDE DECIMAL(11,8),\n",
							"\tUTC_OFFSET CHAR(9),\n",
							"\tICU_TIME_ZONE VARCHAR(50),\n",
							"\tPEOPLE_HOUSED_FLAG CHAR(1),\n",
							"\tREMOTE_SUPPORT_FLAG CHAR(1),\n",
							"\tPRIMARY_CAMPUS_USE_ID CHAR(3),\n",
							"\tPRIMARY_CAMPUS_USE_NAME VARCHAR(32),\n",
							"\tPRIMARY_CAMPUS_USE_DESCR VARCHAR(128),\n",
							"\tCAMPUS_OWNERSHIP VARCHAR(40),\n",
							"\tCAMPUS_ACTIVATION_YEAR SMALLINT,\n",
							"\tCAMPUS_INACTIVATION_YEAR SMALLINT,\n",
							"\tSOURCE_SYS_MODIFIED_DT TIMESTAMP,\n",
							"\tPROCESSED_DT TIMESTAMP NOT NULL,\n",
							"\tCURRENT_IND\tSTRING\n",
							",\tEXTRACT_DT TIMESTAMP\n",
							",\tREC_START_DT TIMESTAMP\n",
							",\tREC_END_DT TIMESTAMP\n",
							",\tSOURCE_SYSTEM STRING\n",
							",\tREC_CHECKSUM STRING\n",
							",\tREC_STATUS STRING\t\n",
							",\tIMG_LST_UPD_DT TIMESTAMP\n",
							",\tIMG_CREATED_DT TIMESTAMP\n",
							",   ACTIVE_IN_SOURCE_IND STRING\n",
							")\n",
							"USING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"LOCATION 'abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/dimensions/DHT_CAMPUS'"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DROP TABLE etlhubConfirmed.DHT_CAMPUS"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"-- RESOWW.CAMPUS definition\n",
							"\n",
							"-- Drop table\n",
							"\n",
							"-- DROP TABLE RESOWW.CAMPUS;\n",
							"\n",
							"CREATE TABLE RESOWW.CAMPUS (\n",
							"\tCAMPUS_KEY INTEGER NOT NULL,\n",
							"\tCAMPUS_ID CHAR(8) NOT NULL,\n",
							"\tCAMPUS_NAME VARCHAR(50) NOT NULL,\n",
							"\tCAMPUS_STATUS VARCHAR(32) NOT NULL,\n",
							"\tSITE_ID CHAR(8) NOT NULL,\n",
							"\tCAMPUS_GROUP CHAR(11),\n",
							"\tPHYSICAL_GEO CHAR(5),\n",
							"\tWORLD_REGION_CODE CHAR(2),\n",
							"\tWORLD_REGION_NAME VARCHAR(60),\n",
							"\tMARKET_TEAM_REGION_CODE CHAR(3),\n",
							"\tMARKET_TEAM_REGION_NAME VARCHAR(60),\n",
							"\tWORK_LOCATION_CODE CHAR(3),\n",
							"\tADDRESS VARCHAR(200),\n",
							"\tCITY VARCHAR(53),\n",
							"\tSTATE_PROVINCE_ID CHAR(3),\n",
							"\tPOSTAL_CODE VARCHAR(15),\n",
							"\tCOUNTRY_CODE CHAR(2),\n",
							"\tLATITUDE DECIMAL(11,8),\n",
							"\tLONGITUDE DECIMAL(11,8),\n",
							"\tUTC_OFFSET CHAR(9),\n",
							"\tICU_TIME_ZONE VARCHAR(50),\n",
							"\tPEOPLE_HOUSED_FLAG CHAR(1),\n",
							"\tREMOTE_SUPPORT_FLAG CHAR(1),\n",
							"\tPRIMARY_CAMPUS_USE_ID CHAR(3),\n",
							"\tPRIMARY_CAMPUS_USE_NAME VARCHAR(32),\n",
							"\tPRIMARY_CAMPUS_USE_DESCR VARCHAR(128),\n",
							"\tCAMPUS_OWNERSHIP VARCHAR(40),\n",
							"\tCAMPUS_ACTIVATION_YEAR SMALLINT,\n",
							"\tCAMPUS_INACTIVATION_YEAR SMALLINT,\n",
							"\tSOURCE_SYS_MODIFIED_DT TIMESTAMP,\n",
							"\tPROCESSED_DT TIMESTAMP NOT NULL,\n",
							"\tCONSTRAINT SQL200724134412910 PRIMARY KEY (CAMPUS_KEY)\n",
							");\n",
							"CREATE INDEX MARKET_TEAM_REGION_IDX ON RESOWW.CAMPUS (MARKET_TEAM_REGION_CODE);\n",
							"CREATE INDEX PHYSICAL_GEO_IDX ON RESOWW.CAMPUS (PHYSICAL_GEO);\n",
							"CREATE INDEX SITE_ID_IDX ON RESOWW.CAMPUS (SITE_ID);\n",
							"CREATE UNIQUE INDEX SQL200724134412900 ON RESOWW.CAMPUS (CAMPUS_KEY);\n",
							"CREATE UNIQUE INDEX SQL200724134412920 ON RESOWW.CAMPUS (CAMPUS_ID);\n",
							"CREATE INDEX WORLD_REGION_IDX ON RESOWW.CAMPUS (WORLD_REGION_CODE);"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DHT_CAMPUS_ddl')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/CreateTableScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPool001494New",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "472g",
					"driverCores": 80,
					"executorMemory": "472g",
					"executorCores": 80,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a78966f5-0ea8-4bd5-8e07-a62b0b708515"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPool001494New",
						"name": "sPool001494New",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPool001494New",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 80,
						"memory": 472,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"-- DBXDH.DHT_CAMPUS definition\r\n",
							"CREATE TABLE etlhubConfirmed.dht_campus (\t\r\n",
							"CAMPUS_KEY INTEGER NOT NULL,\r\n",
							"VERSION INTEGER NOT NULL ,\r\n",
							"GEOGRAPHY_ID VARCHAR(5),\r\n",
							"GEOGRAPHY_NAME VARCHAR(64),\r\n",
							"REGION_ID VARCHAR(8),\r\n",
							"REGION_NAME VARCHAR(64),\r\n",
							"SITE_ID VARCHAR(8),\r\n",
							"SITE_NAME VARCHAR(64),\r\n",
							"WORLD_REGION_CODE VARCHAR(2),\r\n",
							"WORLD_REGION_NAME VARCHAR(64),\r\n",
							"MARKET_TEAM_REGION_CODE VARCHAR(3),\r\n",
							"MARKET_TEAM_REGION_NAME VARCHAR(64),\r\n",
							"COUNTRY_CODE VARCHAR(2),\r\n",
							"COUNTRY_NAME VARCHAR(64),\r\n",
							"CAMPUS_ID VARCHAR(8),\r\n",
							"CAMPUS_NAME VARCHAR(64),\r\n",
							"CAMPUS_STATUS VARCHAR(32),\r\n",
							"WORK_LOCATION_CODE VARCHAR(3),\r\n",
							"ADDRESS VARCHAR(256),\r\n",
							"CITY VARCHAR(64),\r\n",
							"STATE_PROVINCE_ID VARCHAR(3),\r\n",
							"STATE_PROVINCE_NAME VARCHAR(64),\r\n",
							"POSTAL_CODE VARCHAR(16),\r\n",
							"LATITUDE DECIMAL(11,8),\r\n",
							"LONGITUDE DECIMAL(11,8),\r\n",
							"UTC_OFFSET VARCHAR(16),\r\n",
							"ICU_TIME_ZONE VARCHAR(64),\r\n",
							"PEOPLE_HOUSED_FLAG VARCHAR(1),\r\n",
							"REMOTE_SUPPORT_FLAG VARCHAR(1),\r\n",
							"PRIMARY_CAMPUS_USE_NAME VARCHAR(32),\r\n",
							"PRIMARY_CAMPUS_USE_DESCR VARCHAR(128),\r\n",
							"CAMPUS_OWNERSHIP VARCHAR(8),\r\n",
							"CAMPUS_ACTIVATION_YEAR INTEGER,\r\n",
							"CAMPUS_INACTIVATION_YEAR INTEGER,\r\n",
							"CURRENT_IND\tVARCHAR(1), -- Y for active row and N for all rows expired\r\n",
							"EXTRACT_DT TIMESTAMP, -- Load Cycle date, contains same value for all the records inserted in a single batch\r\n",
							"REC_START_DT TIMESTAMP, --SCD TYpe 2 start date\r\n",
							"REC_END_DT TIMESTAMP, -- SCD Type 2 End date, will have high date (9999-12-31) for active dimensional row\r\n",
							"SOURCE_SYSTEM VARCHAR(10), \r\n",
							"REC_CHECKSUM VARCHAR(32), \r\n",
							"IMG_LST_UPD_DT TIMESTAMP, \r\n",
							"IMG_CREATED_DT TIMESTAMP \r\n",
							")\r\n",
							"USING DELTA\r\n",
							"PARTITIONED BY (CURRENT_IND)\r\n",
							"-- specify data lake folder location\r\n",
							"LOCATION 'abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/dimensions/DHT_CAMPUS' "
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from etlhubConfirmed.dht_campus"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DROP TABLE etlhubConfirmed.DHT_CAMPUS"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DELETE FROM etlhubConfirmed.DHT_CAMPUS"
						],
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DHT_COMPANY_ddl')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/CreateTableScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "31f7ecee-c57d-493c-b532-8e5d43abbe81"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\r\n",
							"---- DHT_COMPANY Definition \r\n",
							"\t\t \r\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.DHT_COMPANY  (\r\n",
							"\t\t  COMPANY_KEY INTEGER NOT NULL , \r\n",
							"\t\t  COMPANY_VERSION INTEGER NOT NULL , \r\n",
							"\t\t  COMPANY_LEDGER_CODE VARCHAR(30) ,\r\n",
							"\t\t  COMPANY_COUNTRY_CODE VARCHAR(50) ,\r\n",
							"\t\t  COMPANY_COMPANY_CODE VARCHAR(50) ,\r\n",
							"\t\t  COMPANY_COMPANY_DESC VARCHAR(50) ,\r\n",
							"\t\t  COMPANY_GLBL_ULTMT_CMPNY_CD VARCHAR(8) ,\r\n",
							"\t\t  COMPANY_DVSTR_IND VARCHAR(1) ,\r\n",
							"\t\t  COMPANY_SAP_COMPANY_CD VARCHAR(4) ,\t\r\n",
							"\t\t  CURRENT_IND VARCHAR(1) , \r\n",
							"\t\t  EXTRACT_DT TIMESTAMP  , \r\n",
							"\t\t  REC_START_DT TIMESTAMP  , \r\n",
							"\t\t  REC_END_DT TIMESTAMP  , \r\n",
							"\t\t  SOURCE_SYSTEM VARCHAR(50)  , \r\n",
							"\t\t  REC_CHECKSUM VARCHAR(32)  , \r\n",
							"\t\t  REC_STATUS VARCHAR(1)  , \r\n",
							"\t\t  IMG_LST_UPD_DT TIMESTAMP  , \r\n",
							"\t\t  IMG_CREATED_DT TIMESTAMP  , \r\n",
							"\t\t  DATA_IND varchar(10)  ,\r\n",
							"\t      ACTIVE_IN_SOURCE_IND char(1) )   \r\n",
							"USING DELTA\r\n",
							"PARTITIONED BY (CURRENT_IND)\r\n",
							"-- specify data lake folder location\r\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/deltalake/dimensions/DHT_COMPANY_UPD'   "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\r\n",
							"DROP TABLE etlhubconfirmed.DHT_COMPANY"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\r\n",
							"select * FROM etlhubconfirmed.DHT_COMPANY"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DHT_Create_table_scripts')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/CreateTableScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "cee968a8-fac4-4e13-8a53-4956edf7f8f6"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"-- DBXDH.DHT_DEAL_DIM definition\r\n",
							"\r\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.DHT_DEAL_DIM1  (\r\n",
							"\t\t  DEAL_KEY INTEGER NOT NULL , \r\n",
							"\t\t  DEAL_VERSION_NUMBER INTEGER NOT NULL , \r\n",
							"\t\t  DEAL_IDENTIFIER VARCHAR(50) , \r\n",
							"\t\t  BID_SUPPORT_INTERNET_ID VARCHAR(100) , \r\n",
							"\t\t  BID_SUPPORT_NOTES_ID VARCHAR(100) , \r\n",
							"\t\t  BUSINESS_UNIT_ID VARCHAR(50) , \r\n",
							"\t\t  CLIENT_NAME VARCHAR(50) , \r\n",
							"\t\t  CLIENT_NUMBER VARCHAR(50) , \r\n",
							"\t\t  COMLEXITY_CODE VARCHAR(50) , \r\n",
							"\t\t  COMMENT_TEXT VARCHAR(2500) , \r\n",
							"\t\t  COMPLEXITY_COMMENT_TEXT VARCHAR(1000) , \r\n",
							"\t\t  COMPLEXITY_OVERRIDE_CODE VARCHAR(50) , \r\n",
							"\t\t  CONTRACT_TYPE_DESCRIPTION VARCHAR(300) , \r\n",
							"\t\t  ENGAGEMENT_SUPPORT_TRANSACTION_LINK_TEXT VARCHAR(100) , \r\n",
							"\t\t  ENGAGEMENT_SUPPORT_TRANSACTION_NUMBER VARCHAR(50) , \r\n",
							"\t\t  INDUSTRY_NAME VARCHAR(254) , \r\n",
							"\t\t  INITIAL_SIEBEL_INDICATOR VARCHAR(50) , \r\n",
							"\t\t  ISO_COUNTRY_ID VARCHAR(50) , \r\n",
							"\t\t  LEAD_ACCOUNT_PARTNER_BTT_CODE VARCHAR(50) , \r\n",
							"\t\t  LEAD_PARTNER_BTT_CODE VARCHAR(50) , \r\n",
							"\t\t  LEAD_PARTNER_NOTES_ID VARCHAR(100) , \r\n",
							"\t\t  OLD_OWNER_INTERNET_ID VARCHAR(100) , \r\n",
							"\t\t  OPPORTUNITY_INTERNET_ID VARCHAR(100) , \r\n",
							"\t\t  OPPORTUNITY_NAME VARCHAR(100) , \r\n",
							"\t\t  OPPORTUNITY_NUMBER VARCHAR(50) , \r\n",
							"\t\t  OPPORTUNITY_OWNER_NOTES_ID VARCHAR(100) , \r\n",
							"\t\t  PARENT_DEAL_ID VARCHAR(50) , \r\n",
							"\t\t  PROPOSAL_MANAGER_NOTES_ID VARCHAR(100) , \r\n",
							"\t\t  REASON_FOR_PRIOR_DEALS_BEING_MODIFIED_TEXT VARCHAR(255) , \r\n",
							"\t\t  REMOVED_INDICATOR VARCHAR(50) , \r\n",
							"\t\t  RISK_RATING_NAME VARCHAR(50) , \r\n",
							"\t\t  RISK_RATING_COMMENT_TEXT VARCHAR(1000) , \r\n",
							"\t\t  RISK_RATING_OVERRIDE_CODE VARCHAR(50) , \r\n",
							"\t\t  SALES_STAGE_CODE VARCHAR(50) , \r\n",
							"\t\t  SECONDARY_SERVICE_NAME VARCHAR(50) , \r\n",
							"\t\t  SECTOR_NAME VARCHAR(254) , \r\n",
							"\t\t  SERVICE_NAME VARCHAR(50) , \r\n",
							"\t\t  SIEBEL_INDICATOR VARCHAR(50) , \r\n",
							"\t\t  SOURCE_CODE VARCHAR(50) , \r\n",
							"\t\t  STATUS_CODE VARCHAR(50) , \r\n",
							"\t\t  CURRENT_IND VARCHAR(1) NOT NULL , \r\n",
							"\t\t  EXTRACT_DT TIMESTAMP , \r\n",
							"\t\t  IMG_CREATED_DT TIMESTAMP , \r\n",
							"\t\t  IMG_LST_UPD_DT TIMESTAMP , \r\n",
							"\t\t  REC_END_DT TIMESTAMP , \r\n",
							"\t\t  REC_START_DT TIMESTAMP , \r\n",
							"\t\t  REC_CHECKSUM VARCHAR(32) , \r\n",
							"\t\t  REC_STATUS VARCHAR(1) , \r\n",
							"\t\t  SOURCE_SYSTEM VARCHAR(50) , \r\n",
							"\t\t  RCA1_ANSWER VARCHAR(10) , \r\n",
							"\t\t  RCA1_COMMENT VARCHAR(1000) , \r\n",
							"\t\t  RCA2_ANSWER VARCHAR(10) , \r\n",
							"\t\t  RCA2_COMMENT VARCHAR(1000) , \r\n",
							"\t\t  RCA3_ANSWER VARCHAR(10) , \r\n",
							"\t\t  RCA3_COMMENT VARCHAR(1000) , \r\n",
							"\t\t  RCA4_ANSWER VARCHAR(10) , \r\n",
							"\t\t  RCA4_COMMENT VARCHAR(1000) , \r\n",
							"\t\t  RCA5_ANSWER VARCHAR(10) , \r\n",
							"\t\t  RCA5_COMMENT VARCHAR(1000) , \r\n",
							"\t\t  RCA6_ANSWER VARCHAR(10) , \r\n",
							"\t\t  RCA6_COMMENT VARCHAR(1000) , \r\n",
							"\t\t  RCA7_ANSWER VARCHAR(10) , \r\n",
							"\t\t  RCA7_COMMENT VARCHAR(1000) , \r\n",
							"\t\t  RCA8_ANSWER VARCHAR(10) , \r\n",
							"\t\t  RCA8_COMMENT VARCHAR(1000) , \r\n",
							"\t\t  APPROVER_COUNT VARCHAR(5) , \r\n",
							"\t\t  REVIEWER_COUNT VARCHAR(5) , \r\n",
							"\t\t  DEAL_TYPE VARCHAR(15) , \r\n",
							"\t\t  GEOGRAPHY_CODE VARCHAR(10) , \r\n",
							"\t\t  HYBRD_TYPE_A VARCHAR(9) , \r\n",
							"\t\t  HYBRD_TYPE_B VARCHAR(9) , \r\n",
							"\t\t  LEAD_LINE_OF_BUSINESS VARCHAR(3) , \r\n",
							"\t\t  CREDIT_RATING VARCHAR(5) , \r\n",
							"\t\t  DISCOUNT_DELEGATION_OFFERG_CLOUD_SERVICES_USD_AMOUNT VARCHAR(20) , \r\n",
							"\t\t  DISCOUNT_DELEGATION_OFFERG_INFRASTRUCTURE_SERVICES_USD_AMOUNT VARCHAR(20) , \r\n",
							"\t\t  DISCOUNT_DELEGATION_OFFERG_SECURITY_SERVICES_USD_AMOUNT VARCHAR(20) , \r\n",
							"\t\t  DISCOUNT_DELEGATION_OFFERG_WATSON_CLOUD_PLATFORM_SERVICES_USD_AMOUNT VARCHAR(20) , \r\n",
							"\t\t  RISK_CONSULTANT_NOTES_ID VARCHAR(60) , \r\n",
							"\t\t  SENIOR_SOLUTION_MANAGER_NOTES_ID VARCHAR(4000) , \r\n",
							"\t\t  SIH_GM_NOTES_ID VARCHAR(60) , \r\n",
							"\t\t  RULES_EFFECTIVE_DATE DATE , \r\n",
							"\t\t  DEAL_LEAD_NOTES_ID VARCHAR(60) , \r\n",
							"\t\t  CONTRACT_SIGNOFF_APPROVAL_STATUS_CODE VARCHAR(50) , \r\n",
							"\t\t  PAYBACK_TXT VARCHAR(20),\r\n",
							"          DATA_IND varchar(10)  ,\r\n",
							"\t      ACTIVE_IN_SOURCE_IND char(1) )   \r\n",
							"USING DELTA\r\n",
							"PARTITIONED BY (CURRENT_IND)\r\n",
							"\r\n",
							"-- specify data lake folder location\r\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/deltalake/dimensions/DHT_DEAL_DIM1'\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DHT_EMPLOYEE_ddl')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/CreateTableScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "55c24559-12d7-4daf-aa73-acde1c7f2fac"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"-- DBXDH.DHT_EMPLOYEE definition\n",
							"\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.DHT_EMPLOYEE  (\n",
							"\t\t  EMPLOYEE_KEY INTEGER NOT NULL , \n",
							"\t\t  EMPLOYEE_VERSION INTEGER NOT NULL , \n",
							"\t\t  EMPLOYEE_SERIAL_NUM VARCHAR(20)  , \n",
							"\t\t  EMPLOYEE_COUNTRY_CODE VARCHAR(10)  , \n",
							"\t\t  EMPLOYEE_COMPANY_CD VARCHAR(50)  , \n",
							"\t\t  EMPLOYEE_BUSINESS_CD VARCHAR(100)  , \n",
							"\t\t  NOTES_ID_RAW VARCHAR(100)  , \n",
							"\t\t  NOTES_ID VARCHAR(100)  , \n",
							"\t\t  CURRENT_IND VARCHAR(1) ,\n",
							"\t\t  EXTRACT_DT TIMESTAMP  , \n",
							"\t\t  REC_START_DT TIMESTAMP  , \n",
							"\t\t  REC_END_DT TIMESTAMP  , \n",
							"\t\t  SOURCE_SYSTEM VARCHAR(50)  , \n",
							"\t\t  REC_CHECKSUM VARCHAR(32)  , \n",
							"\t\t  REC_STATUS VARCHAR(1)  , \n",
							"\t\t  IMG_LST_UPD_DT TIMESTAMP ,\n",
							"\t\t  IMG_CREATED_DT TIMESTAMP ,\n",
							"\t\t  EMPLOYEE_INITIALS VARCHAR(50)  , \n",
							"\t\t  EMPLOYEE_LAST_NAME VARCHAR(100)  , \n",
							"\t\t  EMPLOYEE_DISCON_DT TIMESTAMP  , \n",
							"\t\t  EMPLOYEE_EFF_DT TIMESTAMP  , \n",
							"\t\t  EMPLOYEE_STATUS VARCHAR(10)  , \n",
							"\t\t  EMPLOYEE_LEVEL_CD VARCHAR(10)  , \n",
							"\t\t  EMPLOYEE_USER_ID VARCHAR(10)  , \n",
							"\t\t  MANAGER_IND VARCHAR(1)  , \n",
							"\t\t  HOME_NODE_ID VARCHAR(10)  , \n",
							"\t\t  HOME_USER_ID VARCHAR(10)  , \n",
							"\t\t  BURDEN_CD VARCHAR(1)  , \n",
							"\t\t  BURDEN_CD_UPD_IND VARCHAR(1)  , \n",
							"\t\t  FIN_ADMIN_CD VARCHAR(10)  , \n",
							"\t\t  LONGEVITY_CD VARCHAR(1)  , \n",
							"\t\t  GROUP_ID VARCHAR(10)  , \n",
							"\t\t  JOB_FAMILY_CD VARCHAR(10)  , \n",
							"\t\t  PROFESSION_CD VARCHAR(10)  , \n",
							"\t\t  EMF_SOURCE_CD VARCHAR(10)  , \n",
							"\t\t  LBR_RPT_IND VARCHAR(1)  , \n",
							"\t\t  INET_MAIL_ADDR VARCHAR(100)  , \n",
							"\t\t  TEAM_ID VARCHAR(10)  , \n",
							"\t\t  SITE_LOC_CD VARCHAR(10)  , \n",
							"\t\t  EMP_NODE_ID VARCHAR(10)  , \n",
							"\t\t  CNUM_ID VARCHAR(10)  , \n",
							"\t\t  WEEK_SCHEDULE_HRS DECIMAL(5,1)  , \n",
							"\t\t  UNIT_PRICE_AMT DECIMAL(20,2)  , \n",
							"\t\t  SHIFT_1_RATE DECIMAL(20,3)  , \n",
							"\t\t  SHIFT_2_RATE DECIMAL(20,3)  , \n",
							"\t\t  SHIFT_3_RATE DECIMAL(20,3)  , \n",
							"\t\t  STANDBY_RATE DECIMAL(20,3)  , \n",
							"\t\t  OVERTIME_AMT DECIMAL(20,2)  , \n",
							"\t\t  COMPETENCY_SEGMENT_CD VARCHAR(10)  , \n",
							"\t\t  PROFESSION_NAME VARCHAR(30)  , \n",
							"\t\t  ORIG_LOC_CD VARCHAR(10)  , \n",
							"\t\t  DEPT_CATG_CD VARCHAR(15)  , \n",
							"\t\t  LOB_ID VARCHAR(10)  , \n",
							"\t\t  SAP_IND VARCHAR(1)  , \n",
							"\t\t  CHARGE_GROUP_CD VARCHAR(10)  , \n",
							"\t\t  SAP_COMPANY_CD VARCHAR(10)  , \n",
							"\t\t  WORK_WEEK_HRS_MIN DECIMAL(5,1)  , \n",
							"\t\t  WORK_WEEK_HRS_MAX DECIMAL(5,1)  , \n",
							"\t\t  IMG_ACTIVE_EMPLOYEE_STATUS_CD VARCHAR(1)  , \n",
							"\t\t  EMP_FIRST_NM VARCHAR(100)  , \n",
							"\t\t  ISO_CTRY_CD VARCHAR(10)  , \n",
							"\t\t  MGR_CTRY_CD VARCHAR(10)  , \n",
							"\t\t  MGR_CMPNY_CD VARCHAR(50)  , \n",
							"\t\t  MGR_SER_NUM VARCHAR(20)  , \n",
							"\t\t  RDM_CTRY_CD VARCHAR(10)  , \n",
							"\t\t  RDM_CMPNY_CD VARCHAR(50)  , \n",
							"\t\t  RDM_SER_NUM VARCHAR(20)  , \n",
							"\t\t  DIVISION_CODE VARCHAR(2) , \n",
							"\t\t  DEPT_NUMBER VARCHAR(8) , \n",
							"\t\t  MGR_CNUM_ID VARCHAR(10) , \n",
							"\t\t  JOB_ROLE VARCHAR(70),\n",
							"          DATA_IND varchar(10)  ,\n",
							"\t      ACTIVE_IN_SOURCE_IND char(1) )   \n",
							"USING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"LOCATION 'abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/dimensions/DHT_EMPLOYEE7'\n",
							"--option(\"mergeSchema\", \"true\") \n",
							"--#======"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"-- DBXDH.DHT_EMPLOYEE definition\n",
							"--trying new definition to fix data issue\n",
							"\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.DHT_EMPLOYEE  (\n",
							"\t\t  EMPLOYEE_KEY INTEGER NOT NULL , \n",
							"\t\t  EMPLOYEE_VERSION INTEGER NOT NULL , \n",
							"\t\t  EMPLOYEE_SERIAL_NUM VARCHAR(20)  , \n",
							"\t\t  EMPLOYEE_COUNTRY_CODE VARCHAR(10)  , \n",
							"\t\t  EMPLOYEE_COMPANY_CD VARCHAR(50)  , \n",
							"\t\t  EMPLOYEE_BUSINESS_CD VARCHAR(100)  , \n",
							"\t\t  NOTES_ID_RAW VARCHAR(100)  , \n",
							"\t\t  NOTES_ID VARCHAR(100)  , \n",
							"\t\t  CURRENT_IND VARCHAR(1) ,\n",
							"\t\t  EXTRACT_DT TIMESTAMP  , \n",
							"\t\t  REC_START_DT TIMESTAMP  , \n",
							"\t\t  REC_END_DT TIMESTAMP  , \n",
							"\t\t  SOURCE_SYSTEM VARCHAR(50)  , \n",
							"\t\t  REC_CHECKSUM VARCHAR(32)  , \n",
							"\t\t  REC_STATUS VARCHAR(1)  , \n",
							"\t\t  IMG_LST_UPD_DT TIMESTAMP ,\n",
							"\t\t  IMG_CREATED_DT TIMESTAMP ,\n",
							"\t\t  EMPLOYEE_INITIALS VARCHAR(50)  , \n",
							"\t\t  EMPLOYEE_LAST_NAME VARCHAR(100)  , \n",
							"\t\t  EMPLOYEE_DISCON_DT TIMESTAMP  , \n",
							"\t\t  EMPLOYEE_EFF_DT TIMESTAMP  , \n",
							"\t\t  EMPLOYEE_STATUS VARCHAR(10)  , \n",
							"\t\t  EMPLOYEE_LEVEL_CD VARCHAR(10)  , \n",
							"\t\t  EMPLOYEE_USER_ID VARCHAR(10)  , \n",
							"\t\t  MANAGER_IND VARCHAR(1)  , \n",
							"\t\t  HOME_NODE_ID VARCHAR(10)  , \n",
							"\t\t  HOME_USER_ID VARCHAR(10)  , \n",
							"\t\t  BURDEN_CD VARCHAR(1)  , \n",
							"\t\t  BURDEN_CD_UPD_IND VARCHAR(1)  , \n",
							"\t\t  FIN_ADMIN_CD VARCHAR(10)  , \n",
							"\t\t  LONGEVITY_CD VARCHAR(1)  , \n",
							"\t\t  GROUP_ID VARCHAR(10)  , \n",
							"\t\t  JOB_FAMILY_CD VARCHAR(10)  , \n",
							"\t\t  PROFESSION_CD VARCHAR(10)  , \n",
							"\t\t  EMF_SOURCE_CD VARCHAR(10)  , \n",
							"\t\t  LBR_RPT_IND VARCHAR(1)  , \n",
							"\t\t  INET_MAIL_ADDR VARCHAR(100)  , \n",
							"\t\t  TEAM_ID VARCHAR(10)  , \n",
							"\t\t  SITE_LOC_CD VARCHAR(10)  , \n",
							"\t\t  EMP_NODE_ID VARCHAR(10)  , \n",
							"\t\t  CNUM_ID VARCHAR(10)  , \n",
							"\t\t  WEEK_SCHEDULE_HRS DECIMAL(5,1)  , \n",
							"\t\t  UNIT_PRICE_AMT DECIMAL(20,2)  , \n",
							"\t\t  SHIFT_1_RATE DECIMAL(20,3)  , \n",
							"\t\t  SHIFT_2_RATE DECIMAL(20,3)  , \n",
							"\t\t  SHIFT_3_RATE DECIMAL(20,3)  , \n",
							"\t\t  STANDBY_RATE DECIMAL(20,3)  , \n",
							"\t\t  OVERTIME_AMT DECIMAL(20,2)  , \n",
							"\t\t  COMPETENCY_SEGMENT_CD VARCHAR(10)  , \n",
							"\t\t  PROFESSION_NAME VARCHAR(30)  , \n",
							"\t\t  ORIG_LOC_CD VARCHAR(10)  , \n",
							"\t\t  DEPT_CATG_CD VARCHAR(15)  , \n",
							"\t\t  LOB_ID VARCHAR(10)  , \n",
							"\t\t  SAP_IND VARCHAR(1)  , \n",
							"\t\t  CHARGE_GROUP_CD VARCHAR(10)  , \n",
							"\t\t  SAP_COMPANY_CD VARCHAR(10)  , \n",
							"\t\t  WORK_WEEK_HRS_MIN DECIMAL(5,1)  , \n",
							"\t\t  WORK_WEEK_HRS_MAX DECIMAL(5,1)  , \n",
							"\t\t  IMG_ACTIVE_EMPLOYEE_STATUS_CD VARCHAR(1)  , \n",
							"\t\t  EMP_FIRST_NM VARCHAR(100)  , \n",
							"\t\t  ISO_CTRY_CD VARCHAR(10)  , \n",
							"\t\t  MGR_CTRY_CD VARCHAR(10)  , \n",
							"\t\t  MGR_CMPNY_CD VARCHAR(50)  , \n",
							"\t\t  MGR_SER_NUM VARCHAR(20)  , \n",
							"\t\t  RDM_CTRY_CD VARCHAR(10)  , \n",
							"\t\t  RDM_CMPNY_CD VARCHAR(50)  , \n",
							"\t\t  RDM_SER_NUM VARCHAR(20)  , \n",
							"\t\t  DIVISION_CODE VARCHAR(2) , \n",
							"\t\t  DEPT_NUMBER VARCHAR(8) , \n",
							"\t\t  MGR_CNUM_ID VARCHAR(10) , \n",
							"\t\t  JOB_ROLE VARCHAR(70),\n",
							"          DATA_IND varchar(10)  ,\n",
							"\t      ACTIVE_IN_SOURCE_IND char(1) )   \n",
							"USING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/deltalake/dimensions/DHT_EMPLOYEE'\n",
							"--#======"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DROP TABLE etlhubconfirmed.dht_employee"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DELETE FROM etlhubconfirmed.dht_employee"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#sample table\n",
							"# Create table in the metastore\n",
							"from delta.tables import *\n",
							"\n",
							"DeltaTable.createIfNotExists(spark) \\\n",
							"  .tableName(\"etlhubConfirmed.test\") \\\n",
							"  .addColumn(\"id\", \"INT\") \\\n",
							"  .addColumn(\"firstName\", \"STRING\") \\\n",
							"  .addColumn(\"middleName\", \"STRING\") \\\n",
							"  .addColumn(\"lastName\", \"STRING\", comment = \"surname\") \\\n",
							"  .addColumn(\"gender\", \"STRING\") \\\n",
							"  .addColumn(\"birthDate\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"ssn\", \"STRING\") \\\n",
							"  .addColumn(\"salary\", \"INT\") \\\n",
							"  .location('abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/dimensions/TEST') \\\n",
							"  .property(\"mergeSchema\", \"true\") \\\n",
							"  .partitionedBy(\"gender\") \\\n",
							"  .execute()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#sample table\n",
							"# Create table in the metastore\n",
							"from delta.tables import *\n",
							"\n",
							"DeltaTable.createIfNotExists(spark) \\\n",
							"  .tableName(\"etlhubConfirmed.test_timestamp\") \\\n",
							"  .addColumn(\"EMPLOYEE_DISCON_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"EMPLOYEE_EFF_DT\", \"TIMESTAMP\") \\\n",
							"  .location('abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/dimensions/TEST_timestamp') \\\n",
							"  .property(\"mergeSchema\", \"true\") \\\n",
							"  .partitionedBy(\"EMPLOYEE_DISCON_DT\") \\\n",
							"  .execute()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT DISTINCT EMPLOYEE_DISCON_DT\n",
							",EMPLOYEE_EFF_DT from fullJoin A \n",
							"--\n",
							"limit 20"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select * from etlhubConfirmed.test"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 40
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DHT_EXTRACT_CONTROL')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/CreateTableScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "64f7eefa-002f-47d2-895f-7555742d7821"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"-- DBXDH.DHT_EMPLOYEE definition\n",
							"\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.DHT_EXTRACT_CONTROL  (\n",
							"\t\t  EMPLOYEE_KEY INTEGER NOT NULL , \n",
							"\t\t  EMPLOYEE_VERSION INTEGER NOT NULL , \n",
							"\t\t  EMPLOYEE_SERIAL_NUM VARCHAR(20)  , \n",
							"\t\t  EMPLOYEE_COUNTRY_CODE VARCHAR(10)  , \n",
							"\t\t  EMPLOYEE_COMPANY_CD VARCHAR(50)  , \n",
							"\t\t  EMPLOYEE_BUSINESS_CD VARCHAR(100)  , \n",
							"\t\t  NOTES_ID_RAW VARCHAR(100)  , \n",
							"\t\t  NOTES_ID VARCHAR(100)  , \n",
							"\t\t  CURRENT_IND VARCHAR(1) ,\n",
							"\t\t  EXTRACT_DT TIMESTAMP  , \n",
							"\t\t  REC_START_DT TIMESTAMP  , \n",
							"\t\t  REC_END_DT TIMESTAMP  , \n",
							"\t\t  SOURCE_SYSTEM VARCHAR(50)  , \n",
							"\t\t  REC_CHECKSUM VARCHAR(32)  , \n",
							"\t\t  REC_STATUS VARCHAR(1)  , \n",
							"\t\t  IMG_LST_UPD_DT TIMESTAMP ,\n",
							"\t\t  IMG_CREATED_DT TIMESTAMP ,\n",
							"\t\t  EMPLOYEE_INITIALS VARCHAR(50)  , \n",
							"\t\t  EMPLOYEE_LAST_NAME VARCHAR(100)  , \n",
							"\t\t  EMPLOYEE_DISCON_DT TIMESTAMP  , \n",
							"\t\t  EMPLOYEE_EFF_DT TIMESTAMP  , \n",
							"\t\t  EMPLOYEE_STATUS VARCHAR(10)  , \n",
							"\t\t  EMPLOYEE_LEVEL_CD VARCHAR(10)  , \n",
							"\t\t  EMPLOYEE_USER_ID VARCHAR(10)  , \n",
							"\t\t  MANAGER_IND VARCHAR(1)  , \n",
							"\t\t  HOME_NODE_ID VARCHAR(10)  , \n",
							"\t\t  HOME_USER_ID VARCHAR(10)  , \n",
							"\t\t  BURDEN_CD VARCHAR(1)  , \n",
							"\t\t  BURDEN_CD_UPD_IND VARCHAR(1)  , \n",
							"\t\t  FIN_ADMIN_CD VARCHAR(10)  , \n",
							"\t\t  LONGEVITY_CD VARCHAR(1)  , \n",
							"\t\t  GROUP_ID VARCHAR(10)  , \n",
							"\t\t  JOB_FAMILY_CD VARCHAR(10)  , \n",
							"\t\t  PROFESSION_CD VARCHAR(10)  , \n",
							"\t\t  EMF_SOURCE_CD VARCHAR(10)  , \n",
							"\t\t  LBR_RPT_IND VARCHAR(1)  , \n",
							"\t\t  INET_MAIL_ADDR VARCHAR(100)  , \n",
							"\t\t  TEAM_ID VARCHAR(10)  , \n",
							"\t\t  SITE_LOC_CD VARCHAR(10)  , \n",
							"\t\t  EMP_NODE_ID VARCHAR(10)  , \n",
							"\t\t  CNUM_ID VARCHAR(10)  , \n",
							"\t\t  WEEK_SCHEDULE_HRS DECIMAL(5,1)  , \n",
							"\t\t  UNIT_PRICE_AMT DECIMAL(20,2)  , \n",
							"\t\t  SHIFT_1_RATE DECIMAL(20,3)  , \n",
							"\t\t  SHIFT_2_RATE DECIMAL(20,3)  , \n",
							"\t\t  SHIFT_3_RATE DECIMAL(20,3)  , \n",
							"\t\t  STANDBY_RATE DECIMAL(20,3)  , \n",
							"\t\t  OVERTIME_AMT DECIMAL(20,2)  , \n",
							"\t\t  COMPETENCY_SEGMENT_CD VARCHAR(10)  , \n",
							"\t\t  PROFESSION_NAME VARCHAR(30)  , \n",
							"\t\t  ORIG_LOC_CD VARCHAR(10)  , \n",
							"\t\t  DEPT_CATG_CD VARCHAR(15)  , \n",
							"\t\t  LOB_ID VARCHAR(10)  , \n",
							"\t\t  SAP_IND VARCHAR(1)  , \n",
							"\t\t  CHARGE_GROUP_CD VARCHAR(10)  , \n",
							"\t\t  SAP_COMPANY_CD VARCHAR(10)  , \n",
							"\t\t  WORK_WEEK_HRS_MIN DECIMAL(5,1)  , \n",
							"\t\t  WORK_WEEK_HRS_MAX DECIMAL(5,1)  , \n",
							"\t\t  IMG_ACTIVE_EMPLOYEE_STATUS_CD VARCHAR(1)  , \n",
							"\t\t  EMP_FIRST_NM VARCHAR(100)  , \n",
							"\t\t  ISO_CTRY_CD VARCHAR(10)  , \n",
							"\t\t  MGR_CTRY_CD VARCHAR(10)  , \n",
							"\t\t  MGR_CMPNY_CD VARCHAR(50)  , \n",
							"\t\t  MGR_SER_NUM VARCHAR(20)  , \n",
							"\t\t  RDM_CTRY_CD VARCHAR(10)  , \n",
							"\t\t  RDM_CMPNY_CD VARCHAR(50)  , \n",
							"\t\t  RDM_SER_NUM VARCHAR(20)  , \n",
							"\t\t  DIVISION_CODE VARCHAR(2) , \n",
							"\t\t  DEPT_NUMBER VARCHAR(8) , \n",
							"\t\t  MGR_CNUM_ID VARCHAR(10) , \n",
							"\t\t  JOB_ROLE VARCHAR(70),\n",
							"          DATA_IND varchar(10)  ,\n",
							"\t      ACTIVE_IN_SOURCE_IND char(1) )   \n",
							"USING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"LOCATION 'abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/dimensions/DHT_EMPLOYEE7'\n",
							"--option(\"mergeSchema\", \"true\") \n",
							"--#======"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"-- DBXDH.DHT_EMPLOYEE definition\n",
							"--trying new definition to fix data issue\n",
							"\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.DHT_EMPLOYEE  (\n",
							"\t\t  EMPLOYEE_KEY INTEGER NOT NULL , \n",
							"\t\t  EMPLOYEE_VERSION INTEGER NOT NULL , \n",
							"\t\t  EMPLOYEE_SERIAL_NUM VARCHAR(20)  , \n",
							"\t\t  EMPLOYEE_COUNTRY_CODE VARCHAR(10)  , \n",
							"\t\t  EMPLOYEE_COMPANY_CD VARCHAR(50)  , \n",
							"\t\t  EMPLOYEE_BUSINESS_CD VARCHAR(100)  , \n",
							"\t\t  NOTES_ID_RAW VARCHAR(100)  , \n",
							"\t\t  NOTES_ID VARCHAR(100)  , \n",
							"\t\t  CURRENT_IND VARCHAR(1) ,\n",
							"\t\t  EXTRACT_DT TIMESTAMP  , \n",
							"\t\t  REC_START_DT TIMESTAMP  , \n",
							"\t\t  REC_END_DT TIMESTAMP  , \n",
							"\t\t  SOURCE_SYSTEM VARCHAR(50)  , \n",
							"\t\t  REC_CHECKSUM VARCHAR(32)  , \n",
							"\t\t  REC_STATUS VARCHAR(1)  , \n",
							"\t\t  IMG_LST_UPD_DT TIMESTAMP ,\n",
							"\t\t  IMG_CREATED_DT TIMESTAMP ,\n",
							"\t\t  EMPLOYEE_INITIALS VARCHAR(50)  , \n",
							"\t\t  EMPLOYEE_LAST_NAME VARCHAR(100)  , \n",
							"\t\t  EMPLOYEE_DISCON_DT TIMESTAMP  , \n",
							"\t\t  EMPLOYEE_EFF_DT TIMESTAMP  , \n",
							"\t\t  EMPLOYEE_STATUS VARCHAR(10)  , \n",
							"\t\t  EMPLOYEE_LEVEL_CD VARCHAR(10)  , \n",
							"\t\t  EMPLOYEE_USER_ID VARCHAR(10)  , \n",
							"\t\t  MANAGER_IND VARCHAR(1)  , \n",
							"\t\t  HOME_NODE_ID VARCHAR(10)  , \n",
							"\t\t  HOME_USER_ID VARCHAR(10)  , \n",
							"\t\t  BURDEN_CD VARCHAR(1)  , \n",
							"\t\t  BURDEN_CD_UPD_IND VARCHAR(1)  , \n",
							"\t\t  FIN_ADMIN_CD VARCHAR(10)  , \n",
							"\t\t  LONGEVITY_CD VARCHAR(1)  , \n",
							"\t\t  GROUP_ID VARCHAR(10)  , \n",
							"\t\t  JOB_FAMILY_CD VARCHAR(10)  , \n",
							"\t\t  PROFESSION_CD VARCHAR(10)  , \n",
							"\t\t  EMF_SOURCE_CD VARCHAR(10)  , \n",
							"\t\t  LBR_RPT_IND VARCHAR(1)  , \n",
							"\t\t  INET_MAIL_ADDR VARCHAR(100)  , \n",
							"\t\t  TEAM_ID VARCHAR(10)  , \n",
							"\t\t  SITE_LOC_CD VARCHAR(10)  , \n",
							"\t\t  EMP_NODE_ID VARCHAR(10)  , \n",
							"\t\t  CNUM_ID VARCHAR(10)  , \n",
							"\t\t  WEEK_SCHEDULE_HRS DECIMAL(5,1)  , \n",
							"\t\t  UNIT_PRICE_AMT DECIMAL(20,2)  , \n",
							"\t\t  SHIFT_1_RATE DECIMAL(20,3)  , \n",
							"\t\t  SHIFT_2_RATE DECIMAL(20,3)  , \n",
							"\t\t  SHIFT_3_RATE DECIMAL(20,3)  , \n",
							"\t\t  STANDBY_RATE DECIMAL(20,3)  , \n",
							"\t\t  OVERTIME_AMT DECIMAL(20,2)  , \n",
							"\t\t  COMPETENCY_SEGMENT_CD VARCHAR(10)  , \n",
							"\t\t  PROFESSION_NAME VARCHAR(30)  , \n",
							"\t\t  ORIG_LOC_CD VARCHAR(10)  , \n",
							"\t\t  DEPT_CATG_CD VARCHAR(15)  , \n",
							"\t\t  LOB_ID VARCHAR(10)  , \n",
							"\t\t  SAP_IND VARCHAR(1)  , \n",
							"\t\t  CHARGE_GROUP_CD VARCHAR(10)  , \n",
							"\t\t  SAP_COMPANY_CD VARCHAR(10)  , \n",
							"\t\t  WORK_WEEK_HRS_MIN DECIMAL(5,1)  , \n",
							"\t\t  WORK_WEEK_HRS_MAX DECIMAL(5,1)  , \n",
							"\t\t  IMG_ACTIVE_EMPLOYEE_STATUS_CD VARCHAR(1)  , \n",
							"\t\t  EMP_FIRST_NM VARCHAR(100)  , \n",
							"\t\t  ISO_CTRY_CD VARCHAR(10)  , \n",
							"\t\t  MGR_CTRY_CD VARCHAR(10)  , \n",
							"\t\t  MGR_CMPNY_CD VARCHAR(50)  , \n",
							"\t\t  MGR_SER_NUM VARCHAR(20)  , \n",
							"\t\t  RDM_CTRY_CD VARCHAR(10)  , \n",
							"\t\t  RDM_CMPNY_CD VARCHAR(50)  , \n",
							"\t\t  RDM_SER_NUM VARCHAR(20)  , \n",
							"\t\t  DIVISION_CODE VARCHAR(2) , \n",
							"\t\t  DEPT_NUMBER VARCHAR(8) , \n",
							"\t\t  MGR_CNUM_ID VARCHAR(10) , \n",
							"\t\t  JOB_ROLE VARCHAR(70),\n",
							"          DATA_IND varchar(10)  ,\n",
							"\t      ACTIVE_IN_SOURCE_IND char(1) )   \n",
							"USING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/deltalake/dimensions/DHT_EMPLOYEE'\n",
							"--#======"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DROP TABLE etlhubconfirmed.dht_employee"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DELETE FROM etlhubconfirmed.dht_employee"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#sample table\n",
							"# Create table in the metastore\n",
							"from delta.tables import *\n",
							"\n",
							"DeltaTable.createIfNotExists(spark) \\\n",
							"  .tableName(\"etlhubConfirmed.test\") \\\n",
							"  .addColumn(\"id\", \"INT\") \\\n",
							"  .addColumn(\"firstName\", \"STRING\") \\\n",
							"  .addColumn(\"middleName\", \"STRING\") \\\n",
							"  .addColumn(\"lastName\", \"STRING\", comment = \"surname\") \\\n",
							"  .addColumn(\"gender\", \"STRING\") \\\n",
							"  .addColumn(\"birthDate\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"ssn\", \"STRING\") \\\n",
							"  .addColumn(\"salary\", \"INT\") \\\n",
							"  .location('abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/dimensions/TEST') \\\n",
							"  .property(\"mergeSchema\", \"true\") \\\n",
							"  .partitionedBy(\"gender\") \\\n",
							"  .execute()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#sample table\n",
							"# Create table in the metastore\n",
							"from delta.tables import *\n",
							"\n",
							"DeltaTable.createIfNotExists(spark) \\\n",
							"  .tableName(\"etlhubConfirmed.test_timestamp\") \\\n",
							"  .addColumn(\"EMPLOYEE_DISCON_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"EMPLOYEE_EFF_DT\", \"TIMESTAMP\") \\\n",
							"  .location('abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/dimensions/TEST_timestamp') \\\n",
							"  .property(\"mergeSchema\", \"true\") \\\n",
							"  .partitionedBy(\"EMPLOYEE_DISCON_DT\") \\\n",
							"  .execute()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT DISTINCT EMPLOYEE_DISCON_DT\n",
							",EMPLOYEE_EFF_DT from fullJoin A \n",
							"--\n",
							"limit 20"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select * from etlhubConfirmed.test"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 40
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DHT_INDUSTRY_SECTOR')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/CreateTableScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a955891b-0c80-477c-a8e5-f7156f1df19b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"-- DBXDH.DHT_INDUSTRY_SECTOR definition\n",
							"\n",
							"\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.DHT_INDUSTRY_SECTOR  (\t\n",
							"\n",
							"\tINDUSTRY_SECTOR_KEY\tINT NOT NULL\n",
							",\tINDUSTRY_SECTOR_VERSION\tINT NOT NULL\n",
							",\tlevelNum\tSTRING\n",
							",\tparentCode\tSTRING\n",
							",\tcode\tSTRING\n",
							",\tlongDescription\tSTRING\n",
							",\tmediumDescription\tSTRING\n",
							",\tshortDescription\tSTRING\n",
							",\tcomments\tSTRING\n",
							",\trecordStatus\tSTRING\n",
							",\trowCreateTs\tSTRING\n",
							",\trowUpdateTs\tSTRING\n",
							",\tUSAGE_RULE\tSTRING\n",
							",\tALT_DESC_FULL_1\tSTRING\n",
							",\tALT_DESC_FULL_2\tSTRING\n",
							",\tCURRENT_IND\tSTRING\n",
							",\tEXTRACT_DT TIMESTAMP\n",
							",\tREC_START_DT TIMESTAMP\n",
							",\tREC_END_DT TIMESTAMP\n",
							",\tSOURCE_SYSTEM STRING\n",
							",\tREC_CHECKSUM STRING\n",
							",\tREC_STATUS STRING\t\n",
							",\tIMG_LST_UPD_DT TIMESTAMP\n",
							",\tIMG_CREATED_DT TIMESTAMP\n",
							",   ACTIVE_IN_SOURCE_IND STRING\t\n",
							") \n",
							"USING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"LOCATION 'abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/dimensions/DHT_INDUSTRY_SECTOR'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DROP TABLE etlhubConfirmed.DHT_INDUSTRY_SECTOR"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DELETE FROM etlhubConfirmed.DHT_INDUSTRY_SECTOR"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#sample table\n",
							"# Create table in the metastore\n",
							"from delta.tables import *\n",
							"\n",
							"DeltaTable.createIfNotExists(spark) \\\n",
							"  .tableName(\"etlhubConfirmed.test\") \\\n",
							"  .addColumn(\"id\", \"INT\") \\\n",
							"  .addColumn(\"firstName\", \"STRING\") \\\n",
							"  .addColumn(\"middleName\", \"STRING\") \\\n",
							"  .addColumn(\"lastName\", \"STRING\", comment = \"surname\") \\\n",
							"  .addColumn(\"gender\", \"STRING\") \\\n",
							"  .addColumn(\"birthDate\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"ssn\", \"STRING\") \\\n",
							"  .addColumn(\"salary\", \"INT\") \\\n",
							"  .location('abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/dimensions/TEST') \\\n",
							"  .property(\"mergeSchema\", \"true\") \\\n",
							"  .partitionedBy(\"gender\") \\\n",
							"  .execute()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#sample table\n",
							"# Create table in the metastore\n",
							"from delta.tables import *\n",
							"\n",
							"DeltaTable.createIfNotExists(spark) \\\n",
							"  .tableName(\"etlhubConfirmed.test_timestamp\") \\\n",
							"  .addColumn(\"EMPLOYEE_DISCON_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"EMPLOYEE_EFF_DT\", \"TIMESTAMP\") \\\n",
							"  .location('abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/dimensions/TEST_timestamp') \\\n",
							"  .property(\"mergeSchema\", \"true\") \\\n",
							"  .partitionedBy(\"EMPLOYEE_DISCON_DT\") \\\n",
							"  .execute()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT DISTINCT EMPLOYEE_DISCON_DT\n",
							",EMPLOYEE_EFF_DT from fullJoin A \n",
							"--\n",
							"limit 20"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select * from etlhubConfirmed.test"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 40
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DHT_MARKET_ddl')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/CreateTableScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "1fdbc122-538d-44aa-a296-02f4f6bfdc58"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"-- DBXDH.DHT_MARKET definition\n",
							"\n",
							"\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.DHT_MARKET  (\t\n",
							"\t\tMARKET_KEY\tINT \tNOT\tNULL\t,\n",
							"\tMARKET_VERSION\tINT \tNOT\tNULL\t,\n",
							"\tCUSTOMER_NO\tSTRING\t,\t\t\n",
							"\tCUSTOMER_NM\tSTRING\t,\t\t\n",
							"\tINAC_CD\tSTRING\t,\t\t\n",
							"\tINAC_DESC\tSTRING\t,\t\t\n",
							"\tCOUNTRY_CD\tSTRING\t,\t\t\n",
							"\tMARKET_BUSINESS_CD\tSTRING\t,\t\t\n",
							"\tSMB_IND\tSTRING\t,\t\t\n",
							"\tCOMPANY_NO\tSTRING\t,\t\t\n",
							"\tENTERPRISE_NM\tSTRING\t,\t\t\n",
							"\tENTERPRISE_NO\tSTRING\t,\t\t\n",
							"\tCUST_CITY_NM\tSTRING\t,\t\t\n",
							"\tCUST_STATE_PROV_CD\tSTRING\t,\t\t\n",
							"\tCUST_POSTL_CD\tSTRING\t,\t\t\n",
							"\tISO_COUNTRY_CD\tSTRING\t,\t\t\n",
							"\tISO_COUNTRY_DESC\tSTRING\t,\t\t\n",
							"\tCMR_ISU_CD\tSTRING\t,\t\t\n",
							"\tSUB_INDUSTRY_CD\tSTRING\t,\t\t\n",
							"\tCURR_CVRG_TYP_CD\tSTRING\t,\t\t\n",
							"\tCURR_CVRG_TYP_DESC\tSTRING\t,\t\t\n",
							"\tCURR_CVRG_ID\tSTRING\t,\t\t\n",
							"\tCURR_CVRG_NM\tSTRING\t,\t\t\n",
							"\tDLGTD_CVRG_IND\tSTRING\t,\t\t\n",
							"\tBASE_CVRG_TYP_CD\tSTRING\t,\t\t\n",
							"\tBASE_CVRG_TYP_DESC\tSTRING\t,\t\t\n",
							"\tBASE_CVRG_ID\tSTRING\t,\t\t\n",
							"\tBASE_CVRG_NM\tSTRING\t,\t\t\n",
							"\tTERR_ID\tSTRING\t,\t\t\n",
							"\tTERR_NM\tSTRING\t,\t\t\n",
							"\tDBCS_CUSTOMER_NM\tSTRING\t,\t\t\n",
							"\tCLIENT_ID\tSTRING\t,\t\t\n",
							"\tCLIENT_NM\tSTRING\t,\t\t\n",
							"\tCVI_BRNCH_ID\tSTRING\t,\t\t\n",
							"\tCVI_BRNCH_DESC\tSTRING\t,\t\t\n",
							"\tCVI_BRNCH_GRP_CD\tSTRING\t,\t\t\n",
							"\tCVI_BRNCH_GRP_DESC\tSTRING\t,\t\t\n",
							"\tCVI_BRNCH_UNT_ID\tSTRING\t,\t\t\n",
							"\tCVI_BRNCH_UNT_NM\tSTRING\t,\t\t\n",
							"\tSMB_QUADTIER_CD\tSTRING\t,\t\t\n",
							"\tCUST_ACCT_ID\tSTRING\t,\t\t\n",
							"\tRPTS_TO_IMT_ID\tSTRING\t,\t\t\n",
							"\tRPTS_TO_SCTR_CD\tSTRING\t,\t\t\n",
							"\tRPTS_TO_SCTR_DESC\tSTRING\t,\t\t\n",
							"\tCURRENT_IND\tSTRING\t,\t\t\n",
							"\tEXTRACT_DT\tTIMESTAMP\t,\t\t\n",
							"\tREC_START_DT\tTIMESTAMP\t,\t\t\n",
							"\tREC_END_DT\tTIMESTAMP\t,\t\t\n",
							"\tREC_CHECKSUM\tSTRING\t,\t\t\n",
							"\tREC_STATUS\tSTRING\t,\t\t\n",
							"\tSOURCE_SYSTEM\tSTRING\t,\t\t\n",
							"\tCUST_INDUSTRY_SECTOR_KEY\tINT \t,\t\t\n",
							"\tCUST_INDUSTRY_SECTOR_VERSION\tINT \t,\t\t\n",
							"\tTERRITORY_RGN_KEY\tINT \t,\t\t\n",
							"\tREPORTS_TO_IMT_GEO_KEY\tINT \t,\t\t\n",
							"\tFIN_COUNTRY_KEY\tINT \t,\t\t\n",
							"\tIMG_LST_UPD_DT\tTIMESTAMP\t,\t\t\n",
							"\tIMG_CREATED_DT\tTIMESTAMP\t,\t\t\n",
							"\tBOID\tSTRING\t,\t\t\n",
							"\tDOMESTIC_CLIENT_ID\tSTRING\t,\t\t\n",
							"\tDOMESTIC_CLIENT_NM\tSTRING\t,\t\t\n",
							"\tGLOBAL_CLIENT_ID\tSTRING\t,\t\t\n",
							"\tGLOBAL_CLIENT_NM\tSTRING\t,\t\t\n",
							"\tGLOBAL_ULTIMATE_CLIENT_ID\tSTRING\t,\t\t\n",
							"\tGLOBAL_ULTIMATE_CLIENT_NM\tSTRING\t,\t\t\n",
							"\tDOMESTIC_BUYING_GROUP_ID\tSTRING\t,\t\t\n",
							"\tDOMESTIC_BUYING_GROUP_NM\tSTRING\t,\t\t\n",
							"\tGLOBAL_BUYING_GROUP_ID\tSTRING\t,\t\t\n",
							"\tGLOBAL_BUYING_GROUP_NM\tSTRING\t,\t\t\n",
							"\tTOP_TREE_ACCT_ID\tSTRING\t,\t\t\n",
							"\tTOP_TREE_ID\tSTRING\t,\t\t\n",
							"\tTOP_TREE_NM\tSTRING\t,\t\t\n",
							"\tSTANDARD_INDUSTRIAL_CLASSIFICATION_CD\tSTRING\t,\t\t\n",
							"\tCOV_CLIENT_TYPE\tSTRING\t,\t\t\n",
							"\tCOV_CLIENT_TYPE_DESC\tSTRING\t,\t\t\n",
							"\tCOV_CLIENT_SUBTYPE\tSTRING\t,\t\t\n",
							"\tCOV_CLIENT_SUBTYPE_DESC\tSTRING\t,\n",
							"\tACTIVE_IN_SOURCE_IND STRING )   \n",
							"USING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"LOCATION 'abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/dimensions/DHT_MARKET'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DROP TABLE etlhubconfirmed.dht_MARKET"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DELETE FROM etlhubconfirmed.dht_MARKET"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#sample table\n",
							"# Create table in the metastore\n",
							"from delta.tables import *\n",
							"\n",
							"DeltaTable.createIfNotExists(spark) \\\n",
							"  .tableName(\"etlhubConfirmed.test\") \\\n",
							"  .addColumn(\"id\", \"INT\") \\\n",
							"  .addColumn(\"firstName\", \"STRING\") \\\n",
							"  .addColumn(\"middleName\", \"STRING\") \\\n",
							"  .addColumn(\"lastName\", \"STRING\", comment = \"surname\") \\\n",
							"  .addColumn(\"gender\", \"STRING\") \\\n",
							"  .addColumn(\"birthDate\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"ssn\", \"STRING\") \\\n",
							"  .addColumn(\"salary\", \"INT\") \\\n",
							"  .location('abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/dimensions/TEST') \\\n",
							"  .property(\"mergeSchema\", \"true\") \\\n",
							"  .partitionedBy(\"gender\") \\\n",
							"  .execute()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#sample table\n",
							"# Create table in the metastore\n",
							"from delta.tables import *\n",
							"\n",
							"DeltaTable.createIfNotExists(spark) \\\n",
							"  .tableName(\"etlhubConfirmed.test_timestamp\") \\\n",
							"  .addColumn(\"EMPLOYEE_DISCON_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"EMPLOYEE_EFF_DT\", \"TIMESTAMP\") \\\n",
							"  .location('abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/dimensions/TEST_timestamp') \\\n",
							"  .property(\"mergeSchema\", \"true\") \\\n",
							"  .partitionedBy(\"EMPLOYEE_DISCON_DT\") \\\n",
							"  .execute()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT DISTINCT EMPLOYEE_DISCON_DT\n",
							",EMPLOYEE_EFF_DT from fullJoin A \n",
							"--\n",
							"limit 20"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select * from etlhubConfirmed.test"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 40
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DHT_OFFICE_SPACE_ddl')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/CreateTableScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPool001494New",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "472g",
					"driverCores": 80,
					"executorMemory": "472g",
					"executorCores": 80,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "759c6da5-5d89-4a6c-89c7-1e28471b3a49"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPool001494New",
						"name": "sPool001494New",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPool001494New",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 80,
						"memory": 472,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"-- DBXDH.DHT_OFFICE_SPACE definition\r\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.DHT_OFFICE_SPACE  (\t\r\n",
							"SPACE_KEY INT NOT NULL,\r\n",
							"VERSION INT NOT NULL ,\r\n",
							"BUSINESS_ID VARCHAR(64),\r\n",
							"CAMPUS_ID VARCHAR(8),\r\n",
							"BUILDING_ID VARCHAR(8),\r\n",
							"FLOOR_ID VARCHAR(4),\r\n",
							"FLOOR_DESCRIPTION VARCHAR(32),\r\n",
							"SPACE_ID VARCHAR(16),\r\n",
							"SPACE_DESCRIPTION VARCHAR(256),\r\n",
							"SPACE_STATUS VARCHAR(8),\r\n",
							"SPACE_CATEGORY_CODE VARCHAR(4),\r\n",
							"SPACE_CATEGORY_DESCRIPTION VARCHAR(64),\r\n",
							"SPACE_TYPE_CODE VARCHAR(4),\r\n",
							"SPACE_TYPE_DESCRIPTION VARCHAR(64),\r\n",
							"OCCUPANCY_TYPE VARCHAR(16),\r\n",
							"CAPACITY INTEGER,\r\n",
							"AREA_SQUARE_FEET INTEGER ,\r\n",
							"OSCRE_CODE VARCHAR(8),\r\n",
							"WORKPOINT VARCHAR(8),\r\n",
							"RSYP_ID VARCHAR(8),\r\n",
							"CURRENT_IND\tVARCHAR(1), -- Y for active row and N for all rows expired\r\n",
							"EXTRACT_DT TIMESTAMP, -- Load Cycle date, contains same value for all the records inserted in a single batch\r\n",
							"REC_START_DT TIMESTAMP, --SCD TYpe 2 start date\r\n",
							"REC_END_DT TIMESTAMP, -- SCD Type 2 End date, will have high date (9999-12-31) for active dimensional row\r\n",
							"SOURCE_SYSTEM STRING, -- Name of the source system\r\n",
							"REC_CHECKSUM STRING, -- Hash value of all dimensional columns. Used to identify changed records \r\n",
							"IMG_LST_UPD_DT TIMESTAMP, --Timestamp of when the record got updated in table\r\n",
							"IMG_CREATED_DT TIMESTAMP --Timestamp of when the record got inserted in table\r\n",
							")\r\n",
							"USING DELTA\r\n",
							"PARTITIONED BY (CURRENT_IND)\r\n",
							"LOCATION 'abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/dimensions/DHT_OFFICE_SPACE'\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from etlhubConfirmed.DHT_OFFICE_SPACE"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DROP TABLE etlhubConfirmed.DHT_OFFICE_SPACE"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DELETE FROM etlhubConfirmed.DHT_OFFICE_SPACE"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DHT_OPPORTUNITY_ASSIGNMENT_DAILY_FT_ddl')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/CreateTableScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "93174975-a652-4579-b492-874e76cbb4cd"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"-- DBXDH.DHT_OPPORTUNITY_ASSIGNMENT_DAILY_FT definition\n",
							"\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.DHT_OPPORTUNITY_ASSIGNMENT_DAILY_FT  (\n",
							"\t\t  OPPORTUNITY_ASSIGNMENT_KEY INTEGER NOT NULL , \n",
							"\t\t  OPPORTUNITY_ASSIGNMENT_VERSION INTEGER NOT NULL , \n",
							"\t\t  OPPORTUNITY_KEY INTEGER , \n",
							"\t\t  OPPORTUNITY_ROLE_KEY INTEGER , \n",
							"\t\t  CRM_ROLE_KEY INTEGER , \n",
							"\t\t  EMPLOYEE_KEY INTEGER , \n",
							"\t\t  CURRENT_IND VARCHAR(1) , \n",
							"\t\t  EXTRACT_DT TIMESTAMP , \n",
							"\t\t  REC_START_DT TIMESTAMP , \n",
							"\t\t  REC_END_DT TIMESTAMP , \n",
							"\t\t  SOURCE_SYSTEM VARCHAR(50) , \n",
							"\t\t  REC_CHECKSUM VARCHAR(32) , \n",
							"\t\t  REC_STATUS VARCHAR(1) , \n",
							"\t\t  IMG_LST_UPD_DT TIMESTAMP  , \n",
							"\t\t  IMG_CREATED_DT TIMESTAMP  ,\n",
							"          DATA_IND varchar(10)  ,\n",
							"\t      ACTIVE_IN_SOURCE_IND char(1) )   \n",
							"USING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"-- specify data lake folder location\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/DHT_OPPORTUNITY_ASSIGNMENT_DAILY_FT'      \n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DROP TABLE etlhubconfirmed.DHT_OPPORTUNITY_ASSIGNMENT_DAILY_FT"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DELETE FROM etlhubconfirmed.DHT_OPPORTUNITY_ASSIGNMENT_DAILY_FT"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DHT_OPPORTUNITY_ROLE_ddl')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/CreateTableScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9f212310-ffac-4c32-8c3e-92060490d5df"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"-- DBXDH.DHT_OPPORTUNITY_ROLE definition\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.DHT_OPPORTUNITY_ROLE  (\n",
							"\t\t  OPPORTUNITY_ROLE_KEY INTEGER NOT NULL , \n",
							"\t\t  OPPORTUNITY_ROLE_VERSION INTEGER NOT NULL , \n",
							"\t\t  ROLE_CD VARCHAR(254) , \n",
							"\t\t  ROLE_DESC VARCHAR(128) , \n",
							"\t\t  CURRENT_IND VARCHAR(1) , \n",
							"\t\t  EXTRACT_DT TIMESTAMP , \n",
							"\t\t  REC_START_DT TIMESTAMP , \n",
							"\t\t  REC_END_DT TIMESTAMP , \n",
							"\t\t  SOURCE_SYSTEM VARCHAR(50) , \n",
							"\t\t  REC_CHECKSUM VARCHAR(32) , \n",
							"\t\t  REC_STATUS VARCHAR(1) , \n",
							"\t\t  IMG_LST_UPD_DT TIMESTAMP , \n",
							"\t\t  IMG_CREATED_DT TIMESTAMP  ,\n",
							"          DATA_IND varchar(10)  ,\n",
							"\t      ACTIVE_IN_SOURCE_IND char(1) )   \n",
							"USING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"-- specify data lake folder location\n",
							"LOCATION 'abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/dimensions/DHT_OPPORTUNITY_ROLE'      \n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DROP TABLE etlhubconfirmed.dht_opportunity_role"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DELETE FROM etlhubconfirmed.dht_opportunity_role"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DHT_OPPORTUNITY_ddl')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/CreateTableScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "992c4269-9d1c-4683-b6af-6fac9a6eb1d7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"-- DBXDH.DHT_OPPORTUNITY definition\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.DHT_OPPORTUNITY  (\n",
							"\t\t  OPPORTUNITY_KEY INTEGER NOT NULL , \n",
							"\t\t  OPPORTUNITY_VERSION INTEGER NOT NULL , \n",
							"\t\t  OPPORTUNITY_NUM VARCHAR(50)  , \n",
							"\t\t  CURRENT_IND VARCHAR(1) NOT NULL, \n",
							"\t\t  EXTRACT_DT TIMESTAMP  , \n",
							"\t\t  REC_START_DT TIMESTAMP  , \n",
							"\t\t  REC_END_DT TIMESTAMP  , \n",
							"\t\t  SOURCE_SYSTEM VARCHAR(50)  , \n",
							"\t\t  REC_CHECKSUM VARCHAR(32)  , \n",
							"\t\t  REC_STATUS VARCHAR(1)  , \n",
							"\t\t  IMG_LST_UPD_DT TIMESTAMP, \n",
							"\t\t  IMG_CREATED_DT TIMESTAMP , \n",
							"\t\t  ISA_CODE VARCHAR(10)  , \n",
							"\t\t  ARCH_REASON_CD VARCHAR(20)  , \n",
							"\t\t  BUSINESS_TRANSACTION_TYPE VARCHAR(1)  , \n",
							"\t\t  CLIENT_REPRESENTATIVE_NAME VARCHAR(100)  , \n",
							"\t\t  COMPETITOR_LIST VARCHAR(100)  , \n",
							"\t\t  IDENTIFIER_USER_NAME VARCHAR(100)  , \n",
							"\t\t  OPPORTUNITY_NAME VARCHAR(100)  , \n",
							"\t\t  OPPORTUNITY_SOURCE_CD VARCHAR(20)  , \n",
							"\t\t  SBS_SOL_VALID_IND VARCHAR(1)  , \n",
							"\t\t  OPPORTUNITY_IDENTIFIER VARCHAR(100)  , \n",
							"\t\t  ROGUE_IND VARCHAR(1)  , \n",
							"\t\t  REASON_TO_ACT VARCHAR(255) , \n",
							"\t\t  SOLUTION_CATG VARCHAR(1024) , \n",
							"\t\t  BRAND_SPONSOR_LIST VARCHAR(500) , \n",
							"\t\t  GBS_GEOGRAPHY_NAME VARCHAR(60) , \n",
							"\t\t  GBS_BUSINESS_UNIT_GROUP_NAME VARCHAR(60) , \n",
							"\t\t  GBS_BUSINESS_UNIT_NAME VARCHAR(60) , \n",
							"\t\t  TAG_LIST VARCHAR(1000) , \n",
							"\t\t  OPPORTUNITY_LEGACY_NO VARCHAR(100)  ,\n",
							"          DATA_IND varchar(10)  ,\n",
							"\t      ACTIVE_IN_SOURCE_IND char(1) )   \n",
							"\tUSING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"\t-- specify data lake folder location\n",
							"\tLOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/deltalake/dimensions/DHT_OPPORTUNITY'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DROP TABLE etlhubconfirmed.dht_opportunity"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DELETE FROM etlhubconfirmed.dht_opportunity"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DHT_Opportunity_Daily_Fact_ddl')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/CreateTableScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPool001494New",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "472g",
					"driverCores": 80,
					"executorMemory": "472g",
					"executorCores": 80,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0ea5228d-36cb-42f6-826f-6c955833e1b8"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPool001494New",
						"name": "sPool001494New",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPool001494New",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 80,
						"memory": 472,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"-- DBXDH.DHT_OPPORTUNITY_DAILY_FT definition\n",
							"\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.DHT_OPPORTUNITY_DAILY_FT  (\n",
							"\t\t  OPPORTUNITY_FACT_KEY INTEGER NOT NULL , \n",
							"\t\t  OPPORTUNITY_FACT_VERSION INTEGER NOT NULL  , \n",
							"\t\t  OPPORTUNITY_KEY INTEGER  , \n",
							"\t\t  OPPORTUNITY_OWNER_KEY INTEGER  , \n",
							"\t\t  REGION_LEVEL_4_KEY INTEGER  , \n",
							"\t\t  IMG_INDUSTRY_SECTOR_KEY INTEGER  , \n",
							"\t\t  MARKET_KEY INTEGER  , \n",
							"\t\t  OPPORTUNITY_SALES_STAGE_KEY INTEGER  , \n",
							"\t\t  PREVIOUS_OPPORTUNITY_SALES_STAGE_KEY INTEGER  , \n",
							"\t\t  DECISION_DATE_KEY INTEGER  , \n",
							"\t\t  EIW_DATE_KEY INTEGER  , \n",
							"\t\t  OPPORTUNITY_CLOSE_DATE_KEY INTEGER  , \n",
							"\t\t  OPPORTUNITY_CREATE_DATE_KEY INTEGER  , \n",
							"\t\t  OPPORTUNITY_UPDATE_DATE_KEY INTEGER  , \n",
							"\t\t  OPPORTUNITY_SALES_STAGE_UPDATE_DATE_KEY INTEGER  , \n",
							"\t\t  GEOGRAPHY_KEY INTEGER  , \n",
							"\t\t  TOTAL_OPP_VAL_USD DECIMAL(15,0)  , \n",
							"\t\t  CURRENT_IND VARCHAR(1) , \n",
							"\t\t  EXTRACT_DT TIMESTAMP  , \n",
							"\t\t  REC_START_DT TIMESTAMP  , \n",
							"\t\t  REC_END_DT TIMESTAMP  , \n",
							"\t\t  SOURCE_SYSTEM VARCHAR(50)  , \n",
							"\t\t  REC_CHECKSUM VARCHAR(32)  , \n",
							"\t\t  REC_STATUS VARCHAR(1)  , \n",
							"\t\t  IMG_LST_UPD_DT TIMESTAMP  , \n",
							"\t\t  IMG_CREATED_DT TIMESTAMP  , \n",
							"\t\t  PROP_DELIVERY_DATE_KEY INTEGER , \n",
							"\t\t  PERF_CRITERIA_CONF SMALLINT , \n",
							"\t\t  PRICE_AGREEMENT SMALLINT , \n",
							"\t\t  PRICING_SECURED SMALLINT , \n",
							"\t\t  EXECUTIVE_SPONSOR_EMP_KEY VARCHAR(100) , \n",
							"\t\t  OPP_ACTIVE_IN_SOURCE_IND VARCHAR(1) ,\n",
							"          DATA_IND varchar(10)  ,\n",
							"\t      ACTIVE_IN_SOURCE_IND char(1) )   \n",
							"USING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"-- specify data lake folder location\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/deltalake/facts/DHT_OPPORTUNITY_DAILY_FT'      "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DROP TABLE etlhubconfirmed.DHT_OPPORTUNITY_DAILY_FT"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select * FROM etlhubconfirmed.DHT_OPPORTUNITY_DAILY_FT"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DHT_SALES_STAGE_ddl')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/CreateTableScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "f001dbae-c9a7-4bcf-9735-c8c7235adfcf"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"---- DHT_SALES_STAGE\n",
							"\t\t \n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.DHT_SALES_STAGE  (\n",
							"\t\t  SALES_STAGE_KEY INTEGER NOT NULL , \n",
							"\t\t  SALES_STAGE_VERSION INTEGER NOT NULL , \n",
							"\t\t  SALES_STAGE_CD VARCHAR(20)  , \n",
							"\t\t  SALES_STAGE_DESC VARCHAR(60)  , \n",
							"\t\t  SSM_STEP_NO VARCHAR(20)  , \n",
							"\t\t  SSM_STEP_NAME VARCHAR(60)  , \n",
							"\t\t  CURRENT_IND VARCHAR(1) , \n",
							"\t\t  EXTRACT_DT TIMESTAMP  , \n",
							"\t\t  REC_START_DT TIMESTAMP  , \n",
							"\t\t  REC_END_DT TIMESTAMP  , \n",
							"\t\t  SOURCE_SYSTEM VARCHAR(50)  , \n",
							"\t\t  REC_CHECKSUM VARCHAR(32)  , \n",
							"\t\t  REC_STATUS VARCHAR(1)  , \n",
							"\t\t  IMG_LST_UPD_DT TIMESTAMP  , \n",
							"\t\t  IMG_CREATED_DT TIMESTAMP  , \n",
							"\t\t  DATA_IND varchar(10)  ,\n",
							"\t      ACTIVE_IN_SOURCE_IND char(1) )   \n",
							"USING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"-- specify data lake folder location\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/deltalake/dimensions/DHT_SALES_STAGE_UPD'   "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DROP TABLE etlhubconfirmed.DHT_SALES_STAGE"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select * FROM etlhubconfirmed.DHT_SALES_STAGE"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DHT_SERVICE_ddl')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/CreateTableScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a6459e81-ff4f-4ebc-afff-bbee3bf435b5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"-- DBXDH.DHT_SERVICE definition\r\n",
							"\r\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.DHT_SERVICE  (\r\n",
							"\t\t  SERVICE_COMPONENT_KEY INTEGER NOT NULL , \r\n",
							"\t\t  SERVICE_COMPONENT_VERSION INTEGER NOT NULL , \r\n",
							"\t\t  OFFERING_COMPNT_CD VARCHAR(30)  , \r\n",
							"\t\t  OFFERING_COMPNT_DESC VARCHAR(50)  , \r\n",
							"\t\t  OFFERING_CODE_KEY INTEGER  , \r\n",
							"\t\t  OFFERING_CD VARCHAR(10)  , \r\n",
							"\t\t  OFFERING_DESC VARCHAR(50)  , \r\n",
							"\t\t  SERVICE_KEY INTEGER  , \r\n",
							"\t\t  SERVICE_CD VARCHAR(30)  , \r\n",
							"\t\t  SERVICE_DESC VARCHAR(50)  , \r\n",
							"\t\t  OFFERING_CATEGORY_KEY INTEGER  , \r\n",
							"\t\t  OFFERING_CAT_CD VARCHAR(10)  , \r\n",
							"\t\t  OFFERING_CAT_DESC VARCHAR(50)  , \r\n",
							"\t\t  OFFERING_GROUP VARCHAR(50)  , \r\n",
							"\t\t  BUS_MEASMNT_DIV_KEY INTEGER  , \r\n",
							"\t\t  BUS_MEASMNT_DIV_CD VARCHAR(50)  , \r\n",
							"\t\t  BUS_MEASMNT_DIV_DESC VARCHAR(50)  , \r\n",
							"\t\t  LOB_KEY INTEGER  , \r\n",
							"\t\t  LOB_CD VARCHAR(50)  , \r\n",
							"\t\t  LOB_DESC VARCHAR(50)  , \r\n",
							"\t\t  BUSINESS_UNIT_KEY INTEGER  , \r\n",
							"\t\t  BUSINESS_UNIT_ID VARCHAR(50)  , \r\n",
							"\t\t  BUSINESS_UNIT_DESC VARCHAR(100)  , \r\n",
							"\t\t  CURRENT_IND VARCHAR(1) , \r\n",
							"\t\t  EXTRACT_DT TIMESTAMP  , \r\n",
							"\t\t  REC_START_DT TIMESTAMP  , \r\n",
							"\t\t  REC_END_DT TIMESTAMP  , \r\n",
							"\t\t  SOURCE_SYSTEM VARCHAR(50)  , \r\n",
							"\t\t  REC_CHECKSUM VARCHAR(32)  , \r\n",
							"\t\t  REC_STATUS VARCHAR(1)  , \r\n",
							"\t\t  IMG_LST_UPD_DT TIMESTAMP  , \r\n",
							"\t\t  IMG_CREATED_DT TIMESTAMP  , \r\n",
							"\t\t  OFFERING_COMPNT_STATUS_CD VARCHAR(10)  , \r\n",
							"\t\t  OFFERING_COMPNT_STATUS_DESC VARCHAR(30)  , \r\n",
							"\t\t  OFFERING_AREA_CODE VARCHAR(30)  , \r\n",
							"\t\t  OFFERING_AREA_DESC VARCHAR(50)  , \r\n",
							"\t\t  SERVICE_GROUP_CODE VARCHAR(30)  , \r\n",
							"\t\t  SERVICE_GROUP_DESC VARCHAR(50)  , \r\n",
							"\t\t  BRAND_CODE VARCHAR(30)  , \r\n",
							"\t\t  BRAND_DESC VARCHAR(50)  , \r\n",
							"\t\t  SERVICE_GROUP_KEY INTEGER   ,\r\n",
							"          DATA_IND varchar(10)  ,\r\n",
							"\t      ACTIVE_IN_SOURCE_IND char(1) )   \r\n",
							"USING DELTA\r\n",
							"PARTITIONED BY (CURRENT_IND)\r\n",
							"-- specify data lake folder location\r\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/deltalake/dimensions/DHT_SERVICE' "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DROP TABLE etlhubconfirmed.DHT_SERVICE"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select * FROM etlhubconfirmed.DHT_SERVICE"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Delta lake table create statement')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5fdb7c1a-fec5-46be-be57-555e085933e5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--test1\n",
							"-- DBXDH.DHT_PROJECT definition\n",
							"\n",
							"--drop table etlhubConfirmed.customer_dimension;\n",
							"\n",
							"-- Create Delta Lake table, define schema and location\n",
							"/*\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.customer_dimension (\n",
							"    CUSTOMER_KEY INT NOT NULL,\n",
							"\tVERSION INT ,\n",
							"\tCUSTOMER_NO STRING ,\n",
							"\tFINANCIAL_COUNTRY_CD varchar(10) ,\n",
							"\tGBG_ID varchar(10)  ,\n",
							"\tCUSTOMER_NAME varchar(30)  ,\n",
							"\tCURRENT_IND varchar(1)  ,\n",
							"\tEXTRACT_DT TIMESTAMP ,\n",
							"\tREC_START_DT TIMESTAMP ,\n",
							"\tREC_END_DT TIMESTAMP ,\n",
							"\tSOURCE_SYSTEM varchar(50)  ,\n",
							"\tREC_CHECKSUM varchar(32)  ,\n",
							"\tREC_STATUS varchar(1)  ,\n",
							"\tIMG_LST_UPD_DT TIMESTAMP NOT NULL,\n",
							"\tIMG_CREATED_DT TIMESTAMP NOT NULL,\n",
							"\tDATA_IND varchar(10)  ,\n",
							"\tACTIVE_IN_SOURCE_IND char(1)  \n",
							")\n",
							"USING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"-- specify data lake folder location\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer'\n",
							"\n",
							"*/\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"--rename INTEGER with INT\n",
							"--replace VARCHAR, CHAR as STRING\n",
							"--Default is not supported\n",
							"CREATE TABLE IF NOT EXISTS ETLHUBCONFIRMED.DHT_PROJECT  (\n",
							"\t\t  PROJECT_KEY INT NOT NULL , \n",
							"\t\t  PROJECT_VERSION INT NOT NULL , \n",
							"\t\t  PROJECT_ID STRING  , \n",
							"\t\t  FINANCIAL_COUNTRY_CD STRING , \n",
							"\t\t  LEDGER_CD STRING  , \n",
							"\t\t  OFFERING_COMPONENT_CD STRING  , \n",
							"\t\t  OPPORTUNITY_NUM STRING  , \n",
							"\t\t  PROJECT_DESC STRING  , \n",
							"\t\t  SIGNINGS_CD STRING  , \n",
							"\t\t  SIGNINGS_DESC STRING  , \n",
							"\t\t  BUSINESS_TYPE_CD STRING , \n",
							"\t\t  BUSINESS_TYPE_DESC STRING , \n",
							"\t\t  PROJECT_STATUS_CD STRING  , \n",
							"\t\t  PROJECT_STATUS_DESC STRING  , \n",
							"\t\t  PROJECT_CUSTOMER_NO STRING   , \n",
							"\t\t  PROJECT_CREATION_DATE DATE  , \n",
							"\t\t  ACCOUTNING_DIVISION STRING  , \n",
							"\t\t  RESPONSIBLE_SERV_OFFICE STRING , \n",
							"\t\t  PROJECT_LOCAL_CURRENCY STRING , \n",
							"\t\t  LOCAL_ATTRIBUTE07 STRING , \n",
							"\t\t  CURRENT_IND STRING  NOT NULL , \n",
							"\t\t  EXTRACT_DT TIMESTAMP  , \n",
							"\t\t  REC_START_DT TIMESTAMP  , \n",
							"\t\t  REC_END_DT TIMESTAMP  , \n",
							"\t\t  SOURCE_SYSTEM STRING  , \n",
							"\t\t  REC_CHECKSUM STRING , \n",
							"\t\t  REC_STATUS STRING  , \n",
							"\t\t  IMG_LST_UPD_DT TIMESTAMP, -- NOT NULL GENERATED ALWAYS AS CURRENT TIMESTAMP , \n",
							"\t\t  IMG_CREATED_DT TIMESTAMP, -- NOT NULL GENERATED ALWAYS AS CURRENT TIMESTAMP , \n",
							"\t\t  SERVICE_TYPE_CD STRING  , \n",
							"\t\t  SERVICE_TYPE_DESC STRING , \n",
							"\t\t  RATE_TYPE_CD STRING , \n",
							"\t\t  RATE_TYPE_DESC STRING , \n",
							"\t\t  CHARGE_CD STRING  , \n",
							"\t\t  CHARGE_TO_SERVICE_OFFICE_CD STRING   , \n",
							"\t\t  CHARGE_TO_SERVICE_OFFICE_DESC STRING   , \n",
							"\t\t  CUSTOMER_PROJECT_DESC STRING  , \n",
							"\t\t  DESCRIPTION_CD STRING  , \n",
							"\t\t  PROJECT_TITLE STRING , \n",
							"\t\t  CUSTOMER_TYPE_CD STRING , \n",
							"\t\t  CUSTOMER_TYPE_DESC STRING , \n",
							"\t\t  SAP_STATUS_CD STRING , \n",
							"\t\t  DATA_IND STRING   , \n",
							"\t\t  CONTACT_NM STRING   , \n",
							"\t\t  LABOR_CLAIM_IND STRING   , \n",
							"\t\t  COMMENT_TEXT STRING   , \n",
							"\t\t  BID_AND_PROPOSAL_PROJECT_IND STRING   , \n",
							"\t\t  SIGNINGS_EXCEPTION_REASON_CODE STRING , \n",
							"\t\t  CHANNEL_INDICATOR STRING,\n",
							"          ACTIVE_IN_SOURCE_IND STRING  )   \n",
							"\t\t  USING DELTA\n",
							"\t\t  PARTITIONED BY (PROJECT_KEY)\n",
							"\t\t  LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/data/project_dimension'\n",
							" ;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 3')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b641cdfd-41dd-4bc0-8071-55a0c4528a89"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"import pyodbc\r\n",
							"from io import StringIO\r\n",
							"from pyspark.sql.functions import md5, concat_ws\r\n",
							"from sqlite3 import connect\r\n",
							"conn = connect(':memory:')\r\n",
							"columns = [\"SIEBEL_SALES_STAGE_NAME\",\"SSM_STEP_NO\",\"SSM_STEP_NAME\"]\r\n",
							"sas_url = \"https://adls4fsoetlhubdevuseast.blob.core.windows.net/project/sell_cycle.csv?sp=r&st=2022-05-11T09:48:50Z&se=2022-05-11T17:48:50Z&spr=https&sv=2020-08-04&sr=c&sig=ScsDhYGM4HWdCD8kMSMmLfY7Pex8jvmc02LvGatfwPI%3D\"\r\n",
							"blob_client = BlobClient.from_blob_url(sas_url)\r\n",
							"blob_data = blob_client.download_blob()\r\n",
							"df = pd.read_csv(StringIO(blob_data.content_as_text()))\r\n",
							"#print(df)\r\n",
							"sell_cycleDF=spark.createDataFrame(df)\r\n",
							"col_list=[]\r\n",
							"for i in sell_cycleDF.columns:\r\n",
							"    col_list.append(i)\r\n",
							"    #print (col_list)\r\n",
							"sell_cyclenkDF = sell_cycleDF.withColumn(\"nk_hash\",md5(\"SIEBEL_SALES_STAGE_CODE\"))\r\n",
							"sell_cyclecolhashDF = sell_cyclenkDF.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\r\n",
							"#sell_cyclecolhashDF.show()\r\n",
							"sell_cyclecolhashDF.createOrReplaceTempView(\"sellcyclesrc\")\r\n",
							"server = 'sqlserver-kyn-001494-dev-eus-001.database.windows.net' \r\n",
							"database = 'sqldb-etlhub-confirmed' \r\n",
							"username = 'sqladminuser' \r\n",
							"password = 'try2find$5' \r\n",
							"cnxn = pyodbc.connect('DRIVER={com.microsoft.sqlserver.jdbc.SQLServerDriver};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\r\n",
							"#sellcycletgtDF = spark.read.format(\"jdbc\") \\\r\n",
							" #   .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"  #  .option(\"query\", \"SELECT CONVERT(VARCHAR(32),HashBytes('MD5', SIEBEL_SALES_STAGE_CODE),2) as existing_nk_hash,     CONVERT(VARCHAR(32),HashBytes('MD5', SIEBEL_SALES_STAGE_NAME+SSM_STEP_NO+SSM_STEP_NAME),2) as existing_column_hash,tgt.* FROM DBXDH.DHTS_SELL_CYCLE as tgt\") \\\r\n",
							"   # .option(\"user\", \"sqladminuser\") \\\r\n",
							"    #.option(\"password\", \"try2find$5\") \\\r\n",
							"    #.option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    #.load()\r\n",
							"sellcycletgtDF.show()\r\n",
							"sellcycletgtDF.createOrReplaceTempView(\"sellcycletgt\")\r\n",
							"#insertsellcycle=pd.read_sql(\"select a.SIEBEL_SALES_STAGE_CODE,a.SIEBEL_SALES_STAGE_NAME,a.SSM_STEP_NO,a.SSM_STEP_NAME from sellcyclesrc a left join  sellcycletgt b on a.nk_hash = b.existing_nk_hash where b.existing_nk_hash is null\",conn)\r\n",
							"#insertsellcycledf=spark.createDataFrame(insertsellcycle)\r\n",
							"#insertsellcycledf.show()\r\n",
							"#%%sql\r\n",
							"#select * from sellcyclesrc"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 4')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "405445b7-1887-422a-991e-e29084333dc4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"#import required modules\r\n",
							"from pyspark import SparkConf, SparkContext , SQLContext\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql import SQLContext\r\n",
							" \r\n",
							"#Create spark configuration object\r\n",
							"conf = SparkConf()\r\n",
							"conf.setMaster(\"local\").setAppName(\"My app\")\r\n",
							" \r\n",
							"#Create spark context and sparksession\r\n",
							"sc = SparkContext.getOrCreate(conf=conf)\r\n",
							"spark = SparkSession(sc)\r\n",
							"#set variable to be used to connect the database\r\n",
							"database = \"sqldb-etlhub-confirmed\"\r\n",
							"table = \"DBXDH.DHTS_SELL_CYCLE\"\r\n",
							"user = \"sqladminuser\"\r\n",
							"password  = \"try2find$5\"\r\n",
							" \r\n",
							"#read table data into a spark dataframe\r\n",
							"jdbcDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName={database};\") \\\r\n",
							"    .option(\"dbtable\", table) \\\r\n",
							"    .option(\"user\", user) \\\r\n",
							"    .option(\"password\", password) \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()\r\n",
							"#show the data loaded into dataframe\r\n",
							"#jdbcDF.show()\r\n",
							"\r\n",
							"%%sql\r\n",
							"select * from DBXDH.DHTS_SELL_CYCLE\r\n",
							"\r\n",
							"#sqlContext.sql(select * from DBXDH.DHTS_SELL_CYCLE)\r\n",
							" #print(\"update successful\")\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession \r\n",
							"from pyspark.sql.types import * \r\n",
							"from pyspark.sql.functions import when\r\n",
							"from pyspark.sql.context import SQLContext\r\n",
							"\r\n",
							"# Primary storage info \r\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \r\n",
							"container_name = 'project' # fill in your container name \r\n",
							"relative_path = '/' # fill in your relative folder path \r\n",
							"\r\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net%s' % (container_name, account_name, relative_path) \r\n",
							"#adls_path='https://adls4fsoetlhubdevuseast.dfs.core.windows.net/project/sell_cycle_2022.csv'\r\n",
							"print('Primary storage account path: ' + adls_path) \r\n",
							"\r\n",
							"# Read a csv file \r\n",
							"csv_path = adls_path + 'sell_cycle_2022.csv' \r\n",
							"#csv_path='https://adls4fsoetlhubdevuseast.blob.core.windows.net/project/sell_cycle_2022.csv'\r\n",
							"df_csv = spark.read.csv(csv_path, header = 'true')\r\n",
							"#df_csv.show(20,False)\r\n",
							"\r\n",
							"#set variable to be used to connect the database\r\n",
							"database = \"sqldb-etlhub-confirmed\"\r\n",
							"table = \"DBXDH.DHTS_SELL_CYCLE\"\r\n",
							"user = \"sqladminuser\"\r\n",
							"password  = \"try2find$5\"\r\n",
							" \r\n",
							"sqlContext.sql(\"select * from DBXDH.DHTS_SELL_CYCLE\")\r\n",
							"#sqlContext.sql(\"select * from DBXDH.DHTS_SELL_CYCLE\")\r\n",
							" #print(\"update successful\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 33
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 5')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "479488f5-cd51-442a-8e69-ac01e706e0c4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"from pyspark.sql.functions import md5, concat_ws\r\n",
							"from sqlite3 import connect\r\n",
							"from pyspark.sql import functions as F\r\n",
							"conn = connect(':memory:')\r\n",
							"natural_key=\"CUSTOMER_NO\"\r\n",
							"columns = [\"FINANCIAL_COUNTRY_CD\",\"CUSTOMER_DESC\",\"GBG_ID\"]\r\n",
							"sas_url = \"https://adls4fsoetlhubdevuseast.blob.core.windows.net/customer/customer_data.csv?sv=2020-10-02&st=2022-05-15T14%3A59%3A08Z&se=2023-05-16T14%3A59%3A00Z&sr=b&sp=r&sig=RYE103C28iVzI4%2BTmiDyMqJhGGNqBooxZgUc4LITF4U%3D\"\r\n",
							"blob_client = BlobClient.from_blob_url(sas_url)\r\n",
							"blob_data = blob_client.download_blob()\r\n",
							"incrementalData = pd.read_csv(StringIO(blob_data.content_as_text()))\r\n",
							"#print(df)\r\n",
							"incrementalData_DF=spark.createDataFrame(incrementalData)\r\n",
							"col_list=[]\r\n",
							"for i in incrementalData_DF.columns:\r\n",
							"    col_list.append(i)\r\n",
							"    #print (col_list)\r\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\r\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\r\n",
							"#sell_cyclecolhashDF.show()\r\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\r\n",
							"\r\n",
							"#jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;database=dsqlpoolKyn001494DevEtlHubEUS001;user=undefined@asa-kyn-001494-dev-eus-001;password={your_password_here};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\r\n",
							"existingDataDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\r\n",
							"    .option(\"query\", \"SELECT MAX(CUSTOMER_KEY) OVER (ORDER BY CUSTOMER_KEY  ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) AS MAX_KEY, LOWER(CONVERT(VARCHAR(32),HashBytes('MD5', CUSTOMER_NO),2)) as existing_nk_hash,tgt.* FROM DBXDH.DHT_CUSTOMER as tgt WHERE CURRENT_IND='Y'\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()\r\n",
							"#existingDataDF1 = existingDataDF.select([F.col(c).alias(\"`\"'existing_'+c+\"`\") for c in existingDataDF.columns])\r\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\r\n",
							"#existingDataDF1.printSchema()\r\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF')\r\n",
							"\r\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_KEY, \"fullouter\") \r\n",
							"fullJoin1.createOrReplaceTempView('fullJoin')\r\n",
							"\r\n",
							"#Insert for New rows\r\n",
							"\r\n",
							"fullJoin2=sqlContext.sql(\"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \\\r\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\r\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \\\r\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null\")\r\n",
							"fullJoin2.createOrReplaceTempView('fullJoin2')\r\n",
							"\r\n",
							"#insert new rows into database\r\n",
							"\r\n",
							"fullJoin2.write \\\r\n",
							"        .format(\"jdbc\") \\\r\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\r\n",
							"        .option(\"user\", \"sqladminuser\") \\\r\n",
							"        .option(\"password\", \"try2find$5\") \\\r\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\r\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\r\n",
							"        .mode(\"append\") \\\r\n",
							"        .save()\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 6')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d675b475-c43e-4a62-9394-92c8c1ab17a0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"#set the data lake file location:\r\n",
							"file_location = \"abfss://project@adls4fsoetlhubdevuseast.dfs.core.windows.net/sell_cycle_2022.csv\"\r\n",
							" \r\n",
							"#read in the data to dataframe df\r\n",
							"df = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").option(\"delimiter\",\",\").load(file_location)\r\n",
							" \r\n",
							"#display the dataframe\r\n",
							"display(df)\r\n",
							"\r\n",
							"df.printSchema()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"#set the data lake file location:\r\n",
							"file_location = \"abfss://project@adls4fsoetlhubdevuseast.dfs.core.windows.net/sell_cycle_2022.csv\"\r\n",
							" \r\n",
							"#read in the data to dataframe df\r\n",
							"df = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").option(\"delimiter\",\",\").load(file_location)\r\n",
							" \r\n",
							"#display the dataframe\r\n",
							"display(df)\r\n",
							"\r\n",
							"df.printSchema()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"#import necessary python libraries\r\n",
							"\r\n",
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"from pyspark.sql.functions import md5, concat_ws\r\n",
							"from sqlite3 import connect\r\n",
							"from pyspark.sql import functions as F\r\n",
							"#conn = connect(':memory:')\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession \r\n",
							"from pyspark.sql.types import * \r\n",
							"\r\n",
							"\r\n",
							"CREATE TABLE etlhubConfirmed.SELL_CYCLE_2022\r\n",
							"     (SIEBEL_SALES_STAGE_CODE string,  \r\n",
							"      SIEBEL_SALES_STAGE_NAME string,  \r\n",
							"      SSM_STEP_NO string,  \r\n",
							"      SSM_STEP_NAME string)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%sql\r\n",
							"CREATE TABLE IF NOT EXISTS DBXDH.DHTS_SELL_CYCLE_2022\r\n",
							"USING CSV\r\n",
							"LOCATION 'abfss://project@adls4fsoetlhubdevuseast.dfs.core.windows.net/sell_cycle_2022.csv'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 8')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0bca0c97-6475-4fd5-a8b9-7fea7c68ee63"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"# this is latest commit 11th Jul 2022 8:29 PM IST\n",
							"#\n",
							"spark.sql(\"CREATE TABLE etlhubconfirmed.sell_cycle (SIEBEL_SALES_STAGE_CODE string, SIEBEL_SALES_STAGE_NAME string, SSM_STEP_NO string, SSM_STEP_NAME string)\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 9')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e7090b57-bd16-4826-bfd5-7ae0c1d9ece2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"\n",
							"spark.sql(\"SELECT CURRENT_IND,COUNT(*) FROM etlhubconfirmed.customer_dimension GROUP BY CURRENT_IND\")\n",
							"\n",
							"spark.stop()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook_R')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "19142ba3-cf3f-4f0a-a0a6-cc18031d89f8"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1",
						"state": {
							"d39a2370-4c1d-46ed-9a35-67499bf08ce8": {
								"type": "Synapse.DataFrame",
								"sync_state": {
									"table": {
										"rows": [
											{
												"0": "A",
												"1": "22",
												"2": "45000"
											},
											{
												"0": "B",
												"1": "35",
												"2": "65000"
											},
											{
												"0": "C",
												"1": "50",
												"2": "85000"
											}
										],
										"schema": [
											{
												"key": "0",
												"name": "state",
												"type": "string"
											},
											{
												"key": "1",
												"name": "age",
												"type": "bigint"
											},
											{
												"key": "2",
												"name": "salary",
												"type": "bigint"
											}
										],
										"truncated": false
									},
									"isSummary": false,
									"language": "scala"
								},
								"persist_state": {
									"view": {
										"type": "details",
										"chartOptions": {
											"chartType": "bar",
											"aggregationType": "sum",
											"categoryFieldKeys": [
												"0"
											],
											"seriesFieldKeys": [
												"1"
											],
											"isStacked": false
										}
									}
								}
							}
						}
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"new_rows = [('A',22, 45000),(\"B\",35,65000) ,(\"C\",50,85000)]\r\n",
							"demo_df = spark.createDataFrame(new_rows, ['state', 'age', 'salary'])\r\n",
							"demo_df.show() \r\n",
							"demo_df.createOrReplaceTempView('demo_df')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# demo_df.createOrReplaceTempView('demo_df')\r\n",
							" # demo_df.write.csv('demo_df', mode='overwrite')\r\n",
							" # demo_df.write.parquet('abfss://<<TheNameOfAStorageAccountFileSystem>>@<<TheNameOfAStorageAccount>>.dfs.core.windows.net/demodata/demo_df', mode='overwrite')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Language Change**"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(spark.sql('SELECT * FROM demo_df'))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from demo_df;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"new_rows = [('D',42, 55000),(\"E\",65,75000) ,(\"F\",70,95000)]\r\n",
							"demo_dfs = spark.createDataFrame(new_rows, ['state', 'age', 'salary'])\r\n",
							"demo_dfs.show() \r\n",
							"demo_dfs.createOrReplaceTempView('demo_dfs')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from demo_df full join demo_dfs using(state,age,salary) order by state asc;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook_Siva2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Siva"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "bc9a73e8-16ca-4335-821d-6b5e19c920d4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Final Code SCD Type2"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"conn = connect(':memory:')\n",
							"natural_key=\"CUSTOMER_NO\"\n",
							"columns = [\"FINANCIAL_COUNTRY_CD\",\"CUSTOMER_DESC\",\"GBG_ID\"]\n",
							"sas_url = \"https://adls4fsoetlhubdevuseast.blob.core.windows.net/customer/customer_data.csv?sv=2020-10-02&st=2022-05-15T14%3A59%3A08Z&se=2023-05-16T14%3A59%3A00Z&sr=b&sp=r&sig=RYE103C28iVzI4%2BTmiDyMqJhGGNqBooxZgUc4LITF4U%3D\"\n",
							"blob_client = BlobClient.from_blob_url(sas_url)\n",
							"blob_data = blob_client.download_blob()\n",
							"incrementalData = pd.read_csv(StringIO(blob_data.content_as_text()))\n",
							"#print(df)\n",
							"incrementalData_DF=spark.createDataFrame(incrementalData)\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"    #print (col_list)\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\n",
							"\n",
							"#jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;database=dsqlpoolKyn001494DevEtlHubEUS001;user=undefined@asa-kyn-001494-dev-eus-001;password={your_password_here};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\n",
							"existingDataDF = spark.read.format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"    .option(\"query\", \"SELECT MAX(CUSTOMER_KEY) OVER (ORDER BY CUSTOMER_KEY  ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) AS MAX_KEY, LOWER(CONVERT(VARCHAR(32),HashBytes('MD5', CUSTOMER_NO),2)) as existing_nk_hash,tgt.* FROM DBXDH.DHT_CUSTOMER as tgt WHERE CURRENT_IND='Y'\") \\\n",
							"    .option(\"user\", \"sqladminuser\") \\\n",
							"    .option(\"password\", \"try2find$5\") \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .load()\n",
							"#existingDataDF1 = existingDataDF.select([F.col(c).alias(\"`\"'existing_'+c+\"`\") for c in existingDataDF.columns])\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"#existingDataDF1.printSchema()\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_KEY, \"fullouter\") \n",
							"fullJoin1.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"#Insert for New rows\n",
							"\n",
							"fullJoin2=sqlContext.sql(\"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \\\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null\")\n",
							"fullJoin2.createOrReplaceTempView('fullJoin2')\n",
							"\n",
							"#insert new rows into database\n",
							"\n",
							"fullJoin2.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--select * from existingDataDF\n",
							"\n",
							"select * from fullJoin2"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--ALL\n",
							"SELECT * FROM FULLJOIN;\n",
							"\n",
							"--No change records, ignore\n",
							"select * from fullJoin WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							"\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"select * from fullJoin WHERE WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--Soft deletes or rows no longer active in source\n",
							"select * from fullJoin WHERE WHERE nk_hash is null;\n",
							"\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1 \n",
							"select * from fullJoin where existing_existing_nk_hash is null;\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--INSERTS OR NEW ROWS\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION ,\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT,\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM,\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND,\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#test referance\n",
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"query\", \"INSERT INTO DBXDH.DHT_CUSTOMER1 \\\n",
							"        select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION , \\\n",
							"        CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"        CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"        'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"        'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"        from fullJoin A WHERE existing_existing_nk_hash is null\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"overwrite\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"df.write.mode(\"overwrite\") \\\n",
							"    .format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://localhost:1433;databaseName={database};\") \\\n",
							"    .option(\"dbtable\", table) \\\n",
							"    .option(\"user\", user) \\\n",
							"    .option(\"password\", password) \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook_Siva3')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Siva"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "390349d1-5b8a-47c2-90a4-36249c09414a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Try Delta Lake"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from notebookutils import mssparkutils\n",
							"mssparkutils.fs.mount(\n",
							"    \"abfss://customer@adls4fsoetlhubdevuseast.dfs.core.windows.net\",\n",
							"    \"/etlhubadls6\",\n",
							"    {\"linkedService\":\"ls_adls_project_dimension\"}\n",
							")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"jobId=mssparkutils.env.getJobId()\n",
							"print(jobId)\n",
							"df=spark.read.load('synfs:/' + jobId + '/etlhubadls6/customer_data.csv',format='csv',header=True)\n",
							"df.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Define column headers\n",
							"columns = [\"Employee\",\"Salary\"]\n",
							"\n",
							"# Define data for original dataframe\n",
							"empOriginal = [(\"Employee_1\",50000),(\"Employee_2\",55000)]\n",
							"\n",
							"# Define data for updates dataframe\n",
							"empUpdates = [(\"Employee_1\",50000),(\"Employee_2\",60000),(\"Employee_3\",55000)]\n",
							"\n",
							"# Create dataframe with orignial employee data\n",
							"dfOriginal = spark.createDataFrame(data = empOriginal,schema = columns)\n",
							"\n",
							"# Create dataframe with updated employee data\n",
							"dfUpdates = spark.createDataFrame(data = empUpdates,schema = columns)\n",
							"\n",
							"# Display dfOriginal\n",
							"dfOriginal.show()\n",
							"\n",
							"# Display dfUpdates\n",
							"dfUpdates.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create temp views of both the dfOriginal and dfUpdates dataframes \n",
							"# so that we can easily query them with Spark SQL later\n",
							"\n",
							"#dfOriginal\n",
							"dfOriginal.createOrReplaceTempView('Employee_Original')\n",
							"\n",
							"#dfUpdates\n",
							"dfUpdates.createOrReplaceTempView('Employee_Updates')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"-- Create Delta Lake table, define schema and location\n",
							"CREATE TABLE DELTA_Employees (\n",
							"  Employee STRING NOT NULL,\n",
							"  Salary INT NOT NULL,\n",
							"  BeginDate DATE NOT NULL,\n",
							"  EndDate DATE NOT NULL,\n",
							"  CurrentRecord INT NOT NULL \n",
							")\n",
							"USING DELTA\n",
							"-- specify data lake folder location\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/dl1'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"df = spark.sql(\"SELECT * FROM nyctaxi.passengercountstats\")\n",
							"df = df.repartition(1) # This ensures we'll get a single file during write()\n",
							"df.write.mode(\"overwrite\").csv(\"/NYCTaxi/PassengerCountStats_csvformat\")\n",
							"df.write.mode(\"overwrite\").parquet(\"/NYCTaxi/PassengerCountStats_parquetformat\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"CREATE TABLE IF NOT EXISTS default.people10m (\n",
							"  id INT,\n",
							"  firstName STRING,\n",
							"  middleName STRING,\n",
							"  lastName STRING,\n",
							"  gender STRING,\n",
							"  birthDate TIMESTAMP,\n",
							"  ssn STRING,\n",
							"  salary INT\n",
							") USING DELTA"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook_insert')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "166de406-4008-4abd-b552-61725fa7fba1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"from pyspark.sql.functions import md5, concat_ws\r\n",
							"from sqlite3 import connect\r\n",
							"conn = connect(':memory:')\r\n",
							"columns = [\"SIEBEL_SALES_STAGE_NAME\",\"SSM_STEP_NO\",\"SSM_STEP_NAME\"]\r\n",
							"sas_url = \"https://adls4fsoetlhubdevuseast.blob.core.windows.net/project/sell_cycle_2022.csv?sp=r&st=2022-05-16T11:04:46Z&se=2022-05-16T19:04:46Z&spr=https&sv=2020-08-04&sr=b&sig=udm7PnviOcxI9VLp6Kuvyn9AjghiU1sAobpG7EdlNvg%3D\"\r\n",
							"blob_client = BlobClient.from_blob_url(sas_url)\r\n",
							"blob_data = blob_client.download_blob()\r\n",
							"df = pd.read_csv(StringIO(blob_data.content_as_text()))\r\n",
							"df.show(10,False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.session import SparkSession\r\n",
							"\r\n",
							"spark = SparkSession \\\r\n",
							".builder \\\r\n",
							".appName(\"SparkUpsert\") \\\r\n",
							".master(\"Local[*]\") \\\r\n",
							".getOrCreate\r\n",
							"\r\n",
							"df_csv = spark \\\r\n",
							"    .read \\\r\n",
							"    .option(\"header\",\"true\") \\\r\n",
							"    .csv(\"project/sell_cycle_2022.csv\")\r\n",
							"\r\n",
							"df_csv.show(10,False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#import required modules\r\n",
							"from pyspark import SparkConf, SparkContext , SQLContext\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql import SQLContext\r\n",
							" \r\n",
							"#Create spark configuration object\r\n",
							"conf = SparkConf()\r\n",
							"conf.setMaster(\"local\").setAppName(\"My app\")\r\n",
							" \r\n",
							"#Create spark context and sparksession\r\n",
							"sc = SparkContext.getOrCreate(conf=conf)\r\n",
							"spark = SparkSession(sc)\r\n",
							"#set variable to be used to connect the database\r\n",
							"database = \"sqldb-etlhub-confirmed\"\r\n",
							"table = \"DBXDH.DHTS_SELL_CYCLE\"\r\n",
							"user = \"sqladminuser\"\r\n",
							"password  = \"try2find$5\"\r\n",
							" \r\n",
							"#read table data into a spark dataframe\r\n",
							"jdbcDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName={database};\") \\\r\n",
							"    .option(\"dbtable\", table) \\\r\n",
							"    .option(\"user\", user) \\\r\n",
							"    .option(\"password\", password) \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()\r\n",
							"#show the data loaded into dataframe\r\n",
							"#jdbcDF.show()\r\n",
							"\r\n",
							"%%sql\r\n",
							"select * from DBXDH.DHTS_SELL_CYCLE\r\n",
							"\r\n",
							"#sqlContext.sql(select * from DBXDH.DHTS_SELL_CYCLE)\r\n",
							" #print(\"update successful\")\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession \r\n",
							"from pyspark.sql.types import * \r\n",
							"from pyspark.sql.functions import when\r\n",
							"\r\n",
							"# Primary storage info \r\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \r\n",
							"container_name = 'project' # fill in your container name \r\n",
							"relative_path = '/' # fill in your relative folder path \r\n",
							"\r\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net%s' % (container_name, account_name, relative_path) \r\n",
							"#adls_path='https://adls4fsoetlhubdevuseast.dfs.core.windows.net/project/sell_cycle_2022.csv'\r\n",
							"print('Primary storage account path: ' + adls_path) \r\n",
							"\r\n",
							"# Read a csv file \r\n",
							"csv_path = adls_path + 'sell_cycle_2022.csv' \r\n",
							"#csv_path='https://adls4fsoetlhubdevuseast.blob.core.windows.net/project/sell_cycle_2022.csv'\r\n",
							"df_csv = spark.read.csv(csv_path, header = 'true')\r\n",
							"df_csv.show(20,False)\r\n",
							"\r\n",
							"#set variable to be used to connect the database\r\n",
							"database = \"sqldb-etlhub-confirmed\"\r\n",
							"table = \"DBXDH.DHTS_SELL_CYCLE\"\r\n",
							"user = \"sqladminuser\"\r\n",
							"password  = \"try2find$5\"\r\n",
							" \r\n",
							"#write the dataframe into a sql table\r\n",
							"df_csv.write.mode(\"append\") \\\r\n",
							"    .format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName={database};\") \\\r\n",
							"    .option(\"dbtable\", table) \\\r\n",
							"    .option(\"user\", user) \\\r\n",
							"    .option(\"password\", password) \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .save()\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 51
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook_siva')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Siva"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a6ed62c0-bd62-49e6-9df2-663df53c4d97"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1",
						"state": {
							"3009228f-3872-4c4a-8d8a-fa5f8f9fd19f": {
								"type": "Synapse.DataFrame",
								"sync_state": {
									"table": {
										"rows": [
											{
												"0": "EEI0001",
												"1": "602",
												"2": "NG",
												"3": "602EE00119",
												"4": "6950-94G",
												"5": "PX-WR1SFFU",
												"6": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"8": "B024",
												"9": "A",
												"10": "004324",
												"11": "2021-08-31",
												"12": "DD",
												"13": "006C",
												"14": "EUR",
												"16": "2020-01-30",
												"17": "2020-02-15",
												"18": "2023-03-31",
												"26": "EENIS00001",
												"27": "46",
												"28": "2021-09-29 18:32:46.558",
												"29": "IC",
												"30": "147500.00000",
												"31": "0.000",
												"32": "0.00",
												"33": "H",
												"34": "Z042",
												"35": "006C",
												"36": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"37": "        ",
												"38": "147500.00000",
												"40": "0.00000",
												"41": "0.00000",
												"43": "0.00000",
												"45": "LG",
												"46": "A"
											},
											{
												"0": "EEI0002",
												"1": "602",
												"2": "NG",
												"3": "IBM",
												"4": "6940-92A",
												"6": "Ţ�����@م������@Ö������@�����\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"7": "XS",
												"8": "B046",
												"9": "C",
												"10": "003550",
												"11": "2021-09-28",
												"12": "DD",
												"13": "006C",
												"14": "EUR",
												"16": "2021-09-01",
												"17": "2021-09-01",
												"18": "2021-12-31",
												"26": "EENIS00002",
												"27": "30",
												"28": "2022-03-04 20:58:18.790",
												"29": "IC",
												"30": "1517.00000",
												"31": "0.000",
												"32": "0.00",
												"33": "H",
												"35": "006C",
												"36": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"37": "        ",
												"38": "1517.00000",
												"39": "IBM",
												"40": "0.00000",
												"41": "0.00000",
												"43": "0.00000",
												"45": "LG",
												"46": "A"
											},
											{
												"0": "LVI0001",
												"1": "608",
												"2": "NG",
												"3": "608CEMEX",
												"4": "6940-98A",
												"6": "�@�����@⥃�ǣǖ�\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"8": "E011",
												"9": "A",
												"10": "004300",
												"11": "2021-08-31",
												"12": "DD",
												"13": "006O",
												"14": "EUR",
												"16": "2013-02-28",
												"17": "2014-11-24",
												"18": "2022-08-31",
												"26": "LVNIS00001",
												"27": "30",
												"28": "2021-09-16 11:42:34.627",
												"29": "IC",
												"30": "0.00000",
												"31": "0.000",
												"32": "0.00",
												"33": "H",
												"35": "006O",
												"36": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"37": "        ",
												"38": "0.00000",
												"40": "0.00000",
												"41": "0.00000",
												"43": "0.00000",
												"45": "LG",
												"46": "Z"
											},
											{
												"0": "LVI0002",
												"1": "608",
												"2": "NG",
												"3": "608CEMEX",
												"4": "6941-97X",
												"6": "���@���@����@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@��������a�������\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"8": "B048",
												"9": "A",
												"10": "004300",
												"11": "2021-08-31",
												"12": "DD",
												"13": "006O",
												"14": "EUR",
												"16": "2013-02-28",
												"17": "2014-08-12",
												"18": "2022-08-22",
												"26": "LVNIS00001",
												"27": "30",
												"28": "2021-09-16 11:42:34.627",
												"29": "IC",
												"30": "0.00000",
												"31": "0.000",
												"32": "0.00",
												"33": "H",
												"34": "S011",
												"35": "006O",
												"36": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"37": "        ",
												"38": "0.00000",
												"40": "0.00000",
												"41": "0.00000",
												"43": "0.00000",
												"45": "LG",
												"46": "Z"
											},
											{
												"0": "LVI0003",
												"1": "608",
												"2": "NG",
												"3": "608CEMEX",
												"4": "6940-97K",
												"6": "���@���@¤��\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"7": "SR",
												"8": "B048",
												"9": "A",
												"10": "004300",
												"11": "2021-08-31",
												"12": "DD",
												"13": "006O",
												"14": "EUR",
												"16": "2013-02-28",
												"17": "2017-10-20",
												"18": "2022-08-31",
												"26": "LVNIS00001",
												"27": "30",
												"28": "2021-09-20 11:42:33.707",
												"29": "BE",
												"30": "22499.91000",
												"31": "0.000",
												"32": "0.00",
												"33": "H",
												"34": "S011",
												"35": "006O",
												"36": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"37": "        ",
												"38": "0.00000",
												"39": "608CEMEX",
												"40": "22499.91000",
												"41": "0.00000",
												"43": "0.00000",
												"45": "LG",
												"46": "Z"
											},
											{
												"0": "LVI0004",
												"1": "608",
												"2": "NG",
												"3": "608CEMEX",
												"4": "6940-97K",
												"6": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"7": "XS",
												"8": "E011",
												"9": "A",
												"10": "004300",
												"11": "2021-08-31",
												"12": "DD",
												"13": "006O",
												"14": "EUR",
												"16": "2013-02-28",
												"17": "2019-01-01",
												"18": "2022-08-31",
												"26": "LVNIS00001",
												"27": "30",
												"28": "2021-09-16 11:42:34.627",
												"29": "IC",
												"30": "0.00000",
												"31": "0.000",
												"32": "0.00",
												"33": "H",
												"34": "S011",
												"35": "006O",
												"36": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"37": "        ",
												"38": "0.00000",
												"40": "0.00000",
												"41": "0.00000",
												"43": "0.00000",
												"45": "LG",
												"46": "Z"
											},
											{
												"0": "LVI0005",
												"1": "608",
												"2": "NG",
												"3": "NEWCOLV00300124",
												"4": "6950-94G",
												"6": "�����������a�������a������a������aÅ��@⁣������@ę������@ĉ�������a����������@���\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"8": "B008",
												"9": "A",
												"10": "003560",
												"11": "2021-08-31",
												"12": "DD",
												"13": "0076",
												"14": "EUR",
												"16": "2017-06-06",
												"17": "2017-06-08",
												"18": "2022-08-31",
												"22": "608",
												"23": "NEWIS",
												"24": "000090",
												"25": "96DH9",
												"26": "LVNIS00002",
												"27": "46",
												"28": "2021-10-28 17:25:54.303",
												"29": "IC",
												"30": "0.00000",
												"31": "0.000",
												"32": "0.00",
												"33": "H",
												"34": "Z028",
												"35": "0076",
												"36": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"37": "        ",
												"38": "0.00000",
												"39": "608LV000217/L100460",
												"40": "0.00000",
												"41": "0.00000",
												"43": "0.00000",
												"45": "LG",
												"46": "Y"
											},
											{
												"0": "LVI0006",
												"1": "608",
												"2": "NG",
												"3": "NEWCOLV00300124",
												"4": "6950-94G",
												"6": "�����������a�������a������a������aÅ��@⁣������@ę������@ĉ�������a����������@���@@@@@@@@@@@@@@@@@@��@@����������\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"8": "B008",
												"9": "A",
												"10": "003560",
												"11": "2021-08-31",
												"12": "DD",
												"13": "0076",
												"14": "EUR",
												"16": "2017-06-06",
												"17": "2017-06-08",
												"18": "2022-08-31",
												"22": "608",
												"23": "NEWIS",
												"24": "000090",
												"25": "8M2Q6",
												"26": "LVNIS00002",
												"27": "46",
												"28": "2021-10-28 17:25:54.303",
												"29": "IC",
												"30": "520000.00000",
												"31": "0.000",
												"32": "0.00",
												"33": "H",
												"34": "Z028",
												"35": "0076",
												"36": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"37": "        ",
												"38": "520000.00000",
												"39": "608LV000217/L100461",
												"40": "0.00000",
												"41": "0.00000",
												"43": "0.00000",
												"45": "LG",
												"46": "Y"
											},
											{
												"0": "LVI0007",
												"1": "608",
												"2": "NG",
												"3": "NEWCOLV00300124",
												"4": "6950-94G",
												"6": "�����������@�����������@Ȗ�����@▓�����\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"8": "B008",
												"9": "A",
												"10": "003560",
												"11": "2021-08-31",
												"12": "DD",
												"13": "0076",
												"14": "EUR",
												"16": "2017-06-06",
												"17": "2019-01-01",
												"18": "2022-08-31",
												"22": "608",
												"23": "NEWIS",
												"24": "000090",
												"25": "96DH9",
												"26": "LVNIS00002",
												"27": "46",
												"28": "2021-10-28 17:25:54.303",
												"29": "IC",
												"30": "95680.00000",
												"31": "0.000",
												"32": "0.00",
												"33": "H",
												"34": "Z028",
												"35": "0076",
												"36": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
												"37": "        ",
												"38": "95680.00000",
												"39": "608LV000217",
												"40": "0.00000",
												"41": "0.00000",
												"43": "0.00000",
												"45": "LG",
												"46": "K"
											}
										],
										"schema": [
											{
												"key": "0",
												"name": "PROJECT_ID",
												"type": "string"
											},
											{
												"key": "1",
												"name": "FINANCIAL_COUNTRY_CD",
												"type": "string"
											},
											{
												"key": "2",
												"name": "LEDGER_CD",
												"type": "string"
											},
											{
												"key": "3",
												"name": "LEGAL_CONTRACT_ID",
												"type": "string"
											},
											{
												"key": "4",
												"name": "OFFERING_COMPONENT_CD",
												"type": "string"
											},
											{
												"key": "5",
												"name": "OPPORTUNITY_NUM",
												"type": "string"
											},
											{
												"key": "6",
												"name": "PROJECT_DESC",
												"type": "string"
											},
											{
												"key": "7",
												"name": "SIGNINGS_CD",
												"type": "string"
											},
											{
												"key": "8",
												"name": "BUSINESS_TYPE_CD",
												"type": "string"
											},
											{
												"key": "9",
												"name": "PROJECT_STATUS_CD",
												"type": "string"
											},
											{
												"key": "10",
												"name": "PROJECT_CUSTOMER_NO",
												"type": "string"
											},
											{
												"key": "11",
												"name": "PROJECT_CREATION_DATE",
												"type": "string"
											},
											{
												"key": "12",
												"name": "ACCOUTNING_DIVISION",
												"type": "string"
											},
											{
												"key": "13",
												"name": "RESPONSIBLE_SERV_OFFICE",
												"type": "string"
											},
											{
												"key": "14",
												"name": "PROJECT_LOCAL_CURRENCY",
												"type": "string"
											},
											{
												"key": "15",
												"name": "LOCAL_ATTRIBUTE07",
												"type": "string"
											},
											{
												"key": "16",
												"name": "CONTRACT_SIGNED_DATE",
												"type": "string"
											},
											{
												"key": "17",
												"name": "PROJECT_START_DATE",
												"type": "string"
											},
											{
												"key": "18",
												"name": "PROJECT_END_DATE",
												"type": "string"
											},
											{
												"key": "19",
												"name": "LOBPARTNERCOMPANY",
												"type": "string"
											},
											{
												"key": "20",
												"name": "LOBPARTNERCOUNTRY",
												"type": "string"
											},
											{
												"key": "21",
												"name": "LOBPARTNERSERNUM",
												"type": "string"
											},
											{
												"key": "22",
												"name": "PMCOUNTRY",
												"type": "string"
											},
											{
												"key": "23",
												"name": "PMCOMPANY",
												"type": "string"
											},
											{
												"key": "24",
												"name": "PMSERNUM",
												"type": "string"
											},
											{
												"key": "25",
												"name": "BUSINESS_PARTNER_ID",
												"type": "string"
											},
											{
												"key": "26",
												"name": "IBM_CONTRACT_NUM",
												"type": "string"
											},
											{
												"key": "27",
												"name": "BUS_MEASMNT_DIV_CD",
												"type": "string"
											},
											{
												"key": "28",
												"name": "LAST_UPDATE_DT",
												"type": "string"
											},
											{
												"key": "29",
												"name": "SERVICE_TYPE",
												"type": "string"
											},
											{
												"key": "30",
												"name": "TOTAL_TCV",
												"type": "string"
											},
											{
												"key": "31",
												"name": "TOTAL_UNITS",
												"type": "string"
											},
											{
												"key": "32",
												"name": "HOURS_PER_DAY",
												"type": "string"
											},
											{
												"key": "33",
												"name": "RATE_TYPE",
												"type": "string"
											},
											{
												"key": "34",
												"name": "CHARGE_CODE",
												"type": "string"
											},
											{
												"key": "35",
												"name": "CHARGE_TO_SERVOFFICE",
												"type": "string"
											},
											{
												"key": "36",
												"name": "CUSTOMER_PROJECT",
												"type": "string"
											},
											{
												"key": "37",
												"name": "DESCRIPTION_CODE",
												"type": "string"
											},
											{
												"key": "38",
												"name": "LABOR_AMOUNT",
												"type": "string"
											},
											{
												"key": "39",
												"name": "PROJECT_TITLE",
												"type": "string"
											},
											{
												"key": "40",
												"name": "SCHEDULED_CHARGES",
												"type": "string"
											},
											{
												"key": "41",
												"name": "NON_SCHEDULED_CHARGES",
												"type": "string"
											},
											{
												"key": "42",
												"name": "CUSTOMER_TYPE",
												"type": "string"
											},
											{
												"key": "43",
												"name": "TRAVEL_CHARGES",
												"type": "string"
											},
											{
												"key": "44",
												"name": "SAP_STAT_CD",
												"type": "string"
											},
											{
												"key": "45",
												"name": "DATA_IND",
												"type": "string"
											},
											{
												"key": "46",
												"name": "CHANNEL_INDICATOR",
												"type": "string"
											},
											{
												"key": "47",
												"name": "SIGNINGS_EXCEPTION_CODE",
												"type": "string"
											},
											{
												"key": "48",
												"name": "LEAD_PARNTER_SER_NUM",
												"type": "string"
											},
											{
												"key": "49",
												"name": "LEAD_PARNTER_COMPANYCODE",
												"type": "string"
											},
											{
												"key": "50",
												"name": "LEAD_PARNTER_COUNTRY",
												"type": "string"
											}
										],
										"truncated": false
									},
									"isSummary": false,
									"language": "scala"
								},
								"persist_state": {
									"view": {
										"type": "details",
										"chartOptions": {
											"chartType": "bar",
											"aggregationType": "count",
											"categoryFieldKeys": [
												"0"
											],
											"seriesFieldKeys": [
												"0"
											],
											"isStacked": false
										}
									}
								}
							}
						}
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"new_rows=[('Andhra',35,30000),('Telangana',30,50000),('Maharashtra',45,65000)]\n",
							"demo_df=spark.createDataFrame(new_rows,['State', 'Age', 'Salary'])\n",
							"demo_df.createOrReplaceTempView('demo_df')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select * from demo_df;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"read data from ADLS "
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"\n",
							"# Primary storage info \n",
							"account_name = 'Your primary storage account name' # fill in your primary account name \n",
							"container_name = 'Your container name' # fill in your container name \n",
							"relative_path = 'Your relative path' # fill in your relative folder path \n",
							"\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path = adls_path + 'Your file name ' \n",
							"df_csv = spark.read.csv(csv_path, header = 'true') \n",
							"\n",
							"# Read a parquet file \n",
							"parquet_path = adls_path + ' Your file name ' \n",
							"df_parquet = spark.read.parquet(parquet_path) \n",
							"\n",
							"# Read a json file \n",
							"json_path = adls_path + 'Your file name ' \n",
							"df_json = spark.read.json(json_path) "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"read data from dedicated SQL pool"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark \n",
							"import org.apache.spark.sql.SqlAnalyticsConnector._ \n",
							"import com.microsoft.spark.sqlanalytics.utils.Constants \n",
							"\n",
							"val sql_pool_name = \"Your sql pool name\" //fill in your sql pool name \n",
							"val schema_name = \"Your sql schema name\" //fill in your sql schema name \n",
							"val table_name = \"Your sql table name\" //fill in your sql table name \n",
							"\n",
							"// Read the sql table as a Spark dataframe \n",
							"val spark_read = spark.read. \n",
							"    sqlanalytics(s\"$sql_pool_name.$schema_name.$table_name\") \n",
							"spark_read.show(5, truncate = false) "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sql db connection"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"database = \"sqldb1\"\n",
							"table = \"DBXDH.DHT_PROJECT_SIV\"\n",
							"user = \"sqladminuser\"\n",
							"password  = \"try2find$5\"\n",
							"\n",
							"jdbcDF = spark.read.format(\"jdbc\") \\\n",
							"    .option(\"url\", \"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName={sqldb-etlhub-confirmed}\") \\\n",
							"    .option(\"dbtable\", \"DBXDH.DHT_PROJECT_SIV\") \\\n",
							"    .option(\"user\", user) \\\n",
							"    .option(\"password\", password) \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .load()\n",
							"\n",
							"jdbcDF.createOrReplaceTempView('jdbcDF')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select count(*) as cnt from jdbcDF "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"read CSV file"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"\n",
							"# Primary storage info \n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'customer' # fill in your container name \n",
							"relative_path = '/' # fill in your relative folder path \n",
							"\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net%s' % (container_name, account_name, relative_path) \n",
							"#adls_path='https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/customer_data.csv'\n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"#csv_path = adls_path + 'customer_data.csv' \n",
							"csv_path='abfss://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/customer_data.csv'\n",
							"df_csv = spark.read.csv(csv_path, header = 'true')\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.read.csv(\"abfss://customer@adls4fsoetlhubdevuseast.dfs.core.windows.net/customer_data.csv\").count"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"linked_service_name='ls_adls_project_dimension'\n",
							"spark.conf.set(\"spark.storage.synapse.linkedServiceName\",linked_service_name)\n",
							"spark.conf.set(\"fs.azure.account.oauth.provider.type\",\"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedTokenProvider\")\n",
							"\n",
							"spark.read.csv('abfss://https://adls4fsoetlhubdevuseast.blob.core.windows.net/customer/customer_data.csv')\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"\n",
							"sas_url = \"https://adls4fsoetlhubdevuseast.blob.core.windows.net/customer/customer_data.csv?sv=2020-10-02&si=customer-18095228EC2&sr=b&sig=iClJ7jEmBjlM0da5og1b93hnK4rNk1twl3phbFf1clY%3D\"\n",
							"blob_client = BlobClient.from_blob_url(sas_url)\n",
							"blob_data = blob_client.download_blob()\n",
							"df = pd.read_csv(StringIO(blob_data.content_as_text()))\n",
							"#print(df)\n",
							"sparkDF=spark.createDataFrame(df) \n",
							"\n",
							"sparkDF.createOrReplaceTempView('customer_df')\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select * from customer_df"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"\n",
							"# Azure storage access info \n",
							"blob_account_name = 'adls4fsoetlhubdevuseast' # replace with your blob name \n",
							"blob_container_name = 'customer' # replace with your container name \n",
							"blob_relative_path = '' # replace with your relative folder path \n",
							"linked_service_name = 'ls_adls_project_dimension' # replace with your linked service name \n",
							"\n",
							"blob_sas_token = \"https://adls4fsoetlhubdevuseast.blob.core.windows.net/customer/customer_data.csv?sv=2020-10-02&si=customer-18095228EC2&sr=b&sig=iClJ7jEmBjlM0da5og1b93hnK4rNk1twl3phbFf1clY%3D\"\n",
							"#mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name) \n",
							"\n",
							"# Allow SPARK to access from Blob remotely \n",
							"wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path) \n",
							"spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token) \n",
							"print('Remote blob path: ' + wasbs_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path = wasbs_path + 'customer_data.csv ' \n",
							"df_csv = spark.read.csv(csv_path, header = 'true') "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adfsas"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df3=spark.read.load('abfss://project@adls4fsoetlhubdevuseast.dfs.core.windows.net/project_data.csv',format='csv',header=True)\n",
							"display(df3)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook_siva1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Siva"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3c87bc4d-9332-4c94-bfb4-7eb60596a9a3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Incremental source data in CSV file on Azure data lake storage"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"\n",
							"sas_url = \"https://adls4fsoetlhubdevuseast.blob.core.windows.net/project/project_siv_data.csv?sv=2020-10-02&st=2022-05-11T09%3A48%3A28Z&se=2022-05-12T09%3A48%3A28Z&sr=b&sp=r&sig=JhdSw1F%2BODRA4%2B5nko0YwY8RGyjUjbuyA4ms2GblpeE%3D\"\n",
							"\n",
							"blob_client = BlobClient.from_blob_url(sas_url)\n",
							"blob_data = blob_client.download_blob()\n",
							"df = pd.read_csv(StringIO(blob_data.content_as_text()))\n",
							"#print(df)\n",
							"sparkDF=spark.createDataFrame(df) \n",
							"\n",
							"sparkDF.createOrReplaceTempView('customer_df')\n",
							"#sparkDF.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Existing Data in SQL DB table"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"user = \"sqladminuser\"\n",
							"password  = \"try2find$5\"\n",
							"\n",
							"jdbcDF = spark.read.format(\"jdbc\") \\\n",
							"    .option(\"url\", \"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName={sqldb-etlhub-confirmed}\") \\\n",
							"    .option(\"query\", \"SELECT tgt.* FROM DBXDH.DHT_PROJECT_PYSPARK as tgt WHERE CURRENT_IND='Y'\") \\\n",
							"    .option(\"user\", user) \\\n",
							"    .option(\"password\", password) \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .load()\n",
							"#jdbcDF.show()\n",
							"jdbcDF.createOrReplaceTempView('jdbcDF')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Full Outer Join to identify New, Changed, Deleted rows and the ones no longer active in source"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"fullJoin=sparkDF.join(jdbcDF,sparkDF.PROJECT_ID == jdbcDF.PROJECT_ID, \"fullouter\") \n",
							"fullJoin.createOrReplaceTempView('fullJoin')\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select * from fullJoin;\n",
							"\n",
							"select * from customer_df I \n",
							"     full outer join \n",
							"     jdbcDF E on I.PROJECT_ID=E.PROJECT_ID;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Conditional split: New, Changed, Deleted records and no longer active in source"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"fullJoin.spark"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
							"    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
							"    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
							"    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
							"    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
							"      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
							"  ]\n",
							"empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
							"       \"emp_dept_id\",\"gender\",\"salary\"]\n",
							"\n",
							"empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
							"empDF.printSchema()\n",
							"empDF.show(truncate=False)\n",
							"\n",
							"dept = [(\"Finance\",10), \\\n",
							"    (\"Marketing\",20), \\\n",
							"    (\"Sales\",30), \\\n",
							"    (\"IT\",40) \\\n",
							"  ]\n",
							"deptColumns = [\"dept_name\",\"dept_id\"]\n",
							"deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
							"deptDF.printSchema()\n",
							"deptDF.show(truncate=False)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") \\\n",
							"     .show(truncate=False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\") \\\n",
							"    .show(truncate=False)\n",
							"empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"full\") \\\n",
							"    .show(truncate=False)\n",
							"empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullouter\") \\\n",
							"    .show(truncate=False)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"columns = [\"SIEBEL_SALES_STAGE_NAME\",\"SSM_STEP_NO\",\"SSM_STEP_NAME\"]\n",
							"for i in sell_cycleDF.columns:\n",
							"    col_list.append(i)\n",
							"    #print (col_list)\n",
							"sell_cyclenkDF = sell_cycleDF.withColumn(\"nk_hash\",md5(\"SIEBEL_SALES_STAGE_CODE\"))\n",
							"sell_cyclecolhashDF = sell_cyclenkDF.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"sell_cyclecolhashDF.createOrReplaceTempView(\"sellcyclesrc\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Final Code SCD Type2"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"conn = connect(':memory:')\n",
							"natural_key=\"CUSTOMER_NO\"\n",
							"columns = [\"FINANCIAL_COUNTRY_CD\",\"CUSTOMER_DESC\",\"GBG_ID\"]\n",
							"sas_url = \"https://adls4fsoetlhubdevuseast.blob.core.windows.net/customer/customer_data.csv?sv=2020-10-02&st=2022-05-15T14%3A59%3A08Z&se=2023-05-16T14%3A59%3A00Z&sr=b&sp=r&sig=RYE103C28iVzI4%2BTmiDyMqJhGGNqBooxZgUc4LITF4U%3D\"\n",
							"blob_client = BlobClient.from_blob_url(sas_url)\n",
							"blob_data = blob_client.download_blob()\n",
							"incrementalData = pd.read_csv(StringIO(blob_data.content_as_text()))\n",
							"#print(df)\n",
							"incrementalData_DF=spark.createDataFrame(incrementalData)\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"    #print (col_list)\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\n",
							"\n",
							"\n",
							"#insertsellcycle=pd.read_sql(\"select a.SIEBEL_SALES_STAGE_CODE,a.SIEBEL_SALES_STAGE_NAME,a.SSM_STEP_NO,a.SSM_STEP_NAME from sellcyclesrc a left join  sellcycletgt b on a.nk_hash = b.existing_nk_hash where b.existing_nk_hash is null\",conn)\n",
							"#insertsellcycledf=spark.createDataFrame(insertsellcycle)\n",
							"#insertsellcycledf.show()\n",
							"#%%sql\n",
							"#select * from sellcyclesrc\n",
							"\n",
							"\n",
							"existingDataDF = spark.read.format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\n",
							"    .option(\"query\", \"SELECT MAX(CUSTOMER_KEY) OVER (ORDER BY CUSTOMER_KEY  ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) AS MAX_KEY, LOWER(CONVERT(VARCHAR(32),HashBytes('MD5', CUSTOMER_NO),2)) as existing_nk_hash,tgt.* FROM DBXDH.DHT_CUSTOMER1 as tgt WHERE CURRENT_IND='Y'\") \\\n",
							"    .option(\"user\", \"sqladminuser\") \\\n",
							"    .option(\"password\", \"try2find$5\") \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .load()\n",
							"#existingDataDF1 = existingDataDF.select([F.col(c).alias(\"`\"'existing_'+c+\"`\") for c in existingDataDF.columns])\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"#existingDataDF1.printSchema()\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_KEY, \"fullouter\") \n",
							"fullJoin1.createOrReplaceTempView('fullJoin')\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select * from existingDataDF"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--ALL\n",
							"SELECT * FROM FULLJOIN;\n",
							"\n",
							"--No change records, ignore\n",
							"select * from fullJoin WHERE LOWER(nk_hash) = LOWER(existing_nk_hash) and LOWER(column_hash) = LOWER(rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select COALESCE(MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_nk_hash is null;\n",
							"\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"select * from fullJoin WHERE WHERE LOWER(nk_hash) = LOWER(existing_nk_hash) and LOWER(column_hash) <> LOWER(rec_checksum);\n",
							"\n",
							"\n",
							"--Soft deletes or rows no longer active in source\n",
							"select * from fullJoin WHERE WHERE nk_hash is null;\n",
							"\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1 \n",
							"select * from fullJoin where existing_nk_hash is null;\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--INSERTS OR NEW ROWS\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION ,\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT,\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM,\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND,\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\n",
							"        .option(\"query\", \"INSERT INTO DBXDH.DHT_CUSTOMER1 \\\n",
							"        select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION , \\\n",
							"        CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"        CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"        'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"        'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"        from fullJoin A WHERE existing_existing_nk_hash is null\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"overwrite\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"df.write.mode(\"overwrite\") \\\n",
							"    .format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://localhost:1433;databaseName={database};\") \\\n",
							"    .option(\"dbtable\", table) \\\n",
							"    .option(\"user\", user) \\\n",
							"    .option(\"password\", password) \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook_upsert')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7cb888a4-d3ec-4c48-acd6-945ba728ffd9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession \r\n",
							"from pyspark.sql.types import * \r\n",
							"from pyspark.sql.functions import when\r\n",
							"\r\n",
							"# Primary storage info \r\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \r\n",
							"container_name = 'project' # fill in your container name \r\n",
							"relative_path = '/' # fill in your relative folder path \r\n",
							"\r\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net%s' % (container_name, account_name, relative_path) \r\n",
							"#adls_path='https://adls4fsoetlhubdevuseast.dfs.core.windows.net/project/sell_cycle_2022.csv'\r\n",
							"print('Primary storage account path: ' + adls_path) \r\n",
							"\r\n",
							"# Read a csv file \r\n",
							"csv_path = adls_path + 'sell_cycle_2022.csv' \r\n",
							"#csv_path='https://adls4fsoetlhubdevuseast.blob.core.windows.net/project/sell_cycle_2022.csv'\r\n",
							"df_csv = spark.read.csv(csv_path, header = 'true')\r\n",
							"df_csv.show(20,False)\r\n",
							"\r\n",
							"#set variable to be used to connect the database\r\n",
							"database = \"sqldb-etlhub-confirmed\"\r\n",
							"table = \"DBXDH.DHTS_SELL_CYCLE\"\r\n",
							"user = \"sqladminuser\"\r\n",
							"password  = \"try2find$5\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession \r\n",
							"from pyspark.sql.types import * \r\n",
							"from pyspark.sql.functions import when\r\n",
							"\r\n",
							"# Primary storage info \r\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \r\n",
							"container_name = 'project' # fill in your container name \r\n",
							"relative_path = '/' # fill in your relative folder path \r\n",
							"\r\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net%s' % (container_name, account_name, relative_path) \r\n",
							"#adls_path='https://adls4fsoetlhubdevuseast.dfs.core.windows.net/project/sell_cycle_2022.csv'\r\n",
							"print('Primary storage account path: ' + adls_path) \r\n",
							"\r\n",
							"# Read a csv file \r\n",
							"csv_path = adls_path + 'sell_cycle_2022.csv' \r\n",
							"#csv_path='https://adls4fsoetlhubdevuseast.blob.core.windows.net/project/sell_cycle_2022.csv'\r\n",
							"df = spark.read.csv(csv_path, header = 'true')\r\n",
							"df.show(20,False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#package com.flblue.scalatrain\r\n",
							"import java\r\n",
							"import java.sql\r\n",
							"import java.util.properties\r\n",
							"import org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions\r\n",
							"import org.apache.spark.sql.SparkSession\r\n",
							"\r\n",
							"#jdbcUrl = \"jdbc:mysql://sqlserver-kyn-001494-dev-eus-001.database.windows.net:3306/?serverTimezone=ESTSEDT\"\r\n",
							"#jdbcDriver = \"com.mysql.cj.jdbc.Driver\"\r\n",
							"#val connectionProperties = new Properties\r\n",
							"connectionProperties.put(\"user\",\"sqladminuser\")\r\n",
							"connectionProperties.put(\"password\",\"try2find$5\")\r\n",
							"connectionProperties.put(\"jdbcUrl\",\"jdbc:mysql://sqlserver-kyn-001494-dev-eus-001.database.windows.net:3306/?serverTimezone=ESTSEDT\")\r\n",
							"connectionProperties.put(\"jdbcDriver\",\"com.mysql.cj.jdbc.Driver\")\r\n",
							"connectionProperties.put(\"dbname\",\"sqldb-etlhub-confirmed\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#import required modules\r\n",
							"from pyspark import SparkConf, SparkContext\r\n",
							"from pyspark.sql import SparkSession\r\n",
							" \r\n",
							"#Create spark configuration object\r\n",
							"conf = SparkConf()\r\n",
							"conf.setMaster(\"local\").setAppName(\"My app\")\r\n",
							" \r\n",
							"#Create spark context and sparksession\r\n",
							"sc = SparkContext.getOrCreate(conf=conf)\r\n",
							"spark = SparkSession(sc)\r\n",
							"#set variable to be used to connect the database\r\n",
							"database = \"sqldb-etlhub-confirmed\"\r\n",
							"table = \"DBXDH.DHTS_SELL_CYCLE\"\r\n",
							"user = \"sqladminuser\"\r\n",
							"password  = \"try2find$5\"\r\n",
							" \r\n",
							"#read table data into a spark dataframe\r\n",
							"jdbcDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName={database};\") \\\r\n",
							"    .option(\"dbtable\", table) \\\r\n",
							"    .option(\"user\", user) \\\r\n",
							"    .option(\"password\", password) \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()\r\n",
							" \r\n",
							"#show the data loaded into dataframe\r\n",
							"jdbcDF.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Test')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Test",
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "06b0d779-523c-43c4-97c9-3f337a3e6ca5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas as pd\r\n",
							"from azure.storage.blob import BlobClient\r\n",
							"from io import StringIO\r\n",
							"from sqlite3 import connect\r\n",
							"import sys\r\n",
							"import os\r\n",
							"from datetime import datetime\r\n",
							"from pyspark.sql.functions import col\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"print(\"Hello Kitty!\")\r\n",
							"print(\"Siva is genious!\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/account_reports_scala')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "10334c61-cb17-44b6-8520-332ad9299590"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import org.apache.log4j.Logger\r\n",
							"import org.apache.spark.SparkConf\r\n",
							"import org.apache.spark.sql.{DataFrame, SparkSession}\r\n",
							"\r\n",
							"import scala.io.Source\r\n",
							"//import org.ini4j.Ini\r\n",
							"\r\n",
							"import java.io.File\r\n",
							"import java.util.Properties\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"object AccountReports extends Serializable {\r\n",
							"\r\n",
							"  @transient lazy val logger : Logger = Logger.getLogger(getClass.getName)\r\n",
							"\r\n",
							"  def main(args: Array[String] ) = {\r\n",
							"    logger.info(\"Started\")\r\n",
							"    val spark = createSparkSession()\r\n",
							"    val (jdbcURL, jdbcProps) = connectJDBC(spark)\r\n",
							"    val sourceDF = sourceData(spark, jdbcURL, jdbcProps)\r\n",
							"    sourceDF.show(5, false)\r\n",
							"    val lookupDF = lookupData(spark, jdbcURL, jdbcProps)\r\n",
							"    lookupDF.show(5, false)\r\n",
							"    //val (insertDF, updateDF) = changeCapture(sourceDF, lookupDF)\r\n",
							"    //loadTarget(insertDF, updateDF)\r\n",
							"    spark.stop()\r\n",
							"    logger.info(\"Stopped\")\r\n",
							"  }\r\n",
							"\r\n",
							"  def createSparkSession() = {\r\n",
							"    SparkSession.builder()\r\n",
							"      //.master(\"local[5]\")\r\n",
							"      //.config(getSparkAppConf())\r\n",
							"      .getOrCreate()\r\n",
							"  }\r\n",
							"\r\n",
							"  def connectJDBC(spark: SparkSession) = {\r\n",
							"    val props = new Properties()\r\n",
							"    //Ini data = new Ini(new File('properties.ini'))\r\n",
							"    //val driverInfo = spark.read      .option(\"multiline\",true)      .json(\"properties.json\")\r\n",
							"    val url = \"sql-server-pgmp-dev.database.windows.net\" //driverInfo.select(\"url\").collect()(0).mkString(\"\")\r\n",
							"    props.put(\"url\",url)\r\n",
							"    props.put(\"user\",\"pgmpetl\") //driverInfo.select(\"username\").collect()(0).mkString(\"\"))\r\n",
							"    props.put(\"password\",\"Zaq1@wsxZaq1@wsx\") //driverInfo.select(\"password\").collect()(0).mkString(\"\"))\r\n",
							"    //props.setProperty(\"driver\",driverInfo.select(\"driver\").collect()(0).mkString(\"\"))\r\n",
							"    logger.info(\"Props - \" + props)\r\n",
							"    (url, props)\r\n",
							"  }\r\n",
							"\r\n",
							"  def sourceData(spark: SparkSession, jdbcURL: String, jdbcProps: Properties) = {\r\n",
							"    spark.read.jdbc(jdbcURL, \"PGMPDM.ZAUX_ETL_EXCTN\", jdbcProps)\r\n",
							"      .select(\"ETL_EXCTN_ID\", \"ETL_PARAM_START_TMS\", \"ETL_PARAM_END_TMS\")\r\n",
							"      .where(\"IS_CURR_IND = 'Y'\")\r\n",
							"      .createOrReplaceTempView(\"FIL\")\r\n",
							"\r\n",
							"    spark.read.jdbc(jdbcURL, \"PGMPDM.ZAUX_ETL_JOBS\", jdbcProps)\r\n",
							"      .where(\"ETL_JOB_NM = 'BALD0010_ACCTRPTS_DIM'\")\r\n",
							"      .selectExpr(\"coalesce(max(ETL_JOB_ID), -1) as `ETL_JOB_ID`\")\r\n",
							"      .createOrReplaceTempView(\"JOB\")\r\n",
							"\r\n",
							"    spark.read.jdbc(jdbcURL, \"PGMPDM.SRC_SYS_DIM\", jdbcProps)\r\n",
							"      .where(\"SRC_SYS_CD = 'PGMP'\")\r\n",
							"      .selectExpr(\"coalesce(max(SRC_SYS_DIM_UID), -1) as `SRC_SYS_DIM_UID`\")\r\n",
							"      .createOrReplaceTempView(\"SYS\")\r\n",
							"\r\n",
							"    spark.read.jdbc(jdbcURL, \"APPFUN.ACCTRPTS\", jdbcProps)\r\n",
							"      .createOrReplaceTempView(\"ACC\")\r\n",
							"\r\n",
							"    spark.read.jdbc(jdbcURL, \"PGMPDM.ZAUX_DATE_TRIGGERS\", jdbcProps)\r\n",
							"      .createOrReplaceTempView(\"ZDT\")\r\n",
							"\r\n",
							"    spark.read.jdbc(jdbcURL, \"PGMPDM.ZAUX_DELD_PROC_ID\", jdbcProps)\r\n",
							"      .createOrReplaceTempView(\"PDEL\")\r\n",
							"\r\n",
							"    spark.sql(\r\n",
							"      \"\"\"\r\n",
							"        |Select\r\n",
							"        |ACC.PROC_ID as ACCTRPTS_DIM_UID, ACC.PROJECT_ID as PRJCT_ID, cast(ACC.REMARKS as varchar(1024)) as ACCTRPTS_REM_TXT, \tACC.CREATED_TS as SRC_CRETD_TMS, ACC.CREATED_USERID as SRC_CRETD_USER_ID, ACC.UPDATED_TS as SRC_UPDTD_TMS, ACC.UPDATED_USERID as SRC_UPDTD_USER_ID, JOB.ETL_JOB_ID, FIL.ETL_EXCTN_ID, SYS.SRC_SYS_DIM_UID, case when PDEL.PROC_ID is null then 0 else 1 end as IS_DELETED\r\n",
							"        |From ACC\r\n",
							"        |inner join ZDT on ACC.PROC_ID = ZDT.PROC_ID\r\n",
							"        |left join PDEL on ACC.PROC_ID = PDEL.PROC_ID\r\n",
							"        |left join JOB on 1 = 1\r\n",
							"        |left join FIL on 1 = 1\r\n",
							"        |left join SYS on 1 = 1\r\n",
							"        |\"\"\".stripMargin)\r\n",
							"\r\n",
							"  }\r\n",
							"\r\n",
							"  def lookupData(spark: SparkSession, jdbcURL: String, jdbcProps: Properties) = {\r\n",
							"    spark.read.jdbc(jdbcURL, \"PGMPDM.ACCTRPTS_DIM\", jdbcProps)\r\n",
							"      .createOrReplaceTempView(\"S\")\r\n",
							"\r\n",
							"    spark.read.jdbc(jdbcURL, \"PGMPDM.ZAUX_DATE_TRIGGERS\", jdbcProps)\r\n",
							"      .createOrReplaceTempView(\"ZDT\")\r\n",
							"\r\n",
							"    spark.sql(\r\n",
							"      \"\"\"\r\n",
							"        |Select\r\n",
							"        |S.ACCTRPTS_DIM_UID, S.PRJCT_ID, S.ACCTRPTS_REM_TXT, S.SRC_CRETD_TMS, S.SRC_CRETD_USER_ID, S.SRC_UPDTD_TMS, S.SRC_UPDTD_USER_ID, S.SRC_SYS_DIM_UID, S.ETL_JOB_ID\r\n",
							"        |From S\r\n",
							"        |inner join ZDT on S.ACCTRPTS_DIM_UID = ZDT.PROC_ID\r\n",
							"        |order by S.ACCTRPTS_DIM_UID asc\r\n",
							"        |\"\"\".stripMargin)\r\n",
							"  }\r\n",
							"\r\n",
							"  def changeCapture(sourceDF: DataFrame, lookupDF: DataFrame) = {\r\n",
							"    val sourceKey = sourceDF.select(\"ACCTRPTS_DIM_UID\")\r\n",
							"    val lookupKey = lookupDF.select(\"ACCTRPTS_DIM_UID\")\r\n",
							"    val insertDF = sourceDF.join(lookupDF, sourceDF.col(\"ACCTRPTS_DIM_UID\") =!= lookupDF.col(\"ACCTRPTS_DIM_UID\"), \"inner\")\r\n",
							"    val updateDF = sourceDF.join(lookupDF, sourceDF.col(\"ACCTRPTS_DIM_UID\") === lookupDF.col(\"ACCTRPTS_DIM_UID\"), \"inner\")\r\n",
							"    (insertDF,updateDF)\r\n",
							"  }\r\n",
							"\r\n",
							"  def loadTarget(insertDF: DataFrame, updateDF: DataFrame) = {\r\n",
							"    // inner join ZDT on ACC.PROC_ID = ZDT.PROC_ID\r\n",
							"    // inner join ZDT on S.ACCTRPTS_DIM_UID = ZDT.PROC_ID\r\n",
							"  }\r\n",
							"\r\n",
							"}\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bkp_DHT_CAMPUS')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/CreateTableScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "bab3b25d-61ad-4384-9d90-7c0cbc0d253d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"-- DBXDH.DHT_CAMPUS definition\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.DHT_CAMPUS  (\t\n",
							"\tCAMPUS_KEY INTEGER NOT NULL,\n",
							"\tVERSION\tINT NOT NULL,\n",
							"\tCAMPUS_ID CHAR(8) NOT NULL,\n",
							"\tCAMPUS_NAME VARCHAR(50) NOT NULL,\n",
							"\tCAMPUS_STATUS VARCHAR(32) NOT NULL,\n",
							"\tSITE_ID CHAR(8) NOT NULL,\n",
							"\tCAMPUS_GROUP CHAR(11),\n",
							"\tPHYSICAL_GEO CHAR(5),\n",
							"\tWORLD_REGION_CODE CHAR(2),\n",
							"\tWORLD_REGION_NAME VARCHAR(60),\n",
							"\tMARKET_TEAM_REGION_CODE CHAR(3),\n",
							"\tMARKET_TEAM_REGION_NAME VARCHAR(60),\n",
							"\tWORK_LOCATION_CODE CHAR(3),\n",
							"\tADDRESS VARCHAR(200),\n",
							"\tCITY VARCHAR(53),\n",
							"\tSTATE_PROVINCE_ID CHAR(3),\n",
							"\tPOSTAL_CODE VARCHAR(15),\n",
							"\tCOUNTRY_CODE CHAR(2),\n",
							"\tLATITUDE DECIMAL(11,8),\n",
							"\tLONGITUDE DECIMAL(11,8),\n",
							"\tUTC_OFFSET CHAR(9),\n",
							"\tICU_TIME_ZONE VARCHAR(50),\n",
							"\tPEOPLE_HOUSED_FLAG CHAR(1),\n",
							"\tREMOTE_SUPPORT_FLAG CHAR(1),\n",
							"\tPRIMARY_CAMPUS_USE_ID CHAR(3),\n",
							"\tPRIMARY_CAMPUS_USE_NAME VARCHAR(32),\n",
							"\tPRIMARY_CAMPUS_USE_DESCR VARCHAR(128),\n",
							"\tCAMPUS_OWNERSHIP VARCHAR(40),\n",
							"\tCAMPUS_ACTIVATION_YEAR SMALLINT,\n",
							"\tCAMPUS_INACTIVATION_YEAR SMALLINT,\n",
							"\tSOURCE_SYS_MODIFIED_DT TIMESTAMP,\n",
							"\tPROCESSED_DT TIMESTAMP NOT NULL,\n",
							"\tCURRENT_IND\tSTRING\n",
							",\tEXTRACT_DT TIMESTAMP\n",
							",\tREC_START_DT TIMESTAMP\n",
							",\tREC_END_DT TIMESTAMP\n",
							",\tSOURCE_SYSTEM STRING\n",
							",\tREC_CHECKSUM STRING\n",
							",\tIMG_LST_UPD_DT TIMESTAMP\n",
							",\tIMG_CREATED_DT TIMESTAMP\n",
							")\n",
							"USING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"LOCATION 'abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/dimensions/DHT_CAMPUS'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"DROP TABLE etlhubConfirmed.DHT_CAMPUS"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"-- RESOWW.CAMPUS definition\n",
							"\n",
							"-- Drop table\n",
							"\n",
							"-- DROP TABLE RESOWW.CAMPUS;\n",
							"\n",
							"CREATE TABLE RESOWW.CAMPUS (\n",
							"\tCAMPUS_KEY INTEGER NOT NULL,\n",
							"\tCAMPUS_ID CHAR(8) NOT NULL,\n",
							"\tCAMPUS_NAME VARCHAR(50) NOT NULL,\n",
							"\tCAMPUS_STATUS VARCHAR(32) NOT NULL,\n",
							"\tSITE_ID CHAR(8) NOT NULL,\n",
							"\tCAMPUS_GROUP CHAR(11),\n",
							"\tPHYSICAL_GEO CHAR(5),\n",
							"\tWORLD_REGION_CODE CHAR(2),\n",
							"\tWORLD_REGION_NAME VARCHAR(60),\n",
							"\tMARKET_TEAM_REGION_CODE CHAR(3),\n",
							"\tMARKET_TEAM_REGION_NAME VARCHAR(60),\n",
							"\tWORK_LOCATION_CODE CHAR(3),\n",
							"\tADDRESS VARCHAR(200),\n",
							"\tCITY VARCHAR(53),\n",
							"\tSTATE_PROVINCE_ID CHAR(3),\n",
							"\tPOSTAL_CODE VARCHAR(15),\n",
							"\tCOUNTRY_CODE CHAR(2),\n",
							"\tLATITUDE DECIMAL(11,8),\n",
							"\tLONGITUDE DECIMAL(11,8),\n",
							"\tUTC_OFFSET CHAR(9),\n",
							"\tICU_TIME_ZONE VARCHAR(50),\n",
							"\tPEOPLE_HOUSED_FLAG CHAR(1),\n",
							"\tREMOTE_SUPPORT_FLAG CHAR(1),\n",
							"\tPRIMARY_CAMPUS_USE_ID CHAR(3),\n",
							"\tPRIMARY_CAMPUS_USE_NAME VARCHAR(32),\n",
							"\tPRIMARY_CAMPUS_USE_DESCR VARCHAR(128),\n",
							"\tCAMPUS_OWNERSHIP VARCHAR(40),\n",
							"\tCAMPUS_ACTIVATION_YEAR SMALLINT,\n",
							"\tCAMPUS_INACTIVATION_YEAR SMALLINT,\n",
							"\tSOURCE_SYS_MODIFIED_DT TIMESTAMP,\n",
							"\tPROCESSED_DT TIMESTAMP NOT NULL,\n",
							"\tCONSTRAINT SQL200724134412910 PRIMARY KEY (CAMPUS_KEY)\n",
							");\n",
							"CREATE INDEX MARKET_TEAM_REGION_IDX ON RESOWW.CAMPUS (MARKET_TEAM_REGION_CODE);\n",
							"CREATE INDEX PHYSICAL_GEO_IDX ON RESOWW.CAMPUS (PHYSICAL_GEO);\n",
							"CREATE INDEX SITE_ID_IDX ON RESOWW.CAMPUS (SITE_ID);\n",
							"CREATE UNIQUE INDEX SQL200724134412900 ON RESOWW.CAMPUS (CAMPUS_KEY);\n",
							"CREATE UNIQUE INDEX SQL200724134412920 ON RESOWW.CAMPUS (CAMPUS_ID);\n",
							"CREATE INDEX WORLD_REGION_IDX ON RESOWW.CAMPUS (WORLD_REGION_CODE);"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bkp_nb_dht_building')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts/GREIW/backup"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPool001494New",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "472g",
					"driverCores": 80,
					"executorMemory": "472g",
					"executorCores": 80,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ca8b2548-ba7b-4b64-873a-2d84745bae64"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPool001494New",
						"name": "sPool001494New",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPool001494New",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 80,
						"memory": 472,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Building Dimension load**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubConfirmed.DHT_BUILDING by sourcing data from Tririga system.\n",
							"The file is pulled to datastage server using SFTP protocol.\n",
							"\n",
							"The CSV formatted file is uploaded to adls for loading to BUILDING deltalake table created.\n",
							"\n",
							"The script used for file transfer is /home/resodba/trrgprocget.sh\n",
							"\n",
							"File Name: IWTRIRIGABuildingQuery.csv from /GlobalDir/CustomerFiles/tririga"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"from pyspark.sql.functions import col\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"import os\n",
							"import sys\n",
							"from datetime import datetime"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/Tririga/Files/' # fill in your relative folder path \n",
							"relative_archive_path='archive/'\n",
							"file_name='IWTRIRIGABuildingQuery.csv.20220705_upd2' # 20220705, 20220705_upd1, 20220705_upd2\n",
							"natural_key1=\"CAMPUS_ID\"\n",
							"natural_key2=\"BUILDING_ID\"\n",
							"natural_key=\"BUSINESS_ID\"\n",
							"MinimumTririgaBuildingCount=750\n",
							"MinimumTririgaCampusCount=450\n",
							"tablename=\"etlhubConfirmed.dht_building\"\n",
							"stagingtable=\"DHTS_BUILDING\"\n",
							"keycolumn=\"BUILDING_KEY\"\n",
							"date = datetime.now().strftime(\"%Y-%m-%d-%I:%M:%S_%p\")\n",
							"extract_dt = datetime.now().strftime(\"%Y-%m-%d %I:%M:%S\") #This is batch/cycle date. It would be common for all the rows inserted in a single run\n",
							"rec_start_dt=datetime.now().strftime(\"%Y-%m-%d %I:%M:%S\") #This should ideally from the source system. If source doesnt have any date column, use current date \n",
							"print(extract_dt)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Copy source CSV file to archive folder including timestamp in file name"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/ERDM/DS_INDUSTRY_HIERARCHY_DATA.csv\n",
							"\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"adls_arch_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_archive_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"relative_archive_path\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"arch_fil_name=file_name.rsplit(\".\")\n",
							"csv_archive_path = adls_arch_path + arch_fil_name[0] + '_' + date + '.' + arch_fil_name[1]\n",
							"print (csv_archive_path)\n",
							"\n",
							"incrementalData_DF1 = spark.read.csv(csv_path, header = 'true')\n",
							"\n",
							"#incrementalData_DF = incrementalData_DF1.withColumn('BUILDING_BUSINESS_ID', concat_ws(\"CAMPUS_ID\", \"BUILDING_ID\"))\n",
							"\n",
							"incrementalData_DF = incrementalData_DF1.withColumn(natural_key, concat_ws(\"~\", natural_key1, natural_key2))\n",
							"\n",
							"incrementalData_DF.write.csv(csv_archive_path, header = 'true')\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"#Add checks for duplicate data, Check for 1. dups in source data based on natural Key and\n",
							"# 2. Dups in target already\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dim_col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    dim_col_list.append(i)\n",
							"print (dim_col_list)\n",
							"my_string = ','.join(dim_col_list)\n",
							"print (my_string)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Create a dataframe with target deltalake table data with necessary columns\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\n",
							"#existingDataDF=spark.sql(\"SELECT tgt.* from etlhubConfirmed.DHT_EMPLOYEE tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\n",
							"\n",
							"#existingMaxKeyDF=spark.sql(\"SELECT MAX(EMPLOYEE_KEY) existing_MAX_KEY from etlhubConfirmed.DHT_EMPLOYEE tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1, col(natural_key) == col(\"existing_\" + natural_key), \"fullouter\")\n",
							"\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"fullJoin2.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .option(\"overwriteSchema\", \"true\") \\\n",
							"  .option(\"path\", \"abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/staging/{}\".format(stagingtable)) \\\n",
							"  .saveAsTable(stagingtable)\n",
							"   #.option(\"overwriteSchema\", \"true\") \\  .save(\"\") \\"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Data Validation Checks :\n",
							"\n",
							"- Check for duplicates in Source data based on natural key\n",
							"- Check for partial data from source. That is if the file has expected number of rows\n",
							"- Check for duplicates in target delta lake table based on natural key\n",
							"- Referential Integrity checks\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} WHERE CURRENT_IND='Y' GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry5=\"\"\"\n",
							"SELECT * FROM incrementalData_DF2\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"df3=spark.sql(qry3.format(natural_key,natural_key))\n",
							"cnt1=df3.count()\n",
							"\n",
							"print (cnt1)\n",
							"if cnt1 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in source:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(1)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"\n",
							"df5=spark.sql(qry5)\n",
							"cnt3=df5.count()\n",
							"\n",
							"if cnt3 >= MinimumTririgaBuildingCount:\n",
							"    print(\"The number of rows in source data is more than the threshold, data can be processed\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"The source data is having less number of rows than expected. Please check:\" )\n",
							"    print(cnt3)\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							"\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry_4_recon1=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable,tablename,natural_key,natural_key))\n",
							"print('New records or Inserts are:') \n",
							"df_4_recon1.show()\n",
							"\n",
							"qry_4_recon2=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE LOWER({}) = LOWER(existing_{}) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon2=spark.sql(qry_4_recon2.format(stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))\n",
							"print('Changed records or Updates are:') \n",
							"df_4_recon2.show()\n",
							"\n",
							"qry_4_recon3=\"\"\"\n",
							"SELECT COUNT(*) from \n",
							"{} A\n",
							"join fullJoin B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' --AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"\n",
							"\"\"\"\n",
							"\n",
							"df_4_recon3=spark.sql(qry_4_recon3.format(tablename,natural_key,natural_key,natural_key))\n",
							"print('Soft Deletes are:') \n",
							"df_4_recon3.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"#delete FROM etlhubconfirmed.dht_employee;\n",
							"\n",
							"qry_4_recon1=\"\"\"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable, tablename, natural_key, natural_key))\n",
							"\n",
							"qry_ins_new_rows=\"\"\"\n",
							"\n",
							"INSERT INTO {}\n",
							"\tselect \n",
							"\tCOALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY {}) AS {} \n",
							",\t1 AS VERSION\n",
							",   BUSINESS_ID\n",
							",\tCAMPUS_ID\n",
							",\tBUILDING_ID\n",
							",\tBUILDING_NAME\n",
							",\tBUILDING_STATUS\n",
							",\tBUILDING_OWNERSHIP_NAME\n",
							",\tBUILDING_OWNERSHIP_DESCRIPTION\n",
							",\tBUILDING_PRIMARY_USE_NAME AS PRIMARY_BUILDING_USE_NAME\n",
							",\tBUILDING_PRIMARY_USE_DESCRIPTION AS PRIMARY_BUILDING_USE_DESCR\n",
							",\tBUILDING_SECONDARY_USE_NAME AS SECONDARY_BUILDING_USE_NAME\n",
							",\tBUILDING_SECONDARY_USE_DESCRIPTION AS SECONDARY_BUILDING_USE_DESCR\n",
							",\tREPLACE(RENTABLE_AREA,',','') AS RENTABLE_AREA\n",
							",\tACTIVE_YEAR AS BUILDING_ACTIVATION_YEAR\n",
							",\tACTUAL_RETIREMENT_INACTIVATION AS BUILDING_INACTIVATION_YEAR\n",
							",   'Y' AS CURRENT_IND\n",
							",   '{}' AS EXTRACT_DT\n",
							",   '{}' AS REC_START_DT\n",
							",   '9999-12-31 00:00:00' as REC_END_DT\n",
							",   'Tririga' AS SOURCE_SYSTEM\n",
							",    column_hash as REC_CHECKSUM\n",
							",    CURRENT_TIMESTAMP as IMG_LST_UPD_DT\n",
							",    CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							"    from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"d=spark.sql(qry_ins_new_rows.format(tablename,natural_key,keycolumn,extract_dt,rec_start_dt,stagingtable,tablename,natural_key, natural_key))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"qry_ins_new_rows_test=\"\"\"\n",
							"\n",
							"\tselect \n",
							"\tCOALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY {}) AS {} \n",
							",\t1 AS VERSION\n",
							",{}\n",
							",   'Y' AS CURRENT_IND\n",
							",   '{}' AS EXTRACT_DT\n",
							",   '{}' AS REC_START_DT\n",
							",   '9999-12-31 00:00:00' as REC_END_DT\n",
							",   'Tririga' AS SOURCE_SYSTEM\n",
							",    column_hash as REC_CHECKSUM\n",
							",    CURRENT_TIMESTAMP as IMG_LST_UPD_DT\n",
							",    CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							"    from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"d_test=spark.sql(qry_ins_new_rows_test.format(natural_key,keycolumn,my_string,extract_dt,rec_start_dt,stagingtable,tablename,natural_key, natural_key))\n",
							"print(qry_ins_new_rows_test.format(natural_key,keycolumn,my_string,extract_dt,rec_start_dt,stagingtable,tablename,natural_key, natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT min(BUILDING_KEY), MAX(BUILDING_KEY) FROM etlhubconfirmed.dht_building \n",
							"--where CAMPUS_ID ='RTPMAIN'\n",
							"    --like 'RTPMAIN%' --and BUILDING_ID='404'\n",
							"        "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Update or expire the rows with existing instance of the changed rows:\n",
							"qry_4_upd_changes_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.{}\n",
							"AND LOWER(B.{}) = LOWER(B.existing_{}) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= REC_START_DT -  INTERVAL 1 seconds\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\"\"\"\n",
							"f=spark.sql(qry_4_upd_changes_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key,natural_key))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 146
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Insert for changed rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry_ins_changed_rows=\"\"\"\n",
							"INSERT INTO {}\n",
							"\tselect \n",
							"    existing_BUILDING_KEY\n",
							",\t1 + existing_VERSION\n",
							",   BUSINESS_ID\n",
							",\tCAMPUS_ID\n",
							",\tBUILDING_ID\n",
							",\tBUILDING_NAME\n",
							",\tBUILDING_STATUS\n",
							",\tBUILDING_OWNERSHIP_NAME\n",
							",\tBUILDING_OWNERSHIP_DESCRIPTION\n",
							",\tBUILDING_PRIMARY_USE_NAME AS PRIMARY_BUILDING_USE_NAME\n",
							",\tBUILDING_PRIMARY_USE_DESCRIPTION AS PRIMARY_BUILDING_USE_DESCR\n",
							",\tBUILDING_SECONDARY_USE_NAME AS SECONDARY_BUILDING_USE_NAME\n",
							",\tBUILDING_SECONDARY_USE_DESCRIPTION AS SECONDARY_BUILDING_USE_DESCR\n",
							",\tREPLACE(RENTABLE_AREA,',','') AS RENTABLE_AREA\n",
							",\tACTIVE_YEAR AS BUILDING_ACTIVATION_YEAR\n",
							",\tACTUAL_RETIREMENT_INACTIVATION AS BUILDING_INACTIVATION_YEAR\n",
							",   'Y' AS CURRENT_IND\n",
							",   '{}' AS EXTRACT_DT\n",
							",   '{}' AS REC_START_DT\n",
							",   '9999-12-31 00:00:00' as REC_END_DT\n",
							",   'Tririga' AS SOURCE_SYSTEM\n",
							",    column_hash as REC_CHECKSUM\n",
							",    CURRENT_TIMESTAMP as IMG_LST_UPD_DT\n",
							",    existing_IMG_CREATED_DT\n",
							"from {} A \n",
							"WHERE LOWER({}) = LOWER(existing_{}) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"e=spark.sql(qry_ins_changed_rows.format(tablename,extract_dt,rec_start_dt,stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Soft Deletes - Update Active_Ind for the rows no longer active in source:\n",
							"\n",
							"qry_4_upd_deleted_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' --AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"    ,REC_END_DT= REC_START_DT -  INTERVAL 1 seconds\n",
							"\n",
							";\n",
							"\"\"\"\n",
							"g=spark.sql(qry_4_upd_deleted_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} WHERE CURRENT_IND='Y' GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select distinct VERSION,EXTRACT_DT,REC_START_DT,REC_END_DT,IMG_CREATED_DT,IMG_LST_UPD_DT,CURRENT_IND,count(*) \n",
							"from etlhubconfirmed.dht_building --WHERE CURRENT_IND='Y'\n",
							"    group by VERSION,EXTRACT_DT,REC_START_DT,REC_END_DT,IMG_CREATED_DT,IMG_LST_UPD_DT,CURRENT_IND\n",
							"    \n",
							"--Updated File\n",
							"    --BLDMAIN, 006 Updated\n",
							"    --CYNICOSI, 0002 Updated\n",
							"    --RTPMAIN, 002 Updated\n",
							"--Further Updated File\n",
							"    --ITMEDMLN,004 removed\n",
							"    --MQFORTFR,001 Updated\n",
							"    --CYNICOSI,0002 Updated again\n",
							"    --JPATAGOE, 001 Updated\n",
							"    --RTPMAIN, 002 removed\n",
							"    ;\n",
							"\n",
							"SELECT BUILDING_KEY,BUSINESS_ID, BUILDING_OWNERSHIP_DESCR,VERSION,EXTRACT_DT,REC_START_DT,REC_END_DT,IMG_CREATED_DT,IMG_LST_UPD_DT,CURRENT_IND\n",
							" from etlhubconfirmed.dht_building A\n",
							"where BUSINESS_ID in ('BLDMAIN~006','CYNICOSI~0002','RTPMAIN~002','ITMEDMLN~004','MQFORTFR~001','JPATAGOE~001')\n",
							"ORDER BY BUSINESS_ID, REC_START_DT\n",
							"    ;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT @@keycolumn,BUSINESS_ID, BUILDING_OWNERSHIP_DESCR,VERSION,EXTRACT_DT,REC_START_DT,REC_END_DT,IMG_CREATED_DT,IMG_LST_UPD_DT,CURRENT_IND\n",
							" from etlhubconfirmed.dht_building A\n",
							"where BUSINESS_ID in ('BLDMAIN~006','CYNICOSI~0002','RTPMAIN~002','ITMEDMLN~004','MQFORTFR~001','JPATAGOE~001')\n",
							"ORDER BY BUSINESS_ID, REC_START_DT\n",
							"    ;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bkp_nb_dht_company_load')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3026a383-6107-48c8-a118-7f11dc4e9ef9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"from pyspark.sql.functions import md5, concat_ws\r\n",
							"from sqlite3 import connect\r\n",
							"from pyspark.sql import functions as F\r\n",
							"#conn = connect(':memory:')\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession \r\n",
							"from pyspark.sql.types import * \r\n",
							"from delta.tables import *\r\n",
							"#import os\r\n",
							"import sys"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \r\n",
							"container_name = 'etlhubfilestorage' # fill in your container name \r\n",
							"relative_path = 'extract/BMSIW/' # fill in your relative folder path \r\n",
							"file_name='DS_COMPANY_DATA.csv' \r\n",
							"natural_key=\"LEDGER_CODE\"\r\n",
							"tablename=\"etlhubConfirmed.dht_company\"\r\n",
							"#natural_key=\"BUSINESS_PARTNER_ID\"\r\n",
							"keycolumn=\"COMPANY_KEY\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \r\n",
							"print('Primary storage account path: ' + adls_path) \r\n",
							"\r\n",
							"# Read a csv file \r\n",
							"csv_path = adls_path + file_name\r\n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\r\n",
							"\r\n",
							"# Get column list for creating Rec_Checksum\r\n",
							"\r\n",
							"#Add checks for duplicate data, Check for 1. dups in source data based on natural Key and\r\n",
							"# 2. Dups in target already\r\n",
							"\r\n",
							"col_list=[]\r\n",
							"for i in incrementalData_DF.columns:\r\n",
							"    col_list.append(i)\r\n",
							"\r\n",
							"# Add a checsum column to help identify the changed rows\r\n",
							"\r\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\r\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\r\n",
							"\r\n",
							"#sell_cyclecolhashDF.show()\r\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create a dataframe with target deltalake table data with necessary columns\r\n",
							"\r\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\r\n",
							"#existingDataDF=spark.sql(\"SELECT tgt.* from etlhubConfirmed.DHT_EMPLOYEE tgt WHERE CURRENT_IND='Y'\")\r\n",
							"\r\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\r\n",
							"\r\n",
							"#existingMaxKeyDF=spark.sql(\"SELECT MAX(EMPLOYEE_KEY) existing_MAX_KEY from etlhubConfirmed.DHT_EMPLOYEE tgt WHERE CURRENT_IND='Y'\")\r\n",
							"\r\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\r\n",
							"\r\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\r\n",
							"\r\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\r\n",
							"\r\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.EMPLOYEE_BUSINESS_CD == existingDataDF1.existing_EMPLOYEE_BUSINESS_CD, \"fullouter\") \r\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\r\n",
							"\r\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bkp_nb_dht_employee_load_20220624')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "62a31148-6294-4538-a213-9936498167ae"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Employeed Dimension load**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubconfirme.dht_employee by sourceing data from BMSIW's EMF datamart.\n",
							"Since BMSIW is not accessible from Azure, a datastage job src_BMSIW_Employee_Azure is executed on datastage to push the BMSIW extract to adls.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"#import os\n",
							"import sys"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/BMSIW/' # fill in your relative folder path \n",
							"file_name='DS_EMPLOYEE_DATA.csv' \n",
							"natural_key=\"EMPLOYEE_BUSINESS_CD\"\n",
							"tablename=\"etlhubConfirmed.dht_employee\"\n",
							"#natural_key=\"BUSINESS_PARTNER_ID\"\n",
							"keycolumn=\"EMPLOYEE_KEY\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/BMSIW/DS_EMPLOYEE_DATA.csv"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"#Add checks for duplicate data, Check for 1. dups in source data based on natural Key and\n",
							"# 2. Dups in target already\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Create a dataframe with target deltalake table data with necessary columns\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\n",
							"#existingDataDF=spark.sql(\"SELECT tgt.* from etlhubConfirmed.DHT_EMPLOYEE tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\n",
							"\n",
							"#existingMaxKeyDF=spark.sql(\"SELECT MAX(EMPLOYEE_KEY) existing_MAX_KEY from etlhubConfirmed.DHT_EMPLOYEE tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.EMPLOYEE_BUSINESS_CD == existingDataDF1.existing_EMPLOYEE_BUSINESS_CD, \"fullouter\") \n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Data Validation Checks :\n",
							"\n",
							"- Check for duplicates in Source data based on natural key\n",
							"- Check for duplicates in target delta lake table based on natural key\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"df3=spark.sql(qry3.format(natural_key,natural_key))\n",
							"cnt1=df3.count()\n",
							"\n",
							"print (cnt1)\n",
							"if cnt1 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in source:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(1)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							"\n",
							"#try:\n",
							"#  sqlContext.sql(\"create table {}.`{}` as select * from mytempTable\".format(hivedb,table))\n",
							"#except:\n",
							"#   status = 'fail'\n",
							"\n",
							"#assert status == 'success', 'status should be success'\n",
							"\n",
							"#a=spark.sql(qry)\n",
							"\n",
							"#print( df3. || ' Duplicate found ')\n",
							"\n",
							"#a.num_affected_rows\n",
							"#print(numOutputRows)\n",
							"\n",
							"\n",
							"#deltaTable1 = DeltaTable.forPath(spark, 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2')\n",
							"\n",
							"#deltaTable = DeltaTable.forName(spark, 'etlhubConfirmed.customer_dimension')\n",
							"\n",
							"\n",
							"#fullHistoryDF = deltaTable.history()    # get the full history of the table\n",
							"\n",
							"#lastOperationDF = deltaTable.history(1) # get the last operation\n",
							"\n",
							"#print(lastOperationDF.operationMetrics)\n",
							"\n",
							"#lastOperationDF.show()\n",
							"\n",
							"#fullHistoryDF.show()\n",
							"\n",
							"#print(num_affected_rows)\n",
							"\n",
							"#print(num_inserted_rows)\n",
							"\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"\n",
							"\n",
							"remove_col_list=[]\n",
							"for i in fullJoin2.columns:\n",
							"    if \"existing_\" in i:\n",
							"        remove_col_list.append(i)\n",
							"#print (remove_col_list)\n",
							"\n",
							"fullJoin2.alias(\"fullJoin3\")\n",
							"\n",
							"fullJoin3.drop(*remove_col_list)\n",
							"\n",
							"#fullJoin3.printSchema\n",
							"\n",
							"fullJoin3.createOrReplaceTempView(\"fullJoin1\")\n",
							"\n",
							"fullJoin3.printSchema"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 64
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT count(*) from fullJoin A \n",
							"INNER JOIN fullJoin1 C ON A.EMPLOYEE_BUSINESS_CD=C.EMPLOYEE_BUSINESS_CD\n",
							"limit 2\n",
							"--150567\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 61
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"%%sql\n",
							"--INSERT INTO etlhubconfirmed.dht_employee\n",
							"select \n",
							"COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS EMPLOYEE_KEY,1 as VERSION  \n",
							",EMPLOYEE_SERIAL_NUM\n",
							",EMPLOYEE_COUNTRY_CODE\n",
							",EMPLOYEE_COMPANY_CD\n",
							",EMPLOYEE_BUSINESS_CD\n",
							",NOTES_ID_RAW\n",
							",'' AS NOTES_ID\n",
							",'Y' AS CURRENT_IND\n",
							",CURRENT_TIMESTAMP AS EXTRACT_DT\n",
							",CURRENT_TIMESTAMP AS REC_START_DT\n",
							",'9999-12-31 00:00:00.000' as REC_END_DT\n",
							",'BMSIW' AS SOURCE_SYSTEM\n",
							",column_hash as REC_CHECKSUM\n",
							",'I' as REC_STATUS\n",
							",current_timestamp as IMG_LST_UPD_DT\n",
							",CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							",A.EMPLOYEE_INITIAL as EMPLOYEE_INITIALS\n",
							",EMPLOYEE_LAST_NAME\n",
							",EMPLOYEE_DISCON_DT\n",
							",EMPLOYEE_EFF_DT\n",
							",EMPLOYEE_STATUS\n",
							",EMPLOYEE_LEVEL_CD\n",
							",EMPLOYEE_USER_ID\n",
							",MANAGER_IND\n",
							",HOME_NODE_ID\n",
							",HOME_USER_ID\n",
							",BURDEN_CD\n",
							",BURDEN_CD_UPD_IND\n",
							",FIN_ADMIN_CD\n",
							",LONGEVITY_CD\n",
							",GROUP_ID\n",
							",JOB_FAMILY_CD\n",
							",PROFESSION_CD\n",
							",EMF_SOURCE_CD\n",
							",LBR_RPT_IND\n",
							",INET_MAIL_ADDR\n",
							",TEAM_ID\n",
							",SITE_LOC_CD\n",
							",EMP_NODE_ID\n",
							",CNUM_ID\n",
							",WEEK_SCHEDULE_HRS\n",
							",UNIT_PRICE_AMT\n",
							",SHIFT_1_RATE\n",
							",SHIFT_2_RATE\n",
							",SHIFT_3_RATE\n",
							",STANDBY_RATE\n",
							",OVERTIME_AMT\n",
							",COMPETENCY_SEGMENT_CD\n",
							",PROFESSION_NAME\n",
							",ORIG_LOC_CD\n",
							",DEPT_CATG_CD\n",
							",LOB_ID\n",
							",SAP_IND\n",
							",CHARGE_GROUP_CD\n",
							",SAP_COMPANY_CD\n",
							",WORK_WEEK_HRS_MIN\n",
							",WORK_WEEK_HRS_MAX\n",
							",IMG_ACTIVE_EMPLOYEE_STATUS_CD\n",
							",EMP_FIRST_NM\n",
							",ISO_CTRY_CD\n",
							",MGR_CTRY_CD\n",
							",MGR_CMPNY_CD\n",
							",MGR_SER_NUM\n",
							",RDM_CTRY_CD\n",
							",RDM_CMPNY_CD\n",
							",RDM_SER_NUM\n",
							",DIVISION_CODE\n",
							",DEPT_NUMBER\n",
							",MGR_CNUM_ID\n",
							",JOB_ROLE\n",
							",'ED' AS DATA_IND\n",
							",'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"\n",
							"from fullJoin A \n",
							"--INNER JOIN fullJoin1 C ON A.EMPLOYEE_BUSINESS_CD=C.EMPLOYEE_BUSINESS_CD\n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.DHT_EMPLOYEE B\n",
							"WHERE A.EMPLOYEE_BUSINESS_CD=B.EMPLOYEE_BUSINESS_CD\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 69
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--SELECT COUNT(*) as CNT, EMPLOYEE_BUSINESS_CD FROM incrementalData_DF2 GROUP BY EMPLOYEE_BUSINESS_CD HAVING COUNT(*)>1\n",
							"\n",
							"\n",
							"SELECT 'All Rows' as Title, a.* FROM fullJoin a;\n",
							"\n",
							"\n",
							"--No change records, ignore --7ROWS\n",
							"select 'No Change Rows' as Title, a.*  from fullJoin a WHERE LOWER() = LOWER(existing_EMPLOYEE_BUSINESS_CD) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS EMPLOYEE_KEY1, A.* from fullJoin A WHERE existing_EMPLOYEE_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"\n",
							"--INSERT INTO etlhubConfirmed.DHT_EMPLOYEE \n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS EMPLOYEE_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.DHT_EMPLOYEE B\n",
							"WHERE A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert2' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS EMPLOYEE_KEY1, A.* from fullJoin A WHERE existing_EMPLOYEE_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert2' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--select * from etlhubConfirmed.DHT_EMPLOYEE where rec_checksum='c475b27f1b384e1d2289948edad59d84'\n",
							"/*\n",
							"\n",
							"SELECT * FROM etlhubconfirmed.DHT_EMPLOYEE;\n",
							"\n",
							"UPDATE etlhubConfirmed.DHT_EMPLOYEE\n",
							"SET GBG_ID='GB302S66'\n",
							"    ,REC_CHECKSUM='c475b27f1b384e1d2289948edad59d86'\n",
							"    WHERE EMPLOYEE_KEY=5\n",
							"    ;\n",
							"\n",
							"\n",
							"SELECT * FROM etlhubconfirmed.DHT_EMPLOYEE;    \n",
							"*/\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"\n",
							"--select * from fullJoin WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"/*\n",
							"UPDATE etlhubconfirmed.DHT_EMPLOYEE\n",
							"set CURRENT_IND='N'\n",
							"    ,REC_END_DT=current_timestamp \n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"WHERE (CUSTOMER_NO ) =  (select CUSTOMER_NO from fullJoin WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum) )\n",
							"AND CURRENT_IND='Y'\n",
							";\n",
							"\n",
							"*/\n",
							"\n",
							"\n",
							"--INSERT INTO etlhubConfirmed.DHT_EMPLOYEE \n",
							"select existing_EMPLOYEE_KEY,1+existing_VERSION as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'U' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.DHT_EMPLOYEE B\n",
							"WHERE A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert after insert' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS EMPLOYEE_KEY1, A.* from fullJoin A WHERE existing_EMPLOYEE_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert after insert' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"MERGE INTO etlhubconfirmed.DHT_EMPLOYEE A\n",
							"USING fullJoin B\n",
							"ON A.CUSTOMER_NO = B.CUSTOMER_NO\n",
							"AND LOWER(B.CUSTOMER_NO) = LOWER(B.existing_CUSTOMER_NO) and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert After Update' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS EMPLOYEE_KEY1, A.* from fullJoin A WHERE existing_EMPLOYEE_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert After Update' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"--Soft deletes or no longer active in source\n",
							"\n",
							"\n",
							"--SELECT * FROM fullJoin WHERE CUSTOMER_NO is NULL\n",
							"\n",
							"MERGE INTO etlhubconfirmed.DHT_EMPLOYEE A\n",
							"USING fullJoin B\n",
							"ON A.CUSTOMER_NO = B.existing_CUSTOMER_NO\n",
							"AND B.CUSTOMER_NO is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\n",
							"    --,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\n",
							"select 'Final rows in SCD Type2' as Title,a.* from etlhubconfirmed.DHT_EMPLOYEE a;\n",
							"    "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"tags": []
						},
						"source": [
							"%%sql\n",
							"\n",
							"--SELECT COUNT(*) as CNT, EMPLOYEE_BUSINESS_CD FROM incrementalData_DF2 GROUP BY EMPLOYEE_BUSINESS_CD HAVING COUNT(*)>1\n",
							"\n",
							"/*\n",
							"\n",
							"select * from incrementalData_DF2;\n",
							"\n",
							"select * from existingDataDF1;\n",
							"\n",
							"select * from fullJoin;\n",
							"*/\n",
							"\n",
							"--select * from incrementalData_DF2;\n",
							"\n",
							"--select * from etlhubconfirmed.DHT_EMPLOYEE;\n",
							"\n",
							"SELECT 'All Rows' as Title, a.* FROM fullJoin a;\n",
							"\n",
							"\n",
							"--No change records, ignore --7ROWS\n",
							"select 'No Change Rows' as Title, a.*  from fullJoin a WHERE LOWER(EMPLOYEE_BUSINESS_CD) = LOWER(existing_EMPLOYEE_BUSINESS_CD) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS EMPLOYEE_KEY1, A.* from fullJoin A WHERE existing_EMPLOYEE_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"\n",
							"--INSERT INTO etlhubConfirmed.DHT_EMPLOYEE \n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS EMPLOYEE_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.DHT_EMPLOYEE B\n",
							"WHERE A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert2' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS EMPLOYEE_KEY1, A.* from fullJoin A WHERE existing_EMPLOYEE_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert2' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--select * from etlhubConfirmed.DHT_EMPLOYEE where rec_checksum='c475b27f1b384e1d2289948edad59d84'\n",
							"/*\n",
							"\n",
							"SELECT * FROM etlhubconfirmed.DHT_EMPLOYEE;\n",
							"\n",
							"UPDATE etlhubConfirmed.DHT_EMPLOYEE\n",
							"SET GBG_ID='GB302S66'\n",
							"    ,REC_CHECKSUM='c475b27f1b384e1d2289948edad59d86'\n",
							"    WHERE EMPLOYEE_KEY=5\n",
							"    ;\n",
							"\n",
							"\n",
							"SELECT * FROM etlhubconfirmed.DHT_EMPLOYEE;    \n",
							"*/\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"\n",
							"--select * from fullJoin WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"/*\n",
							"UPDATE etlhubconfirmed.DHT_EMPLOYEE\n",
							"set CURRENT_IND='N'\n",
							"    ,REC_END_DT=current_timestamp \n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"WHERE (CUSTOMER_NO ) =  (select CUSTOMER_NO from fullJoin WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum) )\n",
							"AND CURRENT_IND='Y'\n",
							";\n",
							"\n",
							"*/\n",
							"\n",
							"\n",
							"INSERT INTO etlhubConfirmed.DHT_EMPLOYEE \n",
							"select existing_EMPLOYEE_KEY,1+existing_VERSION as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'U' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.DHT_EMPLOYEE B\n",
							"WHERE A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert after insert' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS EMPLOYEE_KEY1, A.* from fullJoin A WHERE existing_EMPLOYEE_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert after insert' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"MERGE INTO etlhubconfirmed.DHT_EMPLOYEE A\n",
							"USING fullJoin B\n",
							"ON A.CUSTOMER_NO = B.CUSTOMER_NO\n",
							"AND LOWER(B.CUSTOMER_NO) = LOWER(B.existing_CUSTOMER_NO) and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert After Update' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS EMPLOYEE_KEY1, A.* from fullJoin A WHERE existing_EMPLOYEE_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert After Update' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"--Soft deletes or no longer active in source\n",
							"\n",
							"\n",
							"--SELECT * FROM fullJoin WHERE CUSTOMER_NO is NULL\n",
							"\n",
							"MERGE INTO etlhubconfirmed.DHT_EMPLOYEE A\n",
							"USING fullJoin B\n",
							"ON A.CUSTOMER_NO = B.existing_CUSTOMER_NO\n",
							"AND B.CUSTOMER_NO is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\n",
							"    --,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\n",
							"select 'Final rows in SCD Type2' as Title,a.* from etlhubconfirmed.DHT_EMPLOYEE a;\n",
							"    "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"select * from etlhubconfirmed.customer_dimension ;\n",
							"--CUSTOMER_NO='C0000216';\n",
							"--group by VERSION having count(*)>1;\n",
							"/*\n",
							"Update etlhubconfirmed.customer_dimension\n",
							"set CURRENT_IND='Y'\n",
							"    WHERE CURRENT_IND='N' and VERSION=2.0;\n",
							"--DELETE FROM etlhubconfirmed.customer_dimension WHERE customer_key in (3.0,4.0);\n",
							"\n",
							"\n",
							"Update etlhubconfirmed.customer_dimension\n",
							"set FINANCIAL_COUNTRY_CD='896'\n",
							"    ,REC_CHECKSUM='3145dfee7cc94e4483b4b0c7244a9949'\n",
							"    WHERE customer_key=7.0;\n",
							"Update etlhubconfirmed.customer_dimension\n",
							"set GBG_ID='GB302S60'\n",
							"    ,customer_name='WALPOLE CO-OPERATIVE BANK1'\n",
							"    ,REC_CHECKSUM='0520612ce8718d5df3b8bb4b165a6548'\n",
							"    WHERE customer_key=8.0;\n",
							"UPDATE etlhubconfirmed.customer_dimension\n",
							"SET CURRENT_IND='N'\n",
							"    WHERE CUSTOMER_KEY=11.0;\n",
							"*/\n",
							"--select * from etlhubconfirmed.customer_dimension;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 90
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"\n",
							"select * from etlhubconfirmed.customer_dimension;\n",
							"\n",
							"\n",
							"\n",
							"select * from fullJoin;\n",
							"\n",
							"--No change records, ignore\n",
							"select * from fullJoin WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							"\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"select * from fullJoin WHERE WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--Soft deletes or rows no longer active in source\n",
							"select * from fullJoin WHERE WHERE nk_hash is null;\n",
							"\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1 \n",
							"select * from fullJoin where existing_existing_nk_hash is null;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"select customer_key,count(*) from etlhubconfirmed.customer_dimension where current_ind='Y' group by customer_key having count(*)>1;\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"etlhubConfirmed.customer_dimension.toDF('abc')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"UPDATE etlhubconfirmed.customer_dimension\n",
							"SET fiNANCIAL_COUNTRY_CD='907'\n",
							"    WHERE CURRENT_IND='Y' AND CUSTOMER_NO='0074657';\n",
							"select * from etlhubconfirmed.customer_dimension;\n",
							"\n",
							"DELETe from etlhubconfirmed.customer_dimension where fiNANCIAL_COUNTRY_CD='905';\n",
							"select * from etlhubconfirmed.customer_dimension;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"MERGE INTO default.people10m\n",
							"USING default.people10m_upload\n",
							"ON default.people10m.id = default.people10m_upload.id\n",
							"WHEN MATCHED THEN UPDATE SET *\n",
							"WHEN NOT MATCHED THEN INSERT *"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create a deltalake table with necessary columns\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--CREATE DATABASE etlhubConfirmed;\n",
							"\n",
							"--drop table etlhubConfirmed.customer_dimension;\n",
							"\n",
							"-- Create Delta Lake table, define schema and location\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.customer_dimension (\n",
							"    CUSTOMER_KEY INT NOT NULL,\n",
							"\tVERSION INT ,\n",
							"\tCUSTOMER_NO STRING ,\n",
							"\tFINANCIAL_COUNTRY_CD varchar(10) ,\n",
							"\tGBG_ID varchar(10)  ,\n",
							"\tCUSTOMER_NAME varchar(30)  ,\n",
							"\tCURRENT_IND varchar(1)  ,\n",
							"\tEXTRACT_DT TIMESTAMP ,\n",
							"\tREC_START_DT TIMESTAMP ,\n",
							"\tREC_END_DT TIMESTAMP ,\n",
							"\tSOURCE_SYSTEM varchar(50)  ,\n",
							"\tREC_CHECKSUM varchar(32)  ,\n",
							"\tREC_STATUS varchar(1)  ,\n",
							"\tIMG_LST_UPD_DT TIMESTAMP NOT NULL,\n",
							"\tIMG_CREATED_DT TIMESTAMP NOT NULL,\n",
							"\tDATA_IND varchar(10)  ,\n",
							"\tACTIVE_IN_SOURCE_IND char(1)  \n",
							")\n",
							"USING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"-- specify data lake folder location\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer'\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"source_data = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"source_format = 'csv'\n",
							"\n",
							"spark.sql(\"COPY INTO \" + table_name + \\\n",
							"  \" FROM '\" + source_data + \"'\" + \\\n",
							"  \" FILEFORMAT = \" + source_format + ';'\n",
							")\n",
							"\n",
							"customer_dim_data = spark.sql(\"SELECT * FROM \" + table_name)\n",
							"\n",
							"display(customer_dim_data)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"read_format = 'csv'\n",
							"write_format = 'delta'\n",
							"load_path = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"save_path = 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2'\n",
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"\n",
							"\n",
							"account_name1 = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name1 = 'customer' # fill in your container name \n",
							"relative_path1 = '' # fill in your relative folder path \n",
							"\n",
							"adls_path1 = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name1, account_name1, relative_path1) \n",
							"#print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path1 = adls_path + 'DHT_CUSTOMER_202205191833.csv' \n",
							"custData_DF1 = spark.read.csv(csv_path1, header = 'true')\n",
							"\n",
							"#custData_DF1.show()\n",
							"\n",
							"# Write the data to its target.\n",
							"\n",
							"custData_DF1.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .save(save_path)\n",
							"# Create the table.\n",
							"#spark.sql(\"DROP TABLE \" + table_name)\n",
							"spark.sql(\"CREATE TABLE IF NOT EXISTS \" + table_name + \" USING DELTA LOCATION '\" + save_path + \"'\" )\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--drop table etlhubConfirmed.customer_dimension;\n",
							"select * from etlhubConfirmed.customer_dimension"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"end"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"source_data = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"source_format = 'CSV'\n",
							"\n",
							"spark.sql(\"DROP TABLE IF EXISTS \" + table_name)\n",
							"\n",
							"spark.sql(\"CREATE TABLE \" + table_name + \" (\" \\\n",
							"  \"loan_id BIGINT, \" + \\\n",
							"  \"funded_amnt INT, \" + \\\n",
							"  \"paid_amnt DOUBLE, \" + \\\n",
							"  \"addr_state STRING)\"\n",
							")\n",
							"\n",
							"spark.sql(\"COPY INTO \" + table_name + \\\n",
							"  \" FROM '\" + source_data + \"'\" + \\\n",
							"  \" FILEFORMAT = \" + source_format\n",
							")\n",
							"\n",
							"loan_risks_upload_data = spark.sql(\"SELECT * FROM \" + table_name)\n",
							"\n",
							"display(loan_risks_upload_data)\n",
							"Load data to datalake table"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"delta_table_path = \"abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer\" \n",
							"data = spark.range(5,10) \n",
							"data.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
							"\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"old code\n",
							"\n",
							"\n",
							"\n",
							"# Create table in the metastore\n",
							"\n",
							"DeltaTable.createIfNotExists(spark) \\\n",
							"  .tableName(\"default.customer_dimension\") \\\n",
							"  .addColumn(\"CUSTOMER_KEY\", \"INT\") \\\n",
							"  .addColumn(\"VERSION\", \"INT\") \\\n",
							"  .addColumn(\"CUSTOMER_NO\", \"STRING\") \\\n",
							"  .addColumn(\"FINANCIAL_COUNTRY_CD\")\\\n",
							"  .addColumn(\"GBG_ID\", \"STRING\") \\\n",
							"  .addColumn(\"CUSTOMER_NAME\", \"STRING\") \\\n",
							"  .addColumn(\"CURRENT_IND\", \"STRING\") \\\n",
							"  .addColumn(\"EXTRACT_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"REC_START_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"REC_END_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"SOURCE_SYSTEM\", \"STRING\") \\\n",
							"  .addColumn(\"REC_STATUS\", \"STRING\") \\\n",
							"  .addColumn(\"IMG_LST_UPD_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"IMG_CREATED_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"DATA_IND\", \"STRING\") \\\n",
							"  .addColumn(\"ACTIVE_IN_SOURCE_IND\", \"STRING\") \\\n",
							"  .execute()\n",
							"\n",
							"######################\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"#jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;database=dsqlpoolKyn001494DevEtlHubEUS001;user=undefined@asa-kyn-001494-dev-eus-001;password={your_password_here};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\n",
							"existingDataDF = spark.read.format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"    .option(\"query\", \"SELECT MAX(CUSTOMER_KEY) OVER (ORDER BY CUSTOMER_KEY  ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) AS MAX_KEY, LOWER(CONVERT(VARCHAR(32),HashBytes('MD5', CUSTOMER_NO),2)) as existing_nk_hash,tgt.* FROM DBXDH.DHT_CUSTOMER as tgt WHERE CURRENT_IND='Y'\") \\\n",
							"    .option(\"user\", \"sqladminuser\") \\\n",
							"    .option(\"password\", \"try2find$5\") \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .load()\n",
							"#existingDataDF1 = existingDataDF.select([F.col(c).alias(\"`\"'existing_'+c+\"`\") for c in existingDataDF.columns])\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"#existingDataDF1.printSchema()\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_KEY, \"fullouter\") \n",
							"fullJoin1.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"#Insert for New rows\n",
							"\n",
							"fullJoin2=sqlContext.sql(\"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \\\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null\")\n",
							"fullJoin2.createOrReplaceTempView('fullJoin2')\n",
							"\n",
							"#insert new rows into database\n",
							"\n",
							"fullJoin2.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()\n",
							"fullJoin2.show()\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--select * from existingDataDF\n",
							"\n",
							"select * from fullJoin2\n",
							"\n",
							"#insert new rows into database\n",
							"/*\n",
							"fullJoin2.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()\n",
							"        */"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--ALL\n",
							"SELECT * FROM FULLJOIN;\n",
							"\n",
							"--No change records, ignore\n",
							"select * from fullJoin WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							"\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"select * from fullJoin WHERE WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--Soft deletes or rows no longer active in source\n",
							"select * from fullJoin WHERE WHERE nk_hash is null;\n",
							"\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1 \n",
							"select * from fullJoin where existing_existing_nk_hash is null;\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--INSERTS OR NEW ROWS\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION ,\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT,\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM,\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND,\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#test referance\n",
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"query\", \"INSERT INTO DBXDH.DHT_CUSTOMER1 \\\n",
							"        select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION , \\\n",
							"        CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"        CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"        'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"        'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"        from fullJoin A WHERE existing_existing_nk_hash is null\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"overwrite\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"df.write.mode(\"overwrite\") \\\n",
							"    .format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://localhost:1433;databaseName={database};\") \\\n",
							"    .option(\"dbtable\", table) \\\n",
							"    .option(\"user\", user) \\\n",
							"    .option(\"password\", password) \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bkp_nb_dht_employee_load_20220701')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "fbab19b2-e4ab-4041-a548-82c556b5d8b4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Employeed Dimension load**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubconfirmed.dht_employee by sourceing data from BMSIW's EMF datamart.\n",
							"Since BMSIW is not accessible from Azure, a datastage job src_BMSIW_Employee_Azure is executed on datastage to push the BMSIW extract to adls.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"import os\n",
							"import sys\n",
							"from datetime import datetime"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/BMSIW/' # fill in your relative folder path \n",
							"file_name='DS_EMPLOYEE_DATA.csv' \n",
							"natural_key=\"EMPLOYEE_BUSINESS_CD\"\n",
							"tablename=\"etlhubConfirmed.dht_employee\"\n",
							"#natural_key=\"BUSINESS_PARTNER_ID\"\n",
							"keycolumn=\"EMPLOYEE_KEY\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 203
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Copy source CSV file to archive folder including timestamp in file name"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"date = datetime.now().strftime(\"%Y_%m_%d-%I:%M:%S_%p\")\n",
							"csv_path = adls_path + file_name\n",
							"csv_archive_path = adls_path + file_name + date\n",
							"print (csv_path,csv_archive_path)\n",
							"#dbutils.fs.mv(csv_path, csv_archive_path )\n",
							"#os.rename(csv_path, csv_archive_path )\n",
							"#dbutils.fs.mv('abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<directory-name>/demo/test.csv', 'abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<directory-name>/destination/renamedtest.csv')\n",
							"#%fs cp csv_path csv_archive_path"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 204
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/BMSIW/DS_EMPLOYEE_DATA.csv"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\n",
							"\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"#Add checks for duplicate data, Check for 1. dups in source data based on natural Key and\n",
							"# 2. Dups in target already\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 205
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Create a dataframe with target deltalake table data with necessary columns\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\n",
							"#existingDataDF=spark.sql(\"SELECT tgt.* from etlhubConfirmed.DHT_EMPLOYEE tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\n",
							"\n",
							"#existingMaxKeyDF=spark.sql(\"SELECT MAX(EMPLOYEE_KEY) existing_MAX_KEY from etlhubConfirmed.DHT_EMPLOYEE tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.EMPLOYEE_BUSINESS_CD == existingDataDF1.existing_EMPLOYEE_BUSINESS_CD, \"fullouter\") \n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"fullJoin2.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .option(\"overwriteSchema\", \"true\") \\\n",
							"  .option(\"path\", \"abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/staging/DHTS_EMPLOYEE\") \\\n",
							"  .saveAsTable(\"DHTS_EMPLOYEE\")\n",
							"   #.option(\"overwriteSchema\", \"true\") \\  .save(\"\") \\"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 215
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select * from existingDataDF1 where EXISTING_employee_serial_num='001518' and EXISTING_employee_country_code='661'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 216
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Data Validation Checks :\n",
							"\n",
							"- Check for duplicates in Source data based on natural key\n",
							"- Check for duplicates in target delta lake table based on natural key\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} WHERE CURRENT_IND='Y' GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"df3=spark.sql(qry3.format(natural_key,natural_key))\n",
							"cnt1=df3.count()\n",
							"\n",
							"print (cnt1)\n",
							"if cnt1 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in source:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(1)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 217
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry_4_recon1=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from fullJoin A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.EMPLOYEE_BUSINESS_CD=B.EMPLOYEE_BUSINESS_CD\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(tablename))\n",
							"print('New records or Inserts are:') \n",
							"df_4_recon1.show()\n",
							"\n",
							"qry_4_recon2=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from fullJoin A \n",
							"WHERE LOWER(EMPLOYEE_BUSINESS_CD) = LOWER(existing_EMPLOYEE_BUSINESS_CD) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.EMPLOYEE_BUSINESS_CD)=LOWER(B.EMPLOYEE_BUSINESS_CD)\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon2=spark.sql(qry_4_recon2.format(tablename))\n",
							"print('Changed records or Updates are:') \n",
							"df_4_recon2.show()\n",
							"\n",
							"qry_4_recon3=\"\"\"\n",
							"SELECT COUNT(*) from \n",
							"etlhubconfirmed.DHT_EMPLOYEE A\n",
							"join fullJoin B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"\n",
							"\"\"\"\n",
							"\n",
							"df_4_recon3=spark.sql(qry_4_recon3.format(natural_key,natural_key,natural_key))\n",
							"print('Soft Deletes are:') \n",
							"df_4_recon3.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 218
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"#delete FROM etlhubconfirmed.dht_employee;\n",
							"\n",
							"qry_4_recon1=\"\"\"\n",
							"SELECT COUNT(*) from fullJoin A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.EMPLOYEE_BUSINESS_CD=B.EMPLOYEE_BUSINESS_CD\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(tablename))\n",
							"\n",
							"qry_ins_new_rows=\"\"\"\n",
							"\n",
							"INSERT INTO etlhubconfirmed.dht_employee\n",
							"select \n",
							"COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS EMPLOYEE_KEY,1 as VERSION  \n",
							",EMPLOYEE_SERIAL_NUM\n",
							",EMPLOYEE_COUNTRY_CODE\n",
							",EMPLOYEE_COMPANY_CD\n",
							",EMPLOYEE_BUSINESS_CD\n",
							",NOTES_ID_RAW\n",
							",NULL AS NOTES_ID\n",
							",'Y' AS CURRENT_IND\n",
							",CURRENT_TIMESTAMP AS EXTRACT_DT\n",
							",CURRENT_TIMESTAMP AS REC_START_DT\n",
							",'9999-12-31 00:00:00.000' as REC_END_DT\n",
							",'BMSIW' AS SOURCE_SYSTEM\n",
							",column_hash as REC_CHECKSUM\n",
							",'I' as REC_STATUS\n",
							",current_timestamp as IMG_LST_UPD_DT\n",
							",CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							",A.EMPLOYEE_INITIAL as EMPLOYEE_INITIALS\n",
							",EMPLOYEE_LAST_NAME\n",
							",CASE WHEN EMPLOYEE_DISCON_DT='0001-01-01 00:00:00' THEN NULL ELSE EMPLOYEE_DISCON_DT END AS EMPLOYEE_DISCON_DT\n",
							",CASE WHEN EMPLOYEE_EFF_DT='0001-01-01 00:00:00' THEN NULL ELSE EMPLOYEE_EFF_DT END AS EMPLOYEE_EFF_DT\n",
							",EMPLOYEE_STATUS\n",
							",EMPLOYEE_LEVEL_CD\n",
							",EMPLOYEE_USER_ID\n",
							",NULL AS MANAGER_IND\n",
							",HOME_NODE_ID\n",
							",HOME_USER_ID\n",
							",BURDEN_CD\n",
							",BURDEN_CD_UPD_IND\n",
							",FIN_ADMIN_CD\n",
							",LONGEVITY_CD\n",
							",GROUP_ID\n",
							",JOB_FAMILY_CD\n",
							",PROFESSION_CD\n",
							",EMF_SOURCE_CD\n",
							",LBR_RPT_IND\n",
							",INET_MAIL_ADDR\n",
							",TEAM_ID\n",
							",'' AS SITE_LOC_CD\n",
							",EMP_NODE_ID\n",
							",CNUM_ID\n",
							",WEEK_SCHEDULE_HRS\n",
							",UNIT_PRICE_AMT\n",
							",SHIFT_1_RATE\n",
							",SHIFT_2_RATE\n",
							",SHIFT_3_RATE\n",
							",STANDBY_RATE\n",
							",OVERTIME_AMT\n",
							",COMPETENCY_SEGMENT_CD\n",
							",PROFESSION_NAME\n",
							",ORIG_LOC_CD\n",
							",DEPT_CATG_CD\n",
							",LOB_ID\n",
							",SAP_IND\n",
							",CHARGE_GROUP_CD\n",
							",SAP_COMPANY_CD\n",
							",WORK_WEEK_HRS_MIN\n",
							",WORK_WEEK_HRS_MAX\n",
							",'Y' AS IMG_ACTIVE_EMPLOYEE_STATUS_CD\n",
							",EMP_FIRST_NM\n",
							",ISO_CTRY_CD\n",
							",MGR_CTRY_CD\n",
							",MGR_CMPNY_CD\n",
							",MGR_SER_NUM\n",
							",RDM_CTRY_CD\n",
							",RDM_CMPNY_CD\n",
							",RDM_SER_NUM\n",
							",NULL AS DIVISION_CODE\n",
							",DEPT_NUMBER\n",
							",MGR_CNUM_ID\n",
							",NULL AS JOB_ROLE\n",
							",'ED' AS DATA_IND\n",
							",'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"\n",
							"from fullJoin A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.DHT_EMPLOYEE B\n",
							"WHERE A.EMPLOYEE_BUSINESS_CD=B.EMPLOYEE_BUSINESS_CD\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"--limit 10\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"d=spark.sql(qry_ins_new_rows)\n",
							"\n",
							"#d.show(10)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 219
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select count(*) from fullJoin where existing_CURRENT_IND='Y';\n",
							"\n",
							"select * from etlhubconfirmed.dht_employee\n",
							"where employee_serial_num='001518' and employee_country_code='661'\n",
							"    ;\n",
							"select * from fullJoin where existing_employee_serial_num='001518' \n",
							"    and existing_employee_country_code='661'\n",
							"    ;\n",
							"\n",
							" select * from DHTS_EMPLOYEE where existing_employee_serial_num='001518' \n",
							"    and existing_employee_country_code='661'\n",
							"    ;   \n",
							"select count(*)\n",
							"from etlhubConfirmed.DHT_EMPLOYEE B\n",
							"join\n",
							"fullJoin A \n",
							"WHERE A.EMPLOYEE_BUSINESS_CD = B.EMPLOYEE_BUSINESS_CD\n",
							"and LOWER(A.EMPLOYEE_BUSINESS_CD) = LOWER(A.existing_EMPLOYEE_BUSINESS_CD) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND B.REC_START_DT=A.existing_REC_START_DT\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.DHT_EMPLOYEE B\n",
							"WHERE LOWER(A.EMPLOYEE_BUSINESS_CD)=LOWER(B.EMPLOYEE_BUSINESS_CD)\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"SELECT COUNT(*) FROM etlhubConfirmed.DHT_EMPLOYEE A\n",
							"JOIN fullJoin B\n",
							"ON A.EMPLOYEE_BUSINESS_CD = B.EMPLOYEE_BUSINESS_CD\n",
							"AND LOWER(B.EMPLOYEE_BUSINESS_CD) = LOWER(B.existing_EMPLOYEE_BUSINESS_CD) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							";"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 220
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Update or expire the rows with existing instance of the changed rows:\n",
							"qry_4_upd_changes_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING fullJoin B\n",
							"ON A.{} = B.{}\n",
							"AND LOWER(B.{}) = LOWER(B.existing_{}) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\"\"\"\n",
							"f=spark.sql(qry_4_upd_changes_rows.format(tablename,natural_key,natural_key,natural_key,natural_key))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 221
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select count(*) from fullJoin where existing_CURRENT_IND='Y';\n",
							"\n",
							"select * from etlhubconfirmed.dht_employee\n",
							"where employee_serial_num='001518' and employee_country_code='661'\n",
							"    ;\n",
							"select * from fullJoin where existing_employee_serial_num='001518' \n",
							"    and existing_employee_country_code='661'\n",
							"    ;\n",
							"\n",
							" select * from DHTS_EMPLOYEE where existing_employee_serial_num='001518' \n",
							"    and existing_employee_country_code='661'\n",
							"    ;   \n",
							"select count(*)\n",
							"from etlhubConfirmed.DHT_EMPLOYEE B\n",
							"join\n",
							"fullJoin A \n",
							"WHERE A.EMPLOYEE_BUSINESS_CD = B.EMPLOYEE_BUSINESS_CD\n",
							"and LOWER(A.EMPLOYEE_BUSINESS_CD) = LOWER(A.existing_EMPLOYEE_BUSINESS_CD) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND B.REC_START_DT=A.existing_REC_START_DT\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.DHT_EMPLOYEE B\n",
							"WHERE LOWER(A.EMPLOYEE_BUSINESS_CD)=LOWER(B.EMPLOYEE_BUSINESS_CD)\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"SELECT COUNT(*) FROM etlhubConfirmed.DHT_EMPLOYEE A\n",
							"JOIN fullJoin B\n",
							"ON A.EMPLOYEE_BUSINESS_CD = B.EMPLOYEE_BUSINESS_CD\n",
							"AND LOWER(B.EMPLOYEE_BUSINESS_CD) = LOWER(B.existing_EMPLOYEE_BUSINESS_CD) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							";"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 222
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Insert for changed rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry_ins_changed_rows=\"\"\"\n",
							"\n",
							"INSERT INTO etlhubconfirmed.dht_employee\n",
							"select \n",
							"existing_EMPLOYEE_KEY\n",
							",1 + existing_EMPLOYEE_VERSION as VERSION\n",
							",EMPLOYEE_SERIAL_NUM\n",
							",EMPLOYEE_COUNTRY_CODE\n",
							",EMPLOYEE_COMPANY_CD\n",
							",EMPLOYEE_BUSINESS_CD\n",
							",NOTES_ID_RAW\n",
							",NULL AS NOTES_ID\n",
							",'Y' AS CURRENT_IND\n",
							",CURRENT_TIMESTAMP AS EXTRACT_DT\n",
							",CURRENT_TIMESTAMP AS REC_START_DT\n",
							",'9999-12-31 00:00:00.000' as REC_END_DT\n",
							",'BMSIW' AS SOURCE_SYSTEM\n",
							",column_hash as REC_CHECKSUM\n",
							",'U' as REC_STATUS\n",
							",CURRENT_TIMESTAMP as IMG_LST_UPD_DT\n",
							",CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							",A.EMPLOYEE_INITIAL as EMPLOYEE_INITIALS\n",
							",EMPLOYEE_LAST_NAME\n",
							",CASE WHEN EMPLOYEE_DISCON_DT='0001-01-01 00:00:00' THEN NULL ELSE EMPLOYEE_DISCON_DT END AS EMPLOYEE_DISCON_DT\n",
							",CASE WHEN EMPLOYEE_EFF_DT='0001-01-01 00:00:00' THEN NULL ELSE EMPLOYEE_EFF_DT END AS EMPLOYEE_EFF_DT\n",
							",EMPLOYEE_STATUS\n",
							",EMPLOYEE_LEVEL_CD\n",
							",EMPLOYEE_USER_ID\n",
							",NULL AS MANAGER_IND\n",
							",HOME_NODE_ID\n",
							",HOME_USER_ID\n",
							",BURDEN_CD\n",
							",BURDEN_CD_UPD_IND\n",
							",FIN_ADMIN_CD\n",
							",LONGEVITY_CD\n",
							",GROUP_ID\n",
							",JOB_FAMILY_CD\n",
							",PROFESSION_CD\n",
							",EMF_SOURCE_CD\n",
							",LBR_RPT_IND\n",
							",INET_MAIL_ADDR\n",
							",TEAM_ID\n",
							",'' AS SITE_LOC_CD\n",
							",EMP_NODE_ID\n",
							",CNUM_ID\n",
							",WEEK_SCHEDULE_HRS\n",
							",UNIT_PRICE_AMT\n",
							",SHIFT_1_RATE\n",
							",SHIFT_2_RATE\n",
							",SHIFT_3_RATE\n",
							",STANDBY_RATE\n",
							",OVERTIME_AMT\n",
							",COMPETENCY_SEGMENT_CD\n",
							",PROFESSION_NAME\n",
							",ORIG_LOC_CD\n",
							",DEPT_CATG_CD\n",
							",LOB_ID\n",
							",SAP_IND\n",
							",CHARGE_GROUP_CD\n",
							",SAP_COMPANY_CD\n",
							",WORK_WEEK_HRS_MIN\n",
							",WORK_WEEK_HRS_MAX\n",
							",'Y' AS IMG_ACTIVE_EMPLOYEE_STATUS_CD\n",
							",EMP_FIRST_NM\n",
							",ISO_CTRY_CD\n",
							",MGR_CTRY_CD\n",
							",MGR_CMPNY_CD\n",
							",MGR_SER_NUM\n",
							",RDM_CTRY_CD\n",
							",RDM_CMPNY_CD\n",
							",RDM_SER_NUM\n",
							",NULL AS DIVISION_CODE\n",
							",DEPT_NUMBER\n",
							",MGR_CNUM_ID\n",
							",NULL AS JOB_ROLE\n",
							",'ED' AS DATA_IND\n",
							",'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from DHTS_EMPLOYEE A \n",
							"WHERE LOWER(EMPLOYEE_BUSINESS_CD) = LOWER(existing_EMPLOYEE_BUSINESS_CD) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.DHT_EMPLOYEE B\n",
							"WHERE LOWER(A.EMPLOYEE_BUSINESS_CD)=LOWER(B.EMPLOYEE_BUSINESS_CD)\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"e=spark.sql(qry_ins_changed_rows)\n",
							"#e.show(4)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 223
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Soft Deletes - Update Active_Ind for the rows no longer active in source:\n",
							"\n",
							"qry_4_upd_deleted_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING fullJoin B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"\n",
							";\n",
							"\"\"\"\n",
							"g=spark.sql(qry_4_upd_deleted_rows.format(tablename,natural_key,natural_key,natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 224
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select 'Number of Active Rows' as Title,count(*) from etlhubconfirmed.dht_employee where CURRENT_IND='Y'\n",
							";\n",
							"\n",
							"\n",
							"select 'Duplicate Rows based on surrogate key' as Title,EMPLOYEE_KEY,count(*) from etlhubconfirmed.dht_employee where CURRENT_IND='Y'\n",
							"    group by EMPLOYEE_KEY\n",
							"    having count(*)>1\n",
							";\n",
							"\n",
							"select 'Duplicate Rows based on natural key' as Title,EMPLOYEE_BUSINESS_CD,count(*) from etlhubconfirmed.dht_employee where CURRENT_IND='Y'\n",
							"    group by EMPLOYEE_BUSINESS_CD\n",
							"    having count(*)>1\n",
							";\n",
							"\n",
							"select 'New rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_employee where CURRENT_IND='Y'\n",
							"    and date(extract_dt)='2022-06-30' and rec_status='I'\n",
							";\n",
							"\n",
							"select 'Changed rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_employee where CURRENT_IND='Y'\n",
							"    and date(extract_dt)='2022-06-30' and rec_status='U'\n",
							";\n",
							"\n",
							"select 'Changed rows updated in this run' as Title,count(*) from etlhubconfirmed.dht_employee where CURRENT_IND='N'\n",
							"    and date(REC_END_DT)='2022-06-30' --and rec_status='I'\n",
							";\n",
							"\n",
							"select 'Rows no longer active in source' as Title,count(*) from etlhubconfirmed.dht_employee where CURRENT_IND='Y'\n",
							"    AND \n",
							"     date(REC_END_DT)='2022-06-30' --and rec_status='I'\n",
							";\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 225
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT COUNT(*) FROM etlhubconfirmed.dht_employee\n",
							"WHERE CURRENT_IND='N' and DATE(REC_END_DT)<>'9999-12-31'\n",
							"    and date(img_lst_upd_dt)='2022-06-30'\n",
							"        limit 1\n",
							";\n",
							"select * from etlhubconfirmed.dht_employee\n",
							"where employee_serial_num='001518' and employee_country_code='661'\n",
							"    ;\n",
							"\n",
							"select count(*) from etlhubconfirmed.dht_employee where CURRENT_IND='Y'\n",
							"   and date(extract_dt)='2022-06-30' --and rec_status='I'\n",
							";\n",
							"\n",
							"select count(*) from etlhubconfirmed.dht_employee\n",
							"WHERE ACTIVE_IN_SOURCE_IND='N'\n",
							"    and date(IMG_LST_UPD_DT)=current_date\n",
							"    ;\n",
							"UPDATE etlhubconfirmed.dht_employee\n",
							"set CURRENT_IND='Y', \n",
							"REC_END_DT='9999-12-31'\n",
							"  WHERE CURRENT_IND='N' and DATE(REC_END_DT)<>'9999-12-31'\n",
							"    and date(img_lst_upd_dt)='2022-06-30'\n",
							"        ;  "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 226
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/copy_xfrm_BP_deltalake_scd_type2_with_parameters')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Siva"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a0227500-03ce-4222-9fcf-53f57ab1e0bf"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"#import necessary python libraries\r\n",
							"\r\n",
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"from pyspark.sql.functions import md5, concat_ws\r\n",
							"from sqlite3 import connect\r\n",
							"from pyspark.sql import functions as F\r\n",
							"#conn = connect(':memory:')\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession \r\n",
							"from pyspark.sql.types import * \r\n",
							"from delta.tables import *\r\n",
							"#import os\r\n",
							"import sys\r\n",
							"\r\n",
							"#Read data from adls csv file extracted from source at https://adls4fsoetlhubdevuseast.blob.core.windows.net/customer/BUSPARTNER_DIM.csv\r\n",
							"\r\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \r\n",
							"container_name = 'customer' # fill in your container name \r\n",
							"relative_path = '' # fill in your relative folder path \r\n",
							"\r\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \r\n",
							"print('Primary storage account path: ' + adls_path) \r\n",
							"\r\n",
							"# Read a csv file \r\n",
							"csv_path = adls_path + 'BUSPARTNER_DIM.csv' \r\n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\r\n",
							"\r\n",
							"tablename=\"etlhubConfirmed.business_partner\"\r\n",
							"natural_key=\"BUSINESS_PARTNER_ID\"\r\n",
							"keycolumn=\"BUSINESS_PARTNER_KEY\"\r\n",
							"#columns1 = [\"BUS_PARTNER_NM\"]\r\n",
							"#columnsDF=spark.createDataFrame(incrementalData_DF)\r\n",
							"col_list=[]\r\n",
							"for i in incrementalData_DF.columns:\r\n",
							"    col_list.append(i)\r\n",
							"#incrementalData_DF.show()\r\n",
							"#print (col_list)\r\n",
							"\r\n",
							"# Add a checsum column to help identify the changed rows\r\n",
							"\r\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\r\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\r\n",
							"#incrementalData_DF2.show()\r\n",
							"\r\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\r\n",
							"\r\n",
							"#Create a deltalake table with necessary columns\r\n",
							"\r\n",
							"#incrementalData_DF2.show()\r\n",
							"\r\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\r\n",
							"#existingDF.createOrReplaceTempView('existingDF')\r\n",
							"#existingDataDF.show()\r\n",
							"\r\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\r\n",
							"#existingMaxKeyDF.show()\r\n",
							"\r\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\r\n",
							"\r\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\r\n",
							"#existingDataDF1.printSchema()\r\n",
							"\r\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\r\n",
							"#existingDataDF1.show()\r\n",
							"\r\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.BUSINESS_PARTNER_ID == existingDataDF1.existing_BUSINESS_PARTNER_ID, \"fullouter\") \r\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\r\n",
							"\r\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\r\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\r\n",
							"fullJoin2.show()\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Get column list for creating Rec_Checksum\r\n",
							"\r\n",
							"#Add checks for duplicate data, Check for 1. dups in source data based on natural Key and\r\n",
							"# 2. Dups in target already\r\n",
							"columns = [\"SIEBEL_SALES_STAGE_NAME\",\"SSM_STEP_NO\",\"SSM_STEP_NAME\"]\r\n",
							"col_list=[]\r\n",
							"for i in incrementalData_DF.columns:\r\n",
							"    col_list.append(i)\r\n",
							"    #print (col_list)\r\n",
							"\r\n",
							"# Add a checsum column to help identify the changed rows\r\n",
							"\r\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\r\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\r\n",
							"\r\n",
							"#sell_cyclecolhashDF.show()\r\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\r\n",
							"\r\n",
							"#Create a deltalake table with necessary columns\r\n",
							"\r\n",
							"#incrementalData_DF2.show()\r\n",
							"\r\n",
							"#existingDataDF=spark.sql(\"SELECT * from $table_name tgt WHERE CURRENT_IND='Y'\")\r\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\r\n",
							"#existingDF.createOrReplaceTempView('existingDF')\r\n",
							"#existingDataDF.show()\r\n",
							"\r\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\r\n",
							"#existingMaxKeyDF.show()\r\n",
							"\r\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\r\n",
							"\r\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\r\n",
							"#existingDataDF1.printSchema()\r\n",
							"\r\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\r\n",
							"\r\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_NO, \"fullouter\") \r\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\r\n",
							"\r\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\r\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\r\n",
							"\r\n",
							"qry= \"\"\"\r\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \r\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \r\n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\r\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\r\n",
							"AND CURRENT_IND='Y'\r\n",
							"WHERE existing_REC_CHECKSUM is null\r\n",
							"AND B.REC_CHECKSUM <> A.column_hash\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"\r\n",
							"qry1= \"\"\"\r\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \r\n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\r\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\r\n",
							"AND CURRENT_IND='Y'\r\n",
							"WHERE existing_REC_CHECKSUM is null\r\n",
							"AND B.REC_CHECKSUM <> A.column_hash\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"\r\n",
							"qry3=\"\"\"\r\n",
							"SELECT COUNT(*) as CNT, CUSTOMER_NO FROM incrementalData_DF2 GROUP BY CUSTOMER_NO HAVING COUNT(*)>1\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"\r\n",
							"df3=spark.sql(qry3)\r\n",
							"cnt1=df3.count()\r\n",
							"\r\n",
							"print (cnt1)\r\n",
							"if cnt1 == 0:\r\n",
							"    print(\"No Duplicates in source data\")\r\n",
							"    status = 'success'\r\n",
							"else:\r\n",
							"    print(\"Below are the duplicates:\")\r\n",
							"    df3.show()\r\n",
							"    status = 'fail'\r\n",
							"    #os.abort() this will take the spark cluster also down\r\n",
							"    sys.exit(1)\r\n",
							"    print(\"This will not be printed\")\r\n",
							"print (\"this will not be printed either\")\r\n",
							"\r\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\r\n",
							"#try:\r\n",
							"#  sqlContext.sql(\"create table {}.`{}` as select * from mytempTable\".format(hivedb,table))\r\n",
							"#except:\r\n",
							"#   status = 'fail'\r\n",
							"\r\n",
							"#assert status == 'success', 'status should be success'\r\n",
							"\r\n",
							"#a=spark.sql(qry)\r\n",
							"\r\n",
							"#print( df3. || ' Duplicate found ')\r\n",
							"\r\n",
							"#a.num_affected_rows\r\n",
							"#print(numOutputRows)\r\n",
							"\r\n",
							"\r\n",
							"#deltaTable1 = DeltaTable.forPath(spark, 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2')\r\n",
							"\r\n",
							"#deltaTable = DeltaTable.forName(spark, 'etlhubConfirmed.customer_dimension')\r\n",
							"\r\n",
							"\r\n",
							"#fullHistoryDF = deltaTable.history()    # get the full history of the table\r\n",
							"\r\n",
							"#lastOperationDF = deltaTable.history(1) # get the last operation\r\n",
							"\r\n",
							"#print(lastOperationDF.operationMetrics)\r\n",
							"\r\n",
							"#lastOperationDF.show()\r\n",
							"\r\n",
							"#fullHistoryDF.show()\r\n",
							"\r\n",
							"#print(num_affected_rows)\r\n",
							"\r\n",
							"#print(num_inserted_rows)\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/datalake_joins')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5588cce5-b44f-4914-9126-60a4673aebba"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"from pyspark.sql.functions import md5, concat_ws\r\n",
							"from sqlite3 import connect\r\n",
							"from pyspark.sql import functions as F\r\n",
							"#conn = connect(':memory:')\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession \r\n",
							"from pyspark.sql.types import *\r\n",
							"\r\n",
							"#set the data lake file location:\r\n",
							"file_location = \"abfss://project@adls4fsoetlhubdevuseast.dfs.core.windows.net/sell_cycle_2022.csv\"\r\n",
							" \r\n",
							"#read in the data to dataframe df\r\n",
							"sellcyclesrcdf = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").option(\"delimiter\",\",\").load(file_location)\r\n",
							"#display the dataframe\r\n",
							"#display(sellcyclesrcdf)\r\n",
							"#sellcyclesrcdf.printSchema()\r\n",
							"\r\n",
							"\r\n",
							"sellcycletgtdf=spark.sql(\"SELECT SIEBEL_SALES_STAGE_CODE,SIEBEL_SALES_STAGE_NAME,SSM_STEP_NO,SSM_STEP_NAME from etlhubConfirmed.dht_sell_cycle\")\r\n",
							"sellcycletgtdf.createOrReplaceTempView('sellcycletgtdfview')\r\n",
							"#sellcycletgtdf.show()\r\n",
							"\r\n",
							"#sellcyclefulljoindf=sellcycletgtdf.join(sellcyclesrcdf,sellcycletgtdf.SIEBEL_SALES_STAGE_CODE == sellcyclesrcdf.SIEBEL_SALES_STAGE_CODE, \"fullouter\") \r\n",
							"#sellcyclefulljoindf.show()\r\n",
							"#sellcyclesrcdf.SIEBEL_SALES_STAGE_CODE,sellcyclesrcdf.SIEBEL_SALES_STAGE_NAME,sellcyclesrcdf.SSM_STEP_NO,sellcyclesrcdf.SSM_STEP_NAME\r\n",
							"\r\n",
							"#sellcycletgtdf.join(sellcyclesrcdf,sellcycletgtdf.SIEBEL_SALES_STAGE_CODE == sellcyclesrcdf.SIEBEL_SALES_STAGE_CODE, \"fullouter\")\\\r\n",
							"#.select(sellcyclesrcdf.SIEBEL_SALES_STAGE_CODE,sellcyclesrcdf.SIEBEL_SALES_STAGE_NAME,sellcyclesrcdf.SSM_STEP_NO,sellcyclesrcdf.SSM_STEP_NAME)\\\r\n",
							"#.show(truncate=False)\r\n",
							"\r\n",
							"sellcycletgtdf.createOrReplaceTempView(\"tgt\")\r\n",
							"sellcyclesrcdf.createOrReplaceTempView(\"src\")\r\n",
							"\r\n",
							"joinDF = spark.sql(\"select t.SIEBEL_SALES_STAGE_CODE as SIEBEL_SALES_STAGE_CODE_tgt,t.SIEBEL_SALES_STAGE_NAME as SIEBEL_SALES_STAGE_NAME_tgt,t.SSM_STEP_NO as SSM_STEP_NO_tgt,t.SSM_STEP_NAME as SSM_STEP_NAME_tgt,s.SIEBEL_SALES_STAGE_CODE as SIEBEL_SALES_STAGE_CODE_src,s.SIEBEL_SALES_STAGE_NAME as SIEBEL_SALES_STAGE_NAME_src,s.SSM_STEP_NO as SSM_STEP_NO_src,s.SSM_STEP_NAME as SSM_STEP_NAME_src from tgt t FULL JOIN src s on t.SIEBEL_SALES_STAGE_CODE == s.SIEBEL_SALES_STAGE_CODE\") \\\r\n",
							"#.show(truncate=False)\r\n",
							"joinDF.toDF\r\n",
							"joinDF.show()\r\n",
							"joinDF.createOrReplaceTempView(\"fulljoinDF\")\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--select SIEBEL_SALES_STAGE_CODE_src,SIEBEL_SALES_STAGE_NAME_src,SSM_STEP_NO_src,SSM_STEP_NAME_src from fulljoinDF\r\n",
							"--INSERT INTO etlhubConfirmed.sell_cycle select SIEBEL_SALES_STAGE_CODE_src,SIEBEL_SALES_STAGE_NAME_src,SSM_STEP_NO_src,SSM_STEP_NAME_src from fulljoinDF ;\r\n",
							"select * from etlhubConfirmed.dht_sell_cycle"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\r\n",
							"update table etlhubConfirmed.sell_cycle set SIEBEL_SALES_STAGE_NAME = SIEBEL_SALES_STAGE_NAME_src\r\n",
							"where SIEBEL_SALES_STAGE_CODE_src = 1000"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\r\n",
							"update etlhubConfirmed.dht_sell_cycle set SIEBEL_SALES_STAGE_NAME = '1200 newly added'\r\n",
							"where SIEBEL_SALES_STAGE_CODE = '1000'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/deltaLake_Customer_SCD_Type2_V1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Siva"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5a9c270f-4324-4ba7-97ff-b254200d3fee"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"SCD Type2 using adls as source and delta lake as target"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#import necessary python libraries\n",
							"\n",
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"#import os\n",
							"import sys\n",
							"\n",
							"#Read data from adls csv file extracted from source at https://adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/data/customer/\n",
							"\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'customer' # fill in your container name \n",
							"relative_path = '' # fill in your relative folder path \n",
							"\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path = adls_path + 'customer_data.csv' \n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\n",
							"\n",
							"natural_key=\"CUSTOMER_NO\"\n",
							"#columns1 = [\"FINANCIAL_COUNTRY_CD\",\"CUSTOMER_DESC\",\"GBG_ID\"]\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"#Add checks for duplicate data, Check for 1. dups in source data based on natural Key and\n",
							"# 2. Dups in target already\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"    #print (col_list)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\n",
							"\n",
							"#Create a deltalake table with necessary columns\n",
							"\n",
							"#incrementalData_DF2.show()\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT tgt.* from etlhubConfirmed.customer_dimension tgt WHERE CURRENT_IND='Y'\")\n",
							"#existingDF.createOrReplaceTempView('existingDF')\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX(CUSTOMER_KEY) existing_MAX_KEY from etlhubConfirmed.customer_dimension tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"#existingDataDF1.printSchema()\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_NO, \"fullouter\") \n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry= \"\"\"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"AND CURRENT_IND='Y'\n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND B.REC_CHECKSUM <> A.column_hash\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry1= \"\"\"\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"AND CURRENT_IND='Y'\n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND B.REC_CHECKSUM <> A.column_hash\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, CUSTOMER_NO FROM incrementalData_DF2 GROUP BY CUSTOMER_NO HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"df3=spark.sql(qry3)\n",
							"cnt1=df3.count()\n",
							"\n",
							"print (cnt1)\n",
							"if cnt1 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(1)\n",
							"    print(\"This will not be printed\")\n",
							"print (\"this will not be printed either\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							"\n",
							"#try:\n",
							"#  sqlContext.sql(\"create table {}.`{}` as select * from mytempTable\".format(hivedb,table))\n",
							"#except:\n",
							"#   status = 'fail'\n",
							"\n",
							"#assert status == 'success', 'status should be success'\n",
							"\n",
							"#a=spark.sql(qry)\n",
							"\n",
							"#print( df3. || ' Duplicate found ')\n",
							"\n",
							"#a.num_affected_rows\n",
							"#print(numOutputRows)\n",
							"\n",
							"\n",
							"#deltaTable1 = DeltaTable.forPath(spark, 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2')\n",
							"\n",
							"#deltaTable = DeltaTable.forName(spark, 'etlhubConfirmed.customer_dimension')\n",
							"\n",
							"\n",
							"#fullHistoryDF = deltaTable.history()    # get the full history of the table\n",
							"\n",
							"#lastOperationDF = deltaTable.history(1) # get the last operation\n",
							"\n",
							"#print(lastOperationDF.operationMetrics)\n",
							"\n",
							"#lastOperationDF.show()\n",
							"\n",
							"#fullHistoryDF.show()\n",
							"\n",
							"#print(num_affected_rows)\n",
							"\n",
							"#print(num_inserted_rows)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--SELECT COUNT(*) as CNT, CUSTOMER_NO FROM incrementalData_DF2 GROUP BY CUSTOMER_NO HAVING COUNT(*)>1\n",
							"SELECT * FROM fullJoin WHERE CUSTOMER_NO is NULL\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--SELECT COUNT(*) as CNT, CUSTOMER_NO FROM incrementalData_DF2 GROUP BY CUSTOMER_NO HAVING COUNT(*)>1\n",
							"\n",
							"/*\n",
							"\n",
							"select * from incrementalData_DF2;\n",
							"\n",
							"select * from existingDataDF1;\n",
							"\n",
							"select * from fullJoin;\n",
							"*/\n",
							"\n",
							"--select * from incrementalData_DF2;\n",
							"\n",
							"--select * from etlhubconfirmed.customer_dimension;\n",
							"\n",
							"SELECT 'All Rows' as Title, a.* FROM fullJoin a;\n",
							"\n",
							"\n",
							"--No change records, ignore --7ROWS\n",
							"select 'No Change Rows' as Title, a.*  from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.customer_dimension B\n",
							"WHERE A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert2' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert2' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--select * from etlhubConfirmed.customer_dimension where rec_checksum='c475b27f1b384e1d2289948edad59d84'\n",
							"/*\n",
							"\n",
							"SELECT * FROM etlhubconfirmed.customer_dimension;\n",
							"\n",
							"UPDATE etlhubConfirmed.customer_dimension\n",
							"SET GBG_ID='GB302S66'\n",
							"    ,REC_CHECKSUM='c475b27f1b384e1d2289948edad59d86'\n",
							"    WHERE CUSTOMER_KEY=5\n",
							"    ;\n",
							"\n",
							"\n",
							"SELECT * FROM etlhubconfirmed.customer_dimension;    \n",
							"*/\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"\n",
							"--select * from fullJoin WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"/*\n",
							"UPDATE etlhubconfirmed.customer_dimension\n",
							"set CURRENT_IND='N'\n",
							"    ,REC_END_DT=current_timestamp \n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"WHERE (CUSTOMER_NO ) =  (select CUSTOMER_NO from fullJoin WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum) )\n",
							"AND CURRENT_IND='Y'\n",
							";\n",
							"\n",
							"*/\n",
							"\n",
							"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select existing_CUSTOMER_KEY,1+existing_VERSION as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'U' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.customer_dimension B\n",
							"WHERE A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert after insert' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert after insert' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"MERGE INTO etlhubconfirmed.customer_dimension A\n",
							"USING fullJoin B\n",
							"ON A.CUSTOMER_NO = B.CUSTOMER_NO\n",
							"AND LOWER(B.CUSTOMER_NO) = LOWER(B.existing_CUSTOMER_NO) and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert After Update' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert After Update' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"--Soft deletes or no longer active in source\n",
							"\n",
							"\n",
							"--SELECT * FROM fullJoin WHERE CUSTOMER_NO is NULL\n",
							"\n",
							"MERGE INTO etlhubconfirmed.customer_dimension A\n",
							"USING fullJoin B\n",
							"ON A.CUSTOMER_NO = B.existing_CUSTOMER_NO\n",
							"AND B.CUSTOMER_NO is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\n",
							"    --,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\n",
							"select 'Final rows in SCD Type2' as Title,a.* from etlhubconfirmed.customer_dimension a;\n",
							"    "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"select * from etlhubconfirmed.customer_dimension ;\n",
							"--CUSTOMER_NO='C0000216';\n",
							"--group by VERSION having count(*)>1;\n",
							"/*\n",
							"Update etlhubconfirmed.customer_dimension\n",
							"set CURRENT_IND='Y'\n",
							"    WHERE CURRENT_IND='N' and VERSION=2.0;\n",
							"--DELETE FROM etlhubconfirmed.customer_dimension WHERE customer_key in (3.0,4.0);\n",
							"\n",
							"\n",
							"Update etlhubconfirmed.customer_dimension\n",
							"set FINANCIAL_COUNTRY_CD='896'\n",
							"    ,REC_CHECKSUM='3145dfee7cc94e4483b4b0c7244a9949'\n",
							"    WHERE customer_key=7.0;\n",
							"Update etlhubconfirmed.customer_dimension\n",
							"set GBG_ID='GB302S60'\n",
							"    ,customer_name='WALPOLE CO-OPERATIVE BANK1'\n",
							"    ,REC_CHECKSUM='0520612ce8718d5df3b8bb4b165a6548'\n",
							"    WHERE customer_key=8.0;\n",
							"UPDATE etlhubconfirmed.customer_dimension\n",
							"SET CURRENT_IND='N'\n",
							"    WHERE CUSTOMER_KEY=11.0;\n",
							"*/\n",
							"--select * from etlhubconfirmed.customer_dimension;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 90
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"\n",
							"select * from etlhubconfirmed.customer_dimension;\n",
							"\n",
							"\n",
							"\n",
							"select * from fullJoin;\n",
							"\n",
							"--No change records, ignore\n",
							"select * from fullJoin WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							"\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"select * from fullJoin WHERE WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--Soft deletes or rows no longer active in source\n",
							"select * from fullJoin WHERE WHERE nk_hash is null;\n",
							"\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1 \n",
							"select * from fullJoin where existing_existing_nk_hash is null;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"select customer_key,count(*) from etlhubconfirmed.customer_dimension where current_ind='Y' group by customer_key having count(*)>1;\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"etlhubConfirmed.customer_dimension.toDF('abc')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"UPDATE etlhubconfirmed.customer_dimension\n",
							"SET fiNANCIAL_COUNTRY_CD='907'\n",
							"    WHERE CURRENT_IND='Y' AND CUSTOMER_NO='0074657';\n",
							"select * from etlhubconfirmed.customer_dimension;\n",
							"\n",
							"DELETe from etlhubconfirmed.customer_dimension where fiNANCIAL_COUNTRY_CD='905';\n",
							"select * from etlhubconfirmed.customer_dimension;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"MERGE INTO default.people10m\n",
							"USING default.people10m_upload\n",
							"ON default.people10m.id = default.people10m_upload.id\n",
							"WHEN MATCHED THEN UPDATE SET *\n",
							"WHEN NOT MATCHED THEN INSERT *"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create a deltalake table with necessary columns\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--CREATE DATABASE etlhubConfirmed;\n",
							"\n",
							"--drop table etlhubConfirmed.customer_dimension;\n",
							"\n",
							"-- Create Delta Lake table, define schema and location\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.customer_dimension (\n",
							"    CUSTOMER_KEY INT NOT NULL,\n",
							"\tVERSION INT ,\n",
							"\tCUSTOMER_NO STRING ,\n",
							"\tFINANCIAL_COUNTRY_CD varchar(10) ,\n",
							"\tGBG_ID varchar(10)  ,\n",
							"\tCUSTOMER_NAME varchar(30)  ,\n",
							"\tCURRENT_IND varchar(1)  ,\n",
							"\tEXTRACT_DT TIMESTAMP ,\n",
							"\tREC_START_DT TIMESTAMP ,\n",
							"\tREC_END_DT TIMESTAMP ,\n",
							"\tSOURCE_SYSTEM varchar(50)  ,\n",
							"\tREC_CHECKSUM varchar(32)  ,\n",
							"\tREC_STATUS varchar(1)  ,\n",
							"\tIMG_LST_UPD_DT TIMESTAMP NOT NULL,\n",
							"\tIMG_CREATED_DT TIMESTAMP NOT NULL,\n",
							"\tDATA_IND varchar(10)  ,\n",
							"\tACTIVE_IN_SOURCE_IND char(1)  \n",
							")\n",
							"USING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"-- specify data lake folder location\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer'\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"source_data = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"source_format = 'csv'\n",
							"\n",
							"spark.sql(\"COPY INTO \" + table_name + \\\n",
							"  \" FROM '\" + source_data + \"'\" + \\\n",
							"  \" FILEFORMAT = \" + source_format + ';'\n",
							")\n",
							"\n",
							"customer_dim_data = spark.sql(\"SELECT * FROM \" + table_name)\n",
							"\n",
							"display(customer_dim_data)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"read_format = 'csv'\n",
							"write_format = 'delta'\n",
							"load_path = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"save_path = 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2'\n",
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"\n",
							"\n",
							"account_name1 = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name1 = 'customer' # fill in your container name \n",
							"relative_path1 = '' # fill in your relative folder path \n",
							"\n",
							"adls_path1 = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name1, account_name1, relative_path1) \n",
							"#print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path1 = adls_path + 'DHT_CUSTOMER_202205191833.csv' \n",
							"custData_DF1 = spark.read.csv(csv_path1, header = 'true')\n",
							"\n",
							"#custData_DF1.show()\n",
							"\n",
							"# Write the data to its target.\n",
							"\n",
							"custData_DF1.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .save(save_path)\n",
							"# Create the table.\n",
							"#spark.sql(\"DROP TABLE \" + table_name)\n",
							"spark.sql(\"CREATE TABLE IF NOT EXISTS \" + table_name + \" USING DELTA LOCATION '\" + save_path + \"'\" )\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--drop table etlhubConfirmed.customer_dimension;\n",
							"select * from etlhubConfirmed.customer_dimension"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"end"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"source_data = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"source_format = 'CSV'\n",
							"\n",
							"spark.sql(\"DROP TABLE IF EXISTS \" + table_name)\n",
							"\n",
							"spark.sql(\"CREATE TABLE \" + table_name + \" (\" \\\n",
							"  \"loan_id BIGINT, \" + \\\n",
							"  \"funded_amnt INT, \" + \\\n",
							"  \"paid_amnt DOUBLE, \" + \\\n",
							"  \"addr_state STRING)\"\n",
							")\n",
							"\n",
							"spark.sql(\"COPY INTO \" + table_name + \\\n",
							"  \" FROM '\" + source_data + \"'\" + \\\n",
							"  \" FILEFORMAT = \" + source_format\n",
							")\n",
							"\n",
							"loan_risks_upload_data = spark.sql(\"SELECT * FROM \" + table_name)\n",
							"\n",
							"display(loan_risks_upload_data)\n",
							"Load data to datalake table"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"delta_table_path = \"abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer\" \n",
							"data = spark.range(5,10) \n",
							"data.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
							"\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"old code\n",
							"\n",
							"\n",
							"\n",
							"# Create table in the metastore\n",
							"\n",
							"DeltaTable.createIfNotExists(spark) \\\n",
							"  .tableName(\"default.customer_dimension\") \\\n",
							"  .addColumn(\"CUSTOMER_KEY\", \"INT\") \\\n",
							"  .addColumn(\"VERSION\", \"INT\") \\\n",
							"  .addColumn(\"CUSTOMER_NO\", \"STRING\") \\\n",
							"  .addColumn(\"FINANCIAL_COUNTRY_CD\")\\\n",
							"  .addColumn(\"GBG_ID\", \"STRING\") \\\n",
							"  .addColumn(\"CUSTOMER_NAME\", \"STRING\") \\\n",
							"  .addColumn(\"CURRENT_IND\", \"STRING\") \\\n",
							"  .addColumn(\"EXTRACT_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"REC_START_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"REC_END_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"SOURCE_SYSTEM\", \"STRING\") \\\n",
							"  .addColumn(\"REC_STATUS\", \"STRING\") \\\n",
							"  .addColumn(\"IMG_LST_UPD_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"IMG_CREATED_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"DATA_IND\", \"STRING\") \\\n",
							"  .addColumn(\"ACTIVE_IN_SOURCE_IND\", \"STRING\") \\\n",
							"  .execute()\n",
							"\n",
							"######################\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"#jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;database=dsqlpoolKyn001494DevEtlHubEUS001;user=undefined@asa-kyn-001494-dev-eus-001;password={your_password_here};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\n",
							"existingDataDF = spark.read.format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"    .option(\"query\", \"SELECT MAX(CUSTOMER_KEY) OVER (ORDER BY CUSTOMER_KEY  ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) AS MAX_KEY, LOWER(CONVERT(VARCHAR(32),HashBytes('MD5', CUSTOMER_NO),2)) as existing_nk_hash,tgt.* FROM DBXDH.DHT_CUSTOMER as tgt WHERE CURRENT_IND='Y'\") \\\n",
							"    .option(\"user\", \"sqladminuser\") \\\n",
							"    .option(\"password\", \"try2find$5\") \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .load()\n",
							"#existingDataDF1 = existingDataDF.select([F.col(c).alias(\"`\"'existing_'+c+\"`\") for c in existingDataDF.columns])\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"#existingDataDF1.printSchema()\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_KEY, \"fullouter\") \n",
							"fullJoin1.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"#Insert for New rows\n",
							"\n",
							"fullJoin2=sqlContext.sql(\"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \\\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null\")\n",
							"fullJoin2.createOrReplaceTempView('fullJoin2')\n",
							"\n",
							"#insert new rows into database\n",
							"\n",
							"fullJoin2.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()\n",
							"fullJoin2.show()\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--select * from existingDataDF\n",
							"\n",
							"select * from fullJoin2\n",
							"\n",
							"#insert new rows into database\n",
							"/*\n",
							"fullJoin2.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()\n",
							"        */"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--ALL\n",
							"SELECT * FROM FULLJOIN;\n",
							"\n",
							"--No change records, ignore\n",
							"select * from fullJoin WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							"\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"select * from fullJoin WHERE WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--Soft deletes or rows no longer active in source\n",
							"select * from fullJoin WHERE WHERE nk_hash is null;\n",
							"\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1 \n",
							"select * from fullJoin where existing_existing_nk_hash is null;\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--INSERTS OR NEW ROWS\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION ,\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT,\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM,\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND,\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#test referance\n",
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"query\", \"INSERT INTO DBXDH.DHT_CUSTOMER1 \\\n",
							"        select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION , \\\n",
							"        CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"        CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"        'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"        'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"        from fullJoin A WHERE existing_existing_nk_hash is null\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"overwrite\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"df.write.mode(\"overwrite\") \\\n",
							"    .format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://localhost:1433;databaseName={database};\") \\\n",
							"    .option(\"dbtable\", table) \\\n",
							"    .option(\"user\", user) \\\n",
							"    .option(\"password\", password) \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/deltaLake_Customer_SCD_Type2_V1_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "2b17a515-12f3-4561-ab44-29f7688759f6"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"SCD Type2 using adls as source and delta lake as target"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#import necessary python libraries\n",
							"\n",
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"#import os\n",
							"import sys\n",
							"\n",
							"#Read data from adls csv file extracted from source at https://adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/data/customer/\n",
							"\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'customer' # fill in your container name \n",
							"relative_path = '' # fill in your relative folder path \n",
							"\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path = adls_path + 'customer_data.csv' \n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\n",
							"\n",
							"natural_key=\"CUSTOMER_NO\"\n",
							"#columns1 = [\"FINANCIAL_COUNTRY_CD\",\"CUSTOMER_DESC\",\"GBG_ID\"]\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"#Add checks for duplicate data, Check for 1. dups in source data based on natural Key and\n",
							"# 2. Dups in target already\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"    #print (col_list)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\n",
							"\n",
							"#Create a deltalake table with necessary columns\n",
							"\n",
							"#incrementalData_DF2.show()\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT tgt.* from etlhubConfirmed.customer_dimension tgt WHERE CURRENT_IND='Y'\")\n",
							"#existingDF.createOrReplaceTempView('existingDF')\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX(CUSTOMER_KEY) existing_MAX_KEY from etlhubConfirmed.customer_dimension tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"#existingDataDF1.printSchema()\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_NO, \"fullouter\") \n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry= \"\"\"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"AND CURRENT_IND='Y'\n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND B.REC_CHECKSUM <> A.column_hash\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry1= \"\"\"\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"AND CURRENT_IND='Y'\n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND B.REC_CHECKSUM <> A.column_hash\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, CUSTOMER_NO FROM incrementalData_DF2 GROUP BY CUSTOMER_NO HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"df3=spark.sql(qry3)\n",
							"cnt1=df3.count()\n",
							"\n",
							"print (cnt1)\n",
							"if cnt1 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(1)\n",
							"    print(\"This will not be printed\")\n",
							"print (\"this will not be printed either\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							"\n",
							"#try:\n",
							"#  sqlContext.sql(\"create table {}.`{}` as select * from mytempTable\".format(hivedb,table))\n",
							"#except:\n",
							"#   status = 'fail'\n",
							"\n",
							"#assert status == 'success', 'status should be success'\n",
							"\n",
							"#a=spark.sql(qry)\n",
							"\n",
							"#print( df3. || ' Duplicate found ')\n",
							"\n",
							"#a.num_affected_rows\n",
							"#print(numOutputRows)\n",
							"\n",
							"\n",
							"#deltaTable1 = DeltaTable.forPath(spark, 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2')\n",
							"\n",
							"#deltaTable = DeltaTable.forName(spark, 'etlhubConfirmed.customer_dimension')\n",
							"\n",
							"\n",
							"#fullHistoryDF = deltaTable.history()    # get the full history of the table\n",
							"\n",
							"#lastOperationDF = deltaTable.history(1) # get the last operation\n",
							"\n",
							"#print(lastOperationDF.operationMetrics)\n",
							"\n",
							"#lastOperationDF.show()\n",
							"\n",
							"#fullHistoryDF.show()\n",
							"\n",
							"#print(num_affected_rows)\n",
							"\n",
							"#print(num_inserted_rows)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--SELECT COUNT(*) as CNT, CUSTOMER_NO FROM incrementalData_DF2 GROUP BY CUSTOMER_NO HAVING COUNT(*)>1\n",
							"SELECT * FROM fullJoin WHERE CUSTOMER_NO is NULL\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--SELECT COUNT(*) as CNT, CUSTOMER_NO FROM incrementalData_DF2 GROUP BY CUSTOMER_NO HAVING COUNT(*)>1\n",
							"\n",
							"/*\n",
							"\n",
							"select * from incrementalData_DF2;\n",
							"\n",
							"select * from existingDataDF1;\n",
							"\n",
							"select * from fullJoin;\n",
							"*/\n",
							"\n",
							"--select * from incrementalData_DF2;\n",
							"\n",
							"--select * from etlhubconfirmed.customer_dimension;\n",
							"\n",
							"SELECT 'All Rows' as Title, a.* FROM fullJoin a;\n",
							"\n",
							"\n",
							"--No change records, ignore --7ROWS\n",
							"select 'No Change Rows' as Title, a.*  from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.customer_dimension B\n",
							"WHERE A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert2' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert2' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--select * from etlhubConfirmed.customer_dimension where rec_checksum='c475b27f1b384e1d2289948edad59d84'\n",
							"/*\n",
							"\n",
							"SELECT * FROM etlhubconfirmed.customer_dimension;\n",
							"\n",
							"UPDATE etlhubConfirmed.customer_dimension\n",
							"SET GBG_ID='GB302S66'\n",
							"    ,REC_CHECKSUM='c475b27f1b384e1d2289948edad59d86'\n",
							"    WHERE CUSTOMER_KEY=5\n",
							"    ;\n",
							"\n",
							"\n",
							"SELECT * FROM etlhubconfirmed.customer_dimension;    \n",
							"*/\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"\n",
							"--select * from fullJoin WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"/*\n",
							"UPDATE etlhubconfirmed.customer_dimension\n",
							"set CURRENT_IND='N'\n",
							"    ,REC_END_DT=current_timestamp \n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"WHERE (CUSTOMER_NO ) =  (select CUSTOMER_NO from fullJoin WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum) )\n",
							"AND CURRENT_IND='Y'\n",
							";\n",
							"\n",
							"*/\n",
							"\n",
							"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select existing_CUSTOMER_KEY,1+existing_VERSION as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'U' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.customer_dimension B\n",
							"WHERE A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert after insert' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert after insert' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"MERGE INTO etlhubconfirmed.customer_dimension A\n",
							"USING fullJoin B\n",
							"ON A.CUSTOMER_NO = B.CUSTOMER_NO\n",
							"AND LOWER(B.CUSTOMER_NO) = LOWER(B.existing_CUSTOMER_NO) and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert After Update' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert After Update' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"--Soft deletes or no longer active in source\n",
							"\n",
							"\n",
							"--SELECT * FROM fullJoin WHERE CUSTOMER_NO is NULL\n",
							"\n",
							"MERGE INTO etlhubconfirmed.customer_dimension A\n",
							"USING fullJoin B\n",
							"ON A.CUSTOMER_NO = B.existing_CUSTOMER_NO\n",
							"AND B.CUSTOMER_NO is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\n",
							"    --,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\n",
							"select 'Final rows in SCD Type2' as Title,a.* from etlhubconfirmed.customer_dimension a;\n",
							"    "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"select * from etlhubconfirmed.customer_dimension ;\n",
							"--CUSTOMER_NO='C0000216';\n",
							"--group by VERSION having count(*)>1;\n",
							"/*\n",
							"Update etlhubconfirmed.customer_dimension\n",
							"set CURRENT_IND='Y'\n",
							"    WHERE CURRENT_IND='N' and VERSION=2.0;\n",
							"--DELETE FROM etlhubconfirmed.customer_dimension WHERE customer_key in (3.0,4.0);\n",
							"\n",
							"\n",
							"Update etlhubconfirmed.customer_dimension\n",
							"set FINANCIAL_COUNTRY_CD='896'\n",
							"    ,REC_CHECKSUM='3145dfee7cc94e4483b4b0c7244a9949'\n",
							"    WHERE customer_key=7.0;\n",
							"Update etlhubconfirmed.customer_dimension\n",
							"set GBG_ID='GB302S60'\n",
							"    ,customer_name='WALPOLE CO-OPERATIVE BANK1'\n",
							"    ,REC_CHECKSUM='0520612ce8718d5df3b8bb4b165a6548'\n",
							"    WHERE customer_key=8.0;\n",
							"UPDATE etlhubconfirmed.customer_dimension\n",
							"SET CURRENT_IND='N'\n",
							"    WHERE CUSTOMER_KEY=11.0;\n",
							"*/\n",
							"--select * from etlhubconfirmed.customer_dimension;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 90
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"\n",
							"select * from etlhubconfirmed.customer_dimension;\n",
							"\n",
							"\n",
							"\n",
							"select * from fullJoin;\n",
							"\n",
							"--No change records, ignore\n",
							"select * from fullJoin WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							"\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"select * from fullJoin WHERE WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--Soft deletes or rows no longer active in source\n",
							"select * from fullJoin WHERE WHERE nk_hash is null;\n",
							"\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1 \n",
							"select * from fullJoin where existing_existing_nk_hash is null;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"select customer_key,count(*) from etlhubconfirmed.customer_dimension where current_ind='Y' group by customer_key having count(*)>1;\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"etlhubConfirmed.customer_dimension.toDF('abc')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"UPDATE etlhubconfirmed.customer_dimension\n",
							"SET fiNANCIAL_COUNTRY_CD='907'\n",
							"    WHERE CURRENT_IND='Y' AND CUSTOMER_NO='0074657';\n",
							"select * from etlhubconfirmed.customer_dimension;\n",
							"\n",
							"DELETe from etlhubconfirmed.customer_dimension where fiNANCIAL_COUNTRY_CD='905';\n",
							"select * from etlhubconfirmed.customer_dimension;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"MERGE INTO default.people10m\n",
							"USING default.people10m_upload\n",
							"ON default.people10m.id = default.people10m_upload.id\n",
							"WHEN MATCHED THEN UPDATE SET *\n",
							"WHEN NOT MATCHED THEN INSERT *"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create a deltalake table with necessary columns\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--CREATE DATABASE etlhubConfirmed;\n",
							"\n",
							"--drop table etlhubConfirmed.customer_dimension;\n",
							"\n",
							"-- Create Delta Lake table, define schema and location\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.customer_dimension (\n",
							"    CUSTOMER_KEY INT NOT NULL,\n",
							"\tVERSION INT ,\n",
							"\tCUSTOMER_NO STRING ,\n",
							"\tFINANCIAL_COUNTRY_CD varchar(10) ,\n",
							"\tGBG_ID varchar(10)  ,\n",
							"\tCUSTOMER_NAME varchar(30)  ,\n",
							"\tCURRENT_IND varchar(1)  ,\n",
							"\tEXTRACT_DT TIMESTAMP ,\n",
							"\tREC_START_DT TIMESTAMP ,\n",
							"\tREC_END_DT TIMESTAMP ,\n",
							"\tSOURCE_SYSTEM varchar(50)  ,\n",
							"\tREC_CHECKSUM varchar(32)  ,\n",
							"\tREC_STATUS varchar(1)  ,\n",
							"\tIMG_LST_UPD_DT TIMESTAMP NOT NULL,\n",
							"\tIMG_CREATED_DT TIMESTAMP NOT NULL,\n",
							"\tDATA_IND varchar(10)  ,\n",
							"\tACTIVE_IN_SOURCE_IND char(1)  \n",
							")\n",
							"USING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"-- specify data lake folder location\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer'\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"source_data = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"source_format = 'csv'\n",
							"\n",
							"spark.sql(\"COPY INTO \" + table_name + \\\n",
							"  \" FROM '\" + source_data + \"'\" + \\\n",
							"  \" FILEFORMAT = \" + source_format + ';'\n",
							")\n",
							"\n",
							"customer_dim_data = spark.sql(\"SELECT * FROM \" + table_name)\n",
							"\n",
							"display(customer_dim_data)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"read_format = 'csv'\n",
							"write_format = 'delta'\n",
							"load_path = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"save_path = 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2'\n",
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"\n",
							"\n",
							"account_name1 = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name1 = 'customer' # fill in your container name \n",
							"relative_path1 = '' # fill in your relative folder path \n",
							"\n",
							"adls_path1 = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name1, account_name1, relative_path1) \n",
							"#print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path1 = adls_path + 'DHT_CUSTOMER_202205191833.csv' \n",
							"custData_DF1 = spark.read.csv(csv_path1, header = 'true')\n",
							"\n",
							"#custData_DF1.show()\n",
							"\n",
							"# Write the data to its target.\n",
							"\n",
							"custData_DF1.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .save(save_path)\n",
							"# Create the table.\n",
							"#spark.sql(\"DROP TABLE \" + table_name)\n",
							"spark.sql(\"CREATE TABLE IF NOT EXISTS \" + table_name + \" USING DELTA LOCATION '\" + save_path + \"'\" )\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--drop table etlhubConfirmed.customer_dimension;\n",
							"select * from etlhubConfirmed.customer_dimension"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"end"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"source_data = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"source_format = 'CSV'\n",
							"\n",
							"spark.sql(\"DROP TABLE IF EXISTS \" + table_name)\n",
							"\n",
							"spark.sql(\"CREATE TABLE \" + table_name + \" (\" \\\n",
							"  \"loan_id BIGINT, \" + \\\n",
							"  \"funded_amnt INT, \" + \\\n",
							"  \"paid_amnt DOUBLE, \" + \\\n",
							"  \"addr_state STRING)\"\n",
							")\n",
							"\n",
							"spark.sql(\"COPY INTO \" + table_name + \\\n",
							"  \" FROM '\" + source_data + \"'\" + \\\n",
							"  \" FILEFORMAT = \" + source_format\n",
							")\n",
							"\n",
							"loan_risks_upload_data = spark.sql(\"SELECT * FROM \" + table_name)\n",
							"\n",
							"display(loan_risks_upload_data)\n",
							"Load data to datalake table"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"delta_table_path = \"abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer\" \n",
							"data = spark.range(5,10) \n",
							"data.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
							"\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"old code\n",
							"\n",
							"\n",
							"\n",
							"# Create table in the metastore\n",
							"\n",
							"DeltaTable.createIfNotExists(spark) \\\n",
							"  .tableName(\"default.customer_dimension\") \\\n",
							"  .addColumn(\"CUSTOMER_KEY\", \"INT\") \\\n",
							"  .addColumn(\"VERSION\", \"INT\") \\\n",
							"  .addColumn(\"CUSTOMER_NO\", \"STRING\") \\\n",
							"  .addColumn(\"FINANCIAL_COUNTRY_CD\")\\\n",
							"  .addColumn(\"GBG_ID\", \"STRING\") \\\n",
							"  .addColumn(\"CUSTOMER_NAME\", \"STRING\") \\\n",
							"  .addColumn(\"CURRENT_IND\", \"STRING\") \\\n",
							"  .addColumn(\"EXTRACT_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"REC_START_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"REC_END_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"SOURCE_SYSTEM\", \"STRING\") \\\n",
							"  .addColumn(\"REC_STATUS\", \"STRING\") \\\n",
							"  .addColumn(\"IMG_LST_UPD_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"IMG_CREATED_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"DATA_IND\", \"STRING\") \\\n",
							"  .addColumn(\"ACTIVE_IN_SOURCE_IND\", \"STRING\") \\\n",
							"  .execute()\n",
							"\n",
							"######################\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"#jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;database=dsqlpoolKyn001494DevEtlHubEUS001;user=undefined@asa-kyn-001494-dev-eus-001;password={your_password_here};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\n",
							"existingDataDF = spark.read.format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"    .option(\"query\", \"SELECT MAX(CUSTOMER_KEY) OVER (ORDER BY CUSTOMER_KEY  ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) AS MAX_KEY, LOWER(CONVERT(VARCHAR(32),HashBytes('MD5', CUSTOMER_NO),2)) as existing_nk_hash,tgt.* FROM DBXDH.DHT_CUSTOMER as tgt WHERE CURRENT_IND='Y'\") \\\n",
							"    .option(\"user\", \"sqladminuser\") \\\n",
							"    .option(\"password\", \"try2find$5\") \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .load()\n",
							"#existingDataDF1 = existingDataDF.select([F.col(c).alias(\"`\"'existing_'+c+\"`\") for c in existingDataDF.columns])\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"#existingDataDF1.printSchema()\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_KEY, \"fullouter\") \n",
							"fullJoin1.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"#Insert for New rows\n",
							"\n",
							"fullJoin2=sqlContext.sql(\"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \\\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null\")\n",
							"fullJoin2.createOrReplaceTempView('fullJoin2')\n",
							"\n",
							"#insert new rows into database\n",
							"\n",
							"fullJoin2.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()\n",
							"fullJoin2.show()\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--select * from existingDataDF\n",
							"\n",
							"select * from fullJoin2\n",
							"\n",
							"#insert new rows into database\n",
							"/*\n",
							"fullJoin2.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()\n",
							"        */"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--ALL\n",
							"SELECT * FROM FULLJOIN;\n",
							"\n",
							"--No change records, ignore\n",
							"select * from fullJoin WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							"\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"select * from fullJoin WHERE WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--Soft deletes or rows no longer active in source\n",
							"select * from fullJoin WHERE WHERE nk_hash is null;\n",
							"\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1 \n",
							"select * from fullJoin where existing_existing_nk_hash is null;\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--INSERTS OR NEW ROWS\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION ,\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT,\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM,\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND,\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#test referance\n",
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"query\", \"INSERT INTO DBXDH.DHT_CUSTOMER1 \\\n",
							"        select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION , \\\n",
							"        CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"        CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"        'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"        'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"        from fullJoin A WHERE existing_existing_nk_hash is null\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"overwrite\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"df.write.mode(\"overwrite\") \\\n",
							"    .format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://localhost:1433;databaseName={database};\") \\\n",
							"    .option(\"dbtable\", table) \\\n",
							"    .option(\"user\", user) \\\n",
							"    .option(\"password\", password) \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/deltaLake_DCA_DEAL_SCD_Type2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "cebe8486-eb4f-4b64-bf0f-66df18d1cc3d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"#import necessary python libraries\r\n",
							"\r\n",
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"from pyspark.sql.functions import md5, concat_ws\r\n",
							"from sqlite3 import connect\r\n",
							"from pyspark.sql import functions as F\r\n",
							"from pyspark.sql.functions import trim,ltrim,rtrim,format\r\n",
							"#conn = connect(':memory:')\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession \r\n",
							"from pyspark.sql.types import * \r\n",
							"from delta.tables import *\r\n",
							"#import os\r\n",
							"import sys\r\n",
							"\r\n",
							"#Read data from adls csv file extracted from source at https://adls4fsoetlhubdevuseast.blob.core.windows.net/etlhubfilestorage/extract/DCA/DS_DEALS_DATA.csv\r\n",
							"\r\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \r\n",
							"container_name = 'etlhubfilestorage' # fill in your container name \r\n",
							"relative_path = '' # fill in your relative folder path \r\n",
							"\r\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \r\n",
							"print('Primary storage account path: ' + adls_path) \r\n",
							"\r\n",
							"# Read a csv file \r\n",
							"csv_path = adls_path + 'extract/DCA/DS_DEALS_DATA_UPD1.csv' \r\n",
							"#incrementalData_DF = spark.read.csv(csv_path, header = 'true', quote='\"') \r\n",
							"incrementalData_DF=spark.read.load(csv_path,format=\"csv\", sep=\"||\", inferSchema=\"true\", header=\"true\")\r\n",
							" \r\n",
							"incrementalData_DF.createOrReplaceTempView('DF_req')\r\n",
							"incrementalData_req_DF=spark.sql(\"select DEAL_ID,BID_SUPPORT_INET_ID,BID_SUPPORT_NOTES_ID,BUS_UNIT_ID,CLNT_NM,CLNT_NUM,COMPLEXITY,COMNTS_TXT,CMPLXTY_CMMNT_TXT,CMPLXTY_OVERRD,CNTRCT_TYP_DESC,ENGG_SPPRT_TXN_LINK_TXT,ENGG_SPPRT_TXN_NUM,INDSTRY_NM,INIT_SIEBEL_IND,CNTRY_ID,LEAD_ACCT_PRTNR_BTT_CD,LEAD_PRTNR_BTT_CD,LEAD_PRTNR_NOTES_ID,OLD_OWNR_INET_ID,OWNR_INET_ID,PROJ_DESC,OPPTNY_ID,OWNR_NOTES_ID,PARENT_DEAL_ID,PRPSL_MGR_NOTES_ID,REAS_TXT,RMVD_IND,RISK_RTNG,RISK_RTNG_CMNT_TXT,RSK_RTNG_OVERRD,SALES_STAGE_CD,SECONDARY_SERVICE,SCTR_NM,SVC_NM,SIEBEL_IND,SRC_CD,STAT_CD,RCA1_ANSWR_TXT,RCA1_ANSWR_CMMNT,RCA2_ANSWR_TXT,RCA2_ANSWR_CMMNT,RCA3_ANSWR_TXT,RCA3_ANSWR_CMMNT,RCA4_ANSWR_TXT,RCA4_ANSWR_CMMNT,RCA5_ANSWR_TXT,RCA5_ANSWR_CMMNT,RCA6_ANSWR_TXT,RCA6_ANSWR_CMMNT,RCA7_ANSWR_TXT,RCA7_ANSWR_CMMNT,RCA8_ANSWR_TXT,RCA8_ANSWR_CMMNT,APPRVR_CNT,RVWR_CNT,DEAL_TYPE,GEO_CD,HYBRD_TYPE_A,HYBRD_TYPE_B,LOAD_LOB,CRDT_RTNG,DD_OFFERG_CS_USD_AMT,DD_OFFERG_IS_USD_AMT,DD_OFFERG_SS_USD_AMT,DD_OFFERG_WCP_USD_AMT,RISK_CONSULTANT_NOTES_ID,SENIOR_SOLUTION_MANAGER_NOTES_ID,SIH_GM_NOTES_ID,RULES_EFFECTIVE_DATE,DEAL_LEAD_NOTES_ID,CNTRCT_SIGNOFF_APPRVL_STAT_CD,PAYBACK_TXT from DF_req\")\r\n",
							"\r\n",
							"tablename=\"etlhubConfirmed.dht_deal_dim\"\r\n",
							"natural_key=\"DEAL_ID\"\r\n",
							"keycolumn=\"DEAL_KEY\"\r\n",
							"\r\n",
							"col_list=[]\r\n",
							"for i in incrementalData_req_DF.columns:\r\n",
							"    col_list.append(i)\r\n",
							"#incrementalData_DF.show()\r\n",
							"#print (col_list)\r\n",
							"\r\n",
							"# Add a checsum column to help identify the changed rows\r\n",
							"\r\n",
							"incrementalData_DF1 = incrementalData_req_DF.withColumn(\"nk_hash\",md5(natural_key))\r\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\r\n",
							"#incrementalData_DF2.show()\r\n",
							"\r\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\r\n",
							"\r\n",
							"#Create a deltalake table with necessary columns\r\n",
							"\r\n",
							"#incrementalData_DF2.show()\r\n",
							"\r\n",
							"existingDataDF=spark.sql(\"SELECT DEAL_KEY,DEAL_VERSION_NUMBER,trim(DEAL_IDENTIFIER) as DEAL_ID,REC_CHECKSUM,IMG_CREATED_DT,REC_START_DT FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\r\n",
							"#existingDF.createOrReplaceTempView('existingDF')\r\n",
							"#existingDataDF.show()\r\n",
							"\r\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\r\n",
							"#existingMaxKeyDF.show()\r\n",
							"\r\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\r\n",
							"\r\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\r\n",
							"#existingDataDF1.printSchema()\r\n",
							"\r\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\r\n",
							"#existingDataDF1.show()\r\n",
							"\r\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,trim(incrementalData_DF2.DEAL_ID) == trim(existingDataDF1.existing_DEAL_ID), \"fullouter\") \r\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\r\n",
							"\r\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\r\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\r\n",
							"\r\n",
							"qry= \"\"\"\r\n",
							"INSERT INTO etlhubConfirmed.dht_deal_dim\r\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS DEAL_KEY,1 as DEAL_VERSION_NUMBER , \r\n",
							"A.DEAL_ID,\r\n",
							"A.BID_SUPPORT_INET_ID,\r\n",
							"A.BID_SUPPORT_NOTES_ID,\r\n",
							"A.BUS_UNIT_ID,\r\n",
							"A.CLNT_NM,\r\n",
							"A.CLNT_NUM,\r\n",
							"REPLACE(A.COMPLEXITY,'\"','') as COMPLEXITY,\r\n",
							"REPLACE(A.COMNTS_TXT,'\"','') as COMNTS_TXT,\r\n",
							"REPLACE(A.CMPLXTY_CMMNT_TXT,'\"','') as CMPLXTY_CMMNT_TXT,\r\n",
							"REPLACE(A.CMPLXTY_OVERRD,'\"','') as CMPLXTY_OVERRD,\r\n",
							"REPLACE(A.CNTRCT_TYP_DESC,'\"','') as CNTRCT_TYP_DESC,\r\n",
							"REPLACE(A.ENGG_SPPRT_TXN_LINK_TXT,'\"','') as ENGG_SPPRT_TXN_LINK_TXT,\r\n",
							"REPLACE(A.ENGG_SPPRT_TXN_NUM,'\"','') as ENGG_SPPRT_TXN_NUM,\r\n",
							"REPLACE(A.INDSTRY_NM,'\"','') as INDSTRY_NM,\r\n",
							"REPLACE(A.INIT_SIEBEL_IND,'\"','') as INIT_SIEBEL_IND,\r\n",
							"REPLACE(A.CNTRY_ID,'\"','') as CNTRY_ID,\r\n",
							"REPLACE(A.LEAD_ACCT_PRTNR_BTT_CD,'\"','') as LEAD_ACCT_PRTNR_BTT_CD,\r\n",
							"REPLACE(A.LEAD_PRTNR_BTT_CD,'\"','') as LEAD_PRTNR_BTT_CD,\r\n",
							"REPLACE(A.LEAD_PRTNR_NOTES_ID,'\"','') as LEAD_PRTNR_NOTES_ID,\r\n",
							"REPLACE(A.OLD_OWNR_INET_ID,'\"','') as OLD_OWNR_INET_ID,\r\n",
							"REPLACE(A.OWNR_INET_ID,'\"','') as OWNR_INET_ID,\r\n",
							"REPLACE(A.proj_desc,'\"','') as PROJ_DESC,\r\n",
							"REPLACE(A.OPPTNY_ID,'\"','') as OPPTNY_ID,\r\n",
							"REPLACE(A.OWNR_NOTES_ID,'\"','') as OWNR_NOTES_ID,\r\n",
							"REPLACE(A.PARENT_DEAL_ID,'\"','') as PARENT_DEAL_ID,\r\n",
							"REPLACE(A.PRPSL_MGR_NOTES_ID,'\"','') as PRPSL_MGR_NOTES_ID,\r\n",
							"REPLACE(A.REAS_TXT,'\"','') as REAS_TXT,\r\n",
							"REPLACE(A.RMVD_IND,'\"','') as RMVD_IND,\r\n",
							"REPLACE(A.RISK_RTNG,'\"','') as RISK_RTNG,\r\n",
							"REPLACE(A.RISK_RTNG_CMNT_TXT,'\"','') as RISK_RTNG_CMNT_TXT,\r\n",
							"REPLACE(A.RSK_RTNG_OVERRD,'\"','') as RSK_RTNG_OVERRD,\r\n",
							"REPLACE(A.SALES_STAGE_CD,'\"','') as SALES_STAGE_CD,\r\n",
							"REPLACE(A.SECONDARY_SERVICE,'\"','') as SECONDARY_SERVICE,\r\n",
							"REPLACE(A.SCTR_NM,'\"','') as SCTR_NM,\r\n",
							"REPLACE(A.SVC_NM,'\"','') as SVC_NM,\r\n",
							"REPLACE(A.SIEBEL_IND,'\"','') as SIEBEL_IND,\r\n",
							"REPLACE(A.SRC_CD,'\"','') as SRC_CD,\r\n",
							"REPLACE(A.STAT_CD,'\"','') as STAT_CD,\r\n",
							"'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT,current_timestamp as IMG_LST_UPD_DT,\r\n",
							"'9999-12-31 00:00:00.000' as REC_END_DT,CURRENT_TIMESTAMP AS REC_START_DT,  A.column_hash as REC_CHECKSUM,'I' as REC_STATUS, 'DCA' AS SOURCE_SYSTEM, \r\n",
							"REPLACE(A.RCA1_ANSWR_TXT,'\"','') as RCA1_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA1_ANSWR_CMMNT,'\"','') as RCA1_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA2_ANSWR_TXT,'\"','') as RCA2_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA2_ANSWR_CMMNT,'\"','') as RCA2_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA3_ANSWR_TXT,'\"','') as RCA3_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA3_ANSWR_CMMNT,'\"','') as RCA3_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA4_ANSWR_TXT,'\"','') as RCA4_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA4_ANSWR_CMMNT,'\"','') as RCA4_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA5_ANSWR_TXT,'\"','') as RCA5_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA5_ANSWR_CMMNT,'\"','') as RCA5_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA6_ANSWR_TXT,'\"','') as RCA6_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA6_ANSWR_CMMNT,'\"','') as RCA6_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA7_ANSWR_TXT,'\"','') as RCA7_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA7_ANSWR_CMMNT,'\"','') as RCA7_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA8_ANSWR_TXT,'\"','') as RCA8_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA8_ANSWR_CMMNT,'\"','') as RCA8_ANSWR_CMMNT,\r\n",
							"REPLACE(A.APPRVR_CNT,'\"','') as APPRVR_CNT,\r\n",
							"REPLACE(A.RVWR_CNT,'\"','') as RVWR_CNT,\r\n",
							"REPLACE(A.DEAL_TYPE,'\"','') as DEAL_TYPE,\r\n",
							"REPLACE(A.GEO_CD,'\"','') as GEO_CD,\r\n",
							"REPLACE(A.HYBRD_TYPE_A,'\"','') as HYBRD_TYPE_A,\r\n",
							"REPLACE(A.HYBRD_TYPE_B,'\"','') as HYBRD_TYPE_B,\r\n",
							"REPLACE(A.LOAD_LOB,'\"','') as LOAD_LOB,\r\n",
							"REPLACE(A.CRDT_RTNG,'\"','') as CRDT_RTNG,\r\n",
							"A.DD_OFFERG_CS_USD_AMT,\r\n",
							"A.DD_OFFERG_IS_USD_AMT,\r\n",
							"A.DD_OFFERG_SS_USD_AMT,\r\n",
							"A.DD_OFFERG_WCP_USD_AMT,\r\n",
							"REPLACE(A.RISK_CONSULTANT_NOTES_ID,'\"','') as RISK_CONSULTANT_NOTES_ID,\r\n",
							"REPLACE(A.SENIOR_SOLUTION_MANAGER_NOTES_ID,'\"','') as SENIOR_SOLUTION_MANAGER_NOTES_ID,\r\n",
							"REPLACE(A.SIH_GM_NOTES_ID,'\"','') as SIH_GM_NOTES_ID,\r\n",
							"A.RULES_EFFECTIVE_DATE,\r\n",
							"REPLACE(A.DEAL_LEAD_NOTES_ID,'\"','') as DEAL_LEAD_NOTES_ID,\r\n",
							"REPLACE(A.CNTRCT_SIGNOFF_APPRVL_STAT_CD,'\"','') as CNTRCT_SIGNOFF_APPRVL_STAT_CD,\r\n",
							"REPLACE(A.PAYBACK_TXT,'\"','') as PAYBACK_TXT,\r\n",
							"'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"LEFT JOIN etlhubConfirmed.dht_deal_dim B\r\n",
							"ON TRIM(A.DEAL_ID)=TRIM(B.DEAL_IDENTIFIER)\r\n",
							"AND CURRENT_IND='Y'\r\n",
							"WHERE existing_REC_CHECKSUM is null\r\n",
							"AND COALESCE(B.REC_CHECKSUM,'') <> COALESCE(A.column_hash,'');\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry)\r\n",
							"\r\n",
							"updins=\"\"\"\r\n",
							"INSERT INTO etlhubConfirmed.dht_deal_dim \r\n",
							"select existing_DEAL_KEY,1+existing_DEAL_VERSION_NUMBER as DEAL_VERSION_NUMBER , \r\n",
							"A.DEAL_ID,\r\n",
							"A.BID_SUPPORT_INET_ID,\r\n",
							"A.BID_SUPPORT_NOTES_ID,\r\n",
							"A.BUS_UNIT_ID,\r\n",
							"A.CLNT_NM,\r\n",
							"A.CLNT_NUM,\r\n",
							"A.COMPLEXITY,\r\n",
							"A.COMNTS_TXT,\r\n",
							"A.CMPLXTY_CMMNT_TXT,\r\n",
							"A.CMPLXTY_OVERRD,\r\n",
							"A.CNTRCT_TYP_DESC,\r\n",
							"A.ENGG_SPPRT_TXN_LINK_TXT,\r\n",
							"A.ENGG_SPPRT_TXN_NUM,\r\n",
							"A.INDSTRY_NM,\r\n",
							"A.INIT_SIEBEL_IND,\r\n",
							"A.CNTRY_ID,\r\n",
							"A.LEAD_ACCT_PRTNR_BTT_CD,\r\n",
							"A.LEAD_PRTNR_BTT_CD,\r\n",
							"A.LEAD_PRTNR_NOTES_ID,\r\n",
							"A.OLD_OWNR_INET_ID,\r\n",
							"A.OWNR_INET_ID,\r\n",
							"A.proj_desc,\r\n",
							"A.OPPTNY_ID,\r\n",
							"A.OWNR_NOTES_ID,\r\n",
							"A.PARENT_DEAL_ID,\r\n",
							"A.PRPSL_MGR_NOTES_ID,\r\n",
							"A.REAS_TXT,\r\n",
							"A.RMVD_IND,\r\n",
							"A.RISK_RTNG,\r\n",
							"A.RISK_RTNG_CMNT_TXT,\r\n",
							"A.RSK_RTNG_OVERRD,\r\n",
							"A.SALES_STAGE_CD,\r\n",
							"A.SECONDARY_SERVICE,\r\n",
							"A.SCTR_NM,\r\n",
							"A.SVC_NM,\r\n",
							"A.SIEBEL_IND,\r\n",
							"A.SRC_CD,\r\n",
							"A.STAT_CD,\r\n",
							"'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT,current_timestamp as IMG_LST_UPD_DT,\r\n",
							"'9999-12-31 00:00:00.000' as REC_END_DT,CURRENT_TIMESTAMP AS REC_START_DT,  A.column_hash as REC_CHECKSUM, \r\n",
							"'U' as REC_STATUS, 'DCA' AS SOURCE_SYSTEM,\r\n",
							"A.RCA1_ANSWR_TXT,\r\n",
							"A.RCA1_ANSWR_CMMNT,\r\n",
							"A.RCA2_ANSWR_TXT,\r\n",
							"A.RCA2_ANSWR_CMMNT, \r\n",
							"A.RCA3_ANSWR_TXT,\r\n",
							"A.RCA3_ANSWR_CMMNT,\r\n",
							"A.RCA4_ANSWR_TXT,\r\n",
							"A.RCA4_ANSWR_CMMNT,\r\n",
							"A.RCA5_ANSWR_TXT,\r\n",
							"A.RCA5_ANSWR_CMMNT,\r\n",
							"A.RCA6_ANSWR_TXT,\r\n",
							"A.RCA6_ANSWR_CMMNT,\r\n",
							"A.RCA7_ANSWR_TXT,\r\n",
							"A.RCA7_ANSWR_CMMNT,\r\n",
							"A.RCA8_ANSWR_TXT,\r\n",
							"A.RCA8_ANSWR_CMMNT,\r\n",
							"A.APPRVR_CNT,\r\n",
							"A.RVWR_CNT,\r\n",
							"A.DEAL_TYPE,\r\n",
							"A.GEO_CD,\r\n",
							"A.HYBRD_TYPE_A,\r\n",
							"A.HYBRD_TYPE_B,\r\n",
							"A.LOAD_LOB,\r\n",
							"A.CRDT_RTNG,\r\n",
							"A.DD_OFFERG_CS_USD_AMT,\r\n",
							"A.DD_OFFERG_IS_USD_AMT,\r\n",
							"A.DD_OFFERG_SS_USD_AMT,\r\n",
							"A.DD_OFFERG_WCP_USD_AMT,\r\n",
							"A.RISK_CONSULTANT_NOTES_ID,\r\n",
							"A.SENIOR_SOLUTION_MANAGER_NOTES_ID,\r\n",
							"A.SIH_GM_NOTES_ID,\r\n",
							"A.RULES_EFFECTIVE_DATE,\r\n",
							"A.DEAL_LEAD_NOTES_ID,\r\n",
							"A.CNTRCT_SIGNOFF_APPRVL_STAT_CD,\r\n",
							"A.PAYBACK_TXT,\r\n",
							"'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"WHERE DEAL_ID = existing_DEAL_ID and LOWER(column_hash) <> LOWER(existing_REC_CHECKSUM)\r\n",
							"AND NOT EXISTS\r\n",
							"(SELECT 1 FROM etlhubConfirmed.dht_deal_dim B\r\n",
							"WHERE TRIM(A.DEAL_ID)=TRIM(B.DEAL_IDENTIFIER)\r\n",
							"and b.CURRENT_IND='Y'\r\n",
							"AND A.column_hash=B.REC_CHECKSUM) ;\r\n",
							"\"\"\"\r\n",
							"spark.sql(updins)\r\n",
							"\r\n",
							"expqry=\"\"\"\r\n",
							"MERGE INTO etlhubConfirmed.dht_deal_dim A\r\n",
							"USING fullJoin B\r\n",
							"ON B.DEAL_ID = A.DEAL_IDENTIFIER\r\n",
							"AND LOWER(A.DEAL_IDENTIFIER) = LOWER(B.existing_DEAL_ID) and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\r\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\r\n",
							",REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\r\n",
							",IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"spark.sql(expqry)\r\n",
							"\r\n",
							"#Soft deletes or no longer active in source\r\n",
							"softdelqry=\"\"\"\r\n",
							"MERGE INTO etlhubConfirmed.dht_deal_dim  A\r\n",
							"USING fullJoin B\r\n",
							"ON A.DEAL_IDENTIFIER = B.existing_DEAL_ID\r\n",
							"AND B.DEAL_ID is NULL\r\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N' ,IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"spark.sql(softdelqry)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from etlhubConfirmed.dht_deal_dim \r\n",
							"where DEAL_IDENTIFIER in (7840900,7840899)\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"INSERT INTO etlhubConfirmed.dht_deal_dim\r\n",
							"select existing_DEAL_KEY,1+existing_DEAL_VERSION_NUMBER as DEAL_VERSION_NUMBER , \r\n",
							"A.DEAL_ID,\r\n",
							"A.BID_SUPPORT_INET_ID,\r\n",
							"A.BID_SUPPORT_NOTES_ID,\r\n",
							"A.BUS_UNIT_ID,\r\n",
							"A.CLNT_NM,\r\n",
							"A.CLNT_NUM,\r\n",
							"REPLACE(A.COMPLEXITY,'\"','') as COMPLEXITY,\r\n",
							"REPLACE(A.COMNTS_TXT,'\"','') as COMNTS_TXT,\r\n",
							"REPLACE(A.CMPLXTY_CMMNT_TXT,'\"','') as CMPLXTY_CMMNT_TXT,\r\n",
							"REPLACE(A.CMPLXTY_OVERRD,'\"','') as CMPLXTY_OVERRD,\r\n",
							"REPLACE(A.CNTRCT_TYP_DESC,'\"','') as CNTRCT_TYP_DESC,\r\n",
							"REPLACE(A.ENGG_SPPRT_TXN_LINK_TXT,'\"','') as ENGG_SPPRT_TXN_LINK_TXT,\r\n",
							"REPLACE(A.ENGG_SPPRT_TXN_NUM,'\"','') as ENGG_SPPRT_TXN_NUM,\r\n",
							"REPLACE(A.INDSTRY_NM,'\"','') as INDSTRY_NM,\r\n",
							"REPLACE(A.INIT_SIEBEL_IND,'\"','') as INIT_SIEBEL_IND,\r\n",
							"REPLACE(A.CNTRY_ID,'\"','') as CNTRY_ID,\r\n",
							"REPLACE(A.LEAD_ACCT_PRTNR_BTT_CD,'\"','') as LEAD_ACCT_PRTNR_BTT_CD,\r\n",
							"REPLACE(A.LEAD_PRTNR_BTT_CD,'\"','') as LEAD_PRTNR_BTT_CD,\r\n",
							"REPLACE(A.LEAD_PRTNR_NOTES_ID,'\"','') as LEAD_PRTNR_NOTES_ID,\r\n",
							"REPLACE(A.OLD_OWNR_INET_ID,'\"','') as OLD_OWNR_INET_ID,\r\n",
							"REPLACE(A.OWNR_INET_ID,'\"','') as OWNR_INET_ID,\r\n",
							"REPLACE(A.proj_desc,'\"','') as PROJ_DESC,\r\n",
							"REPLACE(A.OPPTNY_ID,'\"','') as OPPTNY_ID,\r\n",
							"REPLACE(A.OWNR_NOTES_ID,'\"','') as OWNR_NOTES_ID,\r\n",
							"REPLACE(A.PARENT_DEAL_ID,'\"','') as PARENT_DEAL_ID,\r\n",
							"REPLACE(A.PRPSL_MGR_NOTES_ID,'\"','') as PRPSL_MGR_NOTES_ID,\r\n",
							"REPLACE(A.REAS_TXT,'\"','') as REAS_TXT,\r\n",
							"REPLACE(A.RMVD_IND,'\"','') as RMVD_IND,\r\n",
							"REPLACE(A.RISK_RTNG,'\"','') as RISK_RTNG,\r\n",
							"REPLACE(A.RISK_RTNG_CMNT_TXT,'\"','') as RISK_RTNG_CMNT_TXT,\r\n",
							"REPLACE(A.RSK_RTNG_OVERRD,'\"','') as RSK_RTNG_OVERRD,\r\n",
							"REPLACE(A.SALES_STAGE_CD,'\"','') as SALES_STAGE_CD,\r\n",
							"REPLACE(A.SECONDARY_SERVICE,'\"','') as SECONDARY_SERVICE,\r\n",
							"REPLACE(A.SCTR_NM,'\"','') as SCTR_NM,\r\n",
							"REPLACE(A.SVC_NM,'\"','') as SVC_NM,\r\n",
							"REPLACE(A.SIEBEL_IND,'\"','') as SIEBEL_IND,\r\n",
							"REPLACE(A.SRC_CD,'\"','') as SRC_CD,\r\n",
							"REPLACE(A.STAT_CD,'\"','') as STAT_CD,\r\n",
							"'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT,current_timestamp as IMG_LST_UPD_DT,\r\n",
							"'9999-12-31 00:00:00.000' as REC_END_DT,CURRENT_TIMESTAMP AS REC_START_DT,  A.column_hash as REC_CHECKSUM, \r\n",
							"'U' as REC_STATUS, 'DCA' AS SOURCE_SYSTEM,\r\n",
							"REPLACE(A.RCA1_ANSWR_TXT,'\"','') as RCA1_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA1_ANSWR_CMMNT,'\"','') as RCA1_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA2_ANSWR_TXT,'\"','') as RCA2_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA2_ANSWR_CMMNT,'\"','') as RCA2_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA3_ANSWR_TXT,'\"','') as RCA3_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA3_ANSWR_CMMNT,'\"','') as RCA3_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA4_ANSWR_TXT,'\"','') as RCA4_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA4_ANSWR_CMMNT,'\"','') as RCA4_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA5_ANSWR_TXT,'\"','') as RCA5_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA5_ANSWR_CMMNT,'\"','') as RCA5_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA6_ANSWR_TXT,'\"','') as RCA6_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA6_ANSWR_CMMNT,'\"','') as RCA6_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA7_ANSWR_TXT,'\"','') as RCA7_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA7_ANSWR_CMMNT,'\"','') as RCA7_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA8_ANSWR_TXT,'\"','') as RCA8_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA8_ANSWR_CMMNT,'\"','') as RCA8_ANSWR_CMMNT,\r\n",
							"REPLACE(A.APPRVR_CNT,'\"','') as APPRVR_CNT,\r\n",
							"REPLACE(A.RVWR_CNT,'\"','') as RVWR_CNT,\r\n",
							"REPLACE(A.DEAL_TYPE,'\"','') as DEAL_TYPE,\r\n",
							"REPLACE(A.GEO_CD,'\"','') as GEO_CD,\r\n",
							"REPLACE(A.HYBRD_TYPE_A,'\"','') as HYBRD_TYPE_A,\r\n",
							"REPLACE(A.HYBRD_TYPE_B,'\"','') as HYBRD_TYPE_B,\r\n",
							"REPLACE(A.LOAD_LOB,'\"','') as LOAD_LOB,\r\n",
							"REPLACE(A.CRDT_RTNG,'\"','') as CRDT_RTNG,\r\n",
							"A.DD_OFFERG_CS_USD_AMT,\r\n",
							"A.DD_OFFERG_IS_USD_AMT,\r\n",
							"A.DD_OFFERG_SS_USD_AMT,\r\n",
							"A.DD_OFFERG_WCP_USD_AMT,\r\n",
							"REPLACE(A.RISK_CONSULTANT_NOTES_ID,'\"','') as RISK_CONSULTANT_NOTES_ID,\r\n",
							"REPLACE(A.SENIOR_SOLUTION_MANAGER_NOTES_ID,'\"','') as SENIOR_SOLUTION_MANAGER_NOTES_ID,\r\n",
							"REPLACE(A.SIH_GM_NOTES_ID,'\"','') as SIH_GM_NOTES_ID,\r\n",
							"A.RULES_EFFECTIVE_DATE,\r\n",
							"REPLACE(A.DEAL_LEAD_NOTES_ID,'\"','') as DEAL_LEAD_NOTES_ID,\r\n",
							"REPLACE(A.CNTRCT_SIGNOFF_APPRVL_STAT_CD,'\"','') as CNTRCT_SIGNOFF_APPRVL_STAT_CD,\r\n",
							"REPLACE(A.PAYBACK_TXT,'\"','') as PAYBACK_TXT,\r\n",
							"'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"WHERE LOWER(DEAL_ID) = LOWER(existing_DEAL_ID) and LOWER(column_hash) <> LOWER(existing_REC_CHECKSUM)\r\n",
							"AND NOT EXISTS\r\n",
							"(SELECT 1 FROM etlhubConfirmed.dht_deal_dim B\r\n",
							"WHERE TRIM(A.DEAL_ID)=TRIM(B.DEAL_IDENTIFIER)\r\n",
							"AND b.CURRENT_IND='Y' AND A.column_hash=B.REC_CHECKSUM )\r\n",
							";"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"# expiring old record\r\n",
							"qry2=\"\"\"\r\n",
							"MERGE INTO {} A\r\n",
							"USING fullJoin B\r\n",
							"ON B.DEAL_ID = A.DEAL_IDENTIFIER\r\n",
							"AND LOWER(A.DEAL_IDENTIFIER) = LOWER(B.existing_DEAL_ID) and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\r\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\r\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\r\n",
							"    ,IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry2.format(tablename))\r\n",
							"#Soft deletes or no longer active in source\r\n",
							"qry3=\"\"\"\r\n",
							"MERGE INTO {} A\r\n",
							"USING fullJoin B\r\n",
							"ON A.DEAL_IDENTIFIER = B.existing_DEAL_ID\r\n",
							"AND B.DEAL_ID is NULL\r\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\r\n",
							"    --,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\r\n",
							"    ,IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry3.format(tablename))\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select existing_DEAL_KEY,1+existing_DEAL_VERSION_NUMBER as DEAL_VERSION_NUMBER , \r\n",
							"A.DEAL_ID,\r\n",
							"A.BID_SUPPORT_INET_ID,\r\n",
							"A.BID_SUPPORT_NOTES_ID,\r\n",
							"A.BUS_UNIT_ID,\r\n",
							"A.CLNT_NM,\r\n",
							"A.CLNT_NUM,\r\n",
							"REPLACE(A.COMPLEXITY,'\"','') as COMPLEXITY,\r\n",
							"REPLACE(A.COMNTS_TXT,'\"','') as COMNTS_TXT,\r\n",
							"REPLACE(A.CMPLXTY_CMMNT_TXT,'\"','') as CMPLXTY_CMMNT_TXT,\r\n",
							"REPLACE(A.CMPLXTY_OVERRD,'\"','') as CMPLXTY_OVERRD,\r\n",
							"REPLACE(A.CNTRCT_TYP_DESC,'\"','') as CNTRCT_TYP_DESC,\r\n",
							"REPLACE(A.ENGG_SPPRT_TXN_LINK_TXT,'\"','') as ENGG_SPPRT_TXN_LINK_TXT,\r\n",
							"REPLACE(A.ENGG_SPPRT_TXN_NUM,'\"','') as ENGG_SPPRT_TXN_NUM,\r\n",
							"REPLACE(A.INDSTRY_NM,'\"','') as INDSTRY_NM,\r\n",
							"REPLACE(A.INIT_SIEBEL_IND,'\"','') as INIT_SIEBEL_IND,\r\n",
							"REPLACE(A.CNTRY_ID,'\"','') as CNTRY_ID,\r\n",
							"REPLACE(A.LEAD_ACCT_PRTNR_BTT_CD,'\"','') as LEAD_ACCT_PRTNR_BTT_CD,\r\n",
							"REPLACE(A.LEAD_PRTNR_BTT_CD,'\"','') as LEAD_PRTNR_BTT_CD,\r\n",
							"REPLACE(A.LEAD_PRTNR_NOTES_ID,'\"','') as LEAD_PRTNR_NOTES_ID,\r\n",
							"REPLACE(A.OLD_OWNR_INET_ID,'\"','') as OLD_OWNR_INET_ID,\r\n",
							"REPLACE(A.OWNR_INET_ID,'\"','') as OWNR_INET_ID,\r\n",
							"REPLACE(A.proj_desc,'\"','') as PROJ_DESC,\r\n",
							"REPLACE(A.OPPTNY_ID,'\"','') as OPPTNY_ID,\r\n",
							"REPLACE(A.OWNR_NOTES_ID,'\"','') as OWNR_NOTES_ID,\r\n",
							"REPLACE(A.PARENT_DEAL_ID,'\"','') as PARENT_DEAL_ID,\r\n",
							"REPLACE(A.PRPSL_MGR_NOTES_ID,'\"','') as PRPSL_MGR_NOTES_ID,\r\n",
							"REPLACE(A.REAS_TXT,'\"','') as REAS_TXT,\r\n",
							"REPLACE(A.RMVD_IND,'\"','') as RMVD_IND,\r\n",
							"REPLACE(A.RISK_RTNG,'\"','') as RISK_RTNG,\r\n",
							"REPLACE(A.RISK_RTNG_CMNT_TXT,'\"','') as RISK_RTNG_CMNT_TXT,\r\n",
							"REPLACE(A.RSK_RTNG_OVERRD,'\"','') as RSK_RTNG_OVERRD,\r\n",
							"REPLACE(A.SALES_STAGE_CD,'\"','') as SALES_STAGE_CD,\r\n",
							"REPLACE(A.SECONDARY_SERVICE,'\"','') as SECONDARY_SERVICE,\r\n",
							"REPLACE(A.SCTR_NM,'\"','') as SCTR_NM,\r\n",
							"REPLACE(A.SVC_NM,'\"','') as SVC_NM,\r\n",
							"REPLACE(A.SIEBEL_IND,'\"','') as SIEBEL_IND,\r\n",
							"REPLACE(A.SRC_CD,'\"','') as SRC_CD,\r\n",
							"REPLACE(A.STAT_CD,'\"','') as STAT_CD,\r\n",
							"'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT,current_timestamp as IMG_LST_UPD_DT,\r\n",
							"'9999-12-31 00:00:00.000' as REC_END_DT,CURRENT_TIMESTAMP AS REC_START_DT,  A.column_hash as REC_CHECKSUM, \r\n",
							"'U' as REC_STATUS, 'DCA' AS SOURCE_SYSTEM,\r\n",
							"REPLACE(A.RCA1_ANSWR_TXT,'\"','') as RCA1_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA1_ANSWR_CMMNT,'\"','') as RCA1_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA2_ANSWR_TXT,'\"','') as RCA2_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA2_ANSWR_CMMNT,'\"','') as RCA2_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA3_ANSWR_TXT,'\"','') as RCA3_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA3_ANSWR_CMMNT,'\"','') as RCA3_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA4_ANSWR_TXT,'\"','') as RCA4_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA4_ANSWR_CMMNT,'\"','') as RCA4_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA5_ANSWR_TXT,'\"','') as RCA5_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA5_ANSWR_CMMNT,'\"','') as RCA5_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA6_ANSWR_TXT,'\"','') as RCA6_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA6_ANSWR_CMMNT,'\"','') as RCA6_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA7_ANSWR_TXT,'\"','') as RCA7_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA7_ANSWR_CMMNT,'\"','') as RCA7_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA8_ANSWR_TXT,'\"','') as RCA8_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA8_ANSWR_CMMNT,'\"','') as RCA8_ANSWR_CMMNT,\r\n",
							"REPLACE(A.APPRVR_CNT,'\"','') as APPRVR_CNT,\r\n",
							"REPLACE(A.RVWR_CNT,'\"','') as RVWR_CNT,\r\n",
							"REPLACE(A.DEAL_TYPE,'\"','') as DEAL_TYPE,\r\n",
							"REPLACE(A.GEO_CD,'\"','') as GEO_CD,\r\n",
							"REPLACE(A.HYBRD_TYPE_A,'\"','') as HYBRD_TYPE_A,\r\n",
							"REPLACE(A.HYBRD_TYPE_B,'\"','') as HYBRD_TYPE_B,\r\n",
							"REPLACE(A.LOAD_LOB,'\"','') as LOAD_LOB,\r\n",
							"REPLACE(A.CRDT_RTNG,'\"','') as CRDT_RTNG,\r\n",
							"A.DD_OFFERG_CS_USD_AMT,\r\n",
							"A.DD_OFFERG_IS_USD_AMT,\r\n",
							"A.DD_OFFERG_SS_USD_AMT,\r\n",
							"A.DD_OFFERG_WCP_USD_AMT,\r\n",
							"REPLACE(A.RISK_CONSULTANT_NOTES_ID,'\"','') as RISK_CONSULTANT_NOTES_ID,\r\n",
							"REPLACE(A.SENIOR_SOLUTION_MANAGER_NOTES_ID,'\"','') as SENIOR_SOLUTION_MANAGER_NOTES_ID,\r\n",
							"REPLACE(A.SIH_GM_NOTES_ID,'\"','') as SIH_GM_NOTES_ID,\r\n",
							"A.RULES_EFFECTIVE_DATE,\r\n",
							"REPLACE(A.DEAL_LEAD_NOTES_ID,'\"','') as DEAL_LEAD_NOTES_ID,\r\n",
							"REPLACE(A.CNTRCT_SIGNOFF_APPRVL_STAT_CD,'\"','') as CNTRCT_SIGNOFF_APPRVL_STAT_CD,\r\n",
							"REPLACE(A.PAYBACK_TXT,'\"','') as PAYBACK_TXT,\r\n",
							"'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"WHERE LOWER(DEAL_ID) = LOWER(existing_DEAL_ID) and LOWER(column_hash) <> LOWER(existing_REC_CHECKSUM)\r\n",
							"AND NOT EXISTS\r\n",
							"(SELECT 1 FROM etlhubConfirmed.dht_deal_dim B\r\n",
							"WHERE TRIM(A.DEAL_ID)=TRIM(B.DEAL_IDENTIFIER)\r\n",
							"and b.CURRENT_IND='Y'\r\n",
							"AND A.column_hash=B.REC_CHECKSUM)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select distinct substr(DEAL_IDENTIFIER,2,1) from etlhubConfirmed.dht_deal_dim\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from etlhubConfirmed.dht_deal_dim where deal_key = 418"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/deltaLake_Project_SCD_Type2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "74810b23-c03c-4730-b9f7-f4a50a418aa7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"SCD Type2 using adls as source and delta lake as target"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#import necessary python libraries\n",
							"\n",
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"#import os\n",
							"import sys\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"#Define Parameters\n",
							"\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'project' # fill in your container name \n",
							"relative_path = '' # fill in your relative folder path \n",
							"file_name='project_data.csv' \n",
							"natural_key=\"PROJECT_ID\"\n",
							"tablename=\"etlhubConfirmed.dht_project\"\n",
							"#natural_key=\"BUSINESS_PARTNER_ID\"\n",
							"keycolumn=\"PROJECT_KEY\"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Read data from adls csv file extracted from source at https://adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/data/customer/\n",
							"\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\n",
							"\n",
							"\n",
							"#columns1 = [\"FINANCIAL_COUNTRY_CD\",\"CUSTOMER_DESC\",\"GBG_ID\"]\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"#Add checks for duplicate data, Check for 1. dups in source data based on natural Key and\n",
							"# 2. Dups in target already\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"    #print (col_list)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Create a deltalake table with necessary columns\n",
							"\n",
							"#incrementalData_DF2.show()\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\n",
							"\n",
							"\n",
							"#existingDataDF=spark.sql(\"SELECT tgt.* from etlhubConfirmed.customer_dimension tgt WHERE CURRENT_IND='Y'\")\n",
							"#existingDF.createOrReplaceTempView('existingDF')\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\n",
							"#existingMaxKeyDF=spark.sql(\"SELECT MAX(CUSTOMER_KEY) existing_MAX_KEY from etlhubConfirmed.customer_dimension tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"#existingMaxKeyDF.show()\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"#existingDataDF1.printSchema()\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(natural_key)\n",
							"\n",
							"#incrementalData_DF2.show()\n",
							"\n",
							"#existingDataDF1.show()\n",
							"\n",
							"key1='existing_' + natural_key\n",
							"print(key1)\n",
							"\n",
							"#existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"#fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.PROJECT_ID==existingDataDF1.existing_PROJECT_ID,\"fullouter\")\n",
							"var1=\"incrementalData_DF2.PROJECT_ID\"\n",
							"var2=\"existingDataDF1.existing_PROJECT_ID\"\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,({} == {}).format(var1,var2), \"fullouter\") \n",
							"\n",
							"#fullJoin1=incrementalData_DF2.join(existingDataDF1,{},'fullouter').format(natural_key)\n",
							"#firstDf.join(secondDf, [column], 'inner')\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry= \"\"\"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"AND CURRENT_IND='Y'\n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND B.REC_CHECKSUM <> A.column_hash\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry1= \"\"\"\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"AND CURRENT_IND='Y'\n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND B.REC_CHECKSUM <> A.column_hash\n",
							";\n",
							"\"\"\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Data validation Checks\n",
							"\n",
							"\n",
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"df3=spark.sql(qry3).format(natural_key,natural_key)\n",
							"df4=spark.sql(qry4).format(natural_key,tablename,natural_key)\n",
							"\n",
							"cnt1=df3.count()\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt1)\n",
							"if cnt1 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in source:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(1)\n",
							"\n",
							"if cnt2 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"#print (\"this will not be printed either\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							"\n",
							"#try:\n",
							"#  sqlContext.sql(\"create table {}.`{}` as select * from mytempTable\".format(hivedb,table))\n",
							"#except:\n",
							"#   status = 'fail'\n",
							"\n",
							"#assert status == 'success', 'status should be success'\n",
							"\n",
							"#a=spark.sql(qry)\n",
							"\n",
							"#print( df3. || ' Duplicate found ')\n",
							"\n",
							"#a.num_affected_rows\n",
							"#print(numOutputRows)\n",
							"\n",
							"\n",
							"#deltaTable1 = DeltaTable.forPath(spark, 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2')\n",
							"\n",
							"#deltaTable = DeltaTable.forName(spark, 'etlhubConfirmed.customer_dimension')\n",
							"\n",
							"\n",
							"#fullHistoryDF = deltaTable.history()    # get the full history of the table\n",
							"\n",
							"#lastOperationDF = deltaTable.history(1) # get the last operation\n",
							"\n",
							"#print(lastOperationDF.operationMetrics)\n",
							"\n",
							"#lastOperationDF.show()\n",
							"\n",
							"#fullHistoryDF.show()\n",
							"\n",
							"#print(num_affected_rows)\n",
							"\n",
							"#print(num_inserted_rows)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--SELECT COUNT(*) as CNT, CUSTOMER_NO FROM incrementalData_DF2 GROUP BY CUSTOMER_NO HAVING COUNT(*)>1\n",
							"SELECT * FROM fullJoin WHERE CUSTOMER_NO is NULL\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--SELECT COUNT(*) as CNT, CUSTOMER_NO FROM incrementalData_DF2 GROUP BY CUSTOMER_NO HAVING COUNT(*)>1\n",
							"\n",
							"/*\n",
							"\n",
							"select * from incrementalData_DF2;\n",
							"\n",
							"select * from existingDataDF1;\n",
							"\n",
							"select * from fullJoin;\n",
							"*/\n",
							"\n",
							"--select * from incrementalData_DF2;\n",
							"\n",
							"--select * from etlhubconfirmed.customer_dimension;\n",
							"\n",
							"SELECT 'All Rows' as Title, a.* FROM fullJoin a;\n",
							"\n",
							"\n",
							"--No change records, ignore --7ROWS\n",
							"select 'No Change Rows' as Title, a.*  from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.customer_dimension B\n",
							"WHERE A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert2' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert2' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--select * from etlhubConfirmed.customer_dimension where rec_checksum='c475b27f1b384e1d2289948edad59d84'\n",
							"/*\n",
							"\n",
							"SELECT * FROM etlhubconfirmed.customer_dimension;\n",
							"\n",
							"UPDATE etlhubConfirmed.customer_dimension\n",
							"SET GBG_ID='GB302S66'\n",
							"    ,REC_CHECKSUM='c475b27f1b384e1d2289948edad59d86'\n",
							"    WHERE CUSTOMER_KEY=5\n",
							"    ;\n",
							"\n",
							"\n",
							"SELECT * FROM etlhubconfirmed.customer_dimension;    \n",
							"*/\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"\n",
							"--select * from fullJoin WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"/*\n",
							"UPDATE etlhubconfirmed.customer_dimension\n",
							"set CURRENT_IND='N'\n",
							"    ,REC_END_DT=current_timestamp \n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"WHERE (CUSTOMER_NO ) =  (select CUSTOMER_NO from fullJoin WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum) )\n",
							"AND CURRENT_IND='Y'\n",
							";\n",
							"\n",
							"*/\n",
							"\n",
							"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select existing_CUSTOMER_KEY,1+existing_VERSION as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'U' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.customer_dimension B\n",
							"WHERE A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert after insert' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert after insert' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"MERGE INTO etlhubconfirmed.customer_dimension A\n",
							"USING fullJoin B\n",
							"ON A.CUSTOMER_NO = B.CUSTOMER_NO\n",
							"AND LOWER(B.CUSTOMER_NO) = LOWER(B.existing_CUSTOMER_NO) and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select 'New Rows for Insert After Update' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_CUSTOMER_KEY is null;\n",
							"--UPDATE ROWS\n",
							"select 'Changed Rows for Update/Insert After Update' as Title, a.* from fullJoin a WHERE LOWER(CUSTOMER_NO) = LOWER(existing_CUSTOMER_NO) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"--Soft deletes or no longer active in source\n",
							"\n",
							"\n",
							"--SELECT * FROM fullJoin WHERE CUSTOMER_NO is NULL\n",
							"\n",
							"MERGE INTO etlhubconfirmed.customer_dimension A\n",
							"USING fullJoin B\n",
							"ON A.CUSTOMER_NO = B.existing_CUSTOMER_NO\n",
							"AND B.CUSTOMER_NO is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\n",
							"    --,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\n",
							"select 'Final rows in SCD Type2' as Title,a.* from etlhubconfirmed.customer_dimension a;\n",
							"    "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"select * from etlhubconfirmed.customer_dimension ;\n",
							"--CUSTOMER_NO='C0000216';\n",
							"--group by VERSION having count(*)>1;\n",
							"/*\n",
							"Update etlhubconfirmed.customer_dimension\n",
							"set CURRENT_IND='Y'\n",
							"    WHERE CURRENT_IND='N' and VERSION=2.0;\n",
							"--DELETE FROM etlhubconfirmed.customer_dimension WHERE customer_key in (3.0,4.0);\n",
							"\n",
							"\n",
							"Update etlhubconfirmed.customer_dimension\n",
							"set FINANCIAL_COUNTRY_CD='896'\n",
							"    ,REC_CHECKSUM='3145dfee7cc94e4483b4b0c7244a9949'\n",
							"    WHERE customer_key=7.0;\n",
							"Update etlhubconfirmed.customer_dimension\n",
							"set GBG_ID='GB302S60'\n",
							"    ,customer_name='WALPOLE CO-OPERATIVE BANK1'\n",
							"    ,REC_CHECKSUM='0520612ce8718d5df3b8bb4b165a6548'\n",
							"    WHERE customer_key=8.0;\n",
							"UPDATE etlhubconfirmed.customer_dimension\n",
							"SET CURRENT_IND='N'\n",
							"    WHERE CUSTOMER_KEY=11.0;\n",
							"*/\n",
							"--select * from etlhubconfirmed.customer_dimension;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 90
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"\n",
							"select * from etlhubconfirmed.customer_dimension;\n",
							"\n",
							"\n",
							"\n",
							"select * from fullJoin;\n",
							"\n",
							"--No change records, ignore\n",
							"select * from fullJoin WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							"\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"select * from fullJoin WHERE WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--Soft deletes or rows no longer active in source\n",
							"select * from fullJoin WHERE WHERE nk_hash is null;\n",
							"\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1 \n",
							"select * from fullJoin where existing_existing_nk_hash is null;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"select customer_key,count(*) from etlhubconfirmed.customer_dimension where current_ind='Y' group by customer_key having count(*)>1;\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"etlhubConfirmed.customer_dimension.toDF('abc')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"UPDATE etlhubconfirmed.customer_dimension\n",
							"SET fiNANCIAL_COUNTRY_CD='907'\n",
							"    WHERE CURRENT_IND='Y' AND CUSTOMER_NO='0074657';\n",
							"select * from etlhubconfirmed.customer_dimension;\n",
							"\n",
							"DELETe from etlhubconfirmed.customer_dimension where fiNANCIAL_COUNTRY_CD='905';\n",
							"select * from etlhubconfirmed.customer_dimension;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"MERGE INTO default.people10m\n",
							"USING default.people10m_upload\n",
							"ON default.people10m.id = default.people10m_upload.id\n",
							"WHEN MATCHED THEN UPDATE SET *\n",
							"WHEN NOT MATCHED THEN INSERT *"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create a deltalake table with necessary columns\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--CREATE DATABASE etlhubConfirmed;\n",
							"\n",
							"--drop table etlhubConfirmed.customer_dimension;\n",
							"\n",
							"-- Create Delta Lake table, define schema and location\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.customer_dimension (\n",
							"    CUSTOMER_KEY INT NOT NULL,\n",
							"\tVERSION INT ,\n",
							"\tCUSTOMER_NO STRING ,\n",
							"\tFINANCIAL_COUNTRY_CD varchar(10) ,\n",
							"\tGBG_ID varchar(10)  ,\n",
							"\tCUSTOMER_NAME varchar(30)  ,\n",
							"\tCURRENT_IND varchar(1)  ,\n",
							"\tEXTRACT_DT TIMESTAMP ,\n",
							"\tREC_START_DT TIMESTAMP ,\n",
							"\tREC_END_DT TIMESTAMP ,\n",
							"\tSOURCE_SYSTEM varchar(50)  ,\n",
							"\tREC_CHECKSUM varchar(32)  ,\n",
							"\tREC_STATUS varchar(1)  ,\n",
							"\tIMG_LST_UPD_DT TIMESTAMP NOT NULL,\n",
							"\tIMG_CREATED_DT TIMESTAMP NOT NULL,\n",
							"\tDATA_IND varchar(10)  ,\n",
							"\tACTIVE_IN_SOURCE_IND char(1)  \n",
							")\n",
							"USING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"-- specify data lake folder location\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer'\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"source_data = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"source_format = 'csv'\n",
							"\n",
							"spark.sql(\"COPY INTO \" + table_name + \\\n",
							"  \" FROM '\" + source_data + \"'\" + \\\n",
							"  \" FILEFORMAT = \" + source_format + ';'\n",
							")\n",
							"\n",
							"customer_dim_data = spark.sql(\"SELECT * FROM \" + table_name)\n",
							"\n",
							"display(customer_dim_data)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"read_format = 'csv'\n",
							"write_format = 'delta'\n",
							"load_path = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"save_path = 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2'\n",
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"\n",
							"\n",
							"account_name1 = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name1 = 'customer' # fill in your container name \n",
							"relative_path1 = '' # fill in your relative folder path \n",
							"\n",
							"adls_path1 = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name1, account_name1, relative_path1) \n",
							"#print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path1 = adls_path + 'DHT_CUSTOMER_202205191833.csv' \n",
							"custData_DF1 = spark.read.csv(csv_path1, header = 'true')\n",
							"\n",
							"#custData_DF1.show()\n",
							"\n",
							"# Write the data to its target.\n",
							"\n",
							"custData_DF1.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .save(save_path)\n",
							"# Create the table.\n",
							"#spark.sql(\"DROP TABLE \" + table_name)\n",
							"spark.sql(\"CREATE TABLE IF NOT EXISTS \" + table_name + \" USING DELTA LOCATION '\" + save_path + \"'\" )\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--drop table etlhubConfirmed.customer_dimension;\n",
							"select * from etlhubConfirmed.customer_dimension"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"end"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"source_data = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"source_format = 'CSV'\n",
							"\n",
							"spark.sql(\"DROP TABLE IF EXISTS \" + table_name)\n",
							"\n",
							"spark.sql(\"CREATE TABLE \" + table_name + \" (\" \\\n",
							"  \"loan_id BIGINT, \" + \\\n",
							"  \"funded_amnt INT, \" + \\\n",
							"  \"paid_amnt DOUBLE, \" + \\\n",
							"  \"addr_state STRING)\"\n",
							")\n",
							"\n",
							"spark.sql(\"COPY INTO \" + table_name + \\\n",
							"  \" FROM '\" + source_data + \"'\" + \\\n",
							"  \" FILEFORMAT = \" + source_format\n",
							")\n",
							"\n",
							"loan_risks_upload_data = spark.sql(\"SELECT * FROM \" + table_name)\n",
							"\n",
							"display(loan_risks_upload_data)\n",
							"Load data to datalake table"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"delta_table_path = \"abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer\" \n",
							"data = spark.range(5,10) \n",
							"data.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
							"\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"old code\n",
							"\n",
							"\n",
							"\n",
							"# Create table in the metastore\n",
							"\n",
							"DeltaTable.createIfNotExists(spark) \\\n",
							"  .tableName(\"default.customer_dimension\") \\\n",
							"  .addColumn(\"CUSTOMER_KEY\", \"INT\") \\\n",
							"  .addColumn(\"VERSION\", \"INT\") \\\n",
							"  .addColumn(\"CUSTOMER_NO\", \"STRING\") \\\n",
							"  .addColumn(\"FINANCIAL_COUNTRY_CD\")\\\n",
							"  .addColumn(\"GBG_ID\", \"STRING\") \\\n",
							"  .addColumn(\"CUSTOMER_NAME\", \"STRING\") \\\n",
							"  .addColumn(\"CURRENT_IND\", \"STRING\") \\\n",
							"  .addColumn(\"EXTRACT_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"REC_START_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"REC_END_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"SOURCE_SYSTEM\", \"STRING\") \\\n",
							"  .addColumn(\"REC_STATUS\", \"STRING\") \\\n",
							"  .addColumn(\"IMG_LST_UPD_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"IMG_CREATED_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"DATA_IND\", \"STRING\") \\\n",
							"  .addColumn(\"ACTIVE_IN_SOURCE_IND\", \"STRING\") \\\n",
							"  .execute()\n",
							"\n",
							"######################\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"#jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;database=dsqlpoolKyn001494DevEtlHubEUS001;user=undefined@asa-kyn-001494-dev-eus-001;password={your_password_here};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\n",
							"existingDataDF = spark.read.format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"    .option(\"query\", \"SELECT MAX(CUSTOMER_KEY) OVER (ORDER BY CUSTOMER_KEY  ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) AS MAX_KEY, LOWER(CONVERT(VARCHAR(32),HashBytes('MD5', CUSTOMER_NO),2)) as existing_nk_hash,tgt.* FROM DBXDH.DHT_CUSTOMER as tgt WHERE CURRENT_IND='Y'\") \\\n",
							"    .option(\"user\", \"sqladminuser\") \\\n",
							"    .option(\"password\", \"try2find$5\") \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .load()\n",
							"#existingDataDF1 = existingDataDF.select([F.col(c).alias(\"`\"'existing_'+c+\"`\") for c in existingDataDF.columns])\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"#existingDataDF1.printSchema()\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_KEY, \"fullouter\") \n",
							"fullJoin1.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"#Insert for New rows\n",
							"\n",
							"fullJoin2=sqlContext.sql(\"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \\\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null\")\n",
							"fullJoin2.createOrReplaceTempView('fullJoin2')\n",
							"\n",
							"#insert new rows into database\n",
							"\n",
							"fullJoin2.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()\n",
							"fullJoin2.show()\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--select * from existingDataDF\n",
							"\n",
							"select * from fullJoin2\n",
							"\n",
							"#insert new rows into database\n",
							"/*\n",
							"fullJoin2.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()\n",
							"        */"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--ALL\n",
							"SELECT * FROM FULLJOIN;\n",
							"\n",
							"--No change records, ignore\n",
							"select * from fullJoin WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							"\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"select * from fullJoin WHERE WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--Soft deletes or rows no longer active in source\n",
							"select * from fullJoin WHERE WHERE nk_hash is null;\n",
							"\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1 \n",
							"select * from fullJoin where existing_existing_nk_hash is null;\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--INSERTS OR NEW ROWS\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION ,\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT,\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM,\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND,\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#test referance\n",
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"query\", \"INSERT INTO DBXDH.DHT_CUSTOMER1 \\\n",
							"        select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION , \\\n",
							"        CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"        CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"        'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"        'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"        from fullJoin A WHERE existing_existing_nk_hash is null\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"overwrite\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"df.write.mode(\"overwrite\") \\\n",
							"    .format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://localhost:1433;databaseName={database};\") \\\n",
							"    .option(\"dbtable\", table) \\\n",
							"    .option(\"user\", user) \\\n",
							"    .option(\"password\", password) \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/deltaLake_SCD_Type2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "142d48b3-2f79-443e-a7b5-091984259f3d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"SCD Type2 using adls as source and delta lake as target"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#import necessary python libraries\n",
							"\n",
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"\n",
							"#Read data from adls csv file extracted from source at https://adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/data/customer/\n",
							"\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'customer' # fill in your container name \n",
							"relative_path = '' # fill in your relative folder path \n",
							"\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path = adls_path + 'customer_data.csv' \n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\n",
							"\n",
							"natural_key=\"CUSTOMER_NO\"\n",
							"#columns1 = [\"FINANCIAL_COUNTRY_CD\",\"CUSTOMER_DESC\",\"GBG_ID\"]\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"    #print (col_list)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\n",
							"\n",
							"#Create a deltalake table with necessary columns\n",
							"\n",
							"#incrementalData_DF2.show()\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT MAX(CUSTOMER_KEY) OVER (ORDER BY CUSTOMER_KEY  ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) AS MAX_KEY, tgt.* from etlhubConfirmed.customer_dimension tgt WHERE CURRENT_IND='Y'\")\n",
							"#existingDF.createOrReplaceTempView('existingDF')\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"#existingDataDF1.printSchema()\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_NO, \"fullouter\") \n",
							"fullJoin1.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"#Insert for New rows\n",
							"\n",
							"qry= \"\"\"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"AND CURRENT_IND='Y'\n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND B.REC_CHECKSUM <> A.column_hash\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry1= \"\"\"\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"AND CURRENT_IND='Y'\n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND B.REC_CHECKSUM <> A.column_hash\n",
							";\n",
							"\"\"\"\n",
							"df3=spark.sql(qry1).count()\n",
							"\n",
							"a=spark.sql(qry)\n",
							"\n",
							"print(df3 || ' rows affected ')\n",
							"\n",
							"a.num_affected_rows\n",
							"#print(numOutputRows)\n",
							"\n",
							"\n",
							"deltaTable1 = DeltaTable.forPath(spark, 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2')\n",
							"\n",
							"deltaTable = DeltaTable.forName(spark, 'etlhubConfirmed.customer_dimension')\n",
							"\n",
							"\n",
							"fullHistoryDF = deltaTable.history()    # get the full history of the table\n",
							"\n",
							"lastOperationDF = deltaTable.history(1) # get the last operation\n",
							"\n",
							"print(lastOperationDF.operationMetrics)\n",
							"\n",
							"lastOperationDF.show()\n",
							"\n",
							"#fullHistoryDF.show()\n",
							"\n",
							"#print(num_affected_rows)\n",
							"\n",
							"#print(num_inserted_rows)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"AND CURRENT_IND='Y'\n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND B.REC_CHECKSUM <> A.column_hash\n",
							";\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"etlhubConfirmed.customer_dimension.toDF('abc')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"UPDATE etlhubconfirmed.customer_dimension\n",
							"SET fiNANCIAL_COUNTRY_CD='907'\n",
							"    WHERE CURRENT_IND='Y' AND CUSTOMER_NO='0074657';\n",
							"select * from etlhubconfirmed.customer_dimension;\n",
							"\n",
							"DELETe from etlhubconfirmed.customer_dimension where fiNANCIAL_COUNTRY_CD='905';\n",
							"select * from etlhubconfirmed.customer_dimension;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"MERGE INTO default.people10m\n",
							"USING default.people10m_upload\n",
							"ON default.people10m.id = default.people10m_upload.id\n",
							"WHEN MATCHED THEN UPDATE SET *\n",
							"WHEN NOT MATCHED THEN INSERT *"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create a deltalake table with necessary columns\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--CREATE DATABASE etlhubConfirmed;\n",
							"\n",
							"--drop table etlhubConfirmed.customer_dimension;\n",
							"\n",
							"-- Create Delta Lake table, define schema and location\n",
							"CREATE TABLE IF NOT EXISTS etlhubConfirmed.customer_dimension (\n",
							"    CUSTOMER_KEY INT NOT NULL,\n",
							"\tVERSION INT ,\n",
							"\tCUSTOMER_NO STRING ,\n",
							"\tFINANCIAL_COUNTRY_CD varchar(10) ,\n",
							"\tGBG_ID varchar(10)  ,\n",
							"\tCUSTOMER_NAME varchar(30)  ,\n",
							"\tCURRENT_IND varchar(1)  ,\n",
							"\tEXTRACT_DT TIMESTAMP ,\n",
							"\tREC_START_DT TIMESTAMP ,\n",
							"\tREC_END_DT TIMESTAMP ,\n",
							"\tSOURCE_SYSTEM varchar(50)  ,\n",
							"\tREC_CHECKSUM varchar(32)  ,\n",
							"\tREC_STATUS varchar(1)  ,\n",
							"\tIMG_LST_UPD_DT TIMESTAMP NOT NULL,\n",
							"\tIMG_CREATED_DT TIMESTAMP NOT NULL,\n",
							"\tDATA_IND varchar(10)  ,\n",
							"\tACTIVE_IN_SOURCE_IND char(1)  \n",
							")\n",
							"USING DELTA\n",
							"PARTITIONED BY (CURRENT_IND)\n",
							"-- specify data lake folder location\n",
							"LOCATION 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer'\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"source_data = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"source_format = 'csv'\n",
							"\n",
							"spark.sql(\"COPY INTO \" + table_name + \\\n",
							"  \" FROM '\" + source_data + \"'\" + \\\n",
							"  \" FILEFORMAT = \" + source_format + ';'\n",
							")\n",
							"\n",
							"customer_dim_data = spark.sql(\"SELECT * FROM \" + table_name)\n",
							"\n",
							"display(customer_dim_data)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"read_format = 'csv'\n",
							"write_format = 'delta'\n",
							"load_path = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"save_path = 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2'\n",
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"\n",
							"\n",
							"account_name1 = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name1 = 'customer' # fill in your container name \n",
							"relative_path1 = '' # fill in your relative folder path \n",
							"\n",
							"adls_path1 = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name1, account_name1, relative_path1) \n",
							"#print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path1 = adls_path + 'DHT_CUSTOMER_202205191833.csv' \n",
							"custData_DF1 = spark.read.csv(csv_path1, header = 'true')\n",
							"\n",
							"#custData_DF1.show()\n",
							"\n",
							"# Write the data to its target.\n",
							"\n",
							"custData_DF1.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .save(save_path)\n",
							"# Create the table.\n",
							"#spark.sql(\"DROP TABLE \" + table_name)\n",
							"spark.sql(\"CREATE TABLE IF NOT EXISTS \" + table_name + \" USING DELTA LOCATION '\" + save_path + \"'\" )\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--drop table etlhubConfirmed.customer_dimension;\n",
							"select * from etlhubConfirmed.customer_dimension"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"end"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"table_name = 'etlhubConfirmed.customer_dimension'\n",
							"source_data = 'https://adls4fsoetlhubdevuseast.dfs.core.windows.net/customer/DHT_CUSTOMER_202205191620.csv'\n",
							"source_format = 'CSV'\n",
							"\n",
							"spark.sql(\"DROP TABLE IF EXISTS \" + table_name)\n",
							"\n",
							"spark.sql(\"CREATE TABLE \" + table_name + \" (\" \\\n",
							"  \"loan_id BIGINT, \" + \\\n",
							"  \"funded_amnt INT, \" + \\\n",
							"  \"paid_amnt DOUBLE, \" + \\\n",
							"  \"addr_state STRING)\"\n",
							")\n",
							"\n",
							"spark.sql(\"COPY INTO \" + table_name + \\\n",
							"  \" FROM '\" + source_data + \"'\" + \\\n",
							"  \" FILEFORMAT = \" + source_format\n",
							")\n",
							"\n",
							"loan_risks_upload_data = spark.sql(\"SELECT * FROM \" + table_name)\n",
							"\n",
							"display(loan_risks_upload_data)\n",
							"Load data to datalake table"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"delta_table_path = \"abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer\" \n",
							"data = spark.range(5,10) \n",
							"data.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
							"\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"old code\n",
							"\n",
							"\n",
							"\n",
							"# Create table in the metastore\n",
							"\n",
							"DeltaTable.createIfNotExists(spark) \\\n",
							"  .tableName(\"default.customer_dimension\") \\\n",
							"  .addColumn(\"CUSTOMER_KEY\", \"INT\") \\\n",
							"  .addColumn(\"VERSION\", \"INT\") \\\n",
							"  .addColumn(\"CUSTOMER_NO\", \"STRING\") \\\n",
							"  .addColumn(\"FINANCIAL_COUNTRY_CD\")\\\n",
							"  .addColumn(\"GBG_ID\", \"STRING\") \\\n",
							"  .addColumn(\"CUSTOMER_NAME\", \"STRING\") \\\n",
							"  .addColumn(\"CURRENT_IND\", \"STRING\") \\\n",
							"  .addColumn(\"EXTRACT_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"REC_START_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"REC_END_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"SOURCE_SYSTEM\", \"STRING\") \\\n",
							"  .addColumn(\"REC_STATUS\", \"STRING\") \\\n",
							"  .addColumn(\"IMG_LST_UPD_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"IMG_CREATED_DT\", \"TIMESTAMP\") \\\n",
							"  .addColumn(\"DATA_IND\", \"STRING\") \\\n",
							"  .addColumn(\"ACTIVE_IN_SOURCE_IND\", \"STRING\") \\\n",
							"  .execute()\n",
							"\n",
							"######################\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"\n",
							"#jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;database=dsqlpoolKyn001494DevEtlHubEUS001;user=undefined@asa-kyn-001494-dev-eus-001;password={your_password_here};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\n",
							"existingDataDF = spark.read.format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"    .option(\"query\", \"SELECT MAX(CUSTOMER_KEY) OVER (ORDER BY CUSTOMER_KEY  ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) AS MAX_KEY, LOWER(CONVERT(VARCHAR(32),HashBytes('MD5', CUSTOMER_NO),2)) as existing_nk_hash,tgt.* FROM DBXDH.DHT_CUSTOMER as tgt WHERE CURRENT_IND='Y'\") \\\n",
							"    .option(\"user\", \"sqladminuser\") \\\n",
							"    .option(\"password\", \"try2find$5\") \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .load()\n",
							"#existingDataDF1 = existingDataDF.select([F.col(c).alias(\"`\"'existing_'+c+\"`\") for c in existingDataDF.columns])\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"#existingDataDF1.printSchema()\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_KEY, \"fullouter\") \n",
							"fullJoin1.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"#Insert for New rows\n",
							"\n",
							"fullJoin2=sqlContext.sql(\"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \\\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null\")\n",
							"fullJoin2.createOrReplaceTempView('fullJoin2')\n",
							"\n",
							"#insert new rows into database\n",
							"\n",
							"fullJoin2.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()\n",
							"fullJoin2.show()\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--select * from existingDataDF\n",
							"\n",
							"select * from fullJoin2\n",
							"\n",
							"#insert new rows into database\n",
							"/*\n",
							"fullJoin2.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()\n",
							"        */"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"--ALL\n",
							"SELECT * FROM FULLJOIN;\n",
							"\n",
							"--No change records, ignore\n",
							"select * from fullJoin WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) = LOWER(existing_rec_checksum);\n",
							"\n",
							"--INSERTS OR NEW ROWS\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1, A.* from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							"\n",
							"--Changed records or update old record and insert new with incremented version\n",
							"select * from fullJoin WHERE WHERE LOWER(nk_hash) = LOWER(existing_existing_nk_hash) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\n",
							"\n",
							"\n",
							"--Soft deletes or rows no longer active in source\n",
							"select * from fullJoin WHERE WHERE nk_hash is null;\n",
							"\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1 \n",
							"select * from fullJoin where existing_existing_nk_hash is null;\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--INSERTS OR NEW ROWS\n",
							"--INSERT INTO DBXDH.DHT_CUSTOMER1\n",
							"select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION ,\n",
							"CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT,\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM,\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND,\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND\n",
							"from fullJoin A WHERE existing_existing_nk_hash is null;\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#test referance\n",
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"query\", \"INSERT INTO DBXDH.DHT_CUSTOMER1 \\\n",
							"        select COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY1,1 as VERSION , \\\n",
							"        CUSTOMER_NO,FINANCIAL_COUNTRY_CD,GBG_ID,CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \\\n",
							"        CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, column_hash as REC_CHECKSUM, \\\n",
							"        'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \\\n",
							"        'Y' AS ACTIVE_IN_SOURCE_IND \\\n",
							"        from fullJoin A WHERE existing_existing_nk_hash is null\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"overwrite\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"fullJoin1.write \\\n",
							"        .format(\"jdbc\") \\\n",
							"        .option(\"url\", f\"jdbc:sqlserver://asa-kyn-001494-dev-eus-001.sql.azuresynapse.net:1433;databaseName=dsqlpoolKyn001494DevEtlHubEUS001;\") \\\n",
							"        .option(\"user\", \"sqladminuser\") \\\n",
							"        .option(\"password\", \"try2find$5\") \\\n",
							"        .option(\"dbtable\", \"DBXDH.DHT_CUSTOMER\") \\\n",
							"        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"        .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
							"        .mode(\"append\") \\\n",
							"        .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"\n",
							"df.write.mode(\"overwrite\") \\\n",
							"    .format(\"jdbc\") \\\n",
							"    .option(\"url\", f\"jdbc:sqlserver://localhost:1433;databaseName={database};\") \\\n",
							"    .option(\"dbtable\", table) \\\n",
							"    .option(\"user\", user) \\\n",
							"    .option(\"password\", password) \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .save()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/email_alert')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "64ece5bb-370f-4834-af0a-d8be54e11eb4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import smtplib\n",
							"from pathlib import Path\n",
							"from email.mime.multipart import MIMEMultipart\n",
							"from email.mime.base import MIMEBase\n",
							"from email.mime.text import MIMEText\n",
							"from email.utils import COMMASPACE, formatdate\n",
							"from email import encoders\n",
							"\n",
							"\n",
							"def send_mail(send_from = <from_email>, send_to = <to_email>, subject = \"Test\", message = \"Test\", files=[\"/mnt/<Mounted Point Directory>/\"],\n",
							"              server=\"<SMTP Host>\", port=<SMTP Port>, username='<SMTP Username>', password='<SMTP Password>',\n",
							"              use_tls=True):\n",
							"\n",
							"    msg = MIMEMultipart()\n",
							"    msg['From'] = send_from\n",
							"    msg['To'] = COMMASPACE.join(send_to)\n",
							"    msg['Date'] = formatdate(localtime=True)\n",
							"    msg['Subject'] = subject\n",
							"\n",
							"    msg.attach(MIMEText(message))\n",
							"\n",
							"    for path in files:\n",
							"        part = MIMEBase('application', \"octet-stream\")\n",
							"        with open(path, 'rb') as file:\n",
							"            part.set_payload(file.read())\n",
							"        encoders.encode_base64(part)\n",
							"        part.add_header('Content-Disposition',\n",
							"                        'attachment; filename=\"{}\"'.format(Path(path).name))\n",
							"        msg.attach(part)\n",
							"\n",
							"    smtp = smtplib.SMTP(server, port)\n",
							"    if use_tls:\n",
							"        smtp.starttls()\n",
							"    smtp.login(username, password)\n",
							"    smtp.sendmail(send_from, send_to, msg.as_string())\n",
							"    smtp.quit()\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_dht_Sales_Stage_load')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPool001494New",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "472g",
					"driverCores": 80,
					"executorMemory": "472g",
					"executorCores": 80,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "79801970-eac0-4e30-afb3-794c8f7d18ab"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPool001494New",
						"name": "sPool001494New",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPool001494New",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 80,
						"memory": 472,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Sales Stage Dimension load**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubconfirmed.dht_market by sourceing data from ESA Kyndryl.\n",
							"Since ESA is not accessible from Azure, a datastage job src_ESA_Sales_Stage_Azure is executed on datastage to push the ESA extract to adls.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws,trim\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.functions import col\n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"import os\n",
							"import sys\n",
							"from datetime import datetime"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/ESA/' # fill in your relative folder path \n",
							"relative_archive_path='archive/'\n",
							"file_name='DS_SALES_STAGE_DATA.csv' \n",
							"natural_key=\"SALES_STAGE_CD\"\n",
							"tablename=\"etlhubConfirmed.dht_sales_stage\"\n",
							"stagingtable=\"DHTS_SALES_STAGE\"\n",
							"keycolumn=\"SALES_STAGE_KEY\"\n",
							"date = datetime.now().strftime(\"%Y-%m-%d-%I:%M:%S_%p\")\n",
							"print(date)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Copy source CSV file to archive folder including timestamp in file name"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/ESA/DS_SALES_STAGE_DATA.csv"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"adls_arch_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_archive_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"relative_archive_path\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"arch_fil_name=file_name.rsplit(\".\")\n",
							"csv_archive_path = adls_arch_path + arch_fil_name[0] + '_' + date + '.' + arch_fil_name[1]\n",
							"print (csv_archive_path)\n",
							"incrementalData_DF = spark.read.load(csv_path,format=\"csv\", sep=\"||\", header=\"true\")\n",
							"#incrementalData_DF.show()\n",
							"incrementalData_DF.write.csv(csv_archive_path, header = 'true')\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"print(col_list)\n",
							"incrementalData_SDF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_SDF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Create a dataframe with target deltalake table data with necessary columns\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1, col(natural_key) == col(\"existing_\" + natural_key), \"fullouter\")\n",
							"\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"fullJoin2.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .option(\"overwriteSchema\", \"true\") \\\n",
							"  .option(\"path\", \"abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/staging/{}\".format(stagingtable)) \\\n",
							"  .saveAsTable(stagingtable)\n",
							"   #.option(\"overwriteSchema\", \"true\") \\  .save(\"\") \\"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Data Validation Checks :\n",
							"\n",
							"- Check for duplicates in Source data based on natural key\n",
							"- Check for duplicates in target delta lake table based on natural key\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} WHERE CURRENT_IND='Y' GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"df3=spark.sql(qry3.format(natural_key,natural_key))\n",
							"cnt1=df3.count()\n",
							"\n",
							"print (cnt1)\n",
							"if cnt1 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in source:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(1)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry_4_recon1=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable,tablename,natural_key,natural_key))\n",
							"print('New records or Inserts are:') \n",
							"df_4_recon1.show()\n",
							"\n",
							"qry_4_recon2=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE LOWER({}) = LOWER(existing_{}) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon2=spark.sql(qry_4_recon2.format(stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))\n",
							"print('Changed records or Updates are:') \n",
							"df_4_recon2.show()\n",
							"\n",
							"qry_4_recon3=\"\"\"\n",
							"SELECT COUNT(*) from \n",
							"{} A\n",
							"join fullJoin B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"\n",
							"\"\"\"\n",
							"\n",
							"df_4_recon3=spark.sql(qry_4_recon3.format(tablename,natural_key,natural_key,natural_key))\n",
							"print('Soft Deletes are:') \n",
							"df_4_recon3.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT count(*) FROM etlhubconfirmed.dht_sales_stage where current_ind='Y'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select \r\n",
							"\t* \r\n",
							"    from dhts_sales_stage A \r\n",
							"WHERE existing_REC_CHECKSUM is null"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry_4_recon1=\"\"\"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable, tablename, natural_key, natural_key))\n",
							"\n",
							"qry_ins_new_rows=\"\"\"\n",
							"\n",
							"INSERT INTO {}\n",
							"\tselect \n",
							"\tCOALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS {},1 as SALES_STAGE_VERSION \n",
							"    ,SALES_STAGE_CD\n",
							"\t,SIEBEL_SALES_STAGE_NAME\n",
							"\t,SSM_STEP_NO\n",
							"\t,SSM_STEP_NAME\n",
							"\t,'Y' AS CURRENT_IND\n",
							"\t,CURRENT_TIMESTAMP AS EXTRACT_DT\n",
							"\t,CURRENT_TIMESTAMP AS REC_START_DT\n",
							"\t,'9999-12-31 00:00:00.000' as REC_END_DT\n",
							"\t,'ESA' AS SOURCE_SYSTEM\n",
							"\t,column_hash as REC_CHECKSUM\n",
							"\t,'I' as REC_STATUS\n",
							"\t,current_timestamp as IMG_LST_UPD_DT\n",
							"\t,CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							"\t,'ED' AS DATA_IND\n",
							"\t,'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"    from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"d=spark.sql(qry_ins_new_rows.format(tablename,keycolumn,stagingtable,tablename,natural_key, natural_key ))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"SELECT * from etlhubconfirmed.dht_sales_stage"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Update or expire the rows with existing instance of the changed rows:\n",
							"qry_4_upd_changes_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.{}\n",
							"AND LOWER(B.{}) = LOWER(B.existing_{}) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\"\"\"\n",
							"f=spark.sql(qry_4_upd_changes_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key,natural_key))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--existing data\n",
							"select 'existing data',count(*) from DHTS_SALES_STAGE where existing_CURRENT_IND='Y';\n",
							"  \n",
							"select 'rows for Update', count(*)\n",
							"from etlhubConfirmed.dht_sales_stage B\n",
							"join\n",
							"DHTS_SALES_STAGE A \n",
							"WHERE A.SALES_STAGE_CD = B.SALES_STAGE_CD\n",
							"and LOWER(A.SALES_STAGE_CD) = LOWER(A.existing_SALES_STAGE_CD) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND B.REC_START_DT=A.existing_REC_START_DT\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.DHT_SALES_STAGE B\n",
							"WHERE LOWER(A.SALES_STAGE_CD)=LOWER(B.SALES_STAGE_CD)\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"SELECT COUNT(*) FROM etlhubConfirmed.DHT_SALES_STAGE A\n",
							"JOIN DHTS_SALES_STAGE B\n",
							"ON A.SALES_STAGE_CD = B.SALES_STAGE_CD\n",
							"AND LOWER(B.SALES_STAGE_CD) = LOWER(B.existing_SALES_STAGE_CD) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							";"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Insert for changed rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry_ins_changed_rows=\"\"\"\n",
							"INSERT INTO {}\n",
							"\tselect \n",
							"    existing_SALES_STAGE_KEY\n",
							"    ,1 + existing_SALES_STAGE_VERSION as SALES_STAGE_VERSION\n",
							"    ,SALES_STAGE_CD\n",
							"\t,SIEBEL_SALES_STAGE_NAME\n",
							"\t,SSM_STEP_NO\n",
							"\t,SSM_STEP_NAME\n",
							"\t,'Y' AS CURRENT_IND\n",
							"\t,CURRENT_TIMESTAMP AS EXTRACT_DT\n",
							"\t,CURRENT_TIMESTAMP AS REC_START_DT\n",
							"\t,'9999-12-31 00:00:00.000' as REC_END_DT\n",
							"\t,'ESA' AS SOURCE_SYSTEM\t\n",
							"\t,column_hash as REC_CHECKSUM\n",
							"\t,'U' as REC_STATUS\n",
							"\t,current_timestamp as IMG_LST_UPD_DT\n",
							"\t,existing_IMG_CREATED_DT AS IMG_CREATED_DT\n",
							"\t,'ED' AS DATA_IND\n",
							"\t,'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from {} A \n",
							"WHERE LOWER({}) = LOWER(existing_{}) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"e=spark.sql(qry_ins_changed_rows.format(tablename,stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Soft Deletes - Update Active_Ind for the rows no longer active in source:\n",
							"\n",
							"qry_4_upd_deleted_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"\n",
							";\n",
							"\"\"\"\n",
							"g=spark.sql(qry_4_upd_deleted_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select 'Number of Active Rows' as Title,count(*) from etlhubconfirmed.dht_sales_stage where CURRENT_IND='Y'\n",
							";\n",
							"\n",
							"\n",
							"select 'Duplicate Rows based on surrogate key' as Title,sales_stage_key,count(*) from etlhubconfirmed.dht_sales_stage where CURRENT_IND='Y'\n",
							"    group by sales_stage_key\n",
							"    having count(*)>1\n",
							";\n",
							"\n",
							"select 'Duplicate Rows based on natural key' as Title,SALES_STAGE_CD,count(*) from etlhubconfirmed.dht_sales_stage where CURRENT_IND='Y'\n",
							"    group by SALES_STAGE_CD\n",
							"    having count(*)>1\n",
							";\n",
							"\n",
							"select 'New rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_sales_stage where CURRENT_IND='Y'\n",
							"    and date(extract_dt)=current_date and rec_status='I'\n",
							";\n",
							"\n",
							"select 'Changed rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_sales_stage where CURRENT_IND='Y'\n",
							"    and date(extract_dt)=current_date and rec_status='U'\n",
							";\n",
							"\n",
							"select 'Changed rows updated in this run' as Title,count(*) from etlhubconfirmed.dht_sales_stage where CURRENT_IND='N'\n",
							"    and date(REC_END_DT)=current_date --and rec_status='I'\n",
							";\n",
							"\n",
							"select 'Rows no longer active in source' as Title,count(*) from etlhubconfirmed.dht_sales_stage where CURRENT_IND='Y'\n",
							"    AND \n",
							"     date(REC_END_DT)=current_date --and rec_status='I'\n",
							";\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 49
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT COUNT(*) FROM etlhubconfirmed.dht_sales_stage\n",
							"WHERE CURRENT_IND='N' and DATE(REC_END_DT)<>'9999-12-31'\n",
							"    and date(img_lst_upd_dt)=current_date\n",
							"        limit 1\n",
							";\n",
							"\n",
							"\n",
							"select count(*) from etlhubconfirmed.dht_sales_stage where CURRENT_IND='Y'\n",
							"   and date(extract_dt)=current_date --and rec_status='I'\n",
							";\n",
							"\n",
							"select count(*) from etlhubconfirmed.dht_sales_stage\n",
							"WHERE ACTIVE_IN_SOURCE_IND='N'\n",
							"    and date(IMG_LST_UPD_DT)=current_date\n",
							"    ;\n",
							"UPDATE etlhubconfirmed.dht_sales_stage\n",
							"set CURRENT_IND='Y', \n",
							"REC_END_DT='9999-12-31'\n",
							"  WHERE CURRENT_IND='N' and DATE(REC_END_DT)<>'9999-12-31'\n",
							"    and date(img_lst_upd_dt)=current_date\n",
							"        ;  "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 50
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_dht_Service_load')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPool001494New",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "472g",
					"driverCores": 80,
					"executorMemory": "472g",
					"executorCores": 80,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "57e0568b-4560-485d-8ce3-3f5b8c917f66"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPool001494New",
						"name": "sPool001494New",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPool001494New",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 80,
						"memory": 472,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Service data load**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubconfirmed.dht_market by sourceing data from ESA Kyndryl.\n",
							"Since ESA is not accessible from Azure, a datastage job srcESA_Service_Azure is executed on datastage to push the ESA extract to adls.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws,trim\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.functions import col\n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"import os\n",
							"import sys\n",
							"from datetime import datetime"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/ESA/' # fill in your relative folder path \n",
							"relative_archive_path='archive/'\n",
							"file_name='DS_SERVICE_DATA.csv' \n",
							"natural_key=\"OFFERING_COMPNT_CD\"\n",
							"tablename=\"etlhubConfirmed.dht_service\"\n",
							"stagingtable=\"DHTS_SERVICE\"\n",
							"keycolumn=\"SERVICE_COMPONENT_KEY\"\n",
							"date = datetime.now().strftime(\"%Y-%m-%d-%I:%M:%S_%p\")\n",
							"print(date)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Copy source CSV file to archive folder including timestamp in file name"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/ESA/DS_SALES_STAGE_DATA.csv"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"adls_arch_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_archive_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"relative_archive_path\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"arch_fil_name=file_name.rsplit(\".\")\n",
							"csv_archive_path = adls_arch_path + arch_fil_name[0] + '_' + date + '.' + arch_fil_name[1]\n",
							"print (csv_archive_path)\n",
							"incrementalData_DF = spark.read.load(csv_path,format=\"csv\", sep=\"||\", header=\"true\")\n",
							"#incrementalData_DF.show()\n",
							"incrementalData_DF.write.csv(csv_archive_path, header = 'true')\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"print(col_list)\n",
							"incrementalData_SDF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_SDF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Create a dataframe with target deltalake table data with necessary columns\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1, col(natural_key) == col(\"existing_\" + natural_key), \"fullouter\")\n",
							"\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"fullJoin2.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .option(\"overwriteSchema\", \"true\") \\\n",
							"  .option(\"path\", \"abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/staging/{}\".format(stagingtable)) \\\n",
							"  .saveAsTable(stagingtable)\n",
							"   #.option(\"overwriteSchema\", \"true\") \\  .save(\"\") \\"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Data Validation Checks :\n",
							"\n",
							"- Check for duplicates in Source data based on natural key\n",
							"- Check for duplicates in target delta lake table based on natural key\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} WHERE CURRENT_IND='Y' GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"df3=spark.sql(qry3.format(natural_key,natural_key))\n",
							"cnt1=df3.count()\n",
							"\n",
							"print (cnt1)\n",
							"if cnt1 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in source:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(1)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry_4_recon1=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable,tablename,natural_key,natural_key))\n",
							"print('New records or Inserts are:') \n",
							"df_4_recon1.show()\n",
							"\n",
							"qry_4_recon2=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE LOWER({}) = LOWER(existing_{}) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon2=spark.sql(qry_4_recon2.format(stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))\n",
							"print('Changed records or Updates are:') \n",
							"df_4_recon2.show()\n",
							"\n",
							"qry_4_recon3=\"\"\"\n",
							"SELECT COUNT(*) from \n",
							"{} A\n",
							"join fullJoin B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"\n",
							"\"\"\"\n",
							"\n",
							"df_4_recon3=spark.sql(qry_4_recon3.format(tablename,natural_key,natural_key,natural_key))\n",
							"print('Soft Deletes are:') \n",
							"df_4_recon3.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry_4_recon1=\"\"\"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable, tablename, natural_key, natural_key))\n",
							"\n",
							"qry_ins_new_rows=\"\"\"\n",
							"\n",
							"INSERT INTO {}\n",
							"\tselect \n",
							"\tCOALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS {},1 as SERVICE_COMPONENT_VERSION \n",
							"    ,OFFERING_COMPNT_CD\n",
							"\t,OFFERING_COMPNT_DESC\n",
							"\t,-1 as OFFERING_CODE_KEY\n",
							"\t,OFFERING_CD\n",
							"\t,OFFERING_DESC\n",
							"\t,-1 as SERVICE_KEY\n",
							"\t,SERVICE_CD\n",
							"\t,SERVICE_DESC\n",
							"\t,-1 as OFFERING_CATEGORY_KEY\n",
							"\t,OFFERING_CAT_CD\n",
							"\t,OFFERING_CAT_DESC\n",
							"\t,OFFERING_GROUP\n",
							"\t,-1 as BUS_MEASMNT_DIV_KEY\n",
							"\t,BUS_MEASMNT_DIV_CD\n",
							"\t,'' as BUS_MEASMNT_DIV_DESC\n",
							"\t,-1 as LOB_KEY\n",
							"\t,'' as LOB_CD\n",
							"\t,'' as LOB_DESC\n",
							"\t,-1 as BUSINESS_UNIT_KEY\n",
							"\t,'' as BUSINESS_UNIT_ID\n",
							"\t,'' as BUSINESS_UNIT_DESC\n",
							"\t,'Y' AS CURRENT_IND\n",
							"\t,CURRENT_TIMESTAMP AS EXTRACT_DT\n",
							"\t,CURRENT_TIMESTAMP AS REC_START_DT\n",
							"\t,'9999-12-31 00:00:00.000' as REC_END_DT\n",
							"\t,'ESA' AS SOURCE_SYSTEM\n",
							"\t,column_hash as REC_CHECKSUM\n",
							"\t,'I' as REC_STATUS\n",
							"\t,current_timestamp as IMG_LST_UPD_DT\n",
							"\t,CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							"\t,OFFERING_COMPNT_STATUS_CD\n",
							"\t,OFFERING_COMPNT_STATUS_DESC\n",
							"\t,OFFERING_AREA_CODE\n",
							"\t,OFFERING_AREA_DESC\n",
							"\t,SERVICE_GROUP_CODE\n",
							"\t,SERVICE_GROUP_DESC\n",
							"\t,BRAND_CODE\n",
							"\t,BRAND_DESC\n",
							"\t,-1 as SERVICE_GROUP_KEY\n",
							"\t,'ED' AS DATA_IND\n",
							"\t,'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"    from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"d=spark.sql(qry_ins_new_rows.format(tablename,keycolumn,stagingtable,tablename,natural_key, natural_key ))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from etlhubConfirmed.DHT_SERVICE "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Update or expire the rows with existing instance of the changed rows:\n",
							"qry_4_upd_changes_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.{}\n",
							"AND LOWER(B.{}) = LOWER(B.existing_{}) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= REC_START_DT -  INTERVAL 1 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\"\"\"\n",
							"f=spark.sql(qry_4_upd_changes_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key,natural_key))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--existing data\n",
							"select 'existing data',count(*) from DHTS_SALES_STAGE where existing_CURRENT_IND='Y';\n",
							"  \n",
							"select 'rows for Update', count(*)\n",
							"from etlhubConfirmed.dht_service B\n",
							"join\n",
							"DHTS_SERVICE A \n",
							"WHERE A.OFFERING_COMPNT_CD = B.OFFERING_COMPNT_CD\n",
							"and LOWER(A.OFFERING_COMPNT_CD) = LOWER(A.existing_OFFERING_COMPNT_CD) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND B.REC_START_DT=A.existing_REC_START_DT\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.DHT_SERVICE B\n",
							"WHERE LOWER(A.OFFERING_COMPNT_CD)=LOWER(B.OFFERING_COMPNT_CD)\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"SELECT COUNT(*) FROM etlhubConfirmed.DHT_SERVICE A\n",
							"JOIN DHTS_SERVICE B\n",
							"ON A.OFFERING_COMPNT_CD = B.OFFERING_COMPNT_CD\n",
							"AND LOWER(B.OFFERING_COMPNT_CD) = LOWER(B.existing_OFFERING_COMPNT_CD) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							";"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Insert for changed rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry_ins_changed_rows=\"\"\"\n",
							"INSERT INTO {}\n",
							"\tselect \n",
							"    existing_SERVICE_COMPONENT_KEY\n",
							"    ,1 + existing_SERVICE_COMPONENT_VERSION as SERVICE_COMPONENT_VERSION\n",
							"    ,OFFERING_COMPNT_CD\n",
							"\t,OFFERING_COMPNT_DESC\n",
							"\t,-1\n",
							"\t,OFFERING_CD\n",
							"\t,OFFERING_DESC\n",
							"\t,-1\n",
							"\t,SERVICE_CD\n",
							"\t,SERVICE_DESC\n",
							"\t,-1\n",
							"\t,OFFERING_CAT_CD\n",
							"\t,OFFERING_CAT_DESC\n",
							"\t,OFFERING_GROUP\n",
							"\t,-1\n",
							"\t,BUS_MEASMNT_DIV_CD\n",
							"\t,''\n",
							"\t,-1\n",
							"\t,''\n",
							"\t,''\n",
							"\t,-1\n",
							"\t,''\n",
							"\t,''\n",
							"\t,'Y' AS CURRENT_IND\n",
							"\t,CURRENT_TIMESTAMP AS EXTRACT_DT\n",
							"\t,CURRENT_TIMESTAMP AS REC_START_DT\n",
							"\t,'9999-12-31 00:00:00.000' as REC_END_DT\n",
							"\t,'ESA' AS SOURCE_SYSTEM\t\n",
							"\t,column_hash as REC_CHECKSUM\n",
							"\t,'U' as REC_STATUS\n",
							"\t,current_timestamp as IMG_LST_UPD_DT\n",
							"\t,existing_IMG_CREATED_DT AS IMG_CREATED_DT\n",
							"\t,OFFERING_COMPNT_STATUS_CD\n",
							"\t,OFFERING_COMPNT_STATUS_DESC\n",
							"\t,OFFERING_AREA_CODE\n",
							"\t,OFFERING_AREA_DESC\n",
							"\t,SERVICE_GROUP_CODE\n",
							"\t,SERVICE_GROUP_DESC\n",
							"\t,BRAND_CODE\n",
							"\t,BRAND_DESC\n",
							"\t,-1 \n",
							"\t,'ED' AS DATA_IND\n",
							"\t,'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from {} A \n",
							"WHERE LOWER({}) = LOWER(existing_{}) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"e=spark.sql(qry_ins_changed_rows.format(tablename,stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Soft Deletes - Update Active_Ind for the rows no longer active in source:\n",
							"\n",
							"qry_4_upd_deleted_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"\n",
							";\n",
							"\"\"\"\n",
							"g=spark.sql(qry_4_upd_deleted_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select 'Number of Active Rows' as Title,count(*) from etlhubconfirmed.dht_service where CURRENT_IND='Y'\n",
							";\n",
							"\n",
							"\n",
							"select 'Duplicate Rows based on surrogate key' as Title,SERVICE_COMPONENT_KEY,count(*) from etlhubconfirmed.dht_service where CURRENT_IND='Y'\n",
							"    group by SERVICE_COMPONENT_KEY\n",
							"    having count(*)>1\n",
							";\n",
							"\n",
							"select 'Duplicate Rows based on natural key' as Title,OFFERING_COMPNT_CD,count(*) from etlhubconfirmed.dht_service where CURRENT_IND='Y'\n",
							"    group by OFFERING_COMPNT_CD\n",
							"    having count(*)>1\n",
							";\n",
							"\n",
							"select 'New rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_service where CURRENT_IND='Y'\n",
							"    and date(extract_dt)=current_date and rec_status='I'\n",
							";\n",
							"\n",
							"select 'Changed rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_service where CURRENT_IND='Y'\n",
							"    and date(extract_dt)=current_date and rec_status='U'\n",
							";\n",
							"\n",
							"select 'Changed rows updated in this run' as Title,count(*) from etlhubconfirmed.dht_service where CURRENT_IND='N'\n",
							"    and date(REC_END_DT)=current_date --and rec_status='I'\n",
							";\n",
							"\n",
							"select 'Rows no longer active in source' as Title,count(*) from etlhubconfirmed.dht_service where CURRENT_IND='Y'\n",
							"    AND \n",
							"     date(REC_END_DT)=current_date --and rec_status='I'\n",
							";\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT COUNT(*) FROM etlhubconfirmed.dht_service\n",
							"WHERE CURRENT_IND='N' and DATE(REC_END_DT)<>'9999-12-31'\n",
							"    and date(img_lst_upd_dt)=current_date\n",
							"        limit 1\n",
							";\n",
							"\n",
							"\n",
							"select count(*) from etlhubconfirmed.dht_service where CURRENT_IND='Y'\n",
							"   and date(extract_dt)=current_date --and rec_status='I'\n",
							";\n",
							"\n",
							"select count(*) from etlhubconfirmed.dht_service\n",
							"WHERE ACTIVE_IN_SOURCE_IND='N'\n",
							"    and date(IMG_LST_UPD_DT)=current_date\n",
							"    ;\n",
							"UPDATE etlhubconfirmed.dht_service\n",
							"set CURRENT_IND='Y', \n",
							"REC_END_DT='9999-12-31'\n",
							"  WHERE CURRENT_IND='N' and DATE(REC_END_DT)<>'9999-12-31'\n",
							"    and date(img_lst_upd_dt)=current_date\n",
							"        ;  "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_dht_building')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts/GREIW"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPool001494New",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "472g",
					"driverCores": 80,
					"executorMemory": "472g",
					"executorCores": 80,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "bf594ccd-4e76-44a4-b527-97832c6ce67c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPool001494New",
						"name": "sPool001494New",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPool001494New",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 80,
						"memory": 472,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Building Dimension load**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubConfirmed.DHT_BUILDING by sourcing data from Tririga system.\n",
							"The file is pulled to datastage server using SFTP protocol.\n",
							"\n",
							"The CSV formatted file is uploaded to adls for loading to BUILDING deltalake table created.\n",
							"\n",
							"The script used for file transfer is /home/resodba/trrgprocget.sh\n",
							"\n",
							"File Name: IWTRIRIGABuildingQuery.csv from /GlobalDir/CustomerFiles/tririga"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"from pyspark.sql.functions import col\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"import os\n",
							"import sys\n",
							"from datetime import datetime"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/Tririga/building/' # fill in your relative folder path \n",
							"relative_archive_path='archive/'\n",
							"file_name='IWTRIRIGABuildingQuery.csv' # 20220705, 20220705_upd1, 20220705_upd2\n",
							"natural_key1=\"CAMPUS_ID\"\n",
							"natural_key2=\"BUILDING_ID\"\n",
							"natural_key=\"BUSINESS_ID\"\n",
							"MinimumTririgaBuildingCount=750\n",
							"MinimumTririgaCampusCount=450\n",
							"tablename=\"etlhubConfirmed.dht_building\"\n",
							"stagingtable=\"DHTS_BUILDING\"\n",
							"keycolumn=\"BUILDING_KEY\"\n",
							"date = datetime.now().strftime(\"%Y-%m-%d-%I:%M:%S_%p\")\n",
							"extract_dt = datetime.now().strftime(\"%Y-%m-%d %I:%M:%S\") #This is batch/cycle date. It would be common for all the rows inserted in a single run\n",
							"rec_start_dt=datetime.now().strftime(\"%Y-%m-%d %I:%M:%S\") #This should ideally from the source system. If source doesnt have any date column, use current date \n",
							"print(extract_dt)\n",
							"print(natural_key)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Copy source CSV file to archive folder including timestamp in file name"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/ERDM/DS_INDUSTRY_HIERARCHY_DATA.csv\n",
							"\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"adls_arch_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_archive_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"print(natural_key)\n",
							"\n",
							"relative_archive_path\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"arch_fil_name=file_name.rsplit(\".\")\n",
							"csv_archive_path = adls_arch_path + arch_fil_name[0] + '_' + date + '.' + arch_fil_name[1]\n",
							"print (csv_archive_path)\n",
							"\n",
							"incrementalData_DF1 = spark.read.csv(csv_path, header = 'true')\n",
							"\n",
							"#incrementalData_DF = incrementalData_DF1.withColumn('BUILDING_BUSINESS_ID', concat_ws(\"CAMPUS_ID\", \"BUILDING_ID\"))\n",
							"\n",
							"incrementalData_DF = incrementalData_DF1.withColumn(natural_key, concat_ws(\"~\", natural_key1, natural_key2))\n",
							"\n",
							"#incrementalData_DF.write.csv(csv_archive_path, header = 'true') : Write file into archive directory\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"#Add checks for duplicate data, Check for 1. dups in source data based on natural Key and\n",
							"# 2. Dups in target already\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select * from incrementalData_DF2\n",
							"where BUSINESS_ID in ('BLDMAIN~006','CYNICOSI~0002','RTPMAIN~002','ITMEDMLN~004','MQFORTFR~001','JPATAGOE~001')\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dim_col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    dim_col_list.append(i)\n",
							"print (dim_col_list)\n",
							"my_string = ','.join(dim_col_list)\n",
							"print (my_string)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Create a dataframe with target deltalake table data with necessary columns\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\n",
							"#existingDataDF=spark.sql(\"SELECT tgt.* from etlhubConfirmed.DHT_EMPLOYEE tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\n",
							"\n",
							"#existingMaxKeyDF=spark.sql(\"SELECT MAX(EMPLOYEE_KEY) existing_MAX_KEY from etlhubConfirmed.DHT_EMPLOYEE tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1, col(natural_key) == col(\"existing_\" + natural_key), \"fullouter\")\n",
							"\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"fullJoin2.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .option(\"overwriteSchema\", \"true\") \\\n",
							"  .option(\"path\", \"abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/staging/{}\".format(stagingtable)) \\\n",
							"  .saveAsTable(stagingtable)\n",
							"   #.option(\"overwriteSchema\", \"true\") \\  .save(\"\") \\"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Data Validation Checks :\n",
							"\n",
							"- Check for duplicates in Source data based on natural key\n",
							"- Check for partial data from source. That is if the file has expected number of rows\n",
							"- Check for duplicates in target delta lake table based on natural key\n",
							"- Referential Integrity checks\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} WHERE CURRENT_IND='Y' GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry5=\"\"\"\n",
							"SELECT * FROM incrementalData_DF2\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"df3=spark.sql(qry3.format(natural_key,natural_key))\n",
							"cnt1=df3.count()\n",
							"\n",
							"print (cnt1)\n",
							"if cnt1 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in source:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(1)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"\n",
							"df5=spark.sql(qry5)\n",
							"cnt3=df5.count()\n",
							"\n",
							"if cnt3 >= MinimumTririgaBuildingCount:\n",
							"    print(\"The number of rows in source data is more than the threshold, data can be processed\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"The source data is having less number of rows than expected. Please check:\" )\n",
							"    print(cnt3)\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							"\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry_4_recon1=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable,tablename,natural_key,natural_key))\n",
							"print('New records or Inserts are:') \n",
							"df_4_recon1.show()\n",
							"\n",
							"qry_4_recon2=\"\"\"\n",
							"\n",
							"SELECT count(*) from {} A \n",
							"WHERE LOWER({}) = LOWER(existing_{}) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon2=spark.sql(qry_4_recon2.format(stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))\n",
							"print('Changed records or Updates are:') \n",
							"df_4_recon2.show()\n",
							"\n",
							"qry_4_recon3=\"\"\"\n",
							"SELECT count(*) from \n",
							"{} A\n",
							"join fullJoin B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' --AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"\n",
							"\"\"\"\n",
							"\n",
							"df_4_recon3=spark.sql(qry_4_recon3.format(tablename,natural_key,natural_key,natural_key))\n",
							"print('Soft Deletes are:') \n",
							"df_4_recon3.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"#delete FROM etlhubconfirmed.dht_employee;\n",
							"\n",
							"qry_4_recon1=\"\"\"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable, tablename, natural_key, natural_key))\n",
							"\n",
							"qry_ins_new_rows=\"\"\"\n",
							"\n",
							"INSERT INTO {}\n",
							"\tselect \n",
							"\tCOALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY {}) AS {} \n",
							",\t1 AS VERSION\n",
							",   BUSINESS_ID\n",
							",\tCAMPUS_ID\n",
							",\tBUILDING_ID\n",
							",\tBUILDING_NAME\n",
							",\tBUILDING_STATUS\n",
							",\tBUILDING_OWNERSHIP_NAME\n",
							",\tBUILDING_OWNERSHIP_DESCRIPTION\n",
							",\tBUILDING_PRIMARY_USE_NAME AS PRIMARY_BUILDING_USE_NAME\n",
							",\tBUILDING_PRIMARY_USE_DESCRIPTION AS PRIMARY_BUILDING_USE_DESCR\n",
							",\tBUILDING_SECONDARY_USE_NAME AS SECONDARY_BUILDING_USE_NAME\n",
							",\tBUILDING_SECONDARY_USE_DESCRIPTION AS SECONDARY_BUILDING_USE_DESCR\n",
							",\tREPLACE(RENTABLE_AREA,',','') AS RENTABLE_AREA\n",
							",\tACTIVE_YEAR AS BUILDING_ACTIVATION_YEAR\n",
							",\tACTUAL_RETIREMENT_INACTIVATION AS BUILDING_INACTIVATION_YEAR\n",
							",   'Y' AS CURRENT_IND\n",
							",   '{}' AS EXTRACT_DT\n",
							",   '{}' AS REC_START_DT\n",
							",   '9999-12-31 00:00:00' as REC_END_DT\n",
							",   'Tririga' AS SOURCE_SYSTEM\n",
							",    column_hash as REC_CHECKSUM\n",
							",    CURRENT_TIMESTAMP as IMG_LST_UPD_DT\n",
							",    CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							"    from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"d=spark.sql(qry_ins_new_rows.format(tablename,natural_key,keycolumn,extract_dt,rec_start_dt,stagingtable,tablename,natural_key, natural_key))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"qry_ins_new_rows_test=\"\"\"\n",
							"\n",
							"\tselect \n",
							"\tCOALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY {}) AS {} \n",
							",\t1 AS VERSION\n",
							",{}\n",
							",   'Y' AS CURRENT_IND\n",
							",   '{}' AS EXTRACT_DT\n",
							",   '{}' AS REC_START_DT\n",
							",   '9999-12-31 00:00:00' as REC_END_DT\n",
							",   'Tririga' AS SOURCE_SYSTEM\n",
							",    column_hash as REC_CHECKSUM\n",
							",    CURRENT_TIMESTAMP as IMG_LST_UPD_DT\n",
							",    CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							"    from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"d_test=spark.sql(qry_ins_new_rows_test.format(natural_key,keycolumn,my_string,extract_dt,rec_start_dt,stagingtable,tablename,natural_key, natural_key))\n",
							"print(qry_ins_new_rows_test.format(natural_key,keycolumn,my_string,extract_dt,rec_start_dt,stagingtable,tablename,natural_key, natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT min(BUILDING_KEY), MAX(BUILDING_KEY) FROM etlhubconfirmed.dht_building \n",
							"--where CAMPUS_ID ='RTPMAIN'\n",
							"    --like 'RTPMAIN%' --and BUILDING_ID='404'\n",
							"        "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT a.BUSINESS_ID,B.column_hash,A.BUILDING_KEY, A.VERSION,\n",
							"A.REC_START_DT,B.existing_REC_START_DT\n",
							" FROM etlhubconfirmed.dht_building A\n",
							"JOIN DHTS_BUILDING B\n",
							"ON A.BUSINESS_ID = B.BUSINESS_ID\n",
							"AND LOWER(B.BUSINESS_ID) = LOWER(B.existing_BUSINESS_ID) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' --AND A.REC_START_DT=b.existing_REC_START_DT"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Update or expire the rows with existing instance of the changed rows:\n",
							"qry_4_upd_changes_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.{}\n",
							"AND LOWER(B.{}) = LOWER(B.existing_{}) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= '{}' -  INTERVAL 1 seconds\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\"\"\"\n",
							"f=spark.sql(qry_4_upd_changes_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key,natural_key,rec_start_dt))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Insert for changed rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry_ins_changed_rows=\"\"\"\n",
							"INSERT INTO {}\n",
							"\tselect \n",
							"    existing_BUILDING_KEY\n",
							",\t1 + existing_VERSION\n",
							",   BUSINESS_ID\n",
							",\tCAMPUS_ID\n",
							",\tBUILDING_ID\n",
							",\tBUILDING_NAME\n",
							",\tBUILDING_STATUS\n",
							",\tBUILDING_OWNERSHIP_NAME\n",
							",\tBUILDING_OWNERSHIP_DESCRIPTION\n",
							",\tBUILDING_PRIMARY_USE_NAME AS PRIMARY_BUILDING_USE_NAME\n",
							",\tBUILDING_PRIMARY_USE_DESCRIPTION AS PRIMARY_BUILDING_USE_DESCR\n",
							",\tBUILDING_SECONDARY_USE_NAME AS SECONDARY_BUILDING_USE_NAME\n",
							",\tBUILDING_SECONDARY_USE_DESCRIPTION AS SECONDARY_BUILDING_USE_DESCR\n",
							",\tREPLACE(RENTABLE_AREA,',','') AS RENTABLE_AREA\n",
							",\tACTIVE_YEAR AS BUILDING_ACTIVATION_YEAR\n",
							",\tACTUAL_RETIREMENT_INACTIVATION AS BUILDING_INACTIVATION_YEAR\n",
							",   'Y' AS CURRENT_IND\n",
							",   '{}' AS EXTRACT_DT\n",
							",   '{}' AS REC_START_DT\n",
							",   '9999-12-31 00:00:00' as REC_END_DT\n",
							",   'Tririga' AS SOURCE_SYSTEM\n",
							",    column_hash as REC_CHECKSUM\n",
							",    CURRENT_TIMESTAMP as IMG_LST_UPD_DT\n",
							",    existing_IMG_CREATED_DT\n",
							"from {} A \n",
							"WHERE LOWER({}) = LOWER(existing_{}) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"e=spark.sql(qry_ins_changed_rows.format(tablename,extract_dt,rec_start_dt,stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Soft Deletes - Update Active_Ind for the rows no longer active in source:\n",
							"\n",
							"qry_4_upd_deleted_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' --AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"    ,REC_END_DT= '{}' -  INTERVAL 1 seconds\n",
							"\n",
							";\n",
							"\"\"\"\n",
							"g=spark.sql(qry_4_upd_deleted_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key,rec_start_dt))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} WHERE CURRENT_IND='Y' GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select distinct VERSION,EXTRACT_DT,REC_START_DT,REC_END_DT,IMG_CREATED_DT,IMG_LST_UPD_DT,CURRENT_IND,count(*) \n",
							"from etlhubconfirmed.dht_building --WHERE CURRENT_IND='Y'\n",
							"    group by VERSION,EXTRACT_DT,REC_START_DT,REC_END_DT,IMG_CREATED_DT,IMG_LST_UPD_DT,CURRENT_IND\n",
							"    \n",
							"--Updated File\n",
							"    --BLDMAIN, 006 Updated\n",
							"    --CYNICOSI, 0002 Updated\n",
							"    --RTPMAIN, 002 Updated\n",
							"--Further Updated File\n",
							"    --ITMEDMLN,004 removed\n",
							"    --MQFORTFR,001 Updated\n",
							"    --CYNICOSI,0002 Updated again\n",
							"    --JPATAGOE, 001 Updated\n",
							"    --RTPMAIN, 002 removed\n",
							"    ;\n",
							"\n",
							"SELECT BUILDING_KEY,BUSINESS_ID, BUILDING_OWNERSHIP_DESCR,VERSION,EXTRACT_DT,REC_START_DT,REC_END_DT,IMG_CREATED_DT,IMG_LST_UPD_DT,CURRENT_IND\n",
							" from etlhubconfirmed.dht_building A\n",
							"where BUSINESS_ID in ('BLDMAIN~006','CYNICOSI~0002','RTPMAIN~002','ITMEDMLN~004','MQFORTFR~001','JPATAGOE~001')\n",
							"ORDER BY BUSINESS_ID, REC_START_DT\n",
							"    ;\n",
							"\n",
							"SELECT BUILDING_KEY,BUSINESS_ID, BUILDING_OWNERSHIP_DESCR,VERSION,EXTRACT_DT,REC_START_DT,REC_END_DT,IMG_CREATED_DT,IMG_LST_UPD_DT,CURRENT_IND,A.*\n",
							" from etlhubconfirmed.dht_building A\n",
							"where BUSINESS_ID in ('BLDMAIN~006','CYNICOSI~0002','RTPMAIN~002','ITMEDMLN~004','MQFORTFR~001','JPATAGOE~001'\n",
							"    ,'BLDNEW1~999','POKNEW2~999'\n",
							")\n",
							"ORDER BY BUSINESS_ID, REC_START_DT\n",
							"    ;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"incrementalData_DF.write.csv(csv_archive_path, header = 'true')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT BUILDING_KEY,BUSINESS_ID, BUILDING_OWNERSHIP_DESCR,VERSION,EXTRACT_DT,REC_START_DT,REC_END_DT,IMG_CREATED_DT,IMG_LST_UPD_DT,CURRENT_IND\n",
							" from etlhubconfirmed.dht_building A\n",
							"where BUSINESS_ID in ('BLDMAIN~006','CYNICOSI~0002','RTPMAIN~002','ITMEDMLN~004','MQFORTFR~001','JPATAGOE~001')\n",
							"ORDER BY BUSINESS_ID, REC_START_DT\n",
							"    ;\n",
							"SELECT BUSINESS_ID\n",
							"from etlhubconfirmed.dht_building A\n",
							"where CURRENT_IND='N'\n",
							"    and BUSINESS_ID NOT IN (SELECT BUSINESS_ID FROM etlhubconfirmed.dht_building A WHERE CURRENT_IND='Y')\n",
							"  ;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 49
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_dht_building_final')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts/GREIW/backup"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPool001494New",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "472g",
					"driverCores": 80,
					"executorMemory": "472g",
					"executorCores": 80,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9e464ccf-70fe-4181-ba73-7781041b1fee"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPool001494New",
						"name": "sPool001494New",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPool001494New",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 80,
						"memory": 472,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Building Dimension load**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubConfirmed.DHT_BUILDING by sourcing data from Tririga system.\n",
							"The file is pulled to datastage server using SFTP protocol.\n",
							"\n",
							"The CSV formatted file is uploaded to adls for loading to BUILDING deltalake table created.\n",
							"\n",
							"The script used for file transfer is /home/resodba/trrgprocget.sh\n",
							"\n",
							"File Name: IWTRIRIGABuildingQuery.csv from /GlobalDir/CustomerFiles/tririga"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"from pyspark.sql.functions import col\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"import os\n",
							"import sys\n",
							"from datetime import datetime"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/Tririga/building/' # fill in your relative folder path \n",
							"relative_archive_path='archive/'\n",
							"file_name='IWTRIRIGABuildingQuery.csv' # 20220705, 20220705_upd1, 20220705_upd2\n",
							"natural_key1=\"CAMPUS_ID\"\n",
							"natural_key2=\"BUILDING_ID\"\n",
							"natural_key=\"BUSINESS_ID\"\n",
							"MinimumTririgaBuildingCount=750\n",
							"MinimumTririgaCampusCount=450\n",
							"tablename=\"etlhubConfirmed.dht_building\"\n",
							"stagingtable=\"DHTS_BUILDING\"\n",
							"keycolumn=\"BUILDING_KEY\"\n",
							"date = datetime.now().strftime(\"%Y-%m-%d-%I:%M:%S_%p\")\n",
							"extract_dt = datetime.now().strftime(\"%Y-%m-%d %I:%M:%S\") #This is batch/cycle date. It would be common for all the rows inserted in a single run\n",
							"rec_start_dt=datetime.now().strftime(\"%Y-%m-%d %I:%M:%S\") #This should ideally from the source system. If source doesnt have any date column, use current date \n",
							"print(extract_dt)\n",
							"print(natural_key)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Copy source CSV file to archive folder including timestamp in file name"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/ERDM/DS_INDUSTRY_HIERARCHY_DATA.csv\n",
							"\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"adls_arch_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_archive_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"print(natural_key)\n",
							"\n",
							"relative_archive_path\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"arch_fil_name=file_name.rsplit(\".\")\n",
							"csv_archive_path = adls_arch_path + arch_fil_name[0] + '_' + date + '.' + arch_fil_name[1]\n",
							"print (csv_archive_path)\n",
							"\n",
							"incrementalData_DF1 = spark.read.csv(csv_path, header = 'true')\n",
							"\n",
							"#incrementalData_DF = incrementalData_DF1.withColumn('BUILDING_BUSINESS_ID', concat_ws(\"CAMPUS_ID\", \"BUILDING_ID\"))\n",
							"\n",
							"incrementalData_DF = incrementalData_DF1.withColumn(natural_key, concat_ws(\"~\", natural_key1, natural_key2))\n",
							"\n",
							"#incrementalData_DF.write.csv(csv_archive_path, header = 'true') : Write file into archive directory\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"#Add checks for duplicate data, Check for 1. dups in source data based on natural Key and\n",
							"# 2. Dups in target already\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select * from incrementalData_DF2\n",
							"where BUSINESS_ID in ('BLDMAIN~006','CYNICOSI~0002','RTPMAIN~002','ITMEDMLN~004','MQFORTFR~001','JPATAGOE~001')\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dim_col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    dim_col_list.append(i)\n",
							"print (dim_col_list)\n",
							"my_string = ','.join(dim_col_list)\n",
							"print (my_string)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Create a dataframe with target deltalake table data with necessary columns\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\n",
							"#existingDataDF=spark.sql(\"SELECT tgt.* from etlhubConfirmed.DHT_EMPLOYEE tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\n",
							"\n",
							"#existingMaxKeyDF=spark.sql(\"SELECT MAX(EMPLOYEE_KEY) existing_MAX_KEY from etlhubConfirmed.DHT_EMPLOYEE tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1, col(natural_key) == col(\"existing_\" + natural_key), \"fullouter\")\n",
							"\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"fullJoin2.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .option(\"overwriteSchema\", \"true\") \\\n",
							"  .option(\"path\", \"abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/staging/{}\".format(stagingtable)) \\\n",
							"  .saveAsTable(stagingtable)\n",
							"   #.option(\"overwriteSchema\", \"true\") \\  .save(\"\") \\"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Data Validation Checks :\n",
							"\n",
							"- Check for duplicates in Source data based on natural key\n",
							"- Check for partial data from source. That is if the file has expected number of rows\n",
							"- Check for duplicates in target delta lake table based on natural key\n",
							"- Referential Integrity checks\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} WHERE CURRENT_IND='Y' GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry5=\"\"\"\n",
							"SELECT * FROM incrementalData_DF2\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"df3=spark.sql(qry3.format(natural_key,natural_key))\n",
							"cnt1=df3.count()\n",
							"\n",
							"print (cnt1)\n",
							"if cnt1 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in source:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(1)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"\n",
							"df5=spark.sql(qry5)\n",
							"cnt3=df5.count()\n",
							"\n",
							"if cnt3 >= MinimumTririgaBuildingCount:\n",
							"    print(\"The number of rows in source data is more than the threshold, data can be processed\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"The source data is having less number of rows than expected. Please check:\" )\n",
							"    print(cnt3)\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							"\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry_4_recon1=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable,tablename,natural_key,natural_key))\n",
							"print('New records or Inserts are:') \n",
							"df_4_recon1.show()\n",
							"\n",
							"qry_4_recon2=\"\"\"\n",
							"\n",
							"SELECT count(*) from {} A \n",
							"WHERE LOWER({}) = LOWER(existing_{}) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon2=spark.sql(qry_4_recon2.format(stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))\n",
							"print('Changed records or Updates are:') \n",
							"df_4_recon2.show()\n",
							"\n",
							"qry_4_recon3=\"\"\"\n",
							"SELECT count(*) from \n",
							"{} A\n",
							"join fullJoin B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' --AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"\n",
							"\"\"\"\n",
							"\n",
							"df_4_recon3=spark.sql(qry_4_recon3.format(tablename,natural_key,natural_key,natural_key))\n",
							"print('Soft Deletes are:') \n",
							"df_4_recon3.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"#delete FROM etlhubconfirmed.dht_employee;\n",
							"\n",
							"qry_4_recon1=\"\"\"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable, tablename, natural_key, natural_key))\n",
							"\n",
							"qry_ins_new_rows=\"\"\"\n",
							"\n",
							"INSERT INTO {}\n",
							"\tselect \n",
							"\tCOALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY {}) AS {} \n",
							",\t1 AS VERSION\n",
							",   BUSINESS_ID\n",
							",\tCAMPUS_ID\n",
							",\tBUILDING_ID\n",
							",\tBUILDING_NAME\n",
							",\tBUILDING_STATUS\n",
							",\tBUILDING_OWNERSHIP_NAME\n",
							",\tBUILDING_OWNERSHIP_DESCRIPTION\n",
							",\tBUILDING_PRIMARY_USE_NAME AS PRIMARY_BUILDING_USE_NAME\n",
							",\tBUILDING_PRIMARY_USE_DESCRIPTION AS PRIMARY_BUILDING_USE_DESCR\n",
							",\tBUILDING_SECONDARY_USE_NAME AS SECONDARY_BUILDING_USE_NAME\n",
							",\tBUILDING_SECONDARY_USE_DESCRIPTION AS SECONDARY_BUILDING_USE_DESCR\n",
							",\tREPLACE(RENTABLE_AREA,',','') AS RENTABLE_AREA\n",
							",\tACTIVE_YEAR AS BUILDING_ACTIVATION_YEAR\n",
							",\tACTUAL_RETIREMENT_INACTIVATION AS BUILDING_INACTIVATION_YEAR\n",
							",   'Y' AS CURRENT_IND\n",
							",   '{}' AS EXTRACT_DT\n",
							",   '{}' AS REC_START_DT\n",
							",   '9999-12-31 00:00:00' as REC_END_DT\n",
							",   'Tririga' AS SOURCE_SYSTEM\n",
							",    column_hash as REC_CHECKSUM\n",
							",    CURRENT_TIMESTAMP as IMG_LST_UPD_DT\n",
							",    CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							"    from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"d=spark.sql(qry_ins_new_rows.format(tablename,natural_key,keycolumn,extract_dt,rec_start_dt,stagingtable,tablename,natural_key, natural_key))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"qry_ins_new_rows_test=\"\"\"\n",
							"\n",
							"\tselect \n",
							"\tCOALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY {}) AS {} \n",
							",\t1 AS VERSION\n",
							",{}\n",
							",   'Y' AS CURRENT_IND\n",
							",   '{}' AS EXTRACT_DT\n",
							",   '{}' AS REC_START_DT\n",
							",   '9999-12-31 00:00:00' as REC_END_DT\n",
							",   'Tririga' AS SOURCE_SYSTEM\n",
							",    column_hash as REC_CHECKSUM\n",
							",    CURRENT_TIMESTAMP as IMG_LST_UPD_DT\n",
							",    CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							"    from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"d_test=spark.sql(qry_ins_new_rows_test.format(natural_key,keycolumn,my_string,extract_dt,rec_start_dt,stagingtable,tablename,natural_key, natural_key))\n",
							"print(qry_ins_new_rows_test.format(natural_key,keycolumn,my_string,extract_dt,rec_start_dt,stagingtable,tablename,natural_key, natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT min(BUILDING_KEY), MAX(BUILDING_KEY) FROM etlhubconfirmed.dht_building \n",
							"--where CAMPUS_ID ='RTPMAIN'\n",
							"    --like 'RTPMAIN%' --and BUILDING_ID='404'\n",
							"        "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT a.BUSINESS_ID,B.column_hash,A.BUILDING_KEY, A.VERSION,\n",
							"A.REC_START_DT,B.existing_REC_START_DT\n",
							" FROM etlhubconfirmed.dht_building A\n",
							"JOIN DHTS_BUILDING B\n",
							"ON A.BUSINESS_ID = B.BUSINESS_ID\n",
							"AND LOWER(B.BUSINESS_ID) = LOWER(B.existing_BUSINESS_ID) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' --AND A.REC_START_DT=b.existing_REC_START_DT"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Update or expire the rows with existing instance of the changed rows:\n",
							"qry_4_upd_changes_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.{}\n",
							"AND LOWER(B.{}) = LOWER(B.existing_{}) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= '{}' -  INTERVAL 1 seconds\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\"\"\"\n",
							"f=spark.sql(qry_4_upd_changes_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key,natural_key,rec_start_dt))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Insert for changed rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry_ins_changed_rows=\"\"\"\n",
							"INSERT INTO {}\n",
							"\tselect \n",
							"    existing_BUILDING_KEY\n",
							",\t1 + existing_VERSION\n",
							",   BUSINESS_ID\n",
							",\tCAMPUS_ID\n",
							",\tBUILDING_ID\n",
							",\tBUILDING_NAME\n",
							",\tBUILDING_STATUS\n",
							",\tBUILDING_OWNERSHIP_NAME\n",
							",\tBUILDING_OWNERSHIP_DESCRIPTION\n",
							",\tBUILDING_PRIMARY_USE_NAME AS PRIMARY_BUILDING_USE_NAME\n",
							",\tBUILDING_PRIMARY_USE_DESCRIPTION AS PRIMARY_BUILDING_USE_DESCR\n",
							",\tBUILDING_SECONDARY_USE_NAME AS SECONDARY_BUILDING_USE_NAME\n",
							",\tBUILDING_SECONDARY_USE_DESCRIPTION AS SECONDARY_BUILDING_USE_DESCR\n",
							",\tREPLACE(RENTABLE_AREA,',','') AS RENTABLE_AREA\n",
							",\tACTIVE_YEAR AS BUILDING_ACTIVATION_YEAR\n",
							",\tACTUAL_RETIREMENT_INACTIVATION AS BUILDING_INACTIVATION_YEAR\n",
							",   'Y' AS CURRENT_IND\n",
							",   '{}' AS EXTRACT_DT\n",
							",   '{}' AS REC_START_DT\n",
							",   '9999-12-31 00:00:00' as REC_END_DT\n",
							",   'Tririga' AS SOURCE_SYSTEM\n",
							",    column_hash as REC_CHECKSUM\n",
							",    CURRENT_TIMESTAMP as IMG_LST_UPD_DT\n",
							",    existing_IMG_CREATED_DT\n",
							"from {} A \n",
							"WHERE LOWER({}) = LOWER(existing_{}) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"e=spark.sql(qry_ins_changed_rows.format(tablename,extract_dt,rec_start_dt,stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Soft Deletes - Update Active_Ind for the rows no longer active in source:\n",
							"\n",
							"qry_4_upd_deleted_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' --AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"    ,REC_END_DT= '{}' -  INTERVAL 1 seconds\n",
							"\n",
							";\n",
							"\"\"\"\n",
							"g=spark.sql(qry_4_upd_deleted_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key,rec_start_dt))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} WHERE CURRENT_IND='Y' GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select distinct VERSION,EXTRACT_DT,REC_START_DT,REC_END_DT,IMG_CREATED_DT,IMG_LST_UPD_DT,CURRENT_IND,count(*) \n",
							"from etlhubconfirmed.dht_building --WHERE CURRENT_IND='Y'\n",
							"    group by VERSION,EXTRACT_DT,REC_START_DT,REC_END_DT,IMG_CREATED_DT,IMG_LST_UPD_DT,CURRENT_IND\n",
							"    \n",
							"--Updated File\n",
							"    --BLDMAIN, 006 Updated\n",
							"    --CYNICOSI, 0002 Updated\n",
							"    --RTPMAIN, 002 Updated\n",
							"--Further Updated File\n",
							"    --ITMEDMLN,004 removed\n",
							"    --MQFORTFR,001 Updated\n",
							"    --CYNICOSI,0002 Updated again\n",
							"    --JPATAGOE, 001 Updated\n",
							"    --RTPMAIN, 002 removed\n",
							"    ;\n",
							"\n",
							"SELECT BUILDING_KEY,BUSINESS_ID, BUILDING_OWNERSHIP_DESCR,VERSION,EXTRACT_DT,REC_START_DT,REC_END_DT,IMG_CREATED_DT,IMG_LST_UPD_DT,CURRENT_IND\n",
							" from etlhubconfirmed.dht_building A\n",
							"where BUSINESS_ID in ('BLDMAIN~006','CYNICOSI~0002','RTPMAIN~002','ITMEDMLN~004','MQFORTFR~001','JPATAGOE~001')\n",
							"ORDER BY BUSINESS_ID, REC_START_DT\n",
							"    ;\n",
							"\n",
							"SELECT BUILDING_KEY,BUSINESS_ID, BUILDING_OWNERSHIP_DESCR,VERSION,EXTRACT_DT,REC_START_DT,REC_END_DT,IMG_CREATED_DT,IMG_LST_UPD_DT,CURRENT_IND,A.*\n",
							" from etlhubconfirmed.dht_building A\n",
							"where BUSINESS_ID in ('BLDMAIN~006','CYNICOSI~0002','RTPMAIN~002','ITMEDMLN~004','MQFORTFR~001','JPATAGOE~001'\n",
							"    ,'BLDNEW1~999','POKNEW2~999'\n",
							")\n",
							"ORDER BY BUSINESS_ID, REC_START_DT\n",
							"    ;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"incrementalData_DF.write.csv(csv_archive_path, header = 'true')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT BUILDING_KEY,BUSINESS_ID, BUILDING_OWNERSHIP_DESCR,VERSION,EXTRACT_DT,REC_START_DT,REC_END_DT,IMG_CREATED_DT,IMG_LST_UPD_DT,CURRENT_IND\n",
							" from etlhubconfirmed.dht_building A\n",
							"where BUSINESS_ID in ('BLDMAIN~006','CYNICOSI~0002','RTPMAIN~002','ITMEDMLN~004','MQFORTFR~001','JPATAGOE~001')\n",
							"ORDER BY BUSINESS_ID, REC_START_DT\n",
							"    ;\n",
							"SELECT BUSINESS_ID\n",
							"from etlhubconfirmed.dht_building A\n",
							"where CURRENT_IND='N'\n",
							"    and BUSINESS_ID NOT IN (SELECT BUSINESS_ID FROM etlhubconfirmed.dht_building A WHERE CURRENT_IND='Y')\n",
							"  ;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 49
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_dht_campus')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts/GREIW"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPool001494New",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "472g",
					"driverCores": 80,
					"executorMemory": "472g",
					"executorCores": 80,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ecee3b73-f10d-4569-bad2-04caad341fba"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPool001494New",
						"name": "sPool001494New",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPool001494New",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 80,
						"memory": 472,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Campus load**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubConfirmed.DHT_CMAPUS by sourcing data from Tririga system.\n",
							"The file is pulled to datastage server using SFTP protocol.\n",
							"\n",
							"The CSV formatted file is uploaded to adls for loading to CAMPUS deltalake table created.\n",
							"\n",
							"The script used for file transfer is /home/resodba/trrgprocget.sh\n",
							"\n",
							"File Name: IWTRIRIGACampusQuery.csv from /GlobalDir/CustomerFiles/tririga"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws,trim\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"from pyspark.sql.functions import col\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"import os\n",
							"import sys\n",
							"from datetime import datetime"
						],
						"outputs": [],
						"execution_count": 54
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/Tririga/campus/' # fill in your relative folder path \n",
							"relative_path1 = 'extract/Tririga/Site/'\n",
							"relative_archive_path='archive/'\n",
							"file_name='IWTRIRIGASCampusQuery.csv' # 20220705, 20220705_upd1, 20220705_upd2\n",
							"natural_key=\"CAMPUS_ID\"\n",
							"MinimumTririgaCampusCount=450\n",
							"tablename=\"etlhubConfirmed.dht_campus\"\n",
							"stagingtable=\"DHTS_CAMPUS\"\n",
							"keycolumn=\"CAMPUS_KEY\"\n",
							"date = datetime.now().strftime(\"%Y-%m-%d-%I:%M:%S_%p\")\n",
							"extract_dt = datetime.now().strftime(\"%Y-%m-%d %I:%M:%S\") #This is batch/cycle date. It would be common for all the rows inserted in a single run\n",
							"rec_start_dt=datetime.now().strftime(\"%Y-%m-%d %I:%M:%S\") #This should ideally from the source system. If source doesnt have any date column, use current date \n",
							"print(extract_dt)\n",
							"print(natural_key)"
						],
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Copy source CSV file to archive folder including timestamp in file name"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/Tririga/campus/IWTRIRIGACampusQuery.csv\n",
							"\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"adls_path1 = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path1) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"adls_arch_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_archive_path) \n",
							"print('Primary archive path: ' + adls_arch_path) \n",
							"\n",
							"print(natural_key)\n",
							"\n",
							"relative_archive_path\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"csv_path1= adls_path1 + 'IWTRIRIGASiteQuery.csv'\n",
							"arch_fil_name=file_name.rsplit(\".\")\n",
							"arch_fil_name1='IWTRIRIGASiteQuery.csv'.rsplit(\".\")\n",
							"csv_archive_path = adls_arch_path + arch_fil_name[0] + '_' + date + '.' + arch_fil_name[1]\n",
							"csv_archive_path1 = adls_arch_path + arch_fil_name1[0] + '_' + date + '.' + arch_fil_name1[1]\n",
							"\n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\n",
							"incrementalSiteData_DF = spark.read.csv(csv_path1, header = 'true')\n",
							"\n",
							"incrementalSiteData_DF.createOrReplaceTempView('ParentTable')\n",
							"incrementalData_DF.createOrReplaceTempView('ChildTable')\n",
							"\n",
							"# Capturing recods that are not available in parent table\n",
							"qry_RefIntegrityFailDF=\"\"\"\n",
							"select A.*\n",
							"from ChildTable A\n",
							"left outer join ParentTable B on trim(A.SITE_ID)=trim(B.SITE_ID) \n",
							"where A.SITE_ID IS NULL;\n",
							"\"\"\"\n",
							"RejDataDF=spark.sql(qry_RefIntegrityFailDF)\n",
							"RejDataDF.createOrReplaceTempView('RejDataView')\n",
							"\n",
							"qry_RefIntegrityFailCntryDF=\"\"\"\n",
							"select A.*\n",
							"from ChildTable A\n",
							"left outer join etlhubconfirmed.dhtr_country C ON trim(A.COUNTRY_CODE)=trim(C.CODE)\n",
							"where C.CODE IS NULL;\n",
							"\"\"\"\n",
							"RejCntryDataDF=spark.sql(qry_RefIntegrityFailCntryDF)\n",
							"RejCntryDataDF.createOrReplaceTempView('RejCntryDataView')\n",
							"\n",
							"\n",
							"qry_RefIntegrityFailStteProvDF=\"\"\"\n",
							"select A.*\n",
							"from ChildTable A\n",
							"left outer join etlhubconfirmed.dhtr_stte_prov d ON trim(A.STATE_PROVINCE_ID)=trim(d.STTE_PROV_CD)\n",
							"where d.STTE_PROV_CD IS NULL;\n",
							"\"\"\"\n",
							"RejStteProvDataDF=spark.sql(qry_RefIntegrityFailStteProvDF)\n",
							"RejStteProvDataDF.createOrReplaceTempView('RejStteProvDataView')\n",
							"\n",
							"qry_IncrementalDataDF=\"\"\"\n",
							"select distinct B.GEOGRAPHY_ID, B.GEOGRAPHY_NAME,B.REGION_ID,B.REGION_NAME,B.SITE_ID,B.SITE_NAME,\n",
							"A.WORLD_REGION_CODE, A.WORLD_REGION_NAME, A.MARKET_TEAM_REGION_CODE, A.MARKET_TEAM_REGION_NAME,A.COUNTRY_CODE ,\n",
							"cast(C.mediumDescription AS VARCHAR(64)) as COUNTRY_NAME,A.CAMPUS_ID,A.CAMPUS_NAME,A.CAMPUS_STATUS,A.WORK_LOCATION_CODE, \n",
							"A.ADDRESS, A.CITY, A.STATE_PROVINCE_ID, \n",
							"D.STTE_PROV_DESC as STATE_PROVINCE_NAME,A.POSTAL_CODE,A.LATITUDE,A.LONGITUDE,A.UTC_OFFSET,\n",
							"A.ICU_TIME_ZONE,A.PEOPLE_HOUSED_FLAG,A.REMOTE_SUPPORT_FLAG,A.PRIMARY_CAMPUS_USE_NAME,\n",
							"A.PRIMARY_CAMPUS_USE_DESCRIPTION as PRIMARY_CAMPUS_USE_DESCR,\n",
							"A.CAMPUS_OWNERSHIP,A.CAMPUS_ACTIVATION_YEAR,A.CAMPUS_INACTIVATION_YEAR \n",
							"from ChildTable   A\n",
							"left outer join ParentTable B on trim(A.SITE_ID)=trim(B.SITE_ID)\n",
							"left outer join etlhubconfirmed.dhtr_country C ON trim(A.COUNTRY_CODE)=trim(C.CODE)\n",
							"left outer join etlhubconfirmed.dhtr_stte_prov d ON trim(A.STATE_PROVINCE_ID)=trim(d.STTE_PROV_CD) \n",
							"and trim(A.COUNTRY_CODE) = trim(D.CNTRY_CD)\n",
							"where B.SITE_ID is not null  \n",
							"\"\"\"\n",
							"FinalIncrementalDataDF=spark.sql(qry_IncrementalDataDF)\n",
							"\n",
							"col_list=[]\n",
							"for i in FinalIncrementalDataDF.columns:\n",
							"    col_list.append(i)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1 = FinalIncrementalDataDF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 56
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"SELECT COUNT(*) as CNT, CAMPUS_ID FROM incrementalData_DF2 GROUP BY CAMPUS_ID HAVING COUNT(*)>1"
						],
						"outputs": [],
						"execution_count": 57
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Create a dataframe with target deltalake table data with necessary columns\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT cast(MAX({}) as integer) as existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1, col(natural_key) == col(\"existing_\" + natural_key), \"fullouter\")\n",
							"\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"fullJoin2.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .option(\"overwriteSchema\", \"true\") \\\n",
							"  .option(\"path\", \"abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/staging/{}\".format(stagingtable)) \\\n",
							"  .saveAsTable(stagingtable)\n",
							"   #.option(\"overwriteSchema\", \"true\") \\  .save(\"\") \\\n",
							""
						],
						"outputs": [],
						"execution_count": 59
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from fullJoin"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Data Validation Checks :\n",
							"\n",
							"- Check for duplicates in Source data based on natural key\n",
							"- Check for partial data from source. That is if the file has expected number of rows\n",
							"- Check for duplicates in target delta lake table based on natural key\n",
							"- Referential Integrity checks\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} WHERE CURRENT_IND='Y' GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry5=\"\"\"\n",
							"SELECT * FROM incrementalData_DF2\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"df3=spark.sql(qry3.format(natural_key,natural_key))\n",
							"cnt1=df3.count()\n",
							"\n",
							"print (cnt1)\n",
							"if cnt1 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in source:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    --sys.exit(1)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"\n",
							"df5=spark.sql(qry5)\n",
							"cnt3=df5.count()\n",
							"\n",
							"if cnt3 >= MinimumTririgaCampusCount:\n",
							"    print(\"The number of rows in source data is more than the threshold, data can be processed\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"The source data is having less number of rows than expected. Please check:\" )\n",
							"    print(cnt3)\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							"\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 61
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry_4_recon1=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable,tablename,natural_key,natural_key))\n",
							"print('New records or Inserts are:') \n",
							"df_4_recon1.show()\n",
							"\n",
							"qry_4_recon2=\"\"\"\n",
							"\n",
							"SELECT count(*) from {} A \n",
							"WHERE LOWER({}) = LOWER(existing_{}) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon2=spark.sql(qry_4_recon2.format(stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))\n",
							"print('Changed records or Updates are:') \n",
							"df_4_recon2.show()\n",
							"\n",
							"qry_4_recon3=\"\"\"\n",
							"SELECT count(*) from \n",
							"{} A\n",
							"join fullJoin B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' --AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"\n",
							"\"\"\"\n",
							"\n",
							"df_4_recon3=spark.sql(qry_4_recon3.format(tablename,natural_key,natural_key,natural_key))\n",
							"print('Soft Deletes are:') \n",
							"df_4_recon3.show()\n",
							"\n",
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry_4_recon1=\"\"\"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable, tablename, natural_key, natural_key))"
						],
						"outputs": [],
						"execution_count": 62
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from DHTS_CAMPUS "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"INSERT INTO etlhubConfirmed.dht_campus\n",
							"select cast(COALESCE(A.existing_MAX_KEY,0)+ \n",
							"ROW_NUMBER()OVER(ORDER BY CAMPUS_ID) AS INTEGER) AS CAMPUS_KEY,1 AS VERSION\n",
							",GEOGRAPHY_ID\n",
							",GEOGRAPHY_NAME\n",
							",REGION_ID\n",
							",REGION_NAME\n",
							",SITE_ID\n",
							",SITE_NAME\n",
							",WORLD_REGION_CODE\n",
							",WORLD_REGION_NAME\n",
							",MARKET_TEAM_REGION_CODE\n",
							",MARKET_TEAM_REGION_NAME\n",
							",COUNTRY_CODE\n",
							",COUNTRY_NAME\n",
							",CAMPUS_ID\n",
							",CAMPUS_NAME\n",
							",CAMPUS_STATUS\n",
							",WORK_LOCATION_CODE\n",
							",ADDRESS\n",
							",CITY\n",
							",STATE_PROVINCE_ID\n",
							",STATE_PROVINCE_NAME\n",
							",POSTAL_CODE\n",
							",LATITUDE\n",
							",LONGITUDE\n",
							",UTC_OFFSET\n",
							",ICU_TIME_ZONE\n",
							",PEOPLE_HOUSED_FLAG\n",
							",REMOTE_SUPPORT_FLAG\n",
							",PRIMARY_CAMPUS_USE_NAME\n",
							",PRIMARY_CAMPUS_USE_DESCR\n",
							",CAMPUS_OWNERSHIP\n",
							",CAMPUS_ACTIVATION_YEAR\n",
							",CAMPUS_INACTIVATION_YEAR\n",
							",'Y'\n",
							",CURRENT_TIMESTAMP \n",
							",CURRENT_TIMESTAMP\n",
							",TIMESTAMP_FORMAT('1999-12-31 23:59:59', 'YYYY-MM-DD HH24:MI:SS')\n",
							",'Tririga'\n",
							",column_hash\n",
							",CURRENT_TIMESTAMP\n",
							",CURRENT_TIMESTAMP \n",
							"    from dhts_campus A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubconfirmed.dht_campus B\n",
							"WHERE A.CAMPUS_ID=B.CAMPUS_ID\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"--\"\"\"\n",
							"\n",
							"--dfIns=spark.sql(qry_ins_new_rows.format(extract_dt,rec_start_dt,stagingtable,tablename,natural_key, natural_key))\n",
							""
						],
						"outputs": [],
						"execution_count": 71
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select \r\n",
							"cast(COALESCE(A.existing_MAX_KEY,0)+ \r\n",
							"ROW_NUMBER()OVER(ORDER BY CAMPUS_ID) AS INTEGER) AS CAMPUS_KEY,1 AS VERSION\r\n",
							",GEOGRAPHY_ID\r\n",
							",GEOGRAPHY_NAME\r\n",
							",REGION_ID\r\n",
							",REGION_NAME\r\n",
							",SITE_ID\r\n",
							",SITE_NAME\r\n",
							",WORLD_REGION_CODE\r\n",
							",WORLD_REGION_NAME\r\n",
							",MARKET_TEAM_REGION_CODE\r\n",
							",MARKET_TEAM_REGION_NAME\r\n",
							",COUNTRY_CODE\r\n",
							",COUNTRY_NAME\r\n",
							",CAMPUS_ID\r\n",
							",CAMPUS_NAME\r\n",
							",CAMPUS_STATUS\r\n",
							",WORK_LOCATION_CODE\r\n",
							",ADDRESS\r\n",
							",CITY\r\n",
							",STATE_PROVINCE_ID\r\n",
							",STATE_PROVINCE_NAME\r\n",
							",POSTAL_CODE\r\n",
							",LATITUDE\r\n",
							",LONGITUDE\r\n",
							",UTC_OFFSET\r\n",
							",ICU_TIME_ZONE\r\n",
							",PEOPLE_HOUSED_FLAG\r\n",
							",REMOTE_SUPPORT_FLAG\r\n",
							",PRIMARY_CAMPUS_USE_NAME\r\n",
							",PRIMARY_CAMPUS_USE_DESCR\r\n",
							",CAMPUS_OWNERSHIP\r\n",
							",CAMPUS_ACTIVATION_YEAR\r\n",
							",CAMPUS_INACTIVATION_YEAR\r\n",
							",   'Y' AS CURRENT_IND\r\n",
							",   '2022-08-18 07:17:57' AS EXTRACT_DT\r\n",
							",   '2022-08-18 07:17:57' AS REC_START_DT\r\n",
							",   '9999-12-31 00:00:00' as REC_END_DT\r\n",
							",   'Tririga' AS SOURCE_SYSTEM\r\n",
							",    column_hash as REC_CHECKSUM\r\n",
							",    CURRENT_TIMESTAMP as IMG_LST_UPD_DT\r\n",
							",    CURRENT_TIMESTAMP AS IMG_CREATED_DT\r\n",
							"    from DHTS_CAMPUS A \r\n",
							"WHERE existing_REC_CHECKSUM is null\r\n",
							"AND NOT EXISTS\r\n",
							"(SELECT 1 FROM etlhubConfirmed.dht_campus B\r\n",
							"WHERE A.CAMPUS_ID=B.CAMPUS_ID\r\n",
							"and b.CURRENT_IND='Y'\r\n",
							"AND A.column_hash=B.REC_CHECKSUM)"
						],
						"outputs": [],
						"execution_count": 64
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Update or expire the rows with existing instance of the changed rows:\n",
							"qry_4_upd_changes_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.{}\n",
							"AND LOWER(B.{}) = LOWER(B.existing_{}) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= '{}' -  INTERVAL 1 seconds\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\"\"\"\n",
							"f=spark.sql(qry_4_upd_changes_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key,natural_key,rec_start_dt))\n",
							""
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Insert for changed rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry_ins_changed_rows=\"\"\"\n",
							"INSERT INTO {}\n",
							"\tselect \n",
							"    existing_CAMPUS_KEY\n",
							",\t1 + existing_VERSION\n",
							",GEOGRAPHY_ID\n",
							",GEOGRAPHY_NAME\n",
							",REGION_ID\n",
							",REGION_NAME\n",
							",SITE_ID\n",
							",SITE_NAME\n",
							",WORLD_REGION_CODE\n",
							",WORLD_REGION_NAME\n",
							",MARKET_TEAM_REGION_CODE\n",
							",MARKET_TEAM_REGION_NAME\n",
							",COUNTRY_CODE\n",
							",COUNTRY_NAME\n",
							",CAMPUS_ID\n",
							",CAMPUS_NAME\n",
							",CAMPUS_STATUS\n",
							",WORK_LOCATION_CODE\n",
							",ADDRESS, CITY\n",
							",STATE_PROVINCE_ID\n",
							",STATE_PROVINCE_NAME\n",
							",POSTAL_CODE\n",
							",LATITUDE\n",
							",LONGITUDE\n",
							",UTC_OFFSET\n",
							",ICU_TIME_ZONE\n",
							",PEOPLE_HOUSED_FLAG\n",
							",REMOTE_SUPPORT_FLAG\n",
							",PRIMARY_CAMPUS_USE_NAME\n",
							",PRIMARY_CAMPUS_USE_DESCR\n",
							",CAMPUS_OWNERSHIP\n",
							",CAMPUS_ACTIVATION_YEAR\n",
							",CAMPUS_INACTIVATION_YEAR\n",
							",   'Y' AS CURRENT_IND\n",
							",   '{}' AS EXTRACT_DT\n",
							",   '{}' AS REC_START_DT\n",
							",   '9999-12-31 00:00:00' as REC_END_DT\n",
							",   'Tririga' AS SOURCE_SYSTEM\n",
							",    column_hash as REC_CHECKSUM\n",
							",    CURRENT_TIMESTAMP as IMG_LST_UPD_DT\n",
							",    existing_IMG_CREATED_DT\n",
							"from {} A \n",
							"WHERE LOWER({}) = LOWER(existing_{}) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"e=spark.sql(qry_ins_changed_rows.format(tablename,extract_dt,rec_start_dt,stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Soft Deletes - Update Active_Ind for the rows no longer active in source:\n",
							"\n",
							"qry_4_upd_deleted_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' --AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"    ,REC_END_DT= '{}' -  INTERVAL 1 seconds\n",
							"\n",
							";\n",
							"\"\"\"\n",
							"g=spark.sql(qry_4_upd_deleted_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key,rec_start_dt))"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} WHERE CURRENT_IND='Y' GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							""
						],
						"outputs": [],
						"execution_count": 13
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_dht_deal_dim_load')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPool001494New",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "472g",
					"driverCores": 80,
					"executorMemory": "472g",
					"executorCores": 80,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "690f57ba-81fd-46bc-9bf0-634bf616bb99"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPool001494New",
						"name": "sPool001494New",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPool001494New",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 80,
						"memory": 472,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Sales Stage Dimension load**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubconfirmed.dht_market by sourceing data from DCA Kyndryl.\n",
							"Since DCA is not accessible from Azure, a datastage job src_DCA_DEAL_Azure is executed on datastage to push the DCA extract to adls.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws,trim\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.functions import col\n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"import os\n",
							"import sys\n",
							"from datetime import datetime"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/DCA/' # fill in your relative folder path \n",
							"relative_archive_path='archive/'\n",
							"file_name='DS_DEALS_DATA.csv' \n",
							"natural_key=\"DEAL_IDENTIFIER\"\n",
							"tablename=\"etlhubConfirmed.dht_deal_dim1\"\n",
							"stagingtable=\"DHTS_DEAL_DIM\"\n",
							"keycolumn=\"DEAL_KEY\"\n",
							"date = datetime.now().strftime(\"%Y-%m-%d-%I:%M:%S_%p\")\n",
							"print(date)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Copy source CSV file to archive folder including timestamp in file name"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/DCA/DS_DEALS_DATA.csv"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"adls_arch_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_archive_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"relative_archive_path\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"arch_fil_name=file_name.rsplit(\".\")\n",
							"csv_archive_path = adls_arch_path + arch_fil_name[0] + '_' + date + '.' + arch_fil_name[1]\n",
							"print (csv_archive_path)\n",
							"incrementalData_DF = spark.read.load(csv_path,format=\"csv\", sep=\"||\", header=\"true\")\n",
							"incrementalData_DF.createOrReplaceTempView('DF_req')\n",
							"#incrementalData_DF.printSchema()\n",
							"incrementalData_Srcreq_DF=spark.sql(\"select DEAL_IDENTIFIER,BID_SUPPORT_INET_ID,BID_SUPPORT_NOTES_ID,BUS_UNIT_ID,CLNT_NM,CLNT_NUM,COMPLEXITY,COMNTS_TXT,CMPLXTY_CMMNT_TXT,CMPLXTY_OVERRD,CNTRCT_TYP_DESC,ENGG_SPPRT_TXN_LINK_TXT,ENGG_SPPRT_TXN_NUM,INDSTRY_NM,INIT_SIEBEL_IND,CNTRY_ID,LEAD_ACCT_PRTNR_BTT_CD,LEAD_PRTNR_BTT_CD,LEAD_PRTNR_NOTES_ID,OLD_OWNR_INET_ID,OWNR_INET_ID,PROJ_DESC,OPPTNY_ID,OWNR_NOTES_ID,PARENT_DEAL_ID,PRPSL_MGR_NOTES_ID,REAS_TXT,RMVD_IND,RISK_RTNG,RISK_RTNG_CMNT_TXT,RSK_RTNG_OVERRD,SALES_STAGE_CD,SECONDARY_SERVICE,SCTR_NM,SVC_NM,SIEBEL_IND,SRC_CD,STAT_CD,RCA1_ANSWR_TXT,RCA1_ANSWR_CMMNT,RCA2_ANSWR_TXT,RCA2_ANSWR_CMMNT,RCA3_ANSWR_TXT,RCA3_ANSWR_CMMNT,RCA4_ANSWR_TXT,RCA4_ANSWR_CMMNT,RCA5_ANSWR_TXT,RCA5_ANSWR_CMMNT,RCA6_ANSWR_TXT,RCA6_ANSWR_CMMNT,RCA7_ANSWR_TXT,RCA7_ANSWR_CMMNT,RCA8_ANSWR_TXT,RCA8_ANSWR_CMMNT,APPRVR_CNT,RVWR_CNT,DEAL_TYPE,GEO_CD,HYBRD_TYPE_A,HYBRD_TYPE_B,LOAD_LOB,CRDT_RTNG,DD_OFFERG_CS_USD_AMT,DD_OFFERG_IS_USD_AMT,DD_OFFERG_SS_USD_AMT,DD_OFFERG_WCP_USD_AMT,RISK_CONSULTANT_NOTES_ID,SENIOR_SOLUTION_MANAGER_NOTES_ID,SIH_GM_NOTES_ID,RULES_EFFECTIVE_DATE,DEAL_LEAD_NOTES_ID,CNTRCT_SIGNOFF_APPRVL_STAT_CD,PAYBACK_TXT from DF_req\")\n",
							"#incrementalData_Srcreq_DF.show()\n",
							"changedTypedf = incrementalData_Srcreq_DF.withColumn(natural_key, incrementalData_Srcreq_DF[natural_key].cast(StringType()))\n",
							"#changedTypedf.printSchema()\n",
							"\n",
							"\n",
							"#incrementalData_DF.show()\n",
							"incrementalData_DF.write.csv(csv_archive_path, header = 'true')\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"print(col_list)\n",
							"incrementalData_SDF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_SDF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Create a dataframe with target deltalake table data with necessary columns\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT DEAL_KEY,DEAL_VERSION_NUMBER,trim(DEAL_IDENTIFIER) as DEAL_ID,REC_CHECKSUM,IMG_CREATED_DT,REC_START_DT FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1, col(natural_key) == col(\"existing_\" + natural_key), \"fullouter\")\n",
							"\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"fullJoin2.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .option(\"overwriteSchema\", \"true\") \\\n",
							"  .option(\"path\", \"abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/staging/{}\".format(stagingtable)) \\\n",
							"  .saveAsTable(stagingtable)\n",
							"   #.option(\"overwriteSchema\", \"true\") \\  .save(\"\") \\"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Data Validation Checks :\n",
							"\n",
							"- Check for duplicates in Source data based on natural key\n",
							"- Check for duplicates in target delta lake table based on natural key\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} WHERE CURRENT_IND='Y' GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"df3=spark.sql(qry3.format(natural_key,natural_key))\n",
							"cnt1=df3.count()\n",
							"\n",
							"print (cnt1)\n",
							"if cnt1 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in source:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(1)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry_4_recon1=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable,tablename,natural_key,natural_key))\n",
							"print('New records or Inserts are:') \n",
							"df_4_recon1.show()\n",
							"\n",
							"qry_4_recon2=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE LOWER({}) = LOWER(existing_{}) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon2=spark.sql(qry_4_recon2.format(stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))\n",
							"print('Changed records or Updates are:') \n",
							"df_4_recon2.show()\n",
							"\n",
							"qry_4_recon3=\"\"\"\n",
							"SELECT COUNT(*) from \n",
							"{} A\n",
							"join fullJoin B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"\n",
							"\"\"\"\n",
							"\n",
							"df_4_recon3=spark.sql(qry_4_recon3.format(tablename,natural_key,natural_key,natural_key))\n",
							"print('Soft Deletes are:') \n",
							"df_4_recon3.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT count(*) FROM etlhubconfirmed.dht_deal_dim1 where current_ind='Y'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select \r\n",
							"\t* \r\n",
							"    from dhts_deal_dim A \r\n",
							"WHERE existing_REC_CHECKSUM is null"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry_4_recon1=\"\"\"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable, tablename, natural_key, natural_key))\n",
							"\n",
							"qry_ins_new_rows=\"\"\"\n",
							"\n",
							"INSERT INTO {}\n",
							"\tselect \n",
							"\tCOALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS {},1 as SALES_STAGE_VERSION \n",
							"    ,SALES_STAGE_CD\n",
							"\t,SIEBEL_SALES_STAGE_NAME\n",
							"\t,SSM_STEP_NO\n",
							"\t,SSM_STEP_NAME\n",
							"\t,'Y' AS CURRENT_IND\n",
							"\t,CURRENT_TIMESTAMP AS EXTRACT_DT\n",
							"\t,CURRENT_TIMESTAMP AS REC_START_DT\n",
							"\t,'9999-12-31 00:00:00.000' as REC_END_DT\n",
							"\t,'ESA' AS SOURCE_SYSTEM\n",
							"\t,column_hash as REC_CHECKSUM\n",
							"\t,'I' as REC_STATUS\n",
							"\t,current_timestamp as IMG_LST_UPD_DT\n",
							"\t,CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							"\t,'ED' AS DATA_IND\n",
							"\t,'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"    from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"d=spark.sql(qry_ins_new_rows.format(tablename,keycolumn,stagingtable,tablename,natural_key, natural_key ))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"SELECT * from etlhubconfirmed.dht_sales_stage"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Update or expire the rows with existing instance of the changed rows:\n",
							"qry_4_upd_changes_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.{}\n",
							"AND LOWER(B.{}) = LOWER(B.existing_{}) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\"\"\"\n",
							"f=spark.sql(qry_4_upd_changes_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key,natural_key))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--existing data\n",
							"select 'existing data',count(*) from DHTS_SALES_STAGE where existing_CURRENT_IND='Y';\n",
							"  \n",
							"select 'rows for Update', count(*)\n",
							"from etlhubConfirmed.dht_sales_stage B\n",
							"join\n",
							"DHTS_SALES_STAGE A \n",
							"WHERE A.SALES_STAGE_CD = B.SALES_STAGE_CD\n",
							"and LOWER(A.SALES_STAGE_CD) = LOWER(A.existing_SALES_STAGE_CD) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND B.REC_START_DT=A.existing_REC_START_DT\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.DHT_SALES_STAGE B\n",
							"WHERE LOWER(A.SALES_STAGE_CD)=LOWER(B.SALES_STAGE_CD)\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"SELECT COUNT(*) FROM etlhubConfirmed.DHT_SALES_STAGE A\n",
							"JOIN DHTS_SALES_STAGE B\n",
							"ON A.SALES_STAGE_CD = B.SALES_STAGE_CD\n",
							"AND LOWER(B.SALES_STAGE_CD) = LOWER(B.existing_SALES_STAGE_CD) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							";"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Insert for changed rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry_ins_changed_rows=\"\"\"\n",
							"INSERT INTO {}\n",
							"\tselect \n",
							"    existing_SALES_STAGE_KEY\n",
							"    ,1 + existing_SALES_STAGE_VERSION as SALES_STAGE_VERSION\n",
							"    ,SALES_STAGE_CD\n",
							"\t,SIEBEL_SALES_STAGE_NAME\n",
							"\t,SSM_STEP_NO\n",
							"\t,SSM_STEP_NAME\n",
							"\t,'Y' AS CURRENT_IND\n",
							"\t,CURRENT_TIMESTAMP AS EXTRACT_DT\n",
							"\t,CURRENT_TIMESTAMP AS REC_START_DT\n",
							"\t,'9999-12-31 00:00:00.000' as REC_END_DT\n",
							"\t,'ESA' AS SOURCE_SYSTEM\t\n",
							"\t,column_hash as REC_CHECKSUM\n",
							"\t,'U' as REC_STATUS\n",
							"\t,current_timestamp as IMG_LST_UPD_DT\n",
							"\t,existing_IMG_CREATED_DT AS IMG_CREATED_DT\n",
							"\t,'ED' AS DATA_IND\n",
							"\t,'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from {} A \n",
							"WHERE LOWER({}) = LOWER(existing_{}) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"e=spark.sql(qry_ins_changed_rows.format(tablename,stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Soft Deletes - Update Active_Ind for the rows no longer active in source:\n",
							"\n",
							"qry_4_upd_deleted_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"\n",
							";\n",
							"\"\"\"\n",
							"g=spark.sql(qry_4_upd_deleted_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select 'Number of Active Rows' as Title,count(*) from etlhubconfirmed.dht_sales_stage where CURRENT_IND='Y'\n",
							";\n",
							"\n",
							"\n",
							"select 'Duplicate Rows based on surrogate key' as Title,sales_stage_key,count(*) from etlhubconfirmed.dht_sales_stage where CURRENT_IND='Y'\n",
							"    group by sales_stage_key\n",
							"    having count(*)>1\n",
							";\n",
							"\n",
							"select 'Duplicate Rows based on natural key' as Title,SALES_STAGE_CD,count(*) from etlhubconfirmed.dht_sales_stage where CURRENT_IND='Y'\n",
							"    group by SALES_STAGE_CD\n",
							"    having count(*)>1\n",
							";\n",
							"\n",
							"select 'New rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_sales_stage where CURRENT_IND='Y'\n",
							"    and date(extract_dt)=current_date and rec_status='I'\n",
							";\n",
							"\n",
							"select 'Changed rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_sales_stage where CURRENT_IND='Y'\n",
							"    and date(extract_dt)=current_date and rec_status='U'\n",
							";\n",
							"\n",
							"select 'Changed rows updated in this run' as Title,count(*) from etlhubconfirmed.dht_sales_stage where CURRENT_IND='N'\n",
							"    and date(REC_END_DT)=current_date --and rec_status='I'\n",
							";\n",
							"\n",
							"select 'Rows no longer active in source' as Title,count(*) from etlhubconfirmed.dht_sales_stage where CURRENT_IND='Y'\n",
							"    AND \n",
							"     date(REC_END_DT)=current_date --and rec_status='I'\n",
							";\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 49
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT COUNT(*) FROM etlhubconfirmed.dht_sales_stage\n",
							"WHERE CURRENT_IND='N' and DATE(REC_END_DT)<>'9999-12-31'\n",
							"    and date(img_lst_upd_dt)=current_date\n",
							"        limit 1\n",
							";\n",
							"\n",
							"\n",
							"select count(*) from etlhubconfirmed.dht_sales_stage where CURRENT_IND='Y'\n",
							"   and date(extract_dt)=current_date --and rec_status='I'\n",
							";\n",
							"\n",
							"select count(*) from etlhubconfirmed.dht_sales_stage\n",
							"WHERE ACTIVE_IN_SOURCE_IND='N'\n",
							"    and date(IMG_LST_UPD_DT)=current_date\n",
							"    ;\n",
							"UPDATE etlhubconfirmed.dht_sales_stage\n",
							"set CURRENT_IND='Y', \n",
							"REC_END_DT='9999-12-31'\n",
							"  WHERE CURRENT_IND='N' and DATE(REC_END_DT)<>'9999-12-31'\n",
							"    and date(img_lst_upd_dt)=current_date\n",
							"        ;  "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 50
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_dht_deal_dim_load_old')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "1300b974-68ab-45eb-b9d8-ef8847889aa3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**DEAL Dimension load**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubconfirme.dht_deal_dim by sourceing data from DCA deal data.\n",
							"Since DCA is not accessible from Azure, a datastage job src_DCA_DEAL_Azure is executed on datastage to push the DCA extract to adls.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"from pyspark.sql.types import *\n",
							"#import os\n",
							"import sys"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 205
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/DCA/' # fill in your relative folder path \n",
							"file_name='DS_DEALS_DATA_UPD4.csv' \n",
							"natural_key=\"DEAL_ID\"\n",
							"tablename=\"etlhubConfirmed.dht_deal_dim1\"\n",
							"keycolumn=\"DEAL_KEY\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 206
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/DCA/DS_DEALS_DATA.csv"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"#incrementalData_DF = spark.read.csv(csv_path, header = 'true',quote='\"')\n",
							"incrementalData_DF=spark.read.load(csv_path,format=\"csv\", sep=\"||\", inferSchema=\"true\", header=\"true\")\n",
							"incrementalData_DF.createOrReplaceTempView('DF_req')\n",
							"#incrementalData_DF.printSchema()\n",
							"incrementalData_Srcreq_DF=spark.sql(\"select DEAL_ID,BID_SUPPORT_INET_ID,BID_SUPPORT_NOTES_ID,BUS_UNIT_ID,CLNT_NM,CLNT_NUM,COMPLEXITY,COMNTS_TXT,CMPLXTY_CMMNT_TXT,CMPLXTY_OVERRD,CNTRCT_TYP_DESC,ENGG_SPPRT_TXN_LINK_TXT,ENGG_SPPRT_TXN_NUM,INDSTRY_NM,INIT_SIEBEL_IND,CNTRY_ID,LEAD_ACCT_PRTNR_BTT_CD,LEAD_PRTNR_BTT_CD,LEAD_PRTNR_NOTES_ID,OLD_OWNR_INET_ID,OWNR_INET_ID,PROJ_DESC,OPPTNY_ID,OWNR_NOTES_ID,PARENT_DEAL_ID,PRPSL_MGR_NOTES_ID,REAS_TXT,RMVD_IND,RISK_RTNG,RISK_RTNG_CMNT_TXT,RSK_RTNG_OVERRD,SALES_STAGE_CD,SECONDARY_SERVICE,SCTR_NM,SVC_NM,SIEBEL_IND,SRC_CD,STAT_CD,RCA1_ANSWR_TXT,RCA1_ANSWR_CMMNT,RCA2_ANSWR_TXT,RCA2_ANSWR_CMMNT,RCA3_ANSWR_TXT,RCA3_ANSWR_CMMNT,RCA4_ANSWR_TXT,RCA4_ANSWR_CMMNT,RCA5_ANSWR_TXT,RCA5_ANSWR_CMMNT,RCA6_ANSWR_TXT,RCA6_ANSWR_CMMNT,RCA7_ANSWR_TXT,RCA7_ANSWR_CMMNT,RCA8_ANSWR_TXT,RCA8_ANSWR_CMMNT,APPRVR_CNT,RVWR_CNT,DEAL_TYPE,GEO_CD,HYBRD_TYPE_A,HYBRD_TYPE_B,LOAD_LOB,CRDT_RTNG,DD_OFFERG_CS_USD_AMT,DD_OFFERG_IS_USD_AMT,DD_OFFERG_SS_USD_AMT,DD_OFFERG_WCP_USD_AMT,RISK_CONSULTANT_NOTES_ID,SENIOR_SOLUTION_MANAGER_NOTES_ID,SIH_GM_NOTES_ID,RULES_EFFECTIVE_DATE,DEAL_LEAD_NOTES_ID,CNTRCT_SIGNOFF_APPRVL_STAT_CD,PAYBACK_TXT from DF_req\")\n",
							"#incrementalData_Srcreq_DF.show()\n",
							"changedTypedf = incrementalData_Srcreq_DF.withColumn(natural_key, incrementalData_Srcreq_DF[natural_key].cast(StringType()))\n",
							"#changedTypedf.printSchema()\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"col_list=[]\n",
							"for i in incrementalData_Srcreq_DF.columns:\n",
							"    col_list.append(i)\n",
							"    #print (col_list)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1=changedTypedf.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"#incrementalData_DF2.printSchema()\n",
							"incrementalData_DF2.createOrReplaceTempView('incrementalData_SrcDF')\n",
							"\n",
							"#Create a deltalake table with necessary columns\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT DEAL_KEY,DEAL_VERSION_NUMBER,trim(DEAL_IDENTIFIER) as DEAL_ID,REC_CHECKSUM,IMG_CREATED_DT,REC_START_DT FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\n",
							"existingDataDF.createOrReplaceTempView('existingDF')\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"#existingDataDF1.printSchema()\n",
							"#incrementalData_DF2.printSchema()\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataTgtDF')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.DEAL_ID == existingDataDF1.existing_DEAL_ID, \"fullouter\") \n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 207
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Data Validation Checks :\n",
							"\n",
							"- Check for duplicates in Source data based on natural key\n",
							"- Check for duplicates in target delta lake table based on natural key\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\r\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_SrcDF GROUP BY {} HAVING COUNT(*)>1\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"df3=spark.sql(qry3.format(natural_key,natural_key))\r\n",
							"cnt1=df3.count()\r\n",
							"\r\n",
							"print (cnt1)\r\n",
							"if cnt1 == 0:\r\n",
							"    print(\"No Duplicates in source data\")\r\n",
							"    status = 'success'\r\n",
							"else:\r\n",
							"    print(\"Below are the duplicates in source:\")\r\n",
							"    df3.show()\r\n",
							"    status = 'fail'\r\n",
							"    #os.abort() this will take the spark cluster also down\r\n",
							"    sys.exit(1)\r\n",
							"    print(\"This will not be printed\")\r\n",
							"\r\n",
							"qry4=\"\"\"\r\n",
							"SELECT COUNT(*) as CNT, {} FROM existingDataTgtDF GROUP BY {} HAVING COUNT(*)>1;\r\n",
							"\"\"\"\r\n",
							"df4=spark.sql(qry4.format('existing_'+natural_key,'existing_'+natural_key))\r\n",
							"cnt2=df4.count()\r\n",
							"print (cnt2)\r\n",
							"if cnt2 == 0:\r\n",
							"    print(\"no duplicates in target delta lake table\")\r\n",
							"    status = 'success'\r\n",
							"else:\r\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\r\n",
							"    df4.show()\r\n",
							"    status = 'fail'\r\n",
							"    #os.abort() this will take the spark cluster also down\r\n",
							"    sys.exit(2)\r\n",
							"    print(\"This will not be printed\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 208
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\r\n",
							"#delete FROM etlhubconfirmed.dht_employee;\r\n",
							"qry_insert=\"\"\"\r\n",
							"INSERT INTO etlhubConfirmed.dht_deal_dim1\r\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS DEAL_KEY,1 as DEAL_VERSION_NUMBER , \r\n",
							"A.DEAL_ID,\r\n",
							"A.BID_SUPPORT_INET_ID,\r\n",
							"A.BID_SUPPORT_NOTES_ID,\r\n",
							"A.BUS_UNIT_ID,\r\n",
							"A.CLNT_NM,\r\n",
							"A.CLNT_NUM,\r\n",
							"REPLACE(A.COMPLEXITY,'\"','') as COMPLEXITY,\r\n",
							"REPLACE(A.COMNTS_TXT,'\"','') as COMNTS_TXT,\r\n",
							"REPLACE(A.CMPLXTY_CMMNT_TXT,'\"','') as CMPLXTY_CMMNT_TXT,\r\n",
							"REPLACE(A.CMPLXTY_OVERRD,'\"','') as CMPLXTY_OVERRD,\r\n",
							"REPLACE(A.CNTRCT_TYP_DESC,'\"','') as CNTRCT_TYP_DESC,\r\n",
							"REPLACE(A.ENGG_SPPRT_TXN_LINK_TXT,'\"','') as ENGG_SPPRT_TXN_LINK_TXT,\r\n",
							"REPLACE(A.ENGG_SPPRT_TXN_NUM,'\"','') as ENGG_SPPRT_TXN_NUM,\r\n",
							"REPLACE(A.INDSTRY_NM,'\"','') as INDSTRY_NM,\r\n",
							"REPLACE(A.INIT_SIEBEL_IND,'\"','') as INIT_SIEBEL_IND,\r\n",
							"REPLACE(A.CNTRY_ID,'\"','') as CNTRY_ID,\r\n",
							"REPLACE(A.LEAD_ACCT_PRTNR_BTT_CD,'\"','') as LEAD_ACCT_PRTNR_BTT_CD,\r\n",
							"REPLACE(A.LEAD_PRTNR_BTT_CD,'\"','') as LEAD_PRTNR_BTT_CD,\r\n",
							"REPLACE(A.LEAD_PRTNR_NOTES_ID,'\"','') as LEAD_PRTNR_NOTES_ID,\r\n",
							"REPLACE(A.OLD_OWNR_INET_ID,'\"','') as OLD_OWNR_INET_ID,\r\n",
							"REPLACE(A.OWNR_INET_ID,'\"','') as OWNR_INET_ID,\r\n",
							"REPLACE(A.proj_desc,'\"','') as PROJ_DESC,\r\n",
							"REPLACE(A.OPPTNY_ID,'\"','') as OPPTNY_ID,\r\n",
							"REPLACE(A.OWNR_NOTES_ID,'\"','') as OWNR_NOTES_ID,\r\n",
							"REPLACE(A.PARENT_DEAL_ID,'\"','') as PARENT_DEAL_ID,\r\n",
							"REPLACE(A.PRPSL_MGR_NOTES_ID,'\"','') as PRPSL_MGR_NOTES_ID,\r\n",
							"REPLACE(A.REAS_TXT,'\"','') as REAS_TXT,\r\n",
							"REPLACE(A.RMVD_IND,'\"','') as RMVD_IND,\r\n",
							"REPLACE(A.RISK_RTNG,'\"','') as RISK_RTNG,\r\n",
							"REPLACE(A.RISK_RTNG_CMNT_TXT,'\"','') as RISK_RTNG_CMNT_TXT,\r\n",
							"REPLACE(A.RSK_RTNG_OVERRD,'\"','') as RSK_RTNG_OVERRD,\r\n",
							"REPLACE(A.SALES_STAGE_CD,'\"','') as SALES_STAGE_CD,\r\n",
							"REPLACE(A.SECONDARY_SERVICE,'\"','') as SECONDARY_SERVICE,\r\n",
							"REPLACE(A.SCTR_NM,'\"','') as SCTR_NM,\r\n",
							"REPLACE(A.SVC_NM,'\"','') as SVC_NM,\r\n",
							"REPLACE(A.SIEBEL_IND,'\"','') as SIEBEL_IND,\r\n",
							"REPLACE(A.SRC_CD,'\"','') as SRC_CD,\r\n",
							"REPLACE(A.STAT_CD,'\"','') as STAT_CD,\r\n",
							"'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT,current_timestamp as IMG_LST_UPD_DT,\r\n",
							"'9999-12-31 00:00:00.000' as REC_END_DT,CURRENT_TIMESTAMP AS REC_START_DT,  A.column_hash as REC_CHECKSUM,'I' as REC_STATUS, 'DCA' AS SOURCE_SYSTEM, \r\n",
							"REPLACE(A.RCA1_ANSWR_TXT,'\"','') as RCA1_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA1_ANSWR_CMMNT,'\"','') as RCA1_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA2_ANSWR_TXT,'\"','') as RCA2_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA2_ANSWR_CMMNT,'\"','') as RCA2_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA3_ANSWR_TXT,'\"','') as RCA3_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA3_ANSWR_CMMNT,'\"','') as RCA3_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA4_ANSWR_TXT,'\"','') as RCA4_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA4_ANSWR_CMMNT,'\"','') as RCA4_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA5_ANSWR_TXT,'\"','') as RCA5_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA5_ANSWR_CMMNT,'\"','') as RCA5_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA6_ANSWR_TXT,'\"','') as RCA6_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA6_ANSWR_CMMNT,'\"','') as RCA6_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA7_ANSWR_TXT,'\"','') as RCA7_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA7_ANSWR_CMMNT,'\"','') as RCA7_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA8_ANSWR_TXT,'\"','') as RCA8_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA8_ANSWR_CMMNT,'\"','') as RCA8_ANSWR_CMMNT,\r\n",
							"REPLACE(A.APPRVR_CNT,'\"','') as APPRVR_CNT,\r\n",
							"REPLACE(A.RVWR_CNT,'\"','') as RVWR_CNT,\r\n",
							"REPLACE(A.DEAL_TYPE,'\"','') as DEAL_TYPE,\r\n",
							"REPLACE(A.GEO_CD,'\"','') as GEO_CD,\r\n",
							"REPLACE(A.HYBRD_TYPE_A,'\"','') as HYBRD_TYPE_A,\r\n",
							"REPLACE(A.HYBRD_TYPE_B,'\"','') as HYBRD_TYPE_B,\r\n",
							"REPLACE(A.LOAD_LOB,'\"','') as LOAD_LOB,\r\n",
							"REPLACE(A.CRDT_RTNG,'\"','') as CRDT_RTNG,\r\n",
							"A.DD_OFFERG_CS_USD_AMT,\r\n",
							"A.DD_OFFERG_IS_USD_AMT,\r\n",
							"A.DD_OFFERG_SS_USD_AMT,\r\n",
							"A.DD_OFFERG_WCP_USD_AMT,\r\n",
							"REPLACE(A.RISK_CONSULTANT_NOTES_ID,'\"','') as RISK_CONSULTANT_NOTES_ID,\r\n",
							"REPLACE(A.SENIOR_SOLUTION_MANAGER_NOTES_ID,'\"','') as SENIOR_SOLUTION_MANAGER_NOTES_ID,\r\n",
							"REPLACE(A.SIH_GM_NOTES_ID,'\"','') as SIH_GM_NOTES_ID,\r\n",
							"A.RULES_EFFECTIVE_DATE,\r\n",
							"REPLACE(A.DEAL_LEAD_NOTES_ID,'\"','') as DEAL_LEAD_NOTES_ID,\r\n",
							"REPLACE(A.CNTRCT_SIGNOFF_APPRVL_STAT_CD,'\"','') as CNTRCT_SIGNOFF_APPRVL_STAT_CD,\r\n",
							"REPLACE(A.PAYBACK_TXT,'\"','') as PAYBACK_TXT,\r\n",
							"'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"LEFT JOIN etlhubConfirmed.dht_deal_dim1 B\r\n",
							"ON TRIM(A.DEAL_ID)=TRIM(B.DEAL_IDENTIFIER)\r\n",
							"AND CURRENT_IND='Y'\r\n",
							"WHERE existing_REC_CHECKSUM is null\r\n",
							"AND COALESCE(B.REC_CHECKSUM,'') <> COALESCE(A.column_hash,'');\r\n",
							"\"\"\"\r\n",
							"d=spark.sql(qry_insert)\r\n",
							"#d.show(10)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 209
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#update the existing rows in target table with new values and record status as U and expiry the existing records with current indicator as N.\r\n",
							"#delete FROM etlhubconfirmed.dht_employee;\r\n",
							"\r\n",
							"updins=\"\"\"\r\n",
							"INSERT INTO etlhubConfirmed.dht_deal_dim1 \r\n",
							"select existing_DEAL_KEY,1+existing_DEAL_VERSION_NUMBER as DEAL_VERSION_NUMBER , \r\n",
							"A.DEAL_ID,\r\n",
							"A.BID_SUPPORT_INET_ID,\r\n",
							"A.BID_SUPPORT_NOTES_ID,\r\n",
							"A.BUS_UNIT_ID,\r\n",
							"A.CLNT_NM,\r\n",
							"A.CLNT_NUM,\r\n",
							"A.COMPLEXITY,\r\n",
							"A.COMNTS_TXT,\r\n",
							"A.CMPLXTY_CMMNT_TXT,\r\n",
							"A.CMPLXTY_OVERRD,\r\n",
							"A.CNTRCT_TYP_DESC,\r\n",
							"A.ENGG_SPPRT_TXN_LINK_TXT,\r\n",
							"A.ENGG_SPPRT_TXN_NUM,\r\n",
							"A.INDSTRY_NM,\r\n",
							"A.INIT_SIEBEL_IND,\r\n",
							"A.CNTRY_ID,\r\n",
							"A.LEAD_ACCT_PRTNR_BTT_CD,\r\n",
							"A.LEAD_PRTNR_BTT_CD,\r\n",
							"A.LEAD_PRTNR_NOTES_ID,\r\n",
							"A.OLD_OWNR_INET_ID,\r\n",
							"A.OWNR_INET_ID,\r\n",
							"A.proj_desc,\r\n",
							"A.OPPTNY_ID,\r\n",
							"A.OWNR_NOTES_ID,\r\n",
							"A.PARENT_DEAL_ID,\r\n",
							"A.PRPSL_MGR_NOTES_ID,\r\n",
							"A.REAS_TXT,\r\n",
							"A.RMVD_IND,\r\n",
							"A.RISK_RTNG,\r\n",
							"A.RISK_RTNG_CMNT_TXT,\r\n",
							"A.RSK_RTNG_OVERRD,\r\n",
							"A.SALES_STAGE_CD,\r\n",
							"A.SECONDARY_SERVICE,\r\n",
							"A.SCTR_NM,\r\n",
							"A.SVC_NM,\r\n",
							"A.SIEBEL_IND,\r\n",
							"A.SRC_CD,\r\n",
							"A.STAT_CD,\r\n",
							"'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT,current_timestamp as IMG_LST_UPD_DT,\r\n",
							"'9999-12-31 00:00:00.000' as REC_END_DT,CURRENT_TIMESTAMP AS REC_START_DT,  A.column_hash as REC_CHECKSUM, \r\n",
							"'U' as REC_STATUS, 'DCA' AS SOURCE_SYSTEM,\r\n",
							"A.RCA1_ANSWR_TXT,\r\n",
							"A.RCA1_ANSWR_CMMNT,\r\n",
							"A.RCA2_ANSWR_TXT,\r\n",
							"A.RCA2_ANSWR_CMMNT, \r\n",
							"A.RCA3_ANSWR_TXT,\r\n",
							"A.RCA3_ANSWR_CMMNT,\r\n",
							"A.RCA4_ANSWR_TXT,\r\n",
							"A.RCA4_ANSWR_CMMNT,\r\n",
							"A.RCA5_ANSWR_TXT,\r\n",
							"A.RCA5_ANSWR_CMMNT,\r\n",
							"A.RCA6_ANSWR_TXT,\r\n",
							"A.RCA6_ANSWR_CMMNT,\r\n",
							"A.RCA7_ANSWR_TXT,\r\n",
							"A.RCA7_ANSWR_CMMNT,\r\n",
							"A.RCA8_ANSWR_TXT,\r\n",
							"A.RCA8_ANSWR_CMMNT,\r\n",
							"A.APPRVR_CNT,\r\n",
							"A.RVWR_CNT,\r\n",
							"A.DEAL_TYPE,\r\n",
							"A.GEO_CD,\r\n",
							"A.HYBRD_TYPE_A,\r\n",
							"A.HYBRD_TYPE_B,\r\n",
							"A.LOAD_LOB,\r\n",
							"A.CRDT_RTNG,\r\n",
							"A.DD_OFFERG_CS_USD_AMT,\r\n",
							"A.DD_OFFERG_IS_USD_AMT,\r\n",
							"A.DD_OFFERG_SS_USD_AMT,\r\n",
							"A.DD_OFFERG_WCP_USD_AMT,\r\n",
							"A.RISK_CONSULTANT_NOTES_ID,\r\n",
							"A.SENIOR_SOLUTION_MANAGER_NOTES_ID,\r\n",
							"A.SIH_GM_NOTES_ID,\r\n",
							"A.RULES_EFFECTIVE_DATE,\r\n",
							"A.DEAL_LEAD_NOTES_ID,\r\n",
							"A.CNTRCT_SIGNOFF_APPRVL_STAT_CD,\r\n",
							"A.PAYBACK_TXT,\r\n",
							"'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"WHERE DEAL_ID = existing_DEAL_ID and LOWER(column_hash) <> LOWER(existing_REC_CHECKSUM)\r\n",
							"AND NOT EXISTS\r\n",
							"(SELECT 1 FROM etlhubConfirmed.dht_deal_dim1 B\r\n",
							"WHERE TRIM(A.DEAL_ID)=TRIM(B.DEAL_IDENTIFIER)\r\n",
							"and b.CURRENT_IND='Y'\r\n",
							"AND coalesce(A.column_hash,'')=coalesce(B.REC_CHECKSUM,'')) ;\r\n",
							"\"\"\"\r\n",
							"d1=spark.sql(updins)\r\n",
							"#d1.show()\r\n",
							"\r\n",
							"expqry=\"\"\"\r\n",
							"MERGE INTO etlhubConfirmed.dht_deal_dim1 A\r\n",
							"USING fullJoin B\r\n",
							"ON B.DEAL_ID = A.DEAL_IDENTIFIER\r\n",
							"AND LOWER(A.DEAL_IDENTIFIER) = LOWER(B.existing_DEAL_ID) and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\r\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\r\n",
							",REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds\r\n",
							",IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"d2=spark.sql(expqry)\r\n",
							"#d2.show()\r\n",
							"\r\n",
							"#Soft deletes or no longer active in source\r\n",
							"softdelqry=\"\"\"\r\n",
							"MERGE INTO etlhubConfirmed.dht_deal_dim1  A\r\n",
							"USING fullJoin B\r\n",
							"ON A.DEAL_IDENTIFIER = B.existing_DEAL_ID\r\n",
							"AND B.DEAL_ID is NULL\r\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N' ,IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"df3=spark.sql(softdelqry)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 210
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from etlhubConfirmed.dht_deal_dim1 where DEAL_IDENTIFIER in (7840900,7840899)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 211
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_dht_deal_dim_load_org')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "79cdc02e-5f5d-42be-8bea-d86197745107"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**DEAL Dimension load**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubconfirme.dht_deal_dim by sourceing data from DCA deal data.\n",
							"Since DCA is not accessible from Azure, a datastage job src_DCA_DEAL_Azure is executed on datastage to push the DCA extract to adls.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"from pyspark.sql.types import *\n",
							"#import os\n",
							"import sys"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/DCA/' # fill in your relative folder path \n",
							"file_name='DS_DEALS_DATA.csv' \n",
							"natural_key=\"DEAL_ID\"\n",
							"tgt_natural_key=\"DEAL_IDENTIFIER\"\n",
							"tablename=\"etlhubConfirmed.dht_deal_dim\"\n",
							"keycolumn=\"DEAL_KEY\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/DCA/DS_DEALS_DATA.csv"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"#incrementalData_DF = spark.read.csv(csv_path, header = 'true',quote='\"')\n",
							"incrementalData_DF=spark.read.load(csv_path,format=\"csv\", sep=\"~^\", inferSchema=\"true\", header=\"true\")\n",
							"incrementalData_DF.createOrReplaceTempView('DF_req')\n",
							"#incrementalData_DF.printSchema()\n",
							"incrementalData_Srcreq_DF=spark.sql(\"select DEAL_ID,BID_SUPPORT_INET_ID,BID_SUPPORT_NOTES_ID,BUS_UNIT_ID,CLNT_NM,CLNT_NUM,COMPLEXITY,COMNTS_TXT,CMPLXTY_CMMNT_TXT,CMPLXTY_OVERRD,CNTRCT_TYP_DESC,ENGG_SPPRT_TXN_LINK_TXT,ENGG_SPPRT_TXN_NUM,INDSTRY_NM,INIT_SIEBEL_IND,CNTRY_ID,LEAD_ACCT_PRTNR_BTT_CD,LEAD_PRTNR_BTT_CD,LEAD_PRTNR_NOTES_ID,OLD_OWNR_INET_ID,OWNR_INET_ID,PROJ_DESC,OPPTNY_ID,OWNR_NOTES_ID,PARENT_DEAL_ID,PRPSL_MGR_NOTES_ID,REAS_TXT,RMVD_IND,RISK_RTNG,RISK_RTNG_CMNT_TXT,RSK_RTNG_OVERRD,SALES_STAGE_CD,SECONDARY_SERVICE,SCTR_NM,SVC_NM,SIEBEL_IND,SRC_CD,STAT_CD,RCA1_ANSWR_TXT,RCA1_ANSWR_CMMNT,RCA2_ANSWR_TXT,RCA2_ANSWR_CMMNT,RCA3_ANSWR_TXT,RCA3_ANSWR_CMMNT,RCA4_ANSWR_TXT,RCA4_ANSWR_CMMNT,RCA5_ANSWR_TXT,RCA5_ANSWR_CMMNT,RCA6_ANSWR_TXT,RCA6_ANSWR_CMMNT,RCA7_ANSWR_TXT,RCA7_ANSWR_CMMNT,RCA8_ANSWR_TXT,RCA8_ANSWR_CMMNT,APPRVR_CNT,RVWR_CNT,DEAL_TYPE,GEO_CD,HYBRD_TYPE_A,HYBRD_TYPE_B,LOAD_LOB,CRDT_RTNG,DD_OFFERG_CS_USD_AMT,DD_OFFERG_IS_USD_AMT,DD_OFFERG_SS_USD_AMT,DD_OFFERG_WCP_USD_AMT,RISK_CONSULTANT_NOTES_ID,SENIOR_SOLUTION_MANAGER_NOTES_ID,SIH_GM_NOTES_ID,RULES_EFFECTIVE_DATE,DEAL_LEAD_NOTES_ID,CNTRCT_SIGNOFF_APPRVL_STAT_CD,PAYBACK_TXT from DF_req\")\n",
							"#incrementalData_Srcreq_DF.show()\n",
							"changedTypedf = incrementalData_Srcreq_DF.withColumn(natural_key, incrementalData_Srcreq_DF[natural_key].cast(StringType()))\n",
							"#changedTypedf.printSchema()\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"col_list=[]\n",
							"for i in incrementalData_Srcreq_DF.columns:\n",
							"    col_list.append(i)\n",
							"    #print (col_list)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1=changedTypedf.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"#incrementalData_DF2.printSchema()\n",
							"incrementalData_DF2.createOrReplaceTempView('incrementalData_SrcDF')\n",
							"\n",
							"#Create a deltalake table with necessary columns\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT DEAL_KEY,DEAL_VERSION_NUMBER,trim(DEAL_IDENTIFIER) as DEAL_ID,REC_CHECKSUM,IMG_CREATED_DT,REC_START_DT FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\n",
							"existingDataDF.createOrReplaceTempView('existingDF')\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"#existingDataDF1.printSchema()\n",
							"#incrementalData_DF2.printSchema()\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataTgtDF')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.DEAL_ID == existingDataDF1.existing_DEAL_ID, \"fullouter\") \n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Data Validation Checks :\n",
							"\n",
							"- Check for duplicates in Source data based on natural key\n",
							"- Check for duplicates in target delta lake table based on natural key\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\r\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_SrcDF GROUP BY {} HAVING COUNT(*)>1\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"df3=spark.sql(qry3.format(natural_key,natural_key))\r\n",
							"cnt1=df3.count()\r\n",
							"\r\n",
							"print (cnt1)\r\n",
							"if cnt1 == 0:\r\n",
							"    print(\"No Duplicates in source data\")\r\n",
							"    status = 'success'\r\n",
							"else:\r\n",
							"    print(\"Below are the duplicates in source:\")\r\n",
							"    df3.show()\r\n",
							"    status = 'fail'\r\n",
							"    #os.abort() this will take the spark cluster also down\r\n",
							"    sys.exit(1)\r\n",
							"    print(\"This will not be printed\")\r\n",
							"\r\n",
							"qry4=\"\"\"\r\n",
							"SELECT COUNT(*) as CNT, {} FROM {} where CURRENT_IND= \"Y\" GROUP BY {} HAVING COUNT(*)>1;\r\n",
							"\"\"\"\r\n",
							"df4=spark.sql(qry4.format(tgt_natural_key,tablename,tgt_natural_key))\r\n",
							"cnt2=df4.count()\r\n",
							"print (cnt2)\r\n",
							"if cnt2 == 0:\r\n",
							"    print(\"no duplicates in target delta lake table\")\r\n",
							"    status = 'success'\r\n",
							"else:\r\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\r\n",
							"    df4.show()\r\n",
							"    status = 'fail'\r\n",
							"    #os.abort() this will take the spark cluster also down\r\n",
							"    sys.exit(2)\r\n",
							"    print(\"This will not be printed\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\r\n",
							"qry_insert=\"\"\"\r\n",
							"INSERT INTO etlhubConfirmed.dht_deal_dim\r\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS DEAL_KEY,1 as DEAL_VERSION_NUMBER , \r\n",
							"A.DEAL_ID,\r\n",
							"A.BID_SUPPORT_INET_ID,\r\n",
							"A.BID_SUPPORT_NOTES_ID,\r\n",
							"A.BUS_UNIT_ID,\r\n",
							"A.CLNT_NM,\r\n",
							"A.CLNT_NUM,\r\n",
							"REPLACE(A.COMPLEXITY,'\"','') as COMPLEXITY,\r\n",
							"REPLACE(A.COMNTS_TXT,'\"','') as COMNTS_TXT,\r\n",
							"REPLACE(A.CMPLXTY_CMMNT_TXT,'\"','') as CMPLXTY_CMMNT_TXT,\r\n",
							"REPLACE(A.CMPLXTY_OVERRD,'\"','') as CMPLXTY_OVERRD,\r\n",
							"REPLACE(A.CNTRCT_TYP_DESC,'\"','') as CNTRCT_TYP_DESC,\r\n",
							"REPLACE(A.ENGG_SPPRT_TXN_LINK_TXT,'\"','') as ENGG_SPPRT_TXN_LINK_TXT,\r\n",
							"REPLACE(A.ENGG_SPPRT_TXN_NUM,'\"','') as ENGG_SPPRT_TXN_NUM,\r\n",
							"REPLACE(A.INDSTRY_NM,'\"','') as INDSTRY_NM,\r\n",
							"REPLACE(A.INIT_SIEBEL_IND,'\"','') as INIT_SIEBEL_IND,\r\n",
							"REPLACE(A.CNTRY_ID,'\"','') as CNTRY_ID,\r\n",
							"REPLACE(A.LEAD_ACCT_PRTNR_BTT_CD,'\"','') as LEAD_ACCT_PRTNR_BTT_CD,\r\n",
							"REPLACE(A.LEAD_PRTNR_BTT_CD,'\"','') as LEAD_PRTNR_BTT_CD,\r\n",
							"REPLACE(A.LEAD_PRTNR_NOTES_ID,'\"','') as LEAD_PRTNR_NOTES_ID,\r\n",
							"REPLACE(A.OLD_OWNR_INET_ID,'\"','') as OLD_OWNR_INET_ID,\r\n",
							"REPLACE(A.OWNR_INET_ID,'\"','') as OWNR_INET_ID,\r\n",
							"REPLACE(A.proj_desc,'\"','') as PROJ_DESC,\r\n",
							"REPLACE(A.OPPTNY_ID,'\"','') as OPPTNY_ID,\r\n",
							"REPLACE(A.OWNR_NOTES_ID,'\"','') as OWNR_NOTES_ID,\r\n",
							"REPLACE(A.PARENT_DEAL_ID,'\"','') as PARENT_DEAL_ID,\r\n",
							"REPLACE(A.PRPSL_MGR_NOTES_ID,'\"','') as PRPSL_MGR_NOTES_ID,\r\n",
							"REPLACE(A.REAS_TXT,'\"','') as REAS_TXT,\r\n",
							"REPLACE(A.RMVD_IND,'\"','') as RMVD_IND,\r\n",
							"REPLACE(A.RISK_RTNG,'\"','') as RISK_RTNG,\r\n",
							"REPLACE(A.RISK_RTNG_CMNT_TXT,'\"','') as RISK_RTNG_CMNT_TXT,\r\n",
							"REPLACE(A.RSK_RTNG_OVERRD,'\"','') as RSK_RTNG_OVERRD,\r\n",
							"REPLACE(A.SALES_STAGE_CD,'\"','') as SALES_STAGE_CD,\r\n",
							"REPLACE(A.SECONDARY_SERVICE,'\"','') as SECONDARY_SERVICE,\r\n",
							"REPLACE(A.SCTR_NM,'\"','') as SCTR_NM,\r\n",
							"REPLACE(A.SVC_NM,'\"','') as SVC_NM,\r\n",
							"REPLACE(A.SIEBEL_IND,'\"','') as SIEBEL_IND,\r\n",
							"REPLACE(A.SRC_CD,'\"','') as SRC_CD,\r\n",
							"REPLACE(A.STAT_CD,'\"','') as STAT_CD,\r\n",
							"'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT,current_timestamp as IMG_LST_UPD_DT,\r\n",
							"'9999-12-31 00:00:00.000' as REC_END_DT,CURRENT_TIMESTAMP AS REC_START_DT,  A.column_hash as REC_CHECKSUM,'I' as REC_STATUS, 'DCA' AS SOURCE_SYSTEM, \r\n",
							"REPLACE(A.RCA1_ANSWR_TXT,'\"','') as RCA1_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA1_ANSWR_CMMNT,'\"','') as RCA1_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA2_ANSWR_TXT,'\"','') as RCA2_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA2_ANSWR_CMMNT,'\"','') as RCA2_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA3_ANSWR_TXT,'\"','') as RCA3_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA3_ANSWR_CMMNT,'\"','') as RCA3_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA4_ANSWR_TXT,'\"','') as RCA4_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA4_ANSWR_CMMNT,'\"','') as RCA4_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA5_ANSWR_TXT,'\"','') as RCA5_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA5_ANSWR_CMMNT,'\"','') as RCA5_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA6_ANSWR_TXT,'\"','') as RCA6_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA6_ANSWR_CMMNT,'\"','') as RCA6_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA7_ANSWR_TXT,'\"','') as RCA7_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA7_ANSWR_CMMNT,'\"','') as RCA7_ANSWR_CMMNT,\r\n",
							"REPLACE(A.RCA8_ANSWR_TXT,'\"','') as RCA8_ANSWR_TXT,\r\n",
							"REPLACE(A.RCA8_ANSWR_CMMNT,'\"','') as RCA8_ANSWR_CMMNT,\r\n",
							"REPLACE(A.APPRVR_CNT,'\"','') as APPRVR_CNT,\r\n",
							"REPLACE(A.RVWR_CNT,'\"','') as RVWR_CNT,\r\n",
							"REPLACE(A.DEAL_TYPE,'\"','') as DEAL_TYPE,\r\n",
							"REPLACE(A.GEO_CD,'\"','') as GEO_CD,\r\n",
							"REPLACE(A.HYBRD_TYPE_A,'\"','') as HYBRD_TYPE_A,\r\n",
							"REPLACE(A.HYBRD_TYPE_B,'\"','') as HYBRD_TYPE_B,\r\n",
							"REPLACE(A.LOAD_LOB,'\"','') as LOAD_LOB,\r\n",
							"REPLACE(A.CRDT_RTNG,'\"','') as CRDT_RTNG,\r\n",
							"A.DD_OFFERG_CS_USD_AMT,\r\n",
							"A.DD_OFFERG_IS_USD_AMT,\r\n",
							"A.DD_OFFERG_SS_USD_AMT,\r\n",
							"A.DD_OFFERG_WCP_USD_AMT,\r\n",
							"REPLACE(A.RISK_CONSULTANT_NOTES_ID,'\"','') as RISK_CONSULTANT_NOTES_ID,\r\n",
							"REPLACE(A.SENIOR_SOLUTION_MANAGER_NOTES_ID,'\"','') as SENIOR_SOLUTION_MANAGER_NOTES_ID,\r\n",
							"REPLACE(A.SIH_GM_NOTES_ID,'\"','') as SIH_GM_NOTES_ID,\r\n",
							"A.RULES_EFFECTIVE_DATE,\r\n",
							"REPLACE(A.DEAL_LEAD_NOTES_ID,'\"','') as DEAL_LEAD_NOTES_ID,\r\n",
							"REPLACE(A.CNTRCT_SIGNOFF_APPRVL_STAT_CD,'\"','') as CNTRCT_SIGNOFF_APPRVL_STAT_CD,\r\n",
							"REPLACE(A.PAYBACK_TXT,'\"','') as PAYBACK_TXT,\r\n",
							"'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"LEFT JOIN etlhubConfirmed.dht_deal_dim1 B\r\n",
							"ON TRIM(A.DEAL_ID)=TRIM(B.DEAL_IDENTIFIER)\r\n",
							"AND CURRENT_IND='Y'\r\n",
							"WHERE existing_REC_CHECKSUM is null\r\n",
							"AND COALESCE(B.REC_CHECKSUM,'') <> COALESCE(A.column_hash,'');\r\n",
							"\"\"\"\r\n",
							"d=spark.sql(qry_insert)\r\n",
							"#d.show(10)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#update the existing rows in target table with new values and record status as U and expiry the existing records with current indicator as N.\r\n",
							"\r\n",
							"updins=\"\"\"\r\n",
							"INSERT INTO etlhubConfirmed.dht_deal_dim \r\n",
							"select existing_DEAL_KEY,1+existing_DEAL_VERSION_NUMBER as DEAL_VERSION_NUMBER , \r\n",
							"A.DEAL_ID,\r\n",
							"A.BID_SUPPORT_INET_ID,\r\n",
							"A.BID_SUPPORT_NOTES_ID,\r\n",
							"A.BUS_UNIT_ID,\r\n",
							"A.CLNT_NM,\r\n",
							"A.CLNT_NUM,\r\n",
							"A.COMPLEXITY,\r\n",
							"A.COMNTS_TXT,\r\n",
							"A.CMPLXTY_CMMNT_TXT,\r\n",
							"A.CMPLXTY_OVERRD,\r\n",
							"A.CNTRCT_TYP_DESC,\r\n",
							"A.ENGG_SPPRT_TXN_LINK_TXT,\r\n",
							"A.ENGG_SPPRT_TXN_NUM,\r\n",
							"A.INDSTRY_NM,\r\n",
							"A.INIT_SIEBEL_IND,\r\n",
							"A.CNTRY_ID,\r\n",
							"A.LEAD_ACCT_PRTNR_BTT_CD,\r\n",
							"A.LEAD_PRTNR_BTT_CD,\r\n",
							"A.LEAD_PRTNR_NOTES_ID,\r\n",
							"A.OLD_OWNR_INET_ID,\r\n",
							"A.OWNR_INET_ID,\r\n",
							"A.proj_desc,\r\n",
							"A.OPPTNY_ID,\r\n",
							"A.OWNR_NOTES_ID,\r\n",
							"A.PARENT_DEAL_ID,\r\n",
							"A.PRPSL_MGR_NOTES_ID,\r\n",
							"A.REAS_TXT,\r\n",
							"A.RMVD_IND,\r\n",
							"A.RISK_RTNG,\r\n",
							"A.RISK_RTNG_CMNT_TXT,\r\n",
							"A.RSK_RTNG_OVERRD,\r\n",
							"A.SALES_STAGE_CD,\r\n",
							"A.SECONDARY_SERVICE,\r\n",
							"A.SCTR_NM,\r\n",
							"A.SVC_NM,\r\n",
							"A.SIEBEL_IND,\r\n",
							"A.SRC_CD,\r\n",
							"A.STAT_CD,\r\n",
							"'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT,current_timestamp as IMG_LST_UPD_DT,\r\n",
							"'9999-12-31 00:00:00.000' as REC_END_DT,CURRENT_TIMESTAMP AS REC_START_DT,  A.column_hash as REC_CHECKSUM, \r\n",
							"'U' as REC_STATUS, 'DCA' AS SOURCE_SYSTEM,\r\n",
							"A.RCA1_ANSWR_TXT,\r\n",
							"A.RCA1_ANSWR_CMMNT,\r\n",
							"A.RCA2_ANSWR_TXT,\r\n",
							"A.RCA2_ANSWR_CMMNT, \r\n",
							"A.RCA3_ANSWR_TXT,\r\n",
							"A.RCA3_ANSWR_CMMNT,\r\n",
							"A.RCA4_ANSWR_TXT,\r\n",
							"A.RCA4_ANSWR_CMMNT,\r\n",
							"A.RCA5_ANSWR_TXT,\r\n",
							"A.RCA5_ANSWR_CMMNT,\r\n",
							"A.RCA6_ANSWR_TXT,\r\n",
							"A.RCA6_ANSWR_CMMNT,\r\n",
							"A.RCA7_ANSWR_TXT,\r\n",
							"A.RCA7_ANSWR_CMMNT,\r\n",
							"A.RCA8_ANSWR_TXT,\r\n",
							"A.RCA8_ANSWR_CMMNT,\r\n",
							"A.APPRVR_CNT,\r\n",
							"A.RVWR_CNT,\r\n",
							"A.DEAL_TYPE,\r\n",
							"A.GEO_CD,\r\n",
							"A.HYBRD_TYPE_A,\r\n",
							"A.HYBRD_TYPE_B,\r\n",
							"A.LOAD_LOB,\r\n",
							"A.CRDT_RTNG,\r\n",
							"A.DD_OFFERG_CS_USD_AMT,\r\n",
							"A.DD_OFFERG_IS_USD_AMT,\r\n",
							"A.DD_OFFERG_SS_USD_AMT,\r\n",
							"A.DD_OFFERG_WCP_USD_AMT,\r\n",
							"A.RISK_CONSULTANT_NOTES_ID,\r\n",
							"A.SENIOR_SOLUTION_MANAGER_NOTES_ID,\r\n",
							"A.SIH_GM_NOTES_ID,\r\n",
							"A.RULES_EFFECTIVE_DATE,\r\n",
							"A.DEAL_LEAD_NOTES_ID,\r\n",
							"A.CNTRCT_SIGNOFF_APPRVL_STAT_CD,\r\n",
							"A.PAYBACK_TXT,\r\n",
							"'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"WHERE DEAL_ID = existing_DEAL_ID and LOWER(column_hash) <> LOWER(existing_REC_CHECKSUM)\r\n",
							"AND NOT EXISTS\r\n",
							"(SELECT 1 FROM etlhubConfirmed.dht_deal_dim B\r\n",
							"WHERE TRIM(A.DEAL_ID)=TRIM(B.DEAL_IDENTIFIER)\r\n",
							"and b.CURRENT_IND='Y'\r\n",
							"AND coalesce(A.column_hash,'')=coalesce(B.REC_CHECKSUM,'')) ;\r\n",
							"\"\"\"\r\n",
							"d1=spark.sql(updins)\r\n",
							"#d1.show()\r\n",
							"\r\n",
							"expqry=\"\"\"\r\n",
							"MERGE INTO etlhubConfirmed.dht_deal_dim A\r\n",
							"USING fullJoin B\r\n",
							"ON B.DEAL_ID = A.DEAL_IDENTIFIER\r\n",
							"AND LOWER(A.DEAL_IDENTIFIER) = LOWER(B.existing_DEAL_ID) and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\r\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\r\n",
							",REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds\r\n",
							",IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"d2=spark.sql(expqry)\r\n",
							"#d2.show()\r\n",
							"\r\n",
							"#Soft deletes or no longer active in source\r\n",
							"softdelqry=\"\"\"\r\n",
							"MERGE INTO etlhubConfirmed.dht_deal_dim  A\r\n",
							"USING fullJoin B\r\n",
							"ON A.DEAL_IDENTIFIER = B.existing_DEAL_ID\r\n",
							"AND B.DEAL_ID is NULL\r\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N' ,IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"df3=spark.sql(softdelqry)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\r\n",
							"select 'Number of Active Rows' as Title,count(*) from etlhubconfirmed.dht_deal_dim where CURRENT_IND='Y'\r\n",
							";\r\n",
							"\r\n",
							"\r\n",
							"select 'Duplicate Rows based on surrogate key' as Title,DEAL_KEY,count(*) from etlhubconfirmed.dht_deal_dim where CURRENT_IND='Y'\r\n",
							"    group by DEAL_KEY\r\n",
							"    having count(*)>1\r\n",
							";\r\n",
							"\r\n",
							"select 'Duplicate Rows based on natural key' as Title,DEAL_ID,count(*) from etlhubconfirmed.dht_deal_dim where CURRENT_IND='Y'\r\n",
							"    group by DEAL_ID\r\n",
							"    having count(*)>1\r\n",
							";\r\n",
							"\r\n",
							"select 'New rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_deal_dim where CURRENT_IND='Y'\r\n",
							"    and date(extract_dt)='2022-06-28' and rec_status='I'\r\n",
							";\r\n",
							"\r\n",
							"select 'Changed rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_deal_dim where CURRENT_IND='Y'\r\n",
							"    and date(extract_dt)='2022-06-28' and rec_status='U'\r\n",
							";\r\n",
							"\r\n",
							"select 'Changed rows updated in this run' as Title,count(*) from etlhubconfirmed.dht_deal_dim where CURRENT_IND='N'\r\n",
							"    and date(REC_END_DT)='2022-06-28' --and rec_status='I'\r\n",
							";\r\n",
							"\r\n",
							"select 'Rows no longer active in source' as Title,count(*) from etlhubconfirmed.dht_deal_dim where CURRENT_IND='Y'\r\n",
							"    AND date(REC_END_DT)='2022-06-28' --and rec_status='I'\r\n",
							";\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							" from etlhubConfirmed.dht_deal_dim1 where ACTIVE_IN_SOURCE_IND='N' \r\n",
							"    --where DEAL_IDENTIFIER in (7840900,7840899)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 18
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_dht_employee_load')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ab711c70-0d05-417e-af77-04f09ba7de48"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Employeed Dimension load**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubconfirmed.dht_employee by sourceing data from BMSIW's EMF datamart.\n",
							"Since BMSIW is not accessible from Azure, a datastage job src_BMSIW_Employee_Azure is executed on datastage to push the BMSIW extract to adls.\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"import os\n",
							"import sys\n",
							"from datetime import datetime"
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/BMSIW/' # fill in your relative folder path \n",
							"relative_archive_path='archive/'\n",
							"file_name='DS_EMPLOYEE_DATA.csv' \n",
							"natural_key=\"EMPLOYEE_BUSINESS_CD\"\n",
							"tablename=\"etlhubConfirmed.dht_employee\"\n",
							"#natural_key=\"BUSINESS_PARTNER_ID\"\n",
							"keycolumn=\"EMPLOYEE_KEY\"\n",
							"date = datetime.now().strftime(\"%Y_%m_%d-%I:%M:%S_%p\")"
						],
						"outputs": [],
						"execution_count": 52
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Copy source CSV file to archive folder including timestamp in file name"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/BMSIW/DS_EMPLOYEE_DATA.csv"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"adls_arch_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_archive_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"relative_archive_path\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"arch_fil_name=file_name.rsplit(\".\")\n",
							"csv_archive_path = adls_arch_path + arch_fil_name[0] + '_' + date + '.' + arch_fil_name[1]\n",
							"print (csv_archive_path)\n",
							"\n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\n",
							"\n",
							"incrementalData_DF.write.csv(csv_archive_path, header = 'true')\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"#Add checks for duplicate data, Check for 1. dups in source data based on natural Key and\n",
							"# 2. Dups in target already\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")"
						],
						"outputs": [],
						"execution_count": 53
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Create a dataframe with target deltalake table data with necessary columns\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\n",
							"#existingDataDF=spark.sql(\"SELECT tgt.* from etlhubConfirmed.DHT_EMPLOYEE tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\n",
							"\n",
							"#existingMaxKeyDF=spark.sql(\"SELECT MAX(EMPLOYEE_KEY) existing_MAX_KEY from etlhubConfirmed.DHT_EMPLOYEE tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.EMPLOYEE_BUSINESS_CD == existingDataDF1.existing_EMPLOYEE_BUSINESS_CD, \"fullouter\") \n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"fullJoin2.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .option(\"overwriteSchema\", \"true\") \\\n",
							"  .option(\"path\", \"abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/staging/DHTS_EMPLOYEE\") \\\n",
							"  .saveAsTable(\"DHTS_EMPLOYEE\")\n",
							"   #.option(\"overwriteSchema\", \"true\") \\  .save(\"\") \\"
						],
						"outputs": [],
						"execution_count": 54
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select * from existingDataDF1 where EXISTING_employee_serial_num='001518' and EXISTING_employee_country_code='661'"
						],
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Data Validation Checks :\n",
							"\n",
							"- Check for duplicates in Source data based on natural key\n",
							"- Check for duplicates in target delta lake table based on natural key\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} WHERE CURRENT_IND='Y' GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"df3=spark.sql(qry3.format(natural_key,natural_key))\n",
							"cnt1=df3.count()\n",
							"\n",
							"print (cnt1)\n",
							"if cnt1 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in source:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(1)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							""
						],
						"outputs": [],
						"execution_count": 56
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select * from etlhubconfirmed.dht_employee where EMPLOYEE_BUSINESS_CD='ZZ03FP~693~NEWIS'\n",
							"    --AND CURRENT_IND='Y'"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry_4_recon1=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from DHTS_EMPLOYEE A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.EMPLOYEE_BUSINESS_CD=B.EMPLOYEE_BUSINESS_CD\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(tablename))\n",
							"print('New records or Inserts are:') \n",
							"df_4_recon1.show()\n",
							"\n",
							"qry_4_recon2=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from DHTS_EMPLOYEE A \n",
							"WHERE LOWER(EMPLOYEE_BUSINESS_CD) = LOWER(existing_EMPLOYEE_BUSINESS_CD) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.EMPLOYEE_BUSINESS_CD)=LOWER(B.EMPLOYEE_BUSINESS_CD)\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon2=spark.sql(qry_4_recon2.format(tablename))\n",
							"print('Changed records or Updates are:') \n",
							"df_4_recon2.show()\n",
							"\n",
							"qry_4_recon3=\"\"\"\n",
							"SELECT COUNT(*) from \n",
							"etlhubconfirmed.DHT_EMPLOYEE A\n",
							"join fullJoin B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"\n",
							"\"\"\"\n",
							"\n",
							"df_4_recon3=spark.sql(qry_4_recon3.format(natural_key,natural_key,natural_key))\n",
							"print('Soft Deletes are:') \n",
							"df_4_recon3.show()"
						],
						"outputs": [],
						"execution_count": 66
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"#delete FROM etlhubconfirmed.dht_employee;\n",
							"\n",
							"qry_4_recon1=\"\"\"\n",
							"SELECT COUNT(*) from DHTS_EMPLOYEE A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.EMPLOYEE_BUSINESS_CD=B.EMPLOYEE_BUSINESS_CD\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(tablename))\n",
							"\n",
							"qry_ins_new_rows=\"\"\"\n",
							"\n",
							"INSERT INTO etlhubconfirmed.dht_employee\n",
							"select \n",
							"COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS EMPLOYEE_KEY,1 as VERSION  \n",
							",EMPLOYEE_SERIAL_NUM\n",
							",EMPLOYEE_COUNTRY_CODE\n",
							",EMPLOYEE_COMPANY_CD\n",
							",EMPLOYEE_BUSINESS_CD\n",
							",NOTES_ID_RAW\n",
							",NULL AS NOTES_ID\n",
							",'Y' AS CURRENT_IND\n",
							",CURRENT_TIMESTAMP AS EXTRACT_DT\n",
							",CURRENT_TIMESTAMP AS REC_START_DT\n",
							",'9999-12-31 00:00:00.000' as REC_END_DT\n",
							",'BMSIW' AS SOURCE_SYSTEM\n",
							",column_hash as REC_CHECKSUM\n",
							",'I' as REC_STATUS\n",
							",current_timestamp as IMG_LST_UPD_DT\n",
							",CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							",A.EMPLOYEE_INITIAL as EMPLOYEE_INITIALS\n",
							",EMPLOYEE_LAST_NAME\n",
							",CASE WHEN EMPLOYEE_DISCON_DT='0001-01-01 00:00:00' THEN NULL ELSE EMPLOYEE_DISCON_DT END AS EMPLOYEE_DISCON_DT\n",
							",CASE WHEN EMPLOYEE_EFF_DT='0001-01-01 00:00:00' THEN NULL ELSE EMPLOYEE_EFF_DT END AS EMPLOYEE_EFF_DT\n",
							",EMPLOYEE_STATUS\n",
							",EMPLOYEE_LEVEL_CD\n",
							",EMPLOYEE_USER_ID\n",
							",NULL AS MANAGER_IND\n",
							",HOME_NODE_ID\n",
							",HOME_USER_ID\n",
							",BURDEN_CD\n",
							",BURDEN_CD_UPD_IND\n",
							",FIN_ADMIN_CD\n",
							",LONGEVITY_CD\n",
							",GROUP_ID\n",
							",JOB_FAMILY_CD\n",
							",PROFESSION_CD\n",
							",EMF_SOURCE_CD\n",
							",LBR_RPT_IND\n",
							",INET_MAIL_ADDR\n",
							",TEAM_ID\n",
							",'' AS SITE_LOC_CD\n",
							",EMP_NODE_ID\n",
							",CNUM_ID\n",
							",WEEK_SCHEDULE_HRS\n",
							",UNIT_PRICE_AMT\n",
							",SHIFT_1_RATE\n",
							",SHIFT_2_RATE\n",
							",SHIFT_3_RATE\n",
							",STANDBY_RATE\n",
							",OVERTIME_AMT\n",
							",COMPETENCY_SEGMENT_CD\n",
							",PROFESSION_NAME\n",
							",ORIG_LOC_CD\n",
							",DEPT_CATG_CD\n",
							",LOB_ID\n",
							",SAP_IND\n",
							",CHARGE_GROUP_CD\n",
							",SAP_COMPANY_CD\n",
							",WORK_WEEK_HRS_MIN\n",
							",WORK_WEEK_HRS_MAX\n",
							",'Y' AS IMG_ACTIVE_EMPLOYEE_STATUS_CD\n",
							",EMP_FIRST_NM\n",
							",ISO_CTRY_CD\n",
							",MGR_CTRY_CD\n",
							",MGR_CMPNY_CD\n",
							",MGR_SER_NUM\n",
							",RDM_CTRY_CD\n",
							",RDM_CMPNY_CD\n",
							",RDM_SER_NUM\n",
							",NULL AS DIVISION_CODE\n",
							",DEPT_NUMBER\n",
							",MGR_CNUM_ID\n",
							",NULL AS JOB_ROLE\n",
							",'ED' AS DATA_IND\n",
							",'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"\n",
							"from DHTS_EMPLOYEE A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.DHT_EMPLOYEE B\n",
							"WHERE A.EMPLOYEE_BUSINESS_CD=B.EMPLOYEE_BUSINESS_CD\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"--limit 10\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"d=spark.sql(qry_ins_new_rows)\n",
							"\n",
							"#d.show(10)"
						],
						"outputs": [],
						"execution_count": 58
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Update or expire the rows with existing instance of the changed rows:\n",
							"qry_4_upd_changes_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING DHTS_EMPLOYEE B\n",
							"ON A.{} = B.{}\n",
							"AND LOWER(B.{}) = LOWER(B.existing_{}) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\"\"\"\n",
							"f=spark.sql(qry_4_upd_changes_rows.format(tablename,natural_key,natural_key,natural_key,natural_key))\n",
							""
						],
						"outputs": [],
						"execution_count": 60
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select count(*) from DHTS_EMPLOYEE where existing_CURRENT_IND='Y';\n",
							"\n",
							"select * from etlhubconfirmed.dht_employee\n",
							"where employee_serial_num='001518' and employee_country_code='661'\n",
							"    ;\n",
							"select * from DHTS_EMPLOYEE where existing_employee_serial_num='001518' \n",
							"    and existing_employee_country_code='661'\n",
							"    ;\n",
							"\n",
							" select * from DHTS_EMPLOYEE where existing_employee_serial_num='001518' \n",
							"    and existing_employee_country_code='661'\n",
							"    ;   \n",
							"select count(*)\n",
							"from etlhubConfirmed.DHT_EMPLOYEE B\n",
							"join\n",
							"DHTS_EMPLOYEE A \n",
							"WHERE A.EMPLOYEE_BUSINESS_CD = B.EMPLOYEE_BUSINESS_CD\n",
							"and LOWER(A.EMPLOYEE_BUSINESS_CD) = LOWER(A.existing_EMPLOYEE_BUSINESS_CD) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND B.REC_START_DT=A.existing_REC_START_DT\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.DHT_EMPLOYEE B\n",
							"WHERE LOWER(A.EMPLOYEE_BUSINESS_CD)=LOWER(B.EMPLOYEE_BUSINESS_CD)\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"SELECT COUNT(*) FROM etlhubConfirmed.DHT_EMPLOYEE A\n",
							"JOIN DHTS_EMPLOYEE B\n",
							"ON A.EMPLOYEE_BUSINESS_CD = B.EMPLOYEE_BUSINESS_CD\n",
							"AND LOWER(B.EMPLOYEE_BUSINESS_CD) = LOWER(B.existing_EMPLOYEE_BUSINESS_CD) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							";"
						],
						"outputs": [],
						"execution_count": 61
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Insert for changed rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry_ins_changed_rows=\"\"\"\n",
							"\n",
							"INSERT INTO etlhubconfirmed.dht_employee\n",
							"select \n",
							"existing_EMPLOYEE_KEY\n",
							",1 + existing_EMPLOYEE_VERSION as VERSION\n",
							",EMPLOYEE_SERIAL_NUM\n",
							",EMPLOYEE_COUNTRY_CODE\n",
							",EMPLOYEE_COMPANY_CD\n",
							",EMPLOYEE_BUSINESS_CD\n",
							",NOTES_ID_RAW\n",
							",NULL AS NOTES_ID\n",
							",'Y' AS CURRENT_IND\n",
							",CURRENT_TIMESTAMP AS EXTRACT_DT\n",
							",CURRENT_TIMESTAMP AS REC_START_DT\n",
							",'9999-12-31 00:00:00.000' as REC_END_DT\n",
							",'BMSIW' AS SOURCE_SYSTEM\n",
							",column_hash as REC_CHECKSUM\n",
							",'U' as REC_STATUS\n",
							",CURRENT_TIMESTAMP as IMG_LST_UPD_DT\n",
							",CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							",A.EMPLOYEE_INITIAL as EMPLOYEE_INITIALS\n",
							",EMPLOYEE_LAST_NAME\n",
							",CASE WHEN EMPLOYEE_DISCON_DT='0001-01-01 00:00:00' THEN NULL ELSE EMPLOYEE_DISCON_DT END AS EMPLOYEE_DISCON_DT\n",
							",CASE WHEN EMPLOYEE_EFF_DT='0001-01-01 00:00:00' THEN NULL ELSE EMPLOYEE_EFF_DT END AS EMPLOYEE_EFF_DT\n",
							",EMPLOYEE_STATUS\n",
							",EMPLOYEE_LEVEL_CD\n",
							",EMPLOYEE_USER_ID\n",
							",NULL AS MANAGER_IND\n",
							",HOME_NODE_ID\n",
							",HOME_USER_ID\n",
							",BURDEN_CD\n",
							",BURDEN_CD_UPD_IND\n",
							",FIN_ADMIN_CD\n",
							",LONGEVITY_CD\n",
							",GROUP_ID\n",
							",JOB_FAMILY_CD\n",
							",PROFESSION_CD\n",
							",EMF_SOURCE_CD\n",
							",LBR_RPT_IND\n",
							",INET_MAIL_ADDR\n",
							",TEAM_ID\n",
							",'' AS SITE_LOC_CD\n",
							",EMP_NODE_ID\n",
							",CNUM_ID\n",
							",WEEK_SCHEDULE_HRS\n",
							",UNIT_PRICE_AMT\n",
							",SHIFT_1_RATE\n",
							",SHIFT_2_RATE\n",
							",SHIFT_3_RATE\n",
							",STANDBY_RATE\n",
							",OVERTIME_AMT\n",
							",COMPETENCY_SEGMENT_CD\n",
							",PROFESSION_NAME\n",
							",ORIG_LOC_CD\n",
							",DEPT_CATG_CD\n",
							",LOB_ID\n",
							",SAP_IND\n",
							",CHARGE_GROUP_CD\n",
							",SAP_COMPANY_CD\n",
							",WORK_WEEK_HRS_MIN\n",
							",WORK_WEEK_HRS_MAX\n",
							",'Y' AS IMG_ACTIVE_EMPLOYEE_STATUS_CD\n",
							",EMP_FIRST_NM\n",
							",ISO_CTRY_CD\n",
							",MGR_CTRY_CD\n",
							",MGR_CMPNY_CD\n",
							",MGR_SER_NUM\n",
							",RDM_CTRY_CD\n",
							",RDM_CMPNY_CD\n",
							",RDM_SER_NUM\n",
							",NULL AS DIVISION_CODE\n",
							",DEPT_NUMBER\n",
							",MGR_CNUM_ID\n",
							",NULL AS JOB_ROLE\n",
							",'ED' AS DATA_IND\n",
							",'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from DHTS_EMPLOYEE A \n",
							"WHERE LOWER(EMPLOYEE_BUSINESS_CD) = LOWER(existing_EMPLOYEE_BUSINESS_CD) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.DHT_EMPLOYEE B\n",
							"WHERE LOWER(A.EMPLOYEE_BUSINESS_CD)=LOWER(B.EMPLOYEE_BUSINESS_CD)\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"e=spark.sql(qry_ins_changed_rows)\n",
							"#e.show(4)"
						],
						"outputs": [],
						"execution_count": 62
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Soft Deletes - Update Active_Ind for the rows no longer active in source:\n",
							"\n",
							"qry_4_upd_deleted_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING DHTS_EMPLOYEE B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"\n",
							";\n",
							"\"\"\"\n",
							"g=spark.sql(qry_4_upd_deleted_rows.format(tablename,natural_key,natural_key,natural_key))"
						],
						"outputs": [],
						"execution_count": 63
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select 'Number of Active Rows' as Title,count(*) from etlhubconfirmed.dht_employee where CURRENT_IND='Y'\n",
							";\n",
							"\n",
							"\n",
							"select 'Duplicate Rows based on surrogate key' as Title,EMPLOYEE_KEY,count(*) from etlhubconfirmed.dht_employee where CURRENT_IND='Y'\n",
							"    group by EMPLOYEE_KEY\n",
							"    having count(*)>1\n",
							";\n",
							"\n",
							"select 'Duplicate Rows based on natural key' as Title,EMPLOYEE_BUSINESS_CD,count(*) from etlhubconfirmed.dht_employee where CURRENT_IND='Y'\n",
							"    group by EMPLOYEE_BUSINESS_CD\n",
							"    having count(*)>1\n",
							";\n",
							"\n",
							"select 'New rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_employee where CURRENT_IND='Y'\n",
							"    and date(extract_dt)='2022-07-01' and rec_status='I'\n",
							";\n",
							"\n",
							"select 'Changed rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_employee where CURRENT_IND='Y'\n",
							"    and date(extract_dt)='2022-07-01' and rec_status='U'\n",
							";\n",
							"\n",
							"select 'Changed rows updated in this run' as Title,count(*) from etlhubconfirmed.dht_employee where CURRENT_IND='N'\n",
							"    and date(REC_END_DT)='2022-07-01' --and rec_status='I'\n",
							";\n",
							"\n",
							"select 'Rows no longer active in source' as Title,count(*) from etlhubconfirmed.dht_employee where CURRENT_IND='Y'\n",
							"    AND \n",
							"     date(REC_END_DT)='2022-07-01' --and rec_status='I'\n",
							";\n",
							""
						],
						"outputs": [],
						"execution_count": 64
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT COUNT(*) FROM etlhubconfirmed.dht_employee\n",
							"WHERE CURRENT_IND='N' and DATE(REC_END_DT)<>'9999-12-31'\n",
							"    and date(img_lst_upd_dt)='2022-06-30'\n",
							"        limit 1\n",
							";\n",
							"select * from etlhubconfirmed.dht_employee\n",
							"where employee_serial_num='001518' and employee_country_code='661'\n",
							"    ;\n",
							"\n",
							"select count(*) from etlhubconfirmed.dht_employee where CURRENT_IND='Y'\n",
							"   and date(extract_dt)='2022-06-30' --and rec_status='I'\n",
							";\n",
							"\n",
							"select count(*) from etlhubconfirmed.dht_employee\n",
							"WHERE ACTIVE_IN_SOURCE_IND='N'\n",
							"    and date(IMG_LST_UPD_DT)=current_date\n",
							"    ;\n",
							"UPDATE etlhubconfirmed.dht_employee\n",
							"set CURRENT_IND='Y', \n",
							"REC_END_DT='9999-12-31'\n",
							"  WHERE CURRENT_IND='N' and DATE(REC_END_DT)<>'9999-12-31'\n",
							"    and date(img_lst_upd_dt)='2022-06-30'\n",
							"        ;  "
						],
						"outputs": [],
						"execution_count": 65
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_dht_geography_load')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "51754958-d7c3-4b57-a067-45a3212ebd3b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Geography Dimension load**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubConfirmed.DHT_GEOGRAPHY by sourceing data from ERDM Kyndryl.\n",
							"\n",
							"The Geography has been downloaded from ERDM in a CSV format and uploaded to adls for loading to Industry Dimension.\n",
							"\n",
							"ERDM GeographyLink: pokgsa.ibm.com/gsa/pokgsa/projects/t/tmtproj/output/Approved/ibmww/kyn/bds/kyn_gtm/REFT_CTRY_MKT_REL_KYNDRYL.html\n",
							"\n",
							"ERDM Link: https://erdmuiapp-prod.wdc1a.cirrus.ibm.com/erdmAllStandards/0\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"from pyspark.sql.functions import col\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"import os\n",
							"import sys\n",
							"from datetime import datetime"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/ERDM/' # fill in your relative folder path \n",
							"relative_archive_path='archive/'\n",
							"file_name='DS_INDUSTRY_HIERARCHY_DATA.csv' \n",
							"natural_key=\"code\"\n",
							"tablename=\"etlhubConfirmed.dht_industry_sector\"\n",
							"stagingtable=\"DHTS_INDUSTRY_SECTOR\"\n",
							"keycolumn=\"INDUSTRY_SECTOR_KEY\"\n",
							"date = datetime.now().strftime(\"%Y-%m-%d-%I:%M:%S_%p\")\n",
							"print(date)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Copy source CSV file to archive folder including timestamp in file name"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/ERDM/DS_INDUSTRY_HIERARCHY_DATA.csv\n",
							"\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"adls_arch_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_archive_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"relative_archive_path\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"arch_fil_name=file_name.rsplit(\".\")\n",
							"csv_archive_path = adls_arch_path + arch_fil_name[0] + '_' + date + '.' + arch_fil_name[1]\n",
							"print (csv_archive_path)\n",
							"\n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\n",
							"\n",
							"incrementalData_DF.write.csv(csv_archive_path, header = 'true')\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"#Add checks for duplicate data, Check for 1. dups in source data based on natural Key and\n",
							"# 2. Dups in target already\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Create a dataframe with target deltalake table data with necessary columns\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\n",
							"#existingDataDF=spark.sql(\"SELECT tgt.* from etlhubConfirmed.DHT_EMPLOYEE tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\n",
							"\n",
							"#existingMaxKeyDF=spark.sql(\"SELECT MAX(EMPLOYEE_KEY) existing_MAX_KEY from etlhubConfirmed.DHT_EMPLOYEE tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1, col(natural_key) == col(\"existing_\" + natural_key), \"fullouter\")\n",
							"\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"fullJoin2.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .option(\"overwriteSchema\", \"true\") \\\n",
							"  .option(\"path\", \"abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/staging/{}\".format(stagingtable)) \\\n",
							"  .saveAsTable(stagingtable)\n",
							"   #.option(\"overwriteSchema\", \"true\") \\  .save(\"\") \\"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Data Validation Checks :\n",
							"\n",
							"- Check for duplicates in Source data based on natural key\n",
							"- Check for duplicates in target delta lake table based on natural key\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} WHERE CURRENT_IND='Y' GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"df3=spark.sql(qry3.format(natural_key,natural_key))\n",
							"cnt1=df3.count()\n",
							"\n",
							"print (cnt1)\n",
							"if cnt1 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in source:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(1)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry_4_recon1=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable,tablename,natural_key,natural_key))\n",
							"print('New records or Inserts are:') \n",
							"df_4_recon1.show()\n",
							"\n",
							"qry_4_recon2=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE LOWER({}) = LOWER(existing_{}) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon2=spark.sql(qry_4_recon2.format(stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))\n",
							"print('Changed records or Updates are:') \n",
							"df_4_recon2.show()\n",
							"\n",
							"qry_4_recon3=\"\"\"\n",
							"SELECT COUNT(*) from \n",
							"{} A\n",
							"join fullJoin B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"\n",
							"\"\"\"\n",
							"\n",
							"df_4_recon3=spark.sql(qry_4_recon3.format(tablename,natural_key,natural_key,natural_key))\n",
							"print('Soft Deletes are:') \n",
							"df_4_recon3.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT count(*) FROM etlhubconfirmed.dht_industry_sector where current_ind='Y';\n",
							"\n",
							"select * from fullJoin;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"#delete FROM etlhubconfirmed.dht_employee;\n",
							"\n",
							"qry_4_recon1=\"\"\"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable, tablename, natural_key, natural_key))\n",
							"\n",
							"qry_ins_new_rows=\"\"\"\n",
							"\n",
							"INSERT INTO {}\n",
							"\tselect \n",
							"\tCOALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS {} \n",
							",\t1 AS INDUSTRY_SECTOR_VERSION\n",
							",\tlevelNum\n",
							",\tparentCode\n",
							",\tcode\n",
							",\tlongDescription\n",
							",\tmediumDescription\n",
							",\tshortDescription\n",
							",\tcomments\n",
							",\trecordStatus\n",
							",\trowCreateTs\n",
							",\trowUpdateTs\n",
							",\tUSAGE_RULE\n",
							",\tALT_DESC_FULL_1\n",
							",\tALT_DESC_FULL_2\n",
							",   'Y' AS CURRENT_IND\n",
							",   CURRENT_TIMESTAMP AS EXTRACT_DT\n",
							",   CURRENT_TIMESTAMP AS REC_START_DT\n",
							",   '9999-12-31 00:00:00.000' as REC_END_DT\n",
							",   'ERDM' AS SOURCE_SYSTEM\n",
							",    column_hash as REC_CHECKSUM\n",
							",    'I' as REC_STATUS\n",
							",    current_timestamp as IMG_LST_UPD_DT\n",
							",    CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							",   'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"    from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"d=spark.sql(qry_ins_new_rows.format(tablename,keycolumn,stagingtable,tablename,natural_key, natural_key ))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT count(*) FROM etlhubconfirmed.dht_industry_sector where current_ind='Y'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Update or expire the rows with existing instance of the changed rows:\n",
							"qry_4_upd_changes_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.{}\n",
							"AND LOWER(B.{}) = LOWER(B.existing_{}) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\"\"\"\n",
							"f=spark.sql(qry_4_upd_changes_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key,natural_key))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--existing data\n",
							"select 'existing data',count(*) from DHTS_INDUSTRY_SECTOR where existing_CURRENT_IND='Y';\n",
							"  \n",
							"select 'rows for Update', count(*)\n",
							"from etlhubConfirmed.DHT_INDUSTRY_SECTOR B\n",
							"join\n",
							"DHTS_INDUSTRY_SECTOR A \n",
							"WHERE A.CODE = B.CODE\n",
							"and LOWER(A.CODE) = LOWER(A.existing_CODE) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND B.REC_START_DT=A.existing_REC_START_DT\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.DHT_INDUSTRY_SECTOR B\n",
							"WHERE LOWER(A.code)=LOWER(B.code)\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"SELECT COUNT(*) FROM etlhubConfirmed.DHT_INDUSTRY_SECTOR A\n",
							"JOIN DHTS_INDUSTRY_SECTOR B\n",
							"ON A.code = B.code\n",
							"AND LOWER(B.code) = LOWER(B.existing_code) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							";"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Insert for changed rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry_ins_changed_rows=\"\"\"\n",
							"INSERT INTO {}\n",
							"\tselect \n",
							"    existing_INDUSTRY_SECTOR_KEY\n",
							"    ,1 + existing_INDUSTRY_SECTOR_VERSION as VERSION\n",
							",\tlevelNum\n",
							",\tparentCode\n",
							",\tcode\n",
							",\tlongDescription\n",
							",\tmediumDescription\n",
							",\tshortDescription\n",
							",\tcomments\n",
							",\trecordStatus\n",
							",\trowCreateTs\n",
							",\trowUpdateTs\n",
							",\tUSAGE_RULE\n",
							",\tALT_DESC_FULL_1\n",
							",\tALT_DESC_FULL_2\n",
							",   'Y' AS CURRENT_IND\n",
							",   CURRENT_TIMESTAMP AS EXTRACT_DT\n",
							",   CURRENT_TIMESTAMP AS REC_START_DT\n",
							",   '9999-12-31 00:00:00.000' as REC_END_DT\n",
							",   'ERDM' AS SOURCE_SYSTEM\n",
							",    column_hash as REC_CHECKSUM\n",
							",    'U' as REC_STATUS\n",
							",    current_timestamp as IMG_LST_UPD_DT\n",
							",    CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							",   'Y' AS ACTIVE_IN_SOURCE_IND  \n",
							"from {} A \n",
							"WHERE LOWER({}) = LOWER(existing_{}) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"e=spark.sql(qry_ins_changed_rows.format(tablename,stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Soft Deletes - Update Active_Ind for the rows no longer active in source:\n",
							"\n",
							"qry_4_upd_deleted_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"\n",
							";\n",
							"\"\"\"\n",
							"g=spark.sql(qry_4_upd_deleted_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select 'Number of Active Rows' as Title,count(*) from etlhubconfirmed.dht_INDUSTRY_SECTOR where CURRENT_IND='Y'\n",
							";\n",
							"\n",
							"\n",
							"select 'Duplicate Rows based on surrogate key' as Title,INDUSTRY_SECTOR_KEY,count(*) from etlhubconfirmed.dht_INDUSTRY_SECTOR where CURRENT_IND='Y'\n",
							"    group by INDUSTRY_SECTOR_KEY\n",
							"    having count(*)>1\n",
							";\n",
							"\n",
							"select 'Duplicate Rows based on natural key' as Title,code,count(*) from etlhubconfirmed.dht_INDUSTRY_SECTOR where CURRENT_IND='Y'\n",
							"    group by code\n",
							"    having count(*)>1\n",
							";\n",
							"\n",
							"select 'New rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_INDUSTRY_SECTOR where CURRENT_IND='Y'\n",
							"    and date(extract_dt)=current_date and rec_status='I'\n",
							";\n",
							"\n",
							"select 'Changed rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_INDUSTRY_SECTOR where CURRENT_IND='Y'\n",
							"    and date(extract_dt)=current_date and rec_status='U'\n",
							";\n",
							"\n",
							"select 'Changed rows updated in this run' as Title,count(*) from etlhubconfirmed.dht_INDUSTRY_SECTOR where CURRENT_IND='N'\n",
							"    and date(REC_END_DT)=current_date --and rec_status='I'\n",
							";\n",
							"\n",
							"select 'Rows no longer active in source' as Title,count(*) from etlhubconfirmed.dht_INDUSTRY_SECTOR where CURRENT_IND='Y'\n",
							"    AND \n",
							"     date(REC_END_DT)=current_date --and rec_status='I'\n",
							";\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT COUNT(*) FROM etlhubconfirmed.dht_INDUSTRY_SECTOR\n",
							"WHERE CURRENT_IND='N' and DATE(REC_END_DT)<>'9999-12-31'\n",
							"    and date(img_lst_upd_dt)=current_date\n",
							"        limit 1\n",
							";\n",
							"\n",
							"\n",
							"select count(*) from etlhubconfirmed.dht_INDUSTRY_SECTOR where CURRENT_IND='Y'\n",
							"   and date(extract_dt)=current_date --and rec_status='I'\n",
							";\n",
							"\n",
							"select count(*) from etlhubconfirmed.dht_INDUSTRY_SECTOR\n",
							"WHERE ACTIVE_IN_SOURCE_IND='N'\n",
							"    and date(IMG_LST_UPD_DT)=current_date\n",
							"    ;\n",
							"UPDATE etlhubconfirmed.dht_INDUSTRY_SECTOR\n",
							"set CURRENT_IND='Y', \n",
							"REC_END_DT='9999-12-31'\n",
							"  WHERE CURRENT_IND='N' and DATE(REC_END_DT)<>'9999-12-31'\n",
							"    and date(img_lst_upd_dt)=current_date\n",
							"        ;  "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} WHERE CURRENT_IND='Y' GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT * FROM etlhubconfirmed.dht_INDUSTRY_SECTOR where Code ='008887~613'\n",
							"    --'008915~613'\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 61
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_dht_industry_sector_load')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "2797f7cb-bee7-4314-a75d-f2ca9f8a098b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Industry Hierarchy Dimension load**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubConfirmed.DHT_INDUSTRY_SECTOR by sourceing data from ERDM Kyndryl.\n",
							"\n",
							"The Industry Hierarchy has been downloaded from ERDM in a CSV format and uploaded to adls for loading to Industry Dimension.\n",
							"\n",
							"ERDM Industry Sector Link: https://erdmuiapp-prod.wdc1a.cirrus.ibm.com/erdmStandardDetails/102\n",
							"\n",
							"ERDM Link: https://erdmuiapp-prod.wdc1a.cirrus.ibm.com/erdmAllStandards/0\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"from pyspark.sql.functions import col\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"import os\n",
							"import sys\n",
							"from datetime import datetime"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/ERDM/' # fill in your relative folder path \n",
							"relative_archive_path='archive/'\n",
							"file_name='DS_INDUSTRY_HIERARCHY_DATA.csv' \n",
							"natural_key=\"code\"\n",
							"tablename=\"etlhubConfirmed.dht_industry_sector\"\n",
							"stagingtable=\"DHTS_INDUSTRY_SECTOR\"\n",
							"keycolumn=\"INDUSTRY_SECTOR_KEY\"\n",
							"date = datetime.now().strftime(\"%Y-%m-%d-%I:%M:%S_%p\")\n",
							"print(date)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Copy source CSV file to archive folder including timestamp in file name"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/ERDM/DS_INDUSTRY_HIERARCHY_DATA.csv\n",
							"\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"adls_arch_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_archive_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"relative_archive_path\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"arch_fil_name=file_name.rsplit(\".\")\n",
							"csv_archive_path = adls_arch_path + arch_fil_name[0] + '_' + date + '.' + arch_fil_name[1]\n",
							"print (csv_archive_path)\n",
							"\n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\n",
							"\n",
							"incrementalData_DF.write.csv(csv_archive_path, header = 'true')\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"#Add checks for duplicate data, Check for 1. dups in source data based on natural Key and\n",
							"# 2. Dups in target already\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Create a dataframe with target deltalake table data with necessary columns\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\n",
							"#existingDataDF=spark.sql(\"SELECT tgt.* from etlhubConfirmed.DHT_EMPLOYEE tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\n",
							"\n",
							"#existingMaxKeyDF=spark.sql(\"SELECT MAX(EMPLOYEE_KEY) existing_MAX_KEY from etlhubConfirmed.DHT_EMPLOYEE tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1, col(natural_key) == col(\"existing_\" + natural_key), \"fullouter\")\n",
							"\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"fullJoin2.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .option(\"overwriteSchema\", \"true\") \\\n",
							"  .option(\"path\", \"abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/staging/{}\".format(stagingtable)) \\\n",
							"  .saveAsTable(stagingtable)\n",
							"   #.option(\"overwriteSchema\", \"true\") \\  .save(\"\") \\"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Data Validation Checks :\n",
							"\n",
							"- Check for duplicates in Source data based on natural key\n",
							"- Check for duplicates in target delta lake table based on natural key\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} WHERE CURRENT_IND='Y' GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"df3=spark.sql(qry3.format(natural_key,natural_key))\n",
							"cnt1=df3.count()\n",
							"\n",
							"print (cnt1)\n",
							"if cnt1 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in source:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(1)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry_4_recon1=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable,tablename,natural_key,natural_key))\n",
							"print('New records or Inserts are:') \n",
							"df_4_recon1.show()\n",
							"\n",
							"qry_4_recon2=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE LOWER({}) = LOWER(existing_{}) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon2=spark.sql(qry_4_recon2.format(stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))\n",
							"print('Changed records or Updates are:') \n",
							"df_4_recon2.show()\n",
							"\n",
							"qry_4_recon3=\"\"\"\n",
							"SELECT COUNT(*) from \n",
							"{} A\n",
							"join fullJoin B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"\n",
							"\"\"\"\n",
							"\n",
							"df_4_recon3=spark.sql(qry_4_recon3.format(tablename,natural_key,natural_key,natural_key))\n",
							"print('Soft Deletes are:') \n",
							"df_4_recon3.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT count(*) FROM etlhubconfirmed.dht_industry_sector where current_ind='Y';\n",
							"\n",
							"select * from fullJoin;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"#delete FROM etlhubconfirmed.dht_employee;\n",
							"\n",
							"qry_4_recon1=\"\"\"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable, tablename, natural_key, natural_key))\n",
							"\n",
							"qry_ins_new_rows=\"\"\"\n",
							"\n",
							"INSERT INTO {}\n",
							"\tselect \n",
							"\tCOALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS {} \n",
							",\t1 AS INDUSTRY_SECTOR_VERSION\n",
							",\tlevelNum\n",
							",\tparentCode\n",
							",\tcode\n",
							",\tlongDescription\n",
							",\tmediumDescription\n",
							",\tshortDescription\n",
							",\tcomments\n",
							",\trecordStatus\n",
							",\trowCreateTs\n",
							",\trowUpdateTs\n",
							",\tUSAGE_RULE\n",
							",\tALT_DESC_FULL_1\n",
							",\tALT_DESC_FULL_2\n",
							",   'Y' AS CURRENT_IND\n",
							",   CURRENT_TIMESTAMP AS EXTRACT_DT\n",
							",   CURRENT_TIMESTAMP AS REC_START_DT\n",
							",   '9999-12-31 00:00:00.000' as REC_END_DT\n",
							",   'ERDM' AS SOURCE_SYSTEM\n",
							",    column_hash as REC_CHECKSUM\n",
							",    'I' as REC_STATUS\n",
							",    current_timestamp as IMG_LST_UPD_DT\n",
							",    CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							",   'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"    from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"d=spark.sql(qry_ins_new_rows.format(tablename,keycolumn,stagingtable,tablename,natural_key, natural_key ))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT count(*) FROM etlhubconfirmed.dht_industry_sector where current_ind='Y'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Update or expire the rows with existing instance of the changed rows:\n",
							"qry_4_upd_changes_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.{}\n",
							"AND LOWER(B.{}) = LOWER(B.existing_{}) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\"\"\"\n",
							"f=spark.sql(qry_4_upd_changes_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key,natural_key))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--existing data\n",
							"select 'existing data',count(*) from DHTS_INDUSTRY_SECTOR where existing_CURRENT_IND='Y';\n",
							"  \n",
							"select 'rows for Update', count(*)\n",
							"from etlhubConfirmed.DHT_INDUSTRY_SECTOR B\n",
							"join\n",
							"DHTS_INDUSTRY_SECTOR A \n",
							"WHERE A.CODE = B.CODE\n",
							"and LOWER(A.CODE) = LOWER(A.existing_CODE) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND B.REC_START_DT=A.existing_REC_START_DT\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.DHT_INDUSTRY_SECTOR B\n",
							"WHERE LOWER(A.code)=LOWER(B.code)\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"SELECT COUNT(*) FROM etlhubConfirmed.DHT_INDUSTRY_SECTOR A\n",
							"JOIN DHTS_INDUSTRY_SECTOR B\n",
							"ON A.code = B.code\n",
							"AND LOWER(B.code) = LOWER(B.existing_code) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							";"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Insert for changed rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry_ins_changed_rows=\"\"\"\n",
							"INSERT INTO {}\n",
							"\tselect \n",
							"    existing_INDUSTRY_SECTOR_KEY\n",
							"    ,1 + existing_INDUSTRY_SECTOR_VERSION as VERSION\n",
							",\tlevelNum\n",
							",\tparentCode\n",
							",\tcode\n",
							",\tlongDescription\n",
							",\tmediumDescription\n",
							",\tshortDescription\n",
							",\tcomments\n",
							",\trecordStatus\n",
							",\trowCreateTs\n",
							",\trowUpdateTs\n",
							",\tUSAGE_RULE\n",
							",\tALT_DESC_FULL_1\n",
							",\tALT_DESC_FULL_2\n",
							",   'Y' AS CURRENT_IND\n",
							",   CURRENT_TIMESTAMP AS EXTRACT_DT\n",
							",   CURRENT_TIMESTAMP AS REC_START_DT\n",
							",   '9999-12-31 00:00:00.000' as REC_END_DT\n",
							",   'ERDM' AS SOURCE_SYSTEM\n",
							",    column_hash as REC_CHECKSUM\n",
							",    'U' as REC_STATUS\n",
							",    current_timestamp as IMG_LST_UPD_DT\n",
							",    CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							",   'Y' AS ACTIVE_IN_SOURCE_IND  \n",
							"from {} A \n",
							"WHERE LOWER({}) = LOWER(existing_{}) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"e=spark.sql(qry_ins_changed_rows.format(tablename,stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Soft Deletes - Update Active_Ind for the rows no longer active in source:\n",
							"\n",
							"qry_4_upd_deleted_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"\n",
							";\n",
							"\"\"\"\n",
							"g=spark.sql(qry_4_upd_deleted_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select 'Number of Active Rows' as Title,count(*) from etlhubconfirmed.dht_INDUSTRY_SECTOR where CURRENT_IND='Y'\n",
							";\n",
							"\n",
							"\n",
							"select 'Duplicate Rows based on surrogate key' as Title,INDUSTRY_SECTOR_KEY,count(*) from etlhubconfirmed.dht_INDUSTRY_SECTOR where CURRENT_IND='Y'\n",
							"    group by INDUSTRY_SECTOR_KEY\n",
							"    having count(*)>1\n",
							";\n",
							"\n",
							"select 'Duplicate Rows based on natural key' as Title,code,count(*) from etlhubconfirmed.dht_INDUSTRY_SECTOR where CURRENT_IND='Y'\n",
							"    group by code\n",
							"    having count(*)>1\n",
							";\n",
							"\n",
							"select 'New rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_INDUSTRY_SECTOR where CURRENT_IND='Y'\n",
							"    and date(extract_dt)=current_date and rec_status='I'\n",
							";\n",
							"\n",
							"select 'Changed rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_INDUSTRY_SECTOR where CURRENT_IND='Y'\n",
							"    and date(extract_dt)=current_date and rec_status='U'\n",
							";\n",
							"\n",
							"select 'Changed rows updated in this run' as Title,count(*) from etlhubconfirmed.dht_INDUSTRY_SECTOR where CURRENT_IND='N'\n",
							"    and date(REC_END_DT)=current_date --and rec_status='I'\n",
							";\n",
							"\n",
							"select 'Rows no longer active in source' as Title,count(*) from etlhubconfirmed.dht_INDUSTRY_SECTOR where CURRENT_IND='Y'\n",
							"    AND \n",
							"     date(REC_END_DT)=current_date --and rec_status='I'\n",
							";\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT COUNT(*) FROM etlhubconfirmed.dht_INDUSTRY_SECTOR\n",
							"WHERE CURRENT_IND='N' and DATE(REC_END_DT)<>'9999-12-31'\n",
							"    and date(img_lst_upd_dt)=current_date\n",
							"        limit 1\n",
							";\n",
							"\n",
							"\n",
							"select count(*) from etlhubconfirmed.dht_INDUSTRY_SECTOR where CURRENT_IND='Y'\n",
							"   and date(extract_dt)=current_date --and rec_status='I'\n",
							";\n",
							"\n",
							"select count(*) from etlhubconfirmed.dht_INDUSTRY_SECTOR\n",
							"WHERE ACTIVE_IN_SOURCE_IND='N'\n",
							"    and date(IMG_LST_UPD_DT)=current_date\n",
							"    ;\n",
							"UPDATE etlhubconfirmed.dht_INDUSTRY_SECTOR\n",
							"set CURRENT_IND='Y', \n",
							"REC_END_DT='9999-12-31'\n",
							"  WHERE CURRENT_IND='N' and DATE(REC_END_DT)<>'9999-12-31'\n",
							"    and date(img_lst_upd_dt)=current_date\n",
							"        ;  "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} WHERE CURRENT_IND='Y' GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT * FROM etlhubconfirmed.dht_INDUSTRY_SECTOR where Code ='008887~613'\n",
							"    --'008915~613'\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 61
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_dht_market_load')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "de8a60e1-3324-4cf4-8df3-ca6ab1dbaa97"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Market/Customer Dimension load**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubconfirmed.dht_market by sourceing data from ESA Kyndryl.\n",
							"Since ESA is not accessible from Azure, a datastage job src_ESA_Market_Azure is executed on datastage to push the ESA extract to adls.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"from pyspark.sql.functions import col\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"import os\n",
							"import sys\n",
							"from datetime import datetime"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 133
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/ESA/' # fill in your relative folder path \n",
							"relative_archive_path='archive/'\n",
							"file_name='DS_MARKET_DATA.csv' \n",
							"#file_name='DS_MARKET_DATA_2022-07-25-08:05:43_AM.csv' \n",
							"natural_key=\"MARKET_BUSINESS_CD\"\n",
							"tablename=\"etlhubConfirmed.dht_market\"\n",
							"stagingtable=\"DHTS_MARKET\"\n",
							"keycolumn=\"MARKET_KEY\"\n",
							"date = datetime.now().strftime(\"%Y-%m-%d-%I:%M:%S_%p\")\n",
							"print(date)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 134
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Copy source CSV file to archive folder including timestamp in file name"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/ESA/DS_MARKET_DATA.csv"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"adls_arch_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_archive_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"relative_archive_path\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"arch_fil_name=file_name.rsplit(\".\")\n",
							"csv_archive_path = adls_arch_path + arch_fil_name[0] + '_' + date + '.' + arch_fil_name[1]\n",
							"print (csv_archive_path)\n",
							"\n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\n",
							"\n",
							"incrementalData_DF.write.csv(csv_archive_path, header = 'true')\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"#Add checks for duplicate data, Check for 1. dups in source data based on natural Key and\n",
							"# 2. Dups in target already\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 135
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Create a dataframe with target deltalake table data with necessary columns\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\n",
							"#existingDataDF=spark.sql(\"SELECT tgt.* from etlhubConfirmed.DHT_EMPLOYEE tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\n",
							"\n",
							"#existingMaxKeyDF=spark.sql(\"SELECT MAX(EMPLOYEE_KEY) existing_MAX_KEY from etlhubConfirmed.DHT_EMPLOYEE tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1, col(natural_key) == col(\"existing_\" + natural_key), \"fullouter\")\n",
							"\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"fullJoin2.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .option(\"overwriteSchema\", \"true\") \\\n",
							"  .option(\"path\", \"abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/staging/{}\".format(stagingtable)) \\\n",
							"  .saveAsTable(stagingtable)\n",
							"   #.option(\"overwriteSchema\", \"true\") \\  .save(\"\") \\"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 136
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Data Validation Checks :\n",
							"\n",
							"- Check for duplicates in Source data based on natural key\n",
							"- Check for duplicates in target delta lake table based on natural key\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} WHERE CURRENT_IND='Y' GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"df3=spark.sql(qry3.format(natural_key,natural_key))\n",
							"cnt1=df3.count()\n",
							"\n",
							"print (cnt1)\n",
							"if cnt1 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in source:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(1)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 137
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry_4_recon1=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable,tablename,natural_key,natural_key))\n",
							"print('New records or Inserts are:') \n",
							"df_4_recon1.show()\n",
							"\n",
							"qry_4_recon2=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE LOWER({}) = LOWER(existing_{}) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon2=spark.sql(qry_4_recon2.format(stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))\n",
							"print('Changed records or Updates are:') \n",
							"df_4_recon2.show()\n",
							"\n",
							"qry_4_recon3=\"\"\"\n",
							"SELECT COUNT(*) from \n",
							"{} A\n",
							"join fullJoin B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"\n",
							"\"\"\"\n",
							"\n",
							"df_4_recon3=spark.sql(qry_4_recon3.format(tablename,natural_key,natural_key,natural_key))\n",
							"print('Soft Deletes are:') \n",
							"df_4_recon3.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 138
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT count(*) FROM etlhubconfirmed.dht_market where current_ind='Y'\n",
							";\n",
							"SELECT MARKET_BUSINESS_CD,MARKET_VERSION,CURRENT_IND,REC_START_DT,REC_END_DT,EXTRACT_DT,IMG_LST_UPD_DT,IMG_CREATED_DT,ACTIVE_IN_SOURCE_IND\n",
							" FROM etlhubconfirmed.dht_market A WHERE --CURRENT_IND='N' \n",
							"MARKET_BUSINESS_CD  IN ('008887~613','008915~613')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 139
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"#delete FROM etlhubconfirmed.dht_employee;\n",
							"\n",
							"qry_4_recon1=\"\"\"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable, tablename, natural_key, natural_key))\n",
							"\n",
							"qry_ins_new_rows=\"\"\"\n",
							"\n",
							"INSERT INTO {}\n",
							"\tselect \n",
							"\tCOALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS {},1 as VERSION \n",
							",\tCUSTOMER_NO\n",
							",\tCUSTOMER_NM\n",
							",\tINAC_CD\n",
							",\tINAC_DESC\n",
							",\tCOUNTRY_CD\n",
							",\tMARKET_BUSINESS_CD\n",
							",\tSMB_IND\n",
							",\tCOMPANY_NO\n",
							",\tENTERPRISE_NM\n",
							",\tENTERPRISE_NO\n",
							",\tCUST_CITY_NM\n",
							",\tCUST_STATE_PROV_CD\n",
							",\tCUST_POSTL_CD\n",
							",\tISO_COUNTRY_CD\n",
							",\tISO_COUNTRY_DESC\n",
							",\tCMR_ISU_CD\n",
							",\tSUB_INDUSTRY_CD\n",
							",\tCURR_CVRG_TYP_CD\n",
							",\tCURR_CVRG_TYP_DESC\n",
							",\tCURR_CVRG_ID\n",
							",\tCURR_CVRG_NM\n",
							",\tDLGTD_CVRG_IND\n",
							",\tBASE_CVRG_TYP_CD\n",
							",\tBASE_CVRG_TYP_DESC\n",
							",\tBASE_CVRG_ID\n",
							",\tBASE_CVRG_NM\n",
							",\tTERR_ID\n",
							",\tTERR_NM\n",
							",\tDBCS_CUSTOMER_NM\n",
							",\tCLIENT_ID\n",
							",\tCLIENT_NM\n",
							",\tCVI_BRNCH_ID\n",
							",\tCVI_BRNCH_DESC\n",
							",\tCVI_BRNCH_GRP_CD\n",
							",\tCVI_BRNCH_GRP_DESC\n",
							",\tCVI_BRNCH_UNT_ID\n",
							",\tCVI_BRNCH_UNT_NM\n",
							",\tSMB_QUADTIER_CD\n",
							",\tCUST_ACCT_ID\n",
							",\tRPTS_TO_IMT_ID\n",
							",\tRPTS_TO_SCTR_CD\n",
							",\tRPTS_TO_SCTR_DESC\n",
							"\t,'Y' AS CURRENT_IND\n",
							"\t,CURRENT_TIMESTAMP AS EXTRACT_DT\n",
							"\t,CURRENT_TIMESTAMP AS REC_START_DT\n",
							"\t,'9999-12-31 00:00:00.000' as REC_END_DT\n",
							"\t\n",
							"\t,column_hash as REC_CHECKSUM\n",
							"\t,'I' as REC_STATUS\n",
							"\t,'ESA' AS SOURCE_SYSTEM\n",
							",\tCUST_INDUSTRY_SECTOR_KEY\n",
							",\tCUST_INDUSTRY_SECTOR_VERSION\n",
							",\tTERRITORY_RGN_KEY\n",
							",\tREPORTS_TO_IMT_GEO_KEY\n",
							",\tFIN_COUNTRY_KEY\n",
							"\t,current_timestamp as IMG_LST_UPD_DT\n",
							"\t,CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							",\tBOID\n",
							",\tDOMESTIC_CLIENT_ID\n",
							",\tDOMESTIC_CLIENT_NM\n",
							",\tGLOBAL_CLIENT_ID\n",
							",\tGLOBAL_CLIENT_NM\n",
							",\tGLOBAL_ULTIMATE_CLIENT_ID\n",
							",\tGLOBAL_ULTIMATE_CLIENT_NM\n",
							",\tDOMESTIC_BUYING_GROUP_ID\n",
							",\tDOMESTIC_BUYING_GROUP_NM\n",
							",\tGLOBAL_BUYING_GROUP_ID\n",
							",\tGLOBAL_BUYING_GROUP_NM\n",
							",\tTOP_TREE_ACCT_ID\n",
							",\tTOP_TREE_ID\n",
							",\tTOP_TREE_NM\n",
							",\t'' AS STANDARD_INDUSTRIAL_CLASSIFICATION_CD\n",
							",\tCOV_CLIENT_TYPE\n",
							",\tCOV_CLIENT_TYPE_DESC\n",
							",\tCOV_CLIENT_SUBTYPE\n",
							",\tCOV_CLIENT_SUBTYPE_DESC\n",
							"\t,'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"    from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"d=spark.sql(qry_ins_new_rows.format(tablename,keycolumn,stagingtable,tablename,natural_key, natural_key ))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 140
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT MARKET_BUSINESS_CD,MARKET_VERSION,CURRENT_IND,REC_START_DT,REC_END_DT,EXTRACT_DT,IMG_LST_UPD_DT,IMG_CREATED_DT,ACTIVE_IN_SOURCE_IND\n",
							" FROM etlhubconfirmed.dht_market A WHERE --CURRENT_IND='N' \n",
							"MARKET_BUSINESS_CD  IN ('008887~613','008915~613')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 141
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT extract_dt,count(*) FROM etlhubconfirmed.dht_market where current_ind='Y' group by extract_dt  \n",
							"--and extract_dt like '2022-07-29-02:16%'\n",
							"    --2022-07-29T14:17:38Z"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 142
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT count(*) FROM etlhubconfirmed.dht_market where current_ind='Y'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 143
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Update or expire the rows with existing instance of the changed rows:\n",
							"qry_4_upd_changes_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.{}\n",
							"AND LOWER(B.{}) = LOWER(B.existing_{}) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= REC_START_DT -  INTERVAL 1 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\"\"\"\n",
							"f=spark.sql(qry_4_upd_changes_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key,natural_key))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 144
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT MARKET_BUSINESS_CD,MARKET_VERSION,CURRENT_IND,REC_START_DT,REC_END_DT,EXTRACT_DT,IMG_LST_UPD_DT,IMG_CREATED_DT,ACTIVE_IN_SOURCE_IND\n",
							" FROM etlhubconfirmed.dht_market A WHERE --CURRENT_IND='N' \n",
							"MARKET_BUSINESS_CD  IN ('008887~613','008915~613')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 145
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select * from  etlhubconfirmed.dht_market where rec_end_dt= rec_start_dt - INTERVAL 1 seconds\n",
							"--and extract_dt like '2022-07-29-02:16%'\n",
							"    --2022-07-29T14:17:38Z"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 146
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--existing data\n",
							"select 'existing data',count(*) from DHTS_MARKET where existing_CURRENT_IND='Y';\n",
							"  \n",
							"select 'rows for Update', count(*)\n",
							"from etlhubConfirmed.DHT_MARKET B\n",
							"join\n",
							"DHTS_MARKET A \n",
							"WHERE A.MARKET_BUSINESS_CD = B.MARKET_BUSINESS_CD\n",
							"and LOWER(A.MARKET_BUSINESS_CD) = LOWER(A.existing_MARKET_BUSINESS_CD) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND B.REC_START_DT=A.existing_REC_START_DT\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.DHT_MARKET B\n",
							"WHERE LOWER(A.MARKET_BUSINESS_CD)=LOWER(B.MARKET_BUSINESS_CD)\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"SELECT COUNT(*) FROM etlhubConfirmed.DHT_MARKET A\n",
							"JOIN DHTS_MARKET B\n",
							"ON A.MARKET_BUSINESS_CD = B.MARKET_BUSINESS_CD\n",
							"AND LOWER(B.MARKET_BUSINESS_CD) = LOWER(B.existing_MARKET_BUSINESS_CD) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							";"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 147
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Insert for changed rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry_ins_changed_rows=\"\"\"\n",
							"INSERT INTO {}\n",
							"\tselect \n",
							"    existing_MARKET_KEY\n",
							"    ,1 + existing_MARKET_VERSION as VERSION\n",
							",\tCUSTOMER_NO\n",
							",\tCUSTOMER_NM\n",
							",\tINAC_CD\n",
							",\tINAC_DESC\n",
							",\tCOUNTRY_CD\n",
							",\tMARKET_BUSINESS_CD\n",
							",\tSMB_IND\n",
							",\tCOMPANY_NO\n",
							",\tENTERPRISE_NM\n",
							",\tENTERPRISE_NO\n",
							",\tCUST_CITY_NM\n",
							",\tCUST_STATE_PROV_CD\n",
							",\tCUST_POSTL_CD\n",
							",\tISO_COUNTRY_CD\n",
							",\tISO_COUNTRY_DESC\n",
							",\tCMR_ISU_CD\n",
							",\tSUB_INDUSTRY_CD\n",
							",\tCURR_CVRG_TYP_CD\n",
							",\tCURR_CVRG_TYP_DESC\n",
							",\tCURR_CVRG_ID\n",
							",\tCURR_CVRG_NM\n",
							",\tDLGTD_CVRG_IND\n",
							",\tBASE_CVRG_TYP_CD\n",
							",\tBASE_CVRG_TYP_DESC\n",
							",\tBASE_CVRG_ID\n",
							",\tBASE_CVRG_NM\n",
							",\tTERR_ID\n",
							",\tTERR_NM\n",
							",\tDBCS_CUSTOMER_NM\n",
							",\tCLIENT_ID\n",
							",\tCLIENT_NM\n",
							",\tCVI_BRNCH_ID\n",
							",\tCVI_BRNCH_DESC\n",
							",\tCVI_BRNCH_GRP_CD\n",
							",\tCVI_BRNCH_GRP_DESC\n",
							",\tCVI_BRNCH_UNT_ID\n",
							",\tCVI_BRNCH_UNT_NM\n",
							",\tSMB_QUADTIER_CD\n",
							",\tCUST_ACCT_ID\n",
							",\tRPTS_TO_IMT_ID\n",
							",\tRPTS_TO_SCTR_CD\n",
							",\tRPTS_TO_SCTR_DESC\n",
							",'Y' AS CURRENT_IND\n",
							"\t,CURRENT_TIMESTAMP AS EXTRACT_DT\n",
							"\t,existing_REC_START_DT AS REC_START_DT\n",
							"\t,'9999-12-31 00:00:00.000' as REC_END_DT\t\n",
							"\t,column_hash as REC_CHECKSUM\n",
							"\t,'U' as REC_STATUS\n",
							"\t,'ESA' AS SOURCE_SYSTEM\n",
							",\tCUST_INDUSTRY_SECTOR_KEY\n",
							",\tCUST_INDUSTRY_SECTOR_VERSION\n",
							",\tTERRITORY_RGN_KEY\n",
							",\tREPORTS_TO_IMT_GEO_KEY\n",
							",\tFIN_COUNTRY_KEY\n",
							"\t,current_timestamp as IMG_LST_UPD_DT\n",
							"\t,CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							",\tBOID\n",
							",\tDOMESTIC_CLIENT_ID\n",
							",\tDOMESTIC_CLIENT_NM\n",
							",\tGLOBAL_CLIENT_ID\n",
							",\tGLOBAL_CLIENT_NM\n",
							",\tGLOBAL_ULTIMATE_CLIENT_ID\n",
							",\tGLOBAL_ULTIMATE_CLIENT_NM\n",
							",\tDOMESTIC_BUYING_GROUP_ID\n",
							",\tDOMESTIC_BUYING_GROUP_NM\n",
							",\tGLOBAL_BUYING_GROUP_ID\n",
							",\tGLOBAL_BUYING_GROUP_NM\n",
							",\tTOP_TREE_ACCT_ID\n",
							",\tTOP_TREE_ID\n",
							",\tTOP_TREE_NM\n",
							",\t'' AS STANDARD_INDUSTRIAL_CLASSIFICATION_CD\n",
							",\tCOV_CLIENT_TYPE\n",
							",\tCOV_CLIENT_TYPE_DESC\n",
							",\tCOV_CLIENT_SUBTYPE\n",
							",\tCOV_CLIENT_SUBTYPE_DESC\n",
							"\t,'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from {} A \n",
							"WHERE LOWER({}) = LOWER(existing_{}) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"e=spark.sql(qry_ins_changed_rows.format(tablename,stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 148
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT MARKET_BUSINESS_CD,MARKET_VERSION,CURRENT_IND,REC_START_DT,REC_END_DT,EXTRACT_DT,IMG_LST_UPD_DT,IMG_CREATED_DT,ACTIVE_IN_SOURCE_IND\n",
							" FROM etlhubconfirmed.dht_market A WHERE --CURRENT_IND='N' \n",
							"MARKET_BUSINESS_CD  IN ('008887~613','008915~613')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 149
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT extract_dt,count(*) FROM etlhubconfirmed.dht_market where current_ind='Y' group by extract_dt  \n",
							"--and extract_dt like '2022-07-29-02:16%'\n",
							"    --2022-07-29T14:17:38Z"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 150
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Soft Deletes - Update Active_Ind for the rows no longer active in source:\n",
							"\n",
							"qry_4_upd_deleted_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"\n",
							";\n",
							"\"\"\"\n",
							"g=spark.sql(qry_4_upd_deleted_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 151
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT MARKET_BUSINESS_CD,MARKET_VERSION,CURRENT_IND,REC_START_DT,REC_END_DT,EXTRACT_DT,IMG_LST_UPD_DT,IMG_CREATED_DT,ACTIVE_IN_SOURCE_IND\n",
							" FROM etlhubconfirmed.dht_market A WHERE --CURRENT_IND='N' \n",
							"MARKET_BUSINESS_CD  IN ('008887~613','008915~613')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 152
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT extract_dt,REC_START_DT,REC_END_DT,CURRENT_IND,ACTIVE_IN_SOURCE_IND,count(*) \n",
							"FROM etlhubconfirmed.dht_market group by extract_dt,REC_START_DT,REC_END_DT,CURRENT_IND,ACTIVE_IN_SOURCE_IND\n",
							"--and extract_dt like '2022-07-29-02:16%'\n",
							"    --2022-07-29T14:17:38Z"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 153
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select 'Number of Active Rows' as Title,count(*) from etlhubconfirmed.dht_market where CURRENT_IND='Y'\n",
							";\n",
							"\n",
							"\n",
							"select 'Duplicate Rows based on surrogate key' as Title,market_KEY,count(*) from etlhubconfirmed.dht_market where CURRENT_IND='Y'\n",
							"    group by market_KEY\n",
							"    having count(*)>1\n",
							";\n",
							"\n",
							"select 'Duplicate Rows based on natural key' as Title,market_BUSINESS_CD,count(*) from etlhubconfirmed.dht_market where CURRENT_IND='Y'\n",
							"    group by market_BUSINESS_CD\n",
							"    having count(*)>1\n",
							";\n",
							"\n",
							"select 'New rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_market where CURRENT_IND='Y'\n",
							"    and date(extract_dt)=current_date and rec_status='I'\n",
							";\n",
							"\n",
							"select 'Changed rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_market where CURRENT_IND='Y'\n",
							"    and date(extract_dt)=current_date and rec_status='U'\n",
							";\n",
							"\n",
							"select 'Changed rows updated in this run' as Title,count(*) from etlhubconfirmed.dht_market where CURRENT_IND='N'\n",
							"    and date(REC_END_DT)=current_date --and rec_status='I'\n",
							";\n",
							"\n",
							"select 'Rows no longer active in source' as Title,count(*) from etlhubconfirmed.dht_market where CURRENT_IND='Y'\n",
							"    AND \n",
							"     date(REC_END_DT)=current_date --and rec_status='I'\n",
							";\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 154
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT COUNT(*) FROM etlhubconfirmed.dht_market\n",
							"WHERE CURRENT_IND='N' and DATE(REC_END_DT)<>'9999-12-31'\n",
							"    and date(img_lst_upd_dt)=current_date\n",
							"        limit 1\n",
							";\n",
							"\n",
							"\n",
							"select count(*) from etlhubconfirmed.dht_market where CURRENT_IND='Y'\n",
							"   and date(extract_dt)=current_date --and rec_status='I'\n",
							";\n",
							"\n",
							"select count(*) from etlhubconfirmed.dht_market\n",
							"WHERE ACTIVE_IN_SOURCE_IND='N'\n",
							"    and date(IMG_LST_UPD_DT)=current_date\n",
							"    ;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 155
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT MARKET_BUSINESS_CD,MARKET_VERSION,CURRENT_IND,REC_START_DT,REC_END_DT,EXTRACT_DT,IMG_LST_UPD_DT,IMG_CREATED_DT,ACTIVE_IN_SOURCE_IND\n",
							" FROM etlhubconfirmed.dht_market A WHERE --CURRENT_IND='N' \n",
							"MARKET_BUSINESS_CD  IN ('008887~613','008915~613')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 156
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} WHERE CURRENT_IND='Y' GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 157
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT * FROM etlhubconfirmed.dht_market WHERE --CURRENT_IND='N' \n",
							"MARKET_BUSINESS_CD ='008887~613'\n",
							"    --'008915~613'\n",
							"        --71588\n",
							"        --71590\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 158
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_dht_office_space')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts/GREIW"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPool001494New",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "472g",
					"driverCores": 80,
					"executorMemory": "472g",
					"executorCores": 80,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "eaf0a8b2-7e93-450f-846f-7b5288d0ace4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPool001494New",
						"name": "sPool001494New",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPool001494New",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 80,
						"memory": 472,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Office Space load**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubConfirmed.DHT_OFFICE_SPACE by sourcing data from Tririga system.\n",
							"The file is pulled to datastage server using SFTP protocol.\n",
							"\n",
							"The CSV formatted file is uploaded to adls for loading to OFFICE_SPACE deltalake table created.\n",
							"\n",
							"The script used for file transfer is /home/resodba/trrgprocget.sh\n",
							"\n",
							"File Name: resodb_space_tririga_staging.csv from /GlobalDir/CustomerFiles/tririga"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws,trim\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"from pyspark.sql.functions import col\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"import os\n",
							"import sys\n",
							"from datetime import datetime"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/Tririga/OfficeSpace/' # fill in your relative folder path \n",
							"relative_archive_path='archive/'\n",
							"file_name='ResodbOfficeSpaceTririgaStaging.csv' # 20220705, 20220705_upd1, 20220705_upd2\n",
							"natural_key=\"BUSINESS_ID\"\n",
							"MinimumTririgaCampusCount=450\n",
							"tablename=\"etlhubConfirmed.DHT_OFFICE_SPACE\"\n",
							"stagingtable=\"DHTS_OFFICE_SPACE\"\n",
							"keycolumn=\"SPACE_KEY\"\n",
							"date = datetime.now().strftime(\"%Y-%m-%d-%I:%M:%S_%p\")\n",
							"extract_dt = datetime.now().strftime(\"%Y-%m-%d %I:%M:%S\") #This is batch/cycle date. It would be common for all the rows inserted in a single run\n",
							"rec_start_dt=datetime.now().strftime(\"%Y-%m-%d %I:%M:%S\") #This should ideally from the source system. If source doesnt have any date column, use current date \n",
							"print(extract_dt)\n",
							"print(natural_key)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Copy source CSV file to archive folder including timestamp in file name"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/Tririga/campus/IWTRIRIGACampusQuery.csv\n",
							"\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"adls_arch_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_archive_path) \n",
							"print('Primary archive path: ' + adls_arch_path) \n",
							"\n",
							"print(natural_key)\n",
							"\n",
							"relative_archive_path\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"arch_fil_name=file_name.rsplit(\".\")\n",
							"csv_archive_path = adls_arch_path + arch_fil_name[0] + '_' + date + '.' + arch_fil_name[1]\n",
							"\n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\n",
							"\n",
							"incrementalData_DF.createOrReplaceTempView('SrcData')\n",
							"\n",
							"# Capturing recods that are not available in reference table\n",
							"qry_RefIntegrityFailDF=\"\"\"\n",
							"select A.*\n",
							"from SrcData A\n",
							"left outer join etlhubconfirmed.dhtr_space_floor_level B on trim(A.FLOOR_ID)=trim(B.FLOOR_ID) \n",
							"where B.FLOOR_ID IS NULL;\n",
							"\"\"\"\n",
							"RejDataDF=spark.sql(qry_RefIntegrityFailDF)\n",
							"RejDataDF.createOrReplaceTempView('RejDataView')\n",
							"\n",
							"qry_RefIntegrityFailSpaceClassDF=\"\"\"\n",
							"select A.*\n",
							"from SrcData A\n",
							"left outer join etlhubconfirmed.dhtr_space_class C ON \n",
							"trim(A.SPACE_CLASS_CODE1)=trim(C.SPACE_CLASS_CODE1) AND trim(A.SPACE_CLASS_CODE2)=trim(C.SPACE_CLASS_CODE2)\n",
							"where C.SPACE_CLASS_CODE1 IS NULL;\n",
							"\"\"\"\n",
							"RejSpaceClassDataDF=spark.sql(qry_RefIntegrityFailSpaceClassDF)\n",
							"RejSpaceClassDataDF.createOrReplaceTempView('RejSpaceClassDataView')\n",
							"\n",
							"qry_IncrementalDataDF=\"\"\"\n",
							"select distinct CAST(COALESCE(TRIM(A.CAMPUS_ID),'')||'~'||COALESCE(TRIM(A.BLDG_ID),'')||'~'||COALESCE(TRIM(A.FLOOR_ID),'')||'~'||COALESCE(TRIM(A.SPACE_ID),'') AS VARCHAR(64)) as BUSINESS_ID,\n",
							"A.CAMPUS_ID,A.BLDG_ID as BUILDING_ID, A.FLOOR_ID, B.FLOOR_DESCR as FLOOR_DESCRIPTION,\n",
							"A.SPACE_ID,A.SPACE_DESCR as SPACE_DESCRIPTION, A.SPACE_CLASS_CODE1 as SPACE_CATEGORY_CODE,\n",
							"CASE WHEN LOCATE('-', trim(C.SPACE_CLASS_DESCR)) = 0 THEN C.SPACE_CLASS_DESCR \n",
							"ELSE substr(trim(C.SPACE_CLASS_DESCR),1,LOCATE('-', trim(C.SPACE_CLASS_DESCR))-1) \n",
							"END AS SPACE_CATEGORY_DESCRIPTION,A.SPACE_CLASS_CODE2 as SPACE_TYPE_CODE,\n",
							"CASE WHEN LOCATE('-', trim(C.SPACE_CLASS_DESCR)) = 0 THEN C.SPACE_CLASS_DESCR \n",
							"ELSE substr(trim(C.SPACE_CLASS_DESCR),1,LOCATE('-', trim(C.SPACE_CLASS_DESCR))-1) \n",
							"END AS SPACE_TYPE_DESCRIPTION,C.OCCUPANCY_TYPE , A.CAPACITY,A.AREA as AREA_SQUARE_FEET,\n",
							"A.OSCRE_CODE,A.WORKPOINT\n",
							"from SrcData A\n",
							"left outer join etlhubconfirmed.dhtr_space_floor_level B on trim(A.FLOOR_ID)=trim(B.FLOOR_ID)\n",
							"left outer join etlhubconfirmed.dhtr_space_class C ON \n",
							"trim(A.SPACE_CLASS_CODE1)=trim(C.SPACE_CLASS_CODE1) AND trim(A.SPACE_CLASS_CODE2)=trim(C.SPACE_CLASS_CODE2)\n",
							"where B.FLOOR_ID IS NOT NULL AND C.SPACE_CLASS_CODE1 IS NOT NULL \n",
							"\"\"\"\n",
							"FinalIncrementalDataDF=spark.sql(qry_IncrementalDataDF)\n",
							"\n",
							"col_list=[]\n",
							"for i in FinalIncrementalDataDF.columns:\n",
							"    col_list.append(i)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1 = FinalIncrementalDataDF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from incrementalData_DF2"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"\r\n",
							"qry_IncrementalDataDF=\"\"\"\r\n",
							"select distinct CAST(COALESCE(TRIM(A.CAMPUS_ID),'')||'~'||COALESCE(TRIM(A.BLDG_ID),'')||'~'||COALESCE(TRIM(A.FLOOR_ID),'')||'~'||COALESCE(TRIM(A.SPACE_ID),'') AS VARCHAR(64)) as BUSINESS_ID,\r\n",
							"A.CAMPUS_ID,A.BLDG_ID as BUILDING_ID, A.FLOOR_ID, B.FLOOR_DESCR as FLOOR_DESCRIPTION,\r\n",
							"A.SPACE_ID,A.SPACE_DESCR as SPACE_DESCRIPTION, A.SPACE_CLASS_CODE1 as SPACE_CATEGORY_CODE,\r\n",
							"CASE WHEN LOCATE('-', trim(C.SPACE_CLASS_DESCR)) = 0 THEN C.SPACE_CLASS_DESCR \r\n",
							"ELSE substr(trim(C.SPACE_CLASS_DESCR),1,LOCATE('-', trim(C.SPACE_CLASS_DESCR))-1) \r\n",
							"END AS SPACE_CATEGORY_DESCRIPTION,A.SPACE_CLASS_CODE2 as SPACE_TYPE_CODE,\r\n",
							"CASE WHEN LOCATE('-', trim(C.SPACE_CLASS_DESCR)) = 0 THEN C.SPACE_CLASS_DESCR \r\n",
							"ELSE substr(trim(C.SPACE_CLASS_DESCR),1,LOCATE('-', trim(C.SPACE_CLASS_DESCR))-1) \r\n",
							"END AS SPACE_TYPE_DESCRIPTION,C.OCCUPANCY_TYPE , A.CAPACITY,A.AREA as AREA_SQUARE_FEET,\r\n",
							"A.OSCRE_CODE,A.WORKPOINT\r\n",
							"from SrcData A\r\n",
							"left outer join etlhubconfirmed.dhtr_space_floor_level B on trim(A.FLOOR_ID)=trim(B.FLOOR_ID)\r\n",
							"left outer join etlhubconfirmed.dhtr_space_class C ON \r\n",
							"trim(A.SPACE_CLASS_CODE1)=trim(C.SPACE_CLASS_CODE1) AND trim(A.SPACE_CLASS_CODE2)=trim(C.SPACE_CLASS_CODE2)\r\n",
							"where B.FLOOR_ID IS NOT NULL AND C.SPACE_CLASS_CODE1 IS NOT NULL \r\n",
							"\"\"\"\r\n",
							"FinalIncrementalDataDF=spark.sql(qry_IncrementalDataDF)\r\n",
							"\r\n",
							"col_list=[]\r\n",
							"for i in FinalIncrementalDataDF.columns:\r\n",
							"    col_list.append(i)\r\n",
							"\r\n",
							"# Add a checsum column to help identify the changed rows\r\n",
							"\r\n",
							"incrementalData_DF1 = FinalIncrementalDataDF.withColumn(\"nk_hash\",md5(natural_key))\r\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\r\n",
							"\r\n",
							"#sell_cyclecolhashDF.show()\r\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Create a dataframe with target deltalake table data with necessary columns\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\n",
							"#existingDataDF=spark.sql(\"SELECT tgt.* from etlhubConfirmed.DHT_EMPLOYEE tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\n",
							"\n",
							"#existingMaxKeyDF=spark.sql(\"SELECT MAX(EMPLOYEE_KEY) existing_MAX_KEY from etlhubConfirmed.DHT_EMPLOYEE tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1, col(natural_key) == col(\"existing_\" + natural_key), \"fullouter\")\n",
							"\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"fullJoin2.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .option(\"overwriteSchema\", \"true\") \\\n",
							"  .option(\"path\", \"abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/staging/{}\".format(stagingtable)) \\\n",
							"  .saveAsTable(stagingtable)\n",
							"   #.option(\"overwriteSchema\", \"true\") \\  .save(\"\") \\"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Data Validation Checks :\n",
							"\n",
							"- Check for duplicates in Source data based on natural key\n",
							"- Check for partial data from source. That is if the file has expected number of rows\n",
							"- Check for duplicates in target delta lake table based on natural key\n",
							"- Referential Integrity checks\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} WHERE CURRENT_IND='Y' GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry5=\"\"\"\n",
							"SELECT * FROM incrementalData_DF2\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"df3=spark.sql(qry3.format(natural_key,natural_key))\n",
							"cnt1=df3.count()\n",
							"\n",
							"print (cnt1)\n",
							"if cnt1 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in source:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    --sys.exit(1)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"\n",
							"df5=spark.sql(qry5)\n",
							"cnt3=df5.count()\n",
							"\n",
							"if cnt3 >= MinimumTririgaCampusCount:\n",
							"    print(\"The number of rows in source data is more than the threshold, data can be processed\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"The source data is having less number of rows than expected. Please check:\" )\n",
							"    print(cnt3)\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							"\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry_4_recon1=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable,tablename,natural_key,natural_key))\n",
							"print('New records or Inserts are:') \n",
							"df_4_recon1.show()\n",
							"\n",
							"qry_4_recon2=\"\"\"\n",
							"\n",
							"SELECT count(*) from {} A \n",
							"WHERE LOWER({}) = LOWER(existing_{}) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon2=spark.sql(qry_4_recon2.format(stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))\n",
							"print('Changed records or Updates are:') \n",
							"df_4_recon2.show()\n",
							"\n",
							"qry_4_recon3=\"\"\"\n",
							"SELECT count(*) from \n",
							"{} A\n",
							"join fullJoin B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' --AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"\n",
							"\"\"\"\n",
							"\n",
							"df_4_recon3=spark.sql(qry_4_recon3.format(tablename,natural_key,natural_key,natural_key))\n",
							"print('Soft Deletes are:') \n",
							"df_4_recon3.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry_4_recon1=\"\"\"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable, tablename, natural_key, natural_key))\n",
							"\n",
							"qry_ins_new_rows=\"\"\"\n",
							"INSERT INTO {}\n",
							"select \n",
							"COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY {}) AS {} \n",
							",1 AS VERSION\n",
							",GEOGRAPHY_ID\n",
							",GEOGRAPHY_NAME\n",
							",REGION_ID\n",
							",REGION_NAME\n",
							",SITE_ID\n",
							",SITE_NAME\n",
							",WORLD_REGION_CODE\n",
							",WORLD_REGION_NAME\n",
							",MARKET_TEAM_REGION_CODE\n",
							",MARKET_TEAM_REGION_NAME\n",
							",COUNTRY_CODE\n",
							",COUNTRY_NAME\n",
							",CAMPUS_ID\n",
							",CAMPUS_NAME\n",
							",CAMPUS_STATUS\n",
							",WORK_LOCATION_CODE\n",
							",ADDRESS, CITY\n",
							",STATE_PROVINCE_ID\n",
							",STATE_PROVINCE_NAME\n",
							",POSTAL_CODE\n",
							",LATITUDE\n",
							",LONGITUDE\n",
							",UTC_OFFSET\n",
							",ICU_TIME_ZONE\n",
							",PEOPLE_HOUSED_FLAG\n",
							",REMOTE_SUPPORT_FLAG\n",
							",PRIMARY_CAMPUS_USE_NAME\n",
							",PRIMARY_CAMPUS_USE_DESCR\n",
							",CAMPUS_OWNERSHIP\n",
							",CAMPUS_ACTIVATION_YEAR\n",
							",CAMPUS_INACTIVATION_YEAR\n",
							",   'Y' AS CURRENT_IND\n",
							",   '{}' AS EXTRACT_DT\n",
							",   '{}' AS REC_START_DT\n",
							",   '9999-12-31 00:00:00' as REC_END_DT\n",
							",   'Tririga' AS SOURCE_SYSTEM\n",
							",    column_hash as REC_CHECKSUM\n",
							",    CURRENT_TIMESTAMP as IMG_LST_UPD_DT\n",
							",    CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							"    from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"d=spark.sql(qry_ins_new_rows.format(tablename,natural_key,keycolumn,extract_dt,rec_start_dt,stagingtable,tablename,natural_key, natural_key))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"qry_ins_new_rows_test=\"\"\"\n",
							"\n",
							"\tselect \n",
							"\tCOALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY {}) AS {} \n",
							",\t1 AS VERSION\n",
							",{}\n",
							",   'Y' AS CURRENT_IND\n",
							",   '{}' AS EXTRACT_DT\n",
							",   '{}' AS REC_START_DT\n",
							",   '9999-12-31 00:00:00' as REC_END_DT\n",
							",   'Tririga' AS SOURCE_SYSTEM\n",
							",    column_hash as REC_CHECKSUM\n",
							",    CURRENT_TIMESTAMP as IMG_LST_UPD_DT\n",
							",    CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							"    from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"d_test=spark.sql(qry_ins_new_rows_test.format(natural_key,keycolumn,my_string,extract_dt,rec_start_dt,stagingtable,tablename,natural_key, natural_key))\n",
							"print(qry_ins_new_rows_test.format(natural_key,keycolumn,my_string,extract_dt,rec_start_dt,stagingtable,tablename,natural_key, natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Update or expire the rows with existing instance of the changed rows:\n",
							"qry_4_upd_changes_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.{}\n",
							"AND LOWER(B.{}) = LOWER(B.existing_{}) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= '{}' -  INTERVAL 1 seconds\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\"\"\"\n",
							"f=spark.sql(qry_4_upd_changes_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key,natural_key,rec_start_dt))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Insert for changed rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry_ins_changed_rows=\"\"\"\n",
							"INSERT INTO {}\n",
							"\tselect \n",
							"    existing_CAMPUS_KEY\n",
							",\t1 + existing_VERSION\n",
							",GEOGRAPHY_ID\n",
							",GEOGRAPHY_NAME\n",
							",REGION_ID\n",
							",REGION_NAME\n",
							",SITE_ID\n",
							",SITE_NAME\n",
							",WORLD_REGION_CODE\n",
							",WORLD_REGION_NAME\n",
							",MARKET_TEAM_REGION_CODE\n",
							",MARKET_TEAM_REGION_NAME\n",
							",COUNTRY_CODE\n",
							",COUNTRY_NAME\n",
							",CAMPUS_ID\n",
							",CAMPUS_NAME\n",
							",CAMPUS_STATUS\n",
							",WORK_LOCATION_CODE\n",
							",ADDRESS, CITY\n",
							",STATE_PROVINCE_ID\n",
							",STATE_PROVINCE_NAME\n",
							",POSTAL_CODE\n",
							",LATITUDE\n",
							",LONGITUDE\n",
							",UTC_OFFSET\n",
							",ICU_TIME_ZONE\n",
							",PEOPLE_HOUSED_FLAG\n",
							",REMOTE_SUPPORT_FLAG\n",
							",PRIMARY_CAMPUS_USE_NAME\n",
							",PRIMARY_CAMPUS_USE_DESCR\n",
							",CAMPUS_OWNERSHIP\n",
							",CAMPUS_ACTIVATION_YEAR\n",
							",CAMPUS_INACTIVATION_YEAR\n",
							",   'Y' AS CURRENT_IND\n",
							",   '{}' AS EXTRACT_DT\n",
							",   '{}' AS REC_START_DT\n",
							",   '9999-12-31 00:00:00' as REC_END_DT\n",
							",   'Tririga' AS SOURCE_SYSTEM\n",
							",    column_hash as REC_CHECKSUM\n",
							",    CURRENT_TIMESTAMP as IMG_LST_UPD_DT\n",
							",    existing_IMG_CREATED_DT\n",
							"from {} A \n",
							"WHERE LOWER({}) = LOWER(existing_{}) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"e=spark.sql(qry_ins_changed_rows.format(tablename,extract_dt,rec_start_dt,stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Soft Deletes - Update Active_Ind for the rows no longer active in source:\n",
							"\n",
							"qry_4_upd_deleted_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' --AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"    ,REC_END_DT= '{}' -  INTERVAL 1 seconds\n",
							"\n",
							";\n",
							"\"\"\"\n",
							"g=spark.sql(qry_4_upd_deleted_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key,rec_start_dt))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} WHERE CURRENT_IND='Y' GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_dht_opportunity_assignment_daily_ft _load')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7fc343cd-bdd1-4a77-9b47-1d01f73f0e26"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Opportunity Assignment Daily Fact**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubconfirmed.dht_opportunity_assignment_daily_ft by sourceing data from ESA database.\n",
							"Since ESA is not accessible from Azure, a datastage job srcESA_OpportunityAssignment_DailyFact_Azure is executed on datastage to push the ESA extract to adls.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws,trim\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"import os\n",
							"import sys\n",
							"from datetime import datetime"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/ESA/' # fill in your relative folder path \n",
							"relative_archive_path='archive/'\n",
							"file_name='DS_OPPORTUNITY_ASSIGNMENT_DAILY_FT_DATA.csv' \n",
							"natural_key=['CRM_ROLE_KEY','OPPORTUNITY_KEY','OPPORTUNITY_ROLE_KEY','EMPLOYEE_KEY']\n",
							"tablename=\"etlhubConfirmed.dht_opportunity_assignment_daily_ft\"\n",
							"keycolumn=\"OPPORTUNITY_ASSIGNMENT_KEY\"\n",
							"stagingtable=\"DHTS_OPPORTUNITY_ASSIGNMENT_DAILY_FT\"\n",
							"date = datetime.now().strftime(\"%Y_%m_%d-%I:%M:%S_%p\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Copy source CSV file to archive folder including timestamp in file name"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/ESA/DS_OPPORTUNITY_ASSIGNMENT_DAILY_FT_DATA.csv"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"adls_arch_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_archive_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"relative_archive_path\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"arch_fil_name=file_name.rsplit(\".\")\n",
							"csv_archive_path = adls_arch_path + arch_fil_name[0] + '_' + date + '.' + arch_fil_name[1]\n",
							"print (csv_archive_path)\n",
							"\n",
							"incrementalData_DF = spark.read.load(csv_path,format=\"csv\", sep=\"||\", header=\"true\")\n",
							"incrementalData_DF.createOrReplaceTempView('SourceView')\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"-- trying to insert opportunty number into opportunity dimension table which is available in fact but not in dimensionm table \r\n",
							"INSERT INTO etlhubConfirmed.dht_opportunity \r\n",
							"select COALESCE(max_OPPORTUNITY_KEY + ROW_NUMBER () OVER (ORDER BY 1),0) AS OPPORTUNITY_KEY,1 as OPPORTUNITY_VERSION, \r\n",
							"OPPORTUNITY_NUM,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, '#' as REC_CHECKSUM, \r\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT,\r\n",
							"'' as ISA_CODE,'' as ARCHIVE_REASON_CODE,'' as BUSINESS_TRANSACTION_TYPE,'' as CLIENT_REPRESENTATIVE_NAME,'' as COMPETITOR_LIST, \r\n",
							"'' as IDENTIFIER_NAME,'' as OPPORTUNITY_NAME,'' as OPPORTUNITY_SOURCE_CODE,'' as SBS_SOL_VALID_IND, \r\n",
							"'' as OPPORTUNITY_IDENTIFIER,'' as ROGUE_IND, '' as REASON_TO_ACT,''as SOLUTION_CATG,'' as BRAND_SPONSOR_LIST,'' as GBS_GEOGRAPHY_NAME, \r\n",
							"'' as GBS_BUSINESS_UNIT_GROUP_NAME,'' as GBS_BUSINESS_UNIT_NAME,'' as TAG_LIST,'' as OPPORTUNITY_LEGACY_NO,'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND from \r\n",
							" ( select DISTINCT src.OPPORTUNITY_NUM,max_OPPORTUNITY_KEY from \r\n",
							"(select distinct OPPORTUNITY_NUM from SourceView) as src \r\n",
							"left outer join etlhubConfirmed.dht_opportunity OPP on trim(src.OPPORTUNITY_NUM) = trim(OPP.OPPORTUNITY_NUM)\r\n",
							"left outer join (select max(OPPORTUNITY_KEY) as max_OPPORTUNITY_KEY from etlhubConfirmed.dht_opportunity) max_key ON 1=1\r\n",
							"where OPP.OPPORTUNITY_NUM is null ) as ins_opp ;\r\n",
							"\r\n",
							"INSERT INTO etlhubconfirmed.dht_opportunity_role\r\n",
							"select \r\n",
							"COALESCE(max_oppy_role_key) + ROW_NUMBER () OVER (ORDER BY 1) AS OPPORTUNITY_ROLE_KEY,1 as OPPORTUNITY_ROLE_VERSION \r\n",
							",final.OPP_ROLE as ROLE_CD ,'' as ROLE_DESC\r\n",
							",'Y' AS CURRENT_IND\r\n",
							",CURRENT_TIMESTAMP AS EXTRACT_DT\r\n",
							",CURRENT_TIMESTAMP AS REC_START_DT\r\n",
							",'9999-12-31 00:00:00.000' as REC_END_DT\r\n",
							",'ESA' AS SOURCE_SYSTEM\r\n",
							",'#' as REC_CHECKSUM\r\n",
							",'I' as REC_STATUS\r\n",
							",current_timestamp as IMG_LST_UPD_DT\r\n",
							",CURRENT_TIMESTAMP AS IMG_CREATED_DT\r\n",
							",'ED' AS DATA_IND\r\n",
							",'Y' AS ACTIVE_IN_SOURCE_IND from (\r\n",
							"select distinct OPP_ROLE,max_oppy_role_key from (\r\n",
							"select src.*, max_role.max_oppy_role_key from SourceView src \r\n",
							"LEFT OUTER JOIN etlhubconfirmed.dht_opportunity_role role ON src.OPP_ROLE=role.ROLE_CD\r\n",
							"left outer join (select max(OPPORTUNITY_ROLE_KEY) as max_oppy_role_key from etlhubconfirmed.dht_opportunity_role ) as max_role ON 1=1 \r\n",
							"where role.ROLE_CD is null) as role where coalesce(trim(OPP_ROLE),'') <> ''\r\n",
							") as final ;\r\n",
							"\r\n",
							"\r\n",
							"INSERT INTO etlhubconfirmed.dht_opportunity_role\r\n",
							"select \r\n",
							"COALESCE(max_crm_role_key) + ROW_NUMBER () OVER (ORDER BY 1) AS OPPORTUNITY_ROLE_KEY,1 as OPPORTUNITY_ROLE_VERSION \r\n",
							",final.CRM_ROLE_CODE as ROLE_CD ,'' as ROLE_DESC\r\n",
							",'Y' AS CURRENT_IND\r\n",
							",CURRENT_TIMESTAMP AS EXTRACT_DT\r\n",
							",CURRENT_TIMESTAMP AS REC_START_DT\r\n",
							",'9999-12-31 00:00:00.000' as REC_END_DT\r\n",
							",'ESA' AS SOURCE_SYSTEM\r\n",
							",'#'  as REC_CHECKSUM\r\n",
							",'I' as REC_STATUS\r\n",
							",current_timestamp as IMG_LST_UPD_DT\r\n",
							",CURRENT_TIMESTAMP AS IMG_CREATED_DT\r\n",
							",'ED' AS DATA_IND\r\n",
							",'Y' AS ACTIVE_IN_SOURCE_IND from ( \r\n",
							"select distinct CRM_ROLE_CODE,max_crm_role_key from ( \r\n",
							"select src.*,max_crm_role_key from (select  distinct crm_role_code from SourceView) src \r\n",
							"LEFT OUTER JOIN etlhubconfirmed.dht_opportunity_role crm_role on trim(src.CRM_ROLE_CODE)=trim(crm_role.ROLE_CD) \r\n",
							"left outer join (select max(OPPORTUNITY_ROLE_KEY) as max_crm_role_key from etlhubconfirmed.dht_opportunity_role ) as max_role ON 1=1 \r\n",
							"where crm_role.ROLE_CD is null\r\n",
							") as crm_role where coalesce(trim(crm_role_code),'') <> ''\r\n",
							") as final ; \r\n",
							"\r\n",
							"INSERT INTO etlhubconfirmed.dht_employee (EMPLOYEE_KEY,EMPLOYEE_VERSION,CNUM_ID,CURRENT_IND,EXTRACT_DT, REC_START_DT\r\n",
							", REC_END_DT,SOURCE_SYSTEM,REC_CHECKSUM, REC_STATUS, IMG_LST_UPD_DT,IMG_CREATED_DT)\r\n",
							"select \r\n",
							"COALESCE(emp_key_max,0) + ROW_NUMBER () OVER (ORDER BY 1) AS EMPLOYEE_KEY,1 as VERSION  \r\n",
							",EMP_CNUM,'Y' AS CURRENT_IND,CURRENT_TIMESTAMP AS EXTRACT_DT,CURRENT_TIMESTAMP AS REC_START_DT\r\n",
							",'9999-12-31 00:00:00.000' as REC_END_DT,'BMSIW' AS SOURCE_SYSTEM,'#' as REC_CHECKSUM\r\n",
							",'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT,CURRENT_TIMESTAMP AS IMG_CREATED_DT \r\n",
							"from (\r\n",
							"select DISTINCT EMP_CNUM,emp_key_max \r\n",
							"from (select EMP_CNUM from SourceView) as src \r\n",
							"left outer join (select t2.EMPLOYEE_KEY,t2.EMPLOYEE_VERSION,t2.CNUM_ID\r\n",
							"from (select EMPLOYEE_KEY,EMPLOYEE_VERSION,CNUM_ID,row_number() OVER (PARTITION BY CNUM_ID ORDER BY EMPLOYEE_KEY DESC) AS Seq \r\n",
							"from etlhubConfirmed.DHT_EMPLOYEE where current_ind = 'Y' and IMG_ACTIVE_EMPLOYEE_STATUS_CD='Y' and CNUM_ID is not null) t2 \r\n",
							"where t2.seq = 1) emp ON trim(emp.CNUM_ID)=trim(src.EMP_CNUM)\r\n",
							"left outer join (select max(EMPLOYEE_KEY) as emp_key_max from etlhubconfirmed.DHT_EMPLOYEE ) as max_emp ON 1=1 \r\n",
							"where emp.CNUM_ID is null\r\n",
							") as ins;\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"#joining with dimension tables\r\n",
							"\r\n",
							"qry_DIMOPPDataDF=\"\"\"\r\n",
							"select distinct OPP.OPPORTUNITY_KEY,role.OPPORTUNITY_ROLE_KEY,crm_role.OPPORTUNITY_ROLE_KEY as CRM_ROLE_KEY,emp.EMPLOYEE_KEY \r\n",
							"from SourceView src \r\n",
							"left outer join etlhubConfirmed.dht_opportunity OPP on trim(src.OPPORTUNITY_NUM)=trim(OPP.OPPORTUNITY_NUM) \r\n",
							"left outer join etlhubConfirmed.dht_opportunity_role role on src.OPP_ROLE=role.ROLE_CD \r\n",
							"left outer join etlhubConfirmed.dht_opportunity_role crm_role on src.CRM_ROLE_CODE=crm_role.ROLE_CD \r\n",
							"left outer join (select t2.EMPLOYEE_KEY,t2.EMPLOYEE_VERSION,t2.CNUM_ID\r\n",
							"from (select EMPLOYEE_KEY,EMPLOYEE_VERSION,CNUM_ID,row_number() OVER (PARTITION BY CNUM_ID ORDER BY EMPLOYEE_KEY DESC) AS Seq \r\n",
							"from etlhubConfirmed.DHT_EMPLOYEE where current_ind = 'Y' and IMG_ACTIVE_EMPLOYEE_STATUS_CD='Y' and CNUM_ID is not null) t2 \r\n",
							"where t2.seq = 1) emp ON emp.CNUM_ID=src.EMP_CNUM where OPP.current_ind = 'Y' and role.current_ind = 'Y' and crm_role.current_ind = 'Y';\r\n",
							"\"\"\"\r\n",
							"DIMOPPDataDF=spark.sql(qry_DIMOPPDataDF)\r\n",
							"\r\n",
							"#DIMOPPDataDF=spark.sql(\"select distinct OPP.OPPORTUNITY_KEY,role.OPPORTUNITY_ROLE_KEY,crm_role.OPPORTUNITY_ROLE_KEY as CRM_ROLE_KEY,emp.EMPLOYEE_KEY from SourceView src left outer join etlhubConfirmed.dht_opportunity OPP on trim(src.OPPORTUNITY_NUM)=trim(OPP.OPPORTUNITY_NUM) left outer join etlhubConfirmed.dht_opportunity_role role on src.OPP_ROLE=role.ROLE_CD left outer join etlhubConfirmed.dht_opportunity_role crm_role on src.CRM_ROLE_CODE=crm_role.ROLE_CD left outer join (select t2.EMPLOYEE_KEY,t2.EMPLOYEE_VERSION,t2.CNUM_ID from (select EMPLOYEE_KEY,EMPLOYEE_VERSION,CNUM_ID,row_number() OVER (PARTITION BY CNUM_ID ORDER BY EMPLOYEE_KEY DESC) AS Seq from etlhubConfirmed.DHT_EMPLOYEE where current_ind = 'Y' and IMG_ACTIVE_EMPLOYEE_STATUS_CD='Y' and CNUM_ID is not null) t2 where t2.seq = 1) emp ON emp.CNUM_ID=src.EMP_CNUM where OPP.current_ind = 'Y' and role.current_ind = 'Y' and crm_role.current_ind = 'Y'\")\r\n",
							"#DIMJoin=incrementalData_DF.join(DIMOPPDataDF,(trim(incrementalData_DF.OPPORTUNITY_NUM) == trim(DIMDataDF.OPPORTUNITY_NUM), \"fullouter\") \r\n",
							"DIMOPPDataDF.createOrReplaceTempView('SrcDIMJoinView')\r\n",
							"\r\n",
							"col_list=[]\r\n",
							"for i in DIMOPPDataDF.columns:\r\n",
							"    col_list.append(i)\r\n",
							"# Add a checsum column to help identify the changed rows    \r\n",
							"incrementalData_DF2 = DIMOPPDataDF.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\r\n",
							"incrementalData_DF.write.csv(csv_archive_path, header = 'true')\r\n",
							"\r\n",
							"# Get column list for creating Rec_Checksum\r\n",
							"# Add a checsum column to help identify the changed rows\r\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\r\n",
							"\r\n",
							"existingDIMMaxKeyDF=spark.sql(\"SELECT MAX(OPPORTUNITY_KEY) existing_MAX_KEY from etlhubConfirmed.dht_opportunity WHERE CURRENT_IND='Y'\");\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\r\n",
							"select Src.OPPORTUNITY_NUM,OPP.OPPORTUNITY_KEY,CUST.MARKET_KEY,emp.EMPLOYEE_KEY,emp.EMPLOYEE_VERSION,\r\n",
							"emp_exc.EXECUTIVE_SPONSOR_EMP_KEY,geo.GEOGRAPHY_KEY,rgn4.REGION_LEVEL_4_KEY,sales.SALES_STAGE_KEY,\r\n",
							"prev_sales.SALES_STAGE_KEY as PREV_SALES_STAGE_KEY,INDUS.IMG_INDUSTRY_SECTOR_KEY,\r\n",
							"FROM SourceView Src\r\n",
							"LEFT OUTER JOIN (select OPPORTUNITY_KEY,OPPORTUNITY_VERSION,OPPORTUNITY_NUM\r\n",
							"from  etlhubConfirmed.dht_opportunity where current_ind = 'Y') as OPP ON Src.OPPORTUNITY_NUM=OPP.OPPORTUNITY_NUM\r\n",
							"LEFT OUTER JOIN (select MARKET_KEY,MARKET_VERSION,CUSTOMER_NO,COUNTRY_CD from etlhubConfirmed.DHT_MARKET where current_ind = 'Y') as CUST \r\n",
							"on Src.CUSTOMER_NO=CUST.CUSTOMER_NO and Src.COUNTRY_CODE=CUST.COUNTRY_CD\r\n",
							"LEFT OUTER JOIN (select t2.EMPLOYEE_KEY,t2.EMPLOYEE_VERSION,t2.CNUM_ID\r\n",
							"from( select EMPLOYEE_KEY,EMPLOYEE_VERSION,trim(CNUM_ID) as CNUM_ID,row_number() OVER (PARTITION BY CNUM_ID ORDER BY EMPLOYEE_KEY DESC) AS Seq\r\n",
							"from  etlhubConfirmed.DHT_EMPLOYEE where current_ind = 'Y' and IMG_ACTIVE_EMPLOYEE_STATUS_CD='Y' and CNUM_ID is not null\r\n",
							") t2 where t2.seq = 1) as emp on Src.EMP_CNUM=emp.CNUM_ID\r\n",
							"LEFT OUTER JOIN (select t2.EMPLOYEE_KEY as EXECUTIVE_SPONSOR_EMP_KEY,t2.EMPLOYEE_VERSION as EXECUTIVE_SPONSOR_EMP_VERSION,\r\n",
							"t2.CNUM_ID as EXECUTIVE_SPONSOR_EMP from (select EMPLOYEE_KEY,EMPLOYEE_VERSION,trim(CNUM_ID) as CNUM_ID,\r\n",
							"row_number() OVER (PARTITION BY CNUM_ID ORDER BY EMPLOYEE_KEY DESC) AS Seq\r\n",
							"from  etlhubConfirmed.DHT_EMPLOYEE where current_ind = 'Y' and IMG_ACTIVE_EMPLOYEE_STATUS_CD='Y' and CNUM_ID is not null) t2\r\n",
							"where t2.seq = 1) as emp_exc on emp_exc.EXECUTIVE_SPONSOR_EMP_KEY=Src.EXECUTIVE_SPONSOR_EMP_KEY\r\n",
							"LEFT OUTER JOIN (select distinct GEOGRAPHY_KEY,ISO_COUNTRY_CD from etlhubConfirmed.DHT_GEOGRAPHY where CURRENT_IND='Y' and current date >= EFFECTIVE_START_DT\r\n",
							"and current date <= EFFECTIVE_END_DT) as geo ON geo.ISO_COUNTRY_CD=Src.ISO_COUNTRY_CODE\r\n",
							"LEFT OUTER JOIN (select distinct REGION_LEVEL_4_KEY,REGION_LEVEL_4_CD from etlhubConfirmed.DHT_GEOGRAPHY where CURRENT_IND='Y' and   current date >= EFFECTIVE_START_DT\r\n",
							"and current date <= EFFECTIVE_END_DT) as rgn4 on Src.REGION_ID=rgn4.REGION_LEVEL_4_CD\r\n",
							"LEFT OUTER JOIN (select SALES_STAGE_KEY,SALES_STAGE_CD from etlhubConfirmed.DHT_SALES_STAGE where CURRENT_IND='Y') sales\r\n",
							"ON Src.OPPORTUNITY_SALES_STAGE_CODE = sales.SALES_STAGE_CD\r\n",
							"LEFT OUTER JOIN (select SALES_STAGE_KEY,SALES_STAGE_CD from etlhubConfirmed.DHT_SALES_STAGE where CURRENT_IND='Y') prev_sales\r\n",
							"ON Src.PREV_OPPORTUNITY_SALES_STAGE_CODE = prev_sales.SALES_STAGE_CD\r\n",
							"LEFT OUTER JOIN (select CMR_ISU_CD,IMG_INDUSTRY_SECTOR_KEY,SUB_INDUSTRY_CD from etlhubConfirmed.DHT_IMG_INDUSTRY_SECTOR \r\n",
							"where current_ind = 'Y' and current_date > EFFECTIVE_START_DATE and current_date < EFFECTIVE_END_DATE) as INDUS \r\n",
							"ON Src.CMR_ISU_CODE=INDUS.CMR_ISU_CD AND Src.SUB_INDUSTRY_CODE = INDUS.SUB_INDUSTRY_CD\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Create a dataframe with target deltalake table data with necessary columns\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,((incrementalData_DF2.OPPORTUNITY_KEY == existingDataDF1.existing_OPPORTUNITY_KEY) & (incrementalData_DF2.OPPORTUNITY_ROLE_KEY == existingDataDF1.existing_OPPORTUNITY_ROLE_KEY) & (incrementalData_DF2.CRM_ROLE_KEY == existingDataDF1.existing_CRM_ROLE_KEY) & (incrementalData_DF2.EMPLOYEE_KEY == existingDataDF1.existing_EMPLOYEE_KEY)), \"fullouter\") \n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"fullJoin2.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .option(\"overwriteSchema\", \"true\") \\\n",
							"  .option(\"path\", \"abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/staging/DHTS_OPPORTUNITY_ROLE\") \\\n",
							"  .saveAsTable(stagingtable)\n",
							"   #.option(\"overwriteSchema\", \"true\") \\  .save(\"\") \\"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from DHTS_OPPORTUNITY_ASSIGNMENT_DAILY_FT_DATA"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Data Validation Checks :\n",
							"\n",
							"- Check for duplicates in Source data based on natural key\n",
							"- Check for duplicates in target delta lake table based on natural key\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, CRM_ROLE_KEY,OPPORTUNITY_KEY,OPPORTUNITY_ROLE_KEY,EMPLOYEE_KEY FROM incrementalData_DF2 GROUP BY CRM_ROLE_KEY,OPPORTUNITY_KEY,OPPORTUNITY_ROLE_KEY,EMPLOYEE_KEY HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, CRM_ROLE_KEY,OPPORTUNITY_KEY,OPPORTUNITY_ROLE_KEY,EMPLOYEE_KEY FROM {} WHERE CURRENT_IND='Y' GROUP BY CRM_ROLE_KEY,OPPORTUNITY_KEY,OPPORTUNITY_ROLE_KEY,EMPLOYEE_KEY HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"df3=spark.sql(qry3)\n",
							"cnt1=df3.count()\n",
							"\n",
							"print (cnt1)\n",
							"if cnt1 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in source:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(1)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"df4=spark.sql(qry4.format(tablename))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry_4_recon1=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from DHTS_OPPORTUNITY_ASSIGNMENT_DAILY_FT_DATA A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.OPPORTUNITY_KEY=B.OPPORTUNITY_KEY AND A.OPPORTUNITY_ROLE_KEY = B.OPPORTUNITY_ROLE_KEY \n",
							"AND A.CRM_ROLE_KEY = B.CRM_ROLE_KEY AND A.EMPLOYEE_KEY = B.EMPLOYEE_KEY\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(tablename))\n",
							"print('New records or Inserts are:') \n",
							"df_4_recon1.show()\n",
							"\n",
							"qry_4_recon2=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from DHTS_OPPORTUNITY_ASSIGNMENT_DAILY_FT_DATA A \n",
							"WHERE OPPORTUNITY_KEY=existing_OPPORTUNITY_KEY AND OPPORTUNITY_ROLE_KEY = existing_OPPORTUNITY_ROLE_KEY \n",
							"AND CRM_ROLE_KEY = existing_CRM_ROLE_KEY AND EMPLOYEE_KEY = existing_EMPLOYEE_KEY\n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.OPPORTUNITY_KEY=B.OPPORTUNITY_KEY AND A.OPPORTUNITY_ROLE_KEY = B.OPPORTUNITY_ROLE_KEY \n",
							"AND A.CRM_ROLE_KEY = B.CRM_ROLE_KEY AND A.EMPLOYEE_KEY = B.EMPLOYEE_KEY\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon2=spark.sql(qry_4_recon2.format(tablename))\n",
							"print('Changed records or Updates are:') \n",
							"df_4_recon2.show()\n",
							"\n",
							"qry_4_recon3=\"\"\"\n",
							"SELECT COUNT(*) from \n",
							"etlhubconfirmed.dht_opportunity_assignment_daily_ft A\n",
							"join fullJoin B\n",
							"ON A.OPPORTUNITY_KEY=B.existing_OPPORTUNITY_KEY AND A.OPPORTUNITY_ROLE_KEY = B.existing_OPPORTUNITY_ROLE_KEY \n",
							"AND A.CRM_ROLE_KEY = B.existing_CRM_ROLE_KEY AND A.EMPLOYEE_KEY = B.existing_EMPLOYEE_KEY\n",
							"AND B.OPPORTUNITY_KEY is NULL AND B.OPPORTUNITY_ROLE_KEY is null and B.CRM_ROLE_KEY is null and B.EMPLOYEE_KEY is null\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT ;\n",
							"\"\"\"\n",
							"df_4_recon3=spark.sql(qry_4_recon3)\n",
							"print('Soft Deletes are:') \n",
							"df_4_recon3.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"#delete FROM etlhubconfirmed.dht_opportunity_assignment_daily_ft;\n",
							"\n",
							"qry_4_recon1=\"\"\"\n",
							"SELECT COUNT(*) from DHTS_OPPORTUNITY_ASSIGNMENT_DAILY_FT_DATA A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.OPPORTUNITY_KEY=B.OPPORTUNITY_KEY AND A.OPPORTUNITY_ROLE_KEY = B.OPPORTUNITY_ROLE_KEY \n",
							"AND A.CRM_ROLE_KEY = B.CRM_ROLE_KEY AND A.EMPLOYEE_KEY = B.EMPLOYEE_KEY\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(tablename))\n",
							"\n",
							"qry_ins_new_rows=\"\"\"\n",
							"\n",
							"INSERT INTO etlhubconfirmed.dht_opportunity_assignment_daily_ft\n",
							"select \n",
							"COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS OPPORTUNITY_ASSIGNMENT_KEY,1 as OPPORTUNITY_ASSIGNMENT_VERSION \n",
							",OPPORTUNITY_KEY\n",
							",OPPORTUNITY_ROLE_KEY\n",
							",CRM_ROLE_KEY\n",
							",EMPLOYEE_KEY\n",
							",'Y' AS CURRENT_IND\n",
							",CURRENT_TIMESTAMP AS EXTRACT_DT\n",
							",CURRENT_TIMESTAMP AS REC_START_DT\n",
							",'9999-12-31 00:00:00.000' as REC_END_DT\n",
							",'ESA' AS SOURCE_SYSTEM\n",
							",column_hash as REC_CHECKSUM\n",
							",'I' as REC_STATUS\n",
							",current_timestamp as IMG_LST_UPD_DT\n",
							",CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							",'ED' AS DATA_IND\n",
							",'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from DHTS_OPPORTUNITY_ASSIGNMENT_DAILY_FT_DATA A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.dht_opportunity_assignment_daily_ft B\n",
							"WHERE A.OPPORTUNITY_KEY=B.OPPORTUNITY_KEY AND A.OPPORTUNITY_ROLE_KEY = B.OPPORTUNITY_ROLE_KEY \n",
							"AND A.CRM_ROLE_KEY = B.CRM_ROLE_KEY AND A.EMPLOYEE_KEY = B.EMPLOYEE_KEY\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"--limit 10\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"d=spark.sql(qry_ins_new_rows)\n",
							"\n",
							"#d.show(10)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select count(*) from etlhubConfirmed.dht_opportunity_assignment_daily_ft"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Update or expire the rows with existing instance of the changed rows:\n",
							"qry_4_upd_changes_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING DHTS_OPPORTUNITY_ASSIGNMENT_DAILY_FT_DATA B\n",
							"ON A.OPPORTUNITY_KEY=B.OPPORTUNITY_KEY AND A.OPPORTUNITY_ROLE_KEY = B.OPPORTUNITY_ROLE_KEY \n",
							"AND A.CRM_ROLE_KEY = B.CRM_ROLE_KEY AND A.EMPLOYEE_KEY = B.EMPLOYEE_KEY\n",
							"AND B.OPPORTUNITY_KEY=B.existing_OPPORTUNITY_KEY AND B.OPPORTUNITY_ROLE_KEY = B.existing_OPPORTUNITY_ROLE_KEY \n",
							"AND B.CRM_ROLE_KEY = B.existing_CRM_ROLE_KEY AND B.EMPLOYEE_KEY = B.existing_EMPLOYEE_KEY\n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\"\"\"\n",
							"f=spark.sql(qry_4_upd_changes_rows.format(tablename))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"#Insert for changed rows which are missing in target and present in source based on Natural Key.\n",
							"qry_ins_changed_rows=\"\"\"\n",
							"INSERT INTO etlhubconfirmed.dht_opportunity_assignment_daily_ft\n",
							"select A.existing_OPPORTUNITY_ASSIGNMENT_KEY\n",
							",1 + existing_OPPORTUNITY_ASSIGNMENT_VERSION as OPPORTUNITY_ASSIGNMENT_VERSION\n",
							",OPPORTUNITY_KEY\n",
							",OPPORTUNITY_ROLE_KEY\n",
							",CRM_ROLE_KEY\n",
							",EMPLOYEE_KEY\n",
							",'Y' AS CURRENT_IND\n",
							",CURRENT_TIMESTAMP AS EXTRACT_DT\n",
							",CURRENT_TIMESTAMP AS REC_START_DT\n",
							",'9999-12-31 00:00:00.000' as REC_END_DT\n",
							",'ESA' AS SOURCE_SYSTEM\n",
							",column_hash as REC_CHECKSUM\n",
							",'U' as REC_STATUS\n",
							", CURRENT_TIMESTAMP as IMG_LST_UPD_DT\n",
							",A.existing_IMG_CREATED_DT AS IMG_CREATED_DT\n",
							",'LG' AS DATA_IND\n",
							",'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from DHTS_OPPORTUNITY_ASSIGNMENT_DAILY_FT_DATA A \n",
							"WHERE LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.dht_opportunity_role B\n",
							"WHERE b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\"\"\"\n",
							"e=spark.sql(qry_ins_changed_rows)\n",
							"#e.show(4)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Soft Deletes - Update Active_Ind for the rows no longer active in source:\n",
							"\n",
							"qry_4_upd_deleted_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING DHTS_OPPORTUNITY_ROLE B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"\n",
							";\n",
							"\"\"\"\n",
							"g=spark.sql(qry_4_upd_deleted_rows.format(tablename,natural_key,natural_key,natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select 'Number of Active Rows' as Title,count(*) from etlhubconfirmed.dht_opportunity_role where CURRENT_IND='Y'\n",
							";\n",
							"\n",
							"select 'Duplicate Rows based on surrogate key' as Title,OPPORTUNITY_ROLE_KEY,count(*) from etlhubconfirmed.dht_opportunity_role where CURRENT_IND='Y'\n",
							"    group by OPPORTUNITY_ROLE_KEY\n",
							"    having count(*)>1\n",
							";\n",
							"\n",
							"select 'Duplicate Rows based on natural key' as Title,ROLE_CD,count(*) from etlhubconfirmed.dht_opportunity_role where CURRENT_IND='Y'\n",
							"    group by ROLE_CD\n",
							"    having count(*)>1\n",
							";\n",
							"\n",
							"select 'New rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_opportunity_role where CURRENT_IND='Y'\n",
							"    and date(extract_dt)=CURRENT_DATE and rec_status='I'\n",
							";\n",
							"\n",
							"select 'Changed rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_opportunity_role where CURRENT_IND='Y'\n",
							"    and date(extract_dt)=current_date and rec_status='U'\n",
							";\n",
							"\n",
							"select 'Changed rows updated in this run' as Title,count(*) from etlhubconfirmed.dht_opportunity_role where CURRENT_IND='N'\n",
							"    and date(REC_END_DT)=CURRENT_DATE --and rec_status='I'\n",
							";\n",
							"\n",
							"select 'Rows no longer active in source' as Title,count(*) from etlhubconfirmed.dht_opportunity_role where CURRENT_IND='Y'\n",
							"    AND \n",
							"     date(REC_END_DT)=current_date --and rec_status='I'\n",
							";\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 35
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_dht_opportunity_daily_fact_load')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPool001494New",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "472g",
					"driverCores": 80,
					"executorMemory": "472g",
					"executorCores": 80,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "86f6b3dd-555b-41b9-a044-02d22bce6624"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPool001494New",
						"name": "sPool001494New",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPool001494New",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 80,
						"memory": 472,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Opportunity Fact load**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubconfirmed.dht_market by sourceing data from ESA Kyndryl.\n",
							"Since ESA is not accessible from Azure, a datastage job srcESA_Opportunity_Daily_Fact_Azure is executed on datastage to push the ESA extract to adls.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws,trim\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"from pyspark.sql import SparkSession\n",
							"from pyspark.sql.functions import col\n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"import os\n",
							"import sys\n",
							"from datetime import datetime"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 96
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/ESA/' # fill in your relative folder path \n",
							"relative_archive_path='archive/'\n",
							"file_name='DS_OPPORTUNITY_DAILY_FACT.csv' \n",
							"natural_key=\"OPPORTUNITY_KEY\"\n",
							"tablename=\"etlhubConfirmed.dht_opportunity_daily_ft\"\n",
							"stagingtable=\"DHTS_OPPORTUNITY_DAILY_FT\"\n",
							"keycolumn=\"OPPORTUNITY_FACT_KEY\"\n",
							"date = datetime.now().strftime(\"%Y-%m-%d-%I:%M:%S_%p\")\n",
							"print(date)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 97
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Copy source CSV file to archive folder including timestamp in file name"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/ESA/DS_MARKET_DATA.csv"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"adls_arch_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_archive_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"relative_archive_path\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"arch_fil_name=file_name.rsplit(\".\")\n",
							"csv_archive_path = adls_arch_path + arch_fil_name[0] + '_' + date + '.' + arch_fil_name[1]\n",
							"print (csv_archive_path)\n",
							"\n",
							"incrementalData_DF = spark.read.load(csv_path,format=\"csv\", sep=\"||\", header=\"true\")\n",
							"incrementalData_DF.createOrReplaceTempView('SourceView')\n",
							"#joining with dimension tables\n",
							"DIMOPPDataDF=spark.sql(\"select distinct Src.OPPORTUNITY_NUM,OPP.OPPORTUNITY_KEY as OPPORTUNITY_KEY,CUST.MARKET_KEY,emp.EMPLOYEE_KEY,emp.EMPLOYEE_VERSION,emp_exc.EXECUTIVE_SPONSOR_EMP_KEY,sales.SALES_STAGE_KEY,prev_sales.SALES_STAGE_KEY as PREV_SALES_STAGE_KEY,DECISION_DATE,OPPORTUNITY_CLOSE_DATE,OPPORTUNITY_CREATE_DATE,OPPORTUNITY_UPDATE_DATE,TOTAL_OPP_VAL_USD,OPPORTUNITY_SALES_STAGE_UPDATE_DATE,PERF_CRITERIA_CONF,PROP_DELIVERY_DATE,PRICE_AGREEMENT,PRICING_SECURED,INDUS.INDUSTRY_SECTOR_KEY FROM SourceView Src LEFT OUTER JOIN (select OPPORTUNITY_KEY,OPPORTUNITY_VERSION,OPPORTUNITY_NUM from  etlhubConfirmed.dht_opportunity where current_ind = 'Y') as OPP ON Src.OPPORTUNITY_NUM=OPP.OPPORTUNITY_NUM LEFT OUTER JOIN (select MARKET_KEY,MARKET_VERSION,CUSTOMER_NO,COUNTRY_CD from etlhubConfirmed.DHT_MARKET where current_ind = 'Y') as CUST on Src.CUSTOMER_NO=CUST.CUSTOMER_NO and Src.COUNTRY_CODE=CUST.COUNTRY_CD LEFT OUTER JOIN (select t2.EMPLOYEE_KEY,t2.EMPLOYEE_VERSION,t2.CNUM_ID from (select EMPLOYEE_KEY,EMPLOYEE_VERSION,trim(CNUM_ID) as CNUM_ID,row_number() OVER (PARTITION BY CNUM_ID ORDER BY EMPLOYEE_KEY DESC) AS Seq from  etlhubConfirmed.DHT_EMPLOYEE where current_ind = 'Y' and IMG_ACTIVE_EMPLOYEE_STATUS_CD='Y' and CNUM_ID is not null) t2 where t2.seq = 1) as emp on Src.EMP_CNUM=emp.CNUM_ID LEFT OUTER JOIN (select t2.EMPLOYEE_KEY as EXECUTIVE_SPONSOR_EMP_KEY,t2.EMPLOYEE_VERSION as EXECUTIVE_SPONSOR_EMP_VERSION,t2.CNUM_ID as EXECUTIVE_SPONSOR_EMP from (select EMPLOYEE_KEY,EMPLOYEE_VERSION,trim(CNUM_ID) as CNUM_ID,row_number() OVER (PARTITION BY CNUM_ID ORDER BY EMPLOYEE_KEY DESC) AS Seq from  etlhubConfirmed.DHT_EMPLOYEE where current_ind = 'Y' and IMG_ACTIVE_EMPLOYEE_STATUS_CD='Y' and CNUM_ID is not null) t2 where t2.seq = 1) as emp_exc on emp_exc.EXECUTIVE_SPONSOR_EMP=Src.EXECUTIVE_SPONSOR_EMP_KEY LEFT OUTER JOIN (select SALES_STAGE_KEY,SALES_STAGE_CD from etlhubConfirmed.DHT_SALES_STAGE where CURRENT_IND='Y') sales ON Src.OPPORTUNITY_SALES_STAGE_CODE = sales.SALES_STAGE_CD LEFT OUTER JOIN (select SALES_STAGE_KEY,SALES_STAGE_CD from etlhubConfirmed.DHT_SALES_STAGE where CURRENT_IND='Y') prev_sales ON Src.PREV_OPPORTUNITY_SALES_STAGE_CODE = prev_sales.SALES_STAGE_CD LEFT OUTER JOIN (select CODE,INDUSTRY_SECTOR_KEY,parentCode from etlhubConfirmed.DHT_INDUSTRY_SECTOR where current_ind = 'Y') as INDUS ON Src.CMR_ISU_CODE=INDUS.CODE AND Src.SUB_INDUSTRY_CODE = INDUS.parentCode\")\n",
							"DIMOPPDataDF.createOrReplaceTempView('SrcDIMJoinView')\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"col_list=[]\n",
							"for i in DIMOPPDataDF.columns:\n",
							"    col_list.append(i)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows  \n",
							"incrementalData_DF2 = DIMOPPDataDF.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"incrementalData_DF.write.csv(csv_archive_path, header = 'true')\n",
							"\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 98
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Create a dataframe with target deltalake table data with necessary columns\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1, col(natural_key) == col(\"existing_\" + natural_key), \"fullouter\")\n",
							"\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"fullJoin2.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .option(\"overwriteSchema\", \"true\") \\\n",
							"  .option(\"path\", \"abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/staging/{}\".format(stagingtable)) \\\n",
							"  .saveAsTable(stagingtable)\n",
							"   #.option(\"overwriteSchema\", \"true\") \\  .save(\"\") \\"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 99
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Data Validation Checks :\n",
							"\n",
							"- Check for duplicates in Source data based on natural key\n",
							"- Check for duplicates in target delta lake table based on natural key\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 where {} is not null GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} WHERE CURRENT_IND='Y' GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"df3=spark.sql(qry3.format(natural_key,natural_key,natural_key))\n",
							"cnt1=df3.count()\n",
							"\n",
							"print (cnt1)\n",
							"if cnt1 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in source:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(1)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#print(natural_key)\n",
							"#print(tablename)\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 100
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--select Src.OPPORTUNITY_NUM,OPP.OPPORTUNITY_KEY as OPPORTUNITY_KEY,CUST.MARKET_KEY,emp.EMPLOYEE_KEY,emp.EMPLOYEE_VERSION,emp_exc.EXECUTIVE_SPONSOR_EMP_KEY,sales.SALES_STAGE_KEY,prev_sales.SALES_STAGE_KEY as PREV_SALES_STAGE_KEY,DECISION_DATE,OPPORTUNITY_CLOSE_DATE,OPPORTUNITY_CREATE_DATE,OPPORTUNITY_UPDATE_DATE,TOTAL_OPP_VAL_USD,OPPORTUNITY_SALES_STAGE_UPDATE_DATE,PERF_CRITERIA_CONF,PROP_DELIVERY_DATE,PRICE_AGREEMENT,PRICING_SECURED,INDUS.INDUSTRY_SECTOR_KEY FROM SourceView Src LEFT OUTER JOIN (select OPPORTUNITY_KEY,OPPORTUNITY_VERSION,OPPORTUNITY_NUM from  etlhubConfirmed.dht_opportunity where current_ind = 'Y') as OPP ON Src.OPPORTUNITY_NUM=OPP.OPPORTUNITY_NUM LEFT OUTER JOIN (select MARKET_KEY,MARKET_VERSION,CUSTOMER_NO,COUNTRY_CD from etlhubConfirmed.DHT_MARKET where current_ind = 'Y') as CUST on Src.CUSTOMER_NO=CUST.CUSTOMER_NO and Src.COUNTRY_CODE=CUST.COUNTRY_CD LEFT OUTER JOIN (select t2.EMPLOYEE_KEY,t2.EMPLOYEE_VERSION,t2.CNUM_ID from (select EMPLOYEE_KEY,EMPLOYEE_VERSION,trim(CNUM_ID) as CNUM_ID,row_number() OVER (PARTITION BY CNUM_ID ORDER BY EMPLOYEE_KEY DESC) AS Seq from  etlhubConfirmed.DHT_EMPLOYEE where current_ind = 'Y' and IMG_ACTIVE_EMPLOYEE_STATUS_CD='Y' and CNUM_ID is not null) t2 where t2.seq = 1) as emp on Src.EMP_CNUM=emp.CNUM_ID LEFT OUTER JOIN (select t2.EMPLOYEE_KEY as EXECUTIVE_SPONSOR_EMP_KEY,t2.EMPLOYEE_VERSION as EXECUTIVE_SPONSOR_EMP_VERSION,t2.CNUM_ID as EXECUTIVE_SPONSOR_EMP from (select EMPLOYEE_KEY,EMPLOYEE_VERSION,trim(CNUM_ID) as CNUM_ID,row_number() OVER (PARTITION BY CNUM_ID ORDER BY EMPLOYEE_KEY DESC) AS Seq from  etlhubConfirmed.DHT_EMPLOYEE where current_ind = 'Y' and IMG_ACTIVE_EMPLOYEE_STATUS_CD='Y' and CNUM_ID is not null) t2 where t2.seq = 1) as emp_exc on emp_exc.EXECUTIVE_SPONSOR_EMP=Src.EXECUTIVE_SPONSOR_EMP_KEY LEFT OUTER JOIN (select SALES_STAGE_KEY,SALES_STAGE_CD from etlhubConfirmed.DHT_SALES_STAGE where CURRENT_IND='Y') sales ON Src.OPPORTUNITY_SALES_STAGE_CODE = sales.SALES_STAGE_CD LEFT OUTER JOIN (select SALES_STAGE_KEY,SALES_STAGE_CD from etlhubConfirmed.DHT_SALES_STAGE where CURRENT_IND='Y') prev_sales ON Src.PREV_OPPORTUNITY_SALES_STAGE_CODE = prev_sales.SALES_STAGE_CD LEFT OUTER JOIN (select CODE,INDUSTRY_SECTOR_KEY,parentCode from etlhubConfirmed.DHT_INDUSTRY_SECTOR where current_ind = 'Y') as INDUS ON Src.CMR_ISU_CODE=INDUS.CODE AND Src.SUB_INDUSTRY_CODE = INDUS.parentCode\r\n",
							"--select * from incrementalData_DF2 where opportunity_key=25049\r\n",
							"select Src.OPPORTUNITY_NUM --,OPP.OPPORTUNITY_KEY as OPPORTUNITY_KEY --,CUST.MARKET_KEY,emp.EMPLOYEE_KEY,emp.EMPLOYEE_VERSION,\r\n",
							"--emp_exc.EXECUTIVE_SPONSOR_EMP_KEY,sales.SALES_STAGE_KEY,prev_sales.SALES_STAGE_KEY as PREV_SALES_STAGE_KEY,DECISION_DATE,\r\n",
							"--OPPORTUNITY_CLOSE_DATE,OPPORTUNITY_CREATE_DATE,OPPORTUNITY_UPDATE_DATE,TOTAL_OPP_VAL_USD,OPPORTUNITY_SALES_STAGE_UPDATE_DATE,\r\n",
							"--PERF_CRITERIA_CONF,PROP_DELIVERY_DATE,PRICE_AGREEMENT,PRICING_SECURED,INDUS.INDUSTRY_SECTOR_KEY \r\n",
							"FROM (select * from SourceView where opportunity_num='SOC-WIJKD3I') as Src \r\n",
							"LEFT OUTER JOIN (select OPPORTUNITY_KEY,OPPORTUNITY_VERSION,OPPORTUNITY_NUM from etlhubConfirmed.dht_opportunity where current_ind = 'Y') as OPP ON Src.OPPORTUNITY_NUM=OPP.OPPORTUNITY_NUM \r\n",
							"LEFT OUTER JOIN (select MARKET_KEY,MARKET_VERSION,CUSTOMER_NO,COUNTRY_CD from etlhubConfirmed.DHT_MARKET where current_ind = 'Y') as CUST on Src.CUSTOMER_NO=CUST.CUSTOMER_NO and Src.COUNTRY_CODE=CUST.COUNTRY_CD \r\n",
							"/*LEFT OUTER JOIN (select t2.EMPLOYEE_KEY,t2.EMPLOYEE_VERSION,t2.CNUM_ID from (select EMPLOYEE_KEY,EMPLOYEE_VERSION,trim(CNUM_ID) as CNUM_ID,row_number() OVER (PARTITION BY CNUM_ID ORDER BY EMPLOYEE_KEY DESC) AS Seq from etlhubConfirmed.DHT_EMPLOYEE where current_ind = 'Y' and IMG_ACTIVE_EMPLOYEE_STATUS_CD='Y' and CNUM_ID is not null) t2 where t2.seq = 1) as emp on Src.EMP_CNUM=emp.CNUM_ID \r\n",
							"LEFT OUTER JOIN (select t2.EMPLOYEE_KEY as EXECUTIVE_SPONSOR_EMP_KEY,t2.EMPLOYEE_VERSION as EXECUTIVE_SPONSOR_EMP_VERSION,t2.CNUM_ID as EXECUTIVE_SPONSOR_EMP from (select EMPLOYEE_KEY,EMPLOYEE_VERSION,trim(CNUM_ID) as CNUM_ID,row_number() OVER (PARTITION BY CNUM_ID ORDER BY EMPLOYEE_KEY DESC) AS Seq from etlhubConfirmed.DHT_EMPLOYEE where current_ind = 'Y' and IMG_ACTIVE_EMPLOYEE_STATUS_CD='Y' and CNUM_ID is not null) t2 where t2.seq = 1) as emp_exc on emp_exc.EXECUTIVE_SPONSOR_EMP=Src.EXECUTIVE_SPONSOR_EMP_KEY \r\n",
							"LEFT OUTER JOIN (select SALES_STAGE_KEY,SALES_STAGE_CD from etlhubConfirmed.DHT_SALES_STAGE where CURRENT_IND='Y') sales ON Src.OPPORTUNITY_SALES_STAGE_CODE = sales.SALES_STAGE_CD \r\n",
							"LEFT OUTER JOIN (select SALES_STAGE_KEY,SALES_STAGE_CD from etlhubConfirmed.DHT_SALES_STAGE where CURRENT_IND='Y') prev_sales ON Src.PREV_OPPORTUNITY_SALES_STAGE_CODE = prev_sales.SALES_STAGE_CD \r\n",
							"LEFT OUTER JOIN (select CODE,INDUSTRY_SECTOR_KEY,parentCode from etlhubConfirmed.DHT_INDUSTRY_SECTOR where current_ind = 'Y') as INDUS ON Src.CMR_ISU_CODE=INDUS.CODE AND Src.SUB_INDUSTRY_CODE = INDUS.parentCode\r\n",
							"where opportunity_key=25049 */"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry_4_recon1=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable,tablename,natural_key,natural_key))\n",
							"print('New records or Inserts are:') \n",
							"df_4_recon1.show()\n",
							"\n",
							"qry_4_recon2=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE LOWER({}) = LOWER(existing_{}) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon2=spark.sql(qry_4_recon2.format(stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))\n",
							"print('Changed records or Updates are:') \n",
							"df_4_recon2.show()\n",
							"\n",
							"qry_4_recon3=\"\"\"\n",
							"SELECT COUNT(*) from \n",
							"{} A\n",
							"join fullJoin B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"\n",
							"\"\"\"\n",
							"\n",
							"df_4_recon3=spark.sql(qry_4_recon3.format(tablename,natural_key,natural_key,natural_key))\n",
							"print('Soft Deletes are:') \n",
							"df_4_recon3.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT DECISION_DATE ,\n",
							"date_format(DECISION_DATE,\"yyyyMMdd\") \n",
							"FROM DHTS_OPPORTUNITY_DAILY_FT "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry_4_recon1=\"\"\"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable, tablename, natural_key, natural_key))\n",
							"\n",
							"qry_ins_new_rows=\"\"\"\n",
							"\n",
							"INSERT INTO {}\n",
							"\tselect \n",
							"\tCOALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS {},1 as OPPORTUNITY_FACT_VERSION \n",
							",\tOPPORTUNITY_KEY\n",
							",EMPLOYEE_KEY\n",
							",-1 as REGION_LEVEL_4_KEY\n",
							",INDUSTRY_SECTOR_KEY\n",
							",MARKET_KEY\n",
							",SALES_STAGE_KEY\n",
							",PREV_SALES_STAGE_KEY\n",
							",date_format(DECISION_DATE,\"yyyyMMdd\") as DECISION_DATE_KEY\n",
							",-1 as EIW_DATE_KEY\n",
							",date_format(OPPORTUNITY_CLOSE_DATE,\"yyyyMMdd\") as OPPORTUNITY_CLOSE_DATE_KEY\n",
							",date_format(OPPORTUNITY_CREATE_DATE,\"yyyyMMdd\") as OPPORTUNITY_CREATE_DATE_KEY\n",
							",date_format(OPPORTUNITY_UPDATE_DATE,\"yyyyMMdd\") as OPPORTUNITY_UPDATE_DATE_KEY\n",
							",date_format(OPPORTUNITY_SALES_STAGE_UPDATE_DATE,\"yyyyMMdd\") as OPPORTUNITY_SALES_STAGE_UPDATE_DATE_KEY\n",
							",-1 as GEOGRAPHY_KEY\n",
							",TOTAL_OPP_VAL_USD\n",
							",'Y' AS CURRENT_IND\n",
							"\t,CURRENT_TIMESTAMP AS EXTRACT_DT\n",
							"\t,CURRENT_TIMESTAMP AS REC_START_DT\n",
							"\t,'9999-12-31 00:00:00.000' as REC_END_DT\n",
							"\t,'ESA' AS SOURCE_SYSTEM\n",
							"\t,column_hash as REC_CHECKSUM\n",
							"\t,'I' as REC_STATUS\n",
							"\t,current_timestamp as IMG_LST_UPD_DT\n",
							"\t,CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							"\t,date_format(PROP_DELIVERY_DATE,\"yyyyMMdd\") as PROP_DELIVERY_DATE_KEY\n",
							"\t,PERF_CRITERIA_CONF\n",
							"\t,PRICE_AGREEMENT\n",
							"    ,PRICING_SECURED\n",
							"    ,EXECUTIVE_SPONSOR_EMP_KEY\n",
							"\t,'Y' as OPP_ACTIVE_IN_SOURCE_IND\n",
							"\t,'ED' AS DATA_IND\n",
							"\t,'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"d=spark.sql(qry_ins_new_rows.format(tablename,keycolumn,stagingtable,tablename,natural_key, natural_key ))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 102
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Update or expire the rows with existing instance of the changed rows:\n",
							"qry_4_upd_changes_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.{}\n",
							"AND LOWER(B.{}) = LOWER(B.existing_{}) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\"\"\"\n",
							"f=spark.sql(qry_4_upd_changes_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key,natural_key))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 60
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Insert for changed rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry_ins_changed_rows=\"\"\"\n",
							"INSERT INTO {}\n",
							"\tselect \n",
							"    existing_MARKET_KEY\n",
							"    ,1 + existing_MARKET_VERSION as VERSION\n",
							",\tOPPORTUNITY_KEY\n",
							",EMPLOYEE_KEY\n",
							",-1 as REGION_LEVEL_4_KEY\n",
							",INDUSTRY_SECTOR_KEY\n",
							",MARKET_KEY\n",
							",SALES_STAGE_KEY\n",
							",PREV_SALES_STAGE_KEY\n",
							",date_format(DECISION_DATE,\"yyyyMMdd\") as DECISION_DATE_KEY\n",
							",-1 as EIW_DATE_KEY\n",
							",date_format(OPPORTUNITY_CLOSE_DATE,\"yyyyMMdd\") as OPPORTUNITY_CLOSE_DATE_KEY\n",
							",date_format(OPPORTUNITY_CREATE_DATE,\"yyyyMMdd\") as OPPORTUNITY_CREATE_DATE_KEY\n",
							",date_format(OPPORTUNITY_UPDATE_DATE,\"yyyyMMdd\") as OPPORTUNITY_UPDATE_DATE_KEY\n",
							",date_format(OPPORTUNITY_SALES_STAGE_UPDATE_DATE,\"yyyyMMdd\") as OPPORTUNITY_SALES_STAGE_UPDATE_DATE_KEY\n",
							",-1 as GEOGRAPHY_KEY\n",
							",TOTAL_OPP_VAL_USD\n",
							",'Y' AS CURRENT_IND\n",
							",CURRENT_TIMESTAMP AS EXTRACT_DT\n",
							",CURRENT_TIMESTAMP AS REC_START_DT\n",
							",'9999-12-31 00:00:00.000' as REC_END_DT\n",
							",'ESA' AS SOURCE_SYSTEM\n",
							",column_hash as REC_CHECKSUM\n",
							",'U' as REC_STATUS\n",
							",existing_IMG_LST_UPD_DT as IMG_LST_UPD_DT\n",
							",CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							",date_format(PROP_DELIVERY_DATE,\"yyyyMMdd\") as PROP_DELIVERY_DATE_KEY\n",
							",PERF_CRITERIA_CONF\n",
							",PRICE_AGREEMENT\n",
							",PRICING_SECURED\n",
							",EXECUTIVE_SPONSOR_EMP_KEY\n",
							",'Y' as OPP_ACTIVE_IN_SOURCE_IND\n",
							",'ED' AS DATA_IND\n",
							",'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from {} A \n",
							"WHERE LOWER({}) = LOWER(existing_{}) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"e=spark.sql(qry_ins_changed_rows.format(tablename,stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 52
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Soft Deletes - Update Active_Ind for the rows no longer active in source:\n",
							"\n",
							"qry_4_upd_deleted_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"\n",
							";\n",
							"\"\"\"\n",
							"g=spark.sql(qry_4_upd_deleted_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 53
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select 'Number of Active Rows' as Title,count(*) from etlhubconfirmed.dht_opportunity_daily_ft where CURRENT_IND='Y'\n",
							";\n",
							"\n",
							"\n",
							"select 'Duplicate Rows based on surrogate key' as Title,OPPORTUNITY_FACT_KEY,count(*) from etlhubconfirmed.dht_opportunity_daily_ft \n",
							"where CURRENT_IND='Y'\n",
							"    group by OPPORTUNITY_FACT_KEY\n",
							"    having count(*)>1\n",
							";\n",
							"\n",
							"select 'Duplicate Rows based on natural key' as Title,OPPORTUNITY_KEY,count(*) from etlhubconfirmed.dht_opportunity_daily_ft\n",
							" where CURRENT_IND='Y'\n",
							"    group by OPPORTUNITY_KEY\n",
							"    having count(*)>1\n",
							";\n",
							"\n",
							"select 'New rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_opportunity_daily_ft where CURRENT_IND='Y'\n",
							"    and date(extract_dt)=current_date and rec_status='I'\n",
							";\n",
							"\n",
							"select 'Changed rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_opportunity_daily_ft where CURRENT_IND='Y'\n",
							"    and date(extract_dt)=current_date and rec_status='U'\n",
							";\n",
							"\n",
							"select 'Changed rows updated in this run' as Title,count(*) from etlhubconfirmed.dht_opportunity_daily_ft where CURRENT_IND='N'\n",
							"    and date(REC_END_DT)=current_date --and rec_status='I'\n",
							";\n",
							"\n",
							"select 'Rows no longer active in source' as Title,count(*) from etlhubconfirmed.dht_opportunity_daily_ft where CURRENT_IND='Y'\n",
							"    AND \n",
							"     date(REC_END_DT)=current_date --and rec_status='I'\n",
							";\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 104
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_dht_opportunity_load')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPool001494New",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "472g",
					"driverCores": 80,
					"executorMemory": "472g",
					"executorCores": 80,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5b46505a-a5cb-4d87-be77-bb5df53f2aea"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPool001494New",
						"name": "sPool001494New",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPool001494New",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 80,
						"memory": 472,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Opportunity Data load**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubconfirmed.dht_market by sourceing data from ESA Kyndryl.\n",
							"Since ESA is not accessible from Azure, a datastage job srcESA_Opportunity_Azure is executed on datastage to push the ESA extract to adls.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws,trim\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.functions import col\n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"import os\n",
							"import sys\n",
							"from datetime import datetime"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/ESA/' # fill in your relative folder path \n",
							"relative_archive_path='archive/'\n",
							"file_name='DS_OPPORTUNITY_DATA.csv'  \n",
							"natural_key=\"OPPORTUNITY_NUM\"\n",
							"tablename=\"etlhubConfirmed.dht_opportunity\"\n",
							"stagingtable=\"DHTS_OPPORTUNITY\"\n",
							"keycolumn=\"OPPORTUNITY_KEY\"\n",
							"date = datetime.now().strftime(\"%Y-%m-%d-%I:%M:%S_%p\")\n",
							"print(date)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Copy source CSV file to archive folder including timestamp in file name"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/ESA/DS_SALES_STAGE_DATA.csv"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"adls_arch_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_archive_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"relative_archive_path\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"arch_fil_name=file_name.rsplit(\".\")\n",
							"csv_archive_path = adls_arch_path + arch_fil_name[0] + '_' + date + '.' + arch_fil_name[1]\n",
							"print (csv_archive_path)\n",
							"incrementalData_DF = spark.read.load(csv_path,format=\"csv\", sep=\"||\", header=\"true\")\n",
							"#incrementalData_DF.show()\n",
							"incrementalData_DF.write.csv(csv_archive_path, header = 'true')\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"print(col_list)\n",
							"incrementalData_SDF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_SDF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Create a dataframe with target deltalake table data with necessary columns\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1, col(natural_key) == col(\"existing_\" + natural_key), \"fullouter\")\n",
							"\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"fullJoin2.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .option(\"overwriteSchema\", \"true\") \\\n",
							"  .option(\"path\", \"abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/staging/{}\".format(stagingtable)) \\\n",
							"  .saveAsTable(stagingtable)\n",
							"   #.option(\"overwriteSchema\", \"true\") \\  .save(\"\") \\"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Data Validation Checks :\n",
							"\n",
							"- Check for duplicates in Source data based on natural key\n",
							"- Check for duplicates in target delta lake table based on natural key\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} WHERE CURRENT_IND='Y' GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"df3=spark.sql(qry3.format(natural_key,natural_key))\n",
							"cnt1=df3.count()\n",
							"\n",
							"print (cnt1)\n",
							"if cnt1 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in source:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(1)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry_4_recon1=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable,tablename,natural_key,natural_key))\n",
							"print('New records or Inserts are:') \n",
							"df_4_recon1.show()\n",
							"\n",
							"qry_4_recon2=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE LOWER({}) = LOWER(existing_{}) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon2=spark.sql(qry_4_recon2.format(stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))\n",
							"print('Changed records or Updates are:') \n",
							"df_4_recon2.show()\n",
							"\n",
							"qry_4_recon3=\"\"\"\n",
							"SELECT COUNT(*) from \n",
							"{} A\n",
							"join fullJoin B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"\n",
							"\"\"\"\n",
							"\n",
							"df_4_recon3=spark.sql(qry_4_recon3.format(tablename,natural_key,natural_key,natural_key))\n",
							"print('Soft Deletes are:') \n",
							"df_4_recon3.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"SELECT count(*) FROM etlhubconfirmed.dht_opportunity where current_ind='Y'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select \r\n",
							"\t* \r\n",
							"    from dhts_opportunity A \r\n",
							"WHERE existing_REC_CHECKSUM is null"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry_4_recon1=\"\"\"\n",
							"SELECT COUNT(*) from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.{}=B.{}\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(stagingtable, tablename, natural_key, natural_key))\n",
							"\n",
							"qry_ins_new_rows=\"\"\"\n",
							"\n",
							"INSERT INTO {}\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS {},1 as OPPORTUNITY_VERSION, \n",
							"A.OPPORTUNITY_NUM,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT,\n",
							"A.ISA_CODE,A.ARCHIVE_REASON_CODE,A.BUSINESS_TRANSACTION_TYPE,A.CLIENT_REPRESENTATIVE_NAME,A.COMPETITOR_LIST, \n",
							"A.IDENTIFIER_NAME,A.OPPORTUNITY_NAME,A.OPPORTUNITY_SOURCE_CODE,A.SBS_SOL_VALID_IND, \n",
							"A.OPPORTUNITY_IDENTIFIER,A.ROGUE_IND,A.REASON_TO_ACT,A.SOLUTION_CATG,A.BRAND_SPONSOR_LIST,A.GBS_GEOGRAPHY_NAME, \n",
							"A.GBS_BUSINESS_UNIT_GROUP_NAME,A.GBS_BUSINESS_UNIT_NAME,A.TAG_LIST,A.OPPORTUNITY_LEGACY_NO,'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND  from {} A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE trim(A.{})=trim(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							";\n",
							"\"\"\"\n",
							"d=spark.sql(qry_ins_new_rows.format(tablename,keycolumn,stagingtable,tablename,natural_key, natural_key ))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"SELECT * from etlhubconfirmed.dht_opportunity"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Update or expire the rows with existing instance of the changed rows:\n",
							"qry_4_upd_changes_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.{}\n",
							"AND LOWER(B.{}) = LOWER(B.existing_{}) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\"\"\"\n",
							"f=spark.sql(qry_4_upd_changes_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key,natural_key))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"--existing data\n",
							"select 'existing data',count(*) from dhts_opportunity where existing_CURRENT_IND='Y';\n",
							"  \n",
							"select 'rows for Update', count(*)\n",
							"from etlhubConfirmed.dht_opportunity B\n",
							"join\n",
							"dhts_opportunity A \n",
							"WHERE trim(A.OPPORTUNITY_NUM) = trim(B.OPPORTUNITY_NUM)\n",
							"and LOWER(A.OPPORTUNITY_NUM) = LOWER(A.existing_OPPORTUNITY_NUM) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND B.REC_START_DT=A.existing_REC_START_DT\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.dht_opportunity B\n",
							"WHERE trim(LOWER(A.OPPORTUNITY_NUM))=trim(LOWER(B.OPPORTUNITY_NUM))\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"SELECT COUNT(*) FROM etlhubConfirmed.dht_opportunity A\n",
							"JOIN dhts_opportunity B\n",
							"ON trim(A.OPPORTUNITY_NUM) = trim(B.OPPORTUNITY_NUM)\n",
							"AND trim(LOWER(B.OPPORTUNITY_NUM)) = trim(LOWER(B.existing_OPPORTUNITY_NUM)) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							";"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Insert for changed rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry_ins_changed_rows=\"\"\"\n",
							"INSERT INTO {}\n",
							"\tselect \n",
							"    existing_OPPORTUNITY_KEY\n",
							"    ,1 + existing_OPPORTUNITY_VERSION as OPPORTUNITY_VERSION,\n",
							"    A.OPPORTUNITY_NUM,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'U' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT,\n",
							"A.ISA_CODE,A.ARCHIVE_REASON_CODE,A.BUSINESS_TRANSACTION_TYPE,A.CLIENT_REPRESENTATIVE_NAME,A.COMPETITOR_LIST, \n",
							"A.IDENTIFIER_NAME,A.OPPORTUNITY_NAME,A.OPPORTUNITY_SOURCE_CODE,A.SBS_SOL_VALID_IND, \n",
							"A.OPPORTUNITY_IDENTIFIER,A.ROGUE_IND,A.REASON_TO_ACT,A.SOLUTION_CATG,A.BRAND_SPONSOR_LIST,A.GBS_GEOGRAPHY_NAME, \n",
							"A.GBS_BUSINESS_UNIT_GROUP_NAME,A.GBS_BUSINESS_UNIT_NAME,A.TAG_LIST,A.OPPORTUNITY_LEGACY_NO,'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND  \n",
							"from {} A \n",
							"WHERE trim(LOWER({})) = trim(LOWER(existing_{})) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE LOWER(A.{})=LOWER(B.{})\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"e=spark.sql(qry_ins_changed_rows.format(tablename,stagingtable,natural_key,natural_key,tablename,natural_key,natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Soft Deletes - Update Active_Ind for the rows no longer active in source:\n",
							"\n",
							"qry_4_upd_deleted_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING {} B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"\n",
							";\n",
							"\"\"\"\n",
							"g=spark.sql(qry_4_upd_deleted_rows.format(tablename,stagingtable,natural_key,natural_key,natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select 'Number of Active Rows' as Title,count(*) from etlhubconfirmed.DHT_OPPORTUNITY where CURRENT_IND='Y'\n",
							";\n",
							"\n",
							"\n",
							"select 'Duplicate Rows based on surrogate key' as Title,OPPORTUNITY_KEY,count(*) from etlhubconfirmed.DHT_OPPORTUNITY where CURRENT_IND='Y'\n",
							"    group by OPPORTUNITY_KEY\n",
							"    having count(*)>1\n",
							";\n",
							"\n",
							"select 'Duplicate Rows based on natural key' as Title,OPPORTUNITY_NUM,count(*) from etlhubconfirmed.DHT_OPPORTUNITY where CURRENT_IND='Y'\n",
							"    group by OPPORTUNITY_NUM\n",
							"    having count(*)>1\n",
							";\n",
							"\n",
							"select 'New rows inserted in this run' as Title,count(*) from etlhubconfirmed.DHT_OPPORTUNITY where CURRENT_IND='Y'\n",
							"    and date(extract_dt)=current_date and rec_status='I'\n",
							";\n",
							"\n",
							"select 'Changed rows inserted in this run' as Title,count(*) from etlhubconfirmed.DHT_OPPORTUNITY where CURRENT_IND='Y'\n",
							"    and date(extract_dt)=current_date and rec_status='U'\n",
							";\n",
							"\n",
							"select 'Changed rows updated in this run' as Title,count(*) from etlhubconfirmed.DHT_OPPORTUNITY where CURRENT_IND='N'\n",
							"    and date(REC_END_DT)=current_date --and rec_status='I'\n",
							";\n",
							"\n",
							"select 'Rows no longer active in source' as Title,count(*) from etlhubconfirmed.DHT_OPPORTUNITY where CURRENT_IND='Y'\n",
							"    AND \n",
							"     date(REC_END_DT)=current_date --and rec_status='I'\n",
							";\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT COUNT(*) FROM etlhubconfirmed.DHT_OPPORTUNITY\n",
							"WHERE CURRENT_IND='N' and DATE(REC_END_DT)<>'9999-12-31'\n",
							"    and date(img_lst_upd_dt)=current_date\n",
							"        limit 1\n",
							";\n",
							"\n",
							"\n",
							"select count(*) from etlhubconfirmed.DHT_OPPORTUNITY where CURRENT_IND='Y'\n",
							"   and date(extract_dt)=current_date --and rec_status='I'\n",
							";\n",
							"\n",
							"select count(*) from etlhubconfirmed.DHT_OPPORTUNITY\n",
							"WHERE ACTIVE_IN_SOURCE_IND='N'\n",
							"    and date(IMG_LST_UPD_DT)=current_date\n",
							"    ;\n",
							"UPDATE etlhubconfirmed.DHT_OPPORTUNITY\n",
							"set CURRENT_IND='Y', \n",
							"REC_END_DT='9999-12-31'\n",
							"  WHERE CURRENT_IND='N' and DATE(REC_END_DT)<>'9999-12-31'\n",
							"    and date(img_lst_upd_dt)=current_date\n",
							"        ;  "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 25
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_dht_opportunity_load_org')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "93902ede-f9d2-4f95-a962-0fbaeec2f3cd"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**DEAL Dimension load**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubconfirme.dht_deal_dim by sourceing data from DCA deal data.\n",
							"Since DCA is not accessible from Azure, a datastage job src_DCA_DEAL_Azure is executed on datastage to push the DCA extract to adls.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws,trim\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"from pyspark.sql.types import *\n",
							"#import os\n",
							"import sys"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 128
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/ESA/' # fill in your relative folder path \n",
							"file_name='DS_OPPORTUNITY_DATA_UPD.csv' \n",
							"natural_key=\"OPPORTUNITY_NUM\"\n",
							"#tgt_natural_key=\"DEAL_IDENTIFIER\"\n",
							"tablename=\"etlhubConfirmed.dht_opportunity\"\n",
							"keycolumn=\"OPPORTUNITY_KEY\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 129
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/DCA/DS_DEALS_DATA.csv"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"incrementalData_DF=spark.read.load(csv_path,format=\"csv\", sep=\"||\", header=\"true\")\n",
							"#incrementalData_DF.printSchema()\n",
							"incrementalData_DF.createOrReplaceTempView(\"incrementalSrcData_DF\")\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"    #print (col_list)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"incrementalData_DF1=incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"#incrementalData_DF2.printSchema()\n",
							"incrementalData_DF2.createOrReplaceTempView('incrementalData_SrcDF')\n",
							"\n",
							"#Create a deltalake table with necessary columns\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\n",
							"existingDataDF.createOrReplaceTempView('existingDF')\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"#existingDataDF1.printSchema()\n",
							"#incrementalData_DF2.printSchema()\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataTgtDF')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,trim(incrementalData_DF2.OPPORTUNITY_NUM) == trim(existingDataDF1.existing_OPPORTUNITY_NUM), \"fullouter\") \n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 130
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from incrementalData_SrcDF where opportunity_num in ('SOC-0XKCQTT','SOC-0TYPD7Z')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 131
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Data Validation Checks :\n",
							"\n",
							"- Check for duplicates in Source data based on natural key\n",
							"- Check for duplicates in target delta lake table based on natural key\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\r\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalSrcData_DF GROUP BY {} HAVING COUNT(*)>1\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"df3=spark.sql(qry3.format(natural_key,natural_key))\r\n",
							"cnt1=df3.count()\r\n",
							"\r\n",
							"print (cnt1)\r\n",
							"if cnt1 == 0:\r\n",
							"    print(\"No Duplicates in source data\")\r\n",
							"    status = 'success'\r\n",
							"else:\r\n",
							"    print(\"Below are the duplicates in source:\")\r\n",
							"    df3.show()\r\n",
							"    status = 'fail'\r\n",
							"    #os.abort() this will take the spark cluster also down\r\n",
							"    sys.exit(1)\r\n",
							"    print(\"This will not be printed\")\r\n",
							"\r\n",
							"qry4=\"\"\"\r\n",
							"SELECT COUNT(*) as CNT, {} FROM {} where CURRENT_IND= \"Y\" GROUP BY {} HAVING COUNT(*)>1;\r\n",
							"\"\"\"\r\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\r\n",
							"cnt2=df4.count()\r\n",
							"print (cnt2)\r\n",
							"if cnt2 == 0:\r\n",
							"    print(\"no duplicates in target delta lake table\")\r\n",
							"    status = 'success'\r\n",
							"else:\r\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\r\n",
							"    df4.show()\r\n",
							"    status = 'fail'\r\n",
							"    #os.abort() this will take the spark cluster also down\r\n",
							"    sys.exit(2)\r\n",
							"    print(\"This will not be printed\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 132
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\r\n",
							"qry_insert=\"\"\"\r\n",
							"INSERT INTO etlhubConfirmed.dht_opportunity\r\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS OPPORTUNITY_KEY,1 as OPPORTUNITY_VERSION , \r\n",
							"A.OPPORTUNITY_NUM,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT,\r\n",
							"A.ISA_CODE,A.ARCHIVE_REASON_CODE,A.BUSINESS_TRANSACTION_TYPE,A.CLIENT_REPRESENTATIVE_NAME,A.COMPETITOR_LIST, \r\n",
							"A.IDENTIFIER_NAME,A.OPPORTUNITY_NAME,A.OPPORTUNITY_SOURCE_CODE,A.SBS_SOL_VALID_IND, \r\n",
							"A.OPPORTUNITY_IDENTIFIER,A.ROGUE_IND,A.REASON_TO_ACT,A.SOLUTION_CATG,A.BRAND_SPONSOR_LIST,A.GBS_GEOGRAPHY_NAME, \r\n",
							"A.GBS_BUSINESS_UNIT_GROUP_NAME,A.GBS_BUSINESS_UNIT_NAME,A.TAG_LIST,A.OPPORTUNITY_LEGACY_NO,'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND  \r\n",
							"from fullJoin A \r\n",
							"LEFT JOIN etlhubConfirmed.dht_opportunity B\r\n",
							"ON TRIM(A.OPPORTUNITY_NUM)=TRIM(B.OPPORTUNITY_NUM)\r\n",
							"AND CURRENT_IND='Y' \r\n",
							"WHERE existing_REC_CHECKSUM is null\r\n",
							"AND COALESCE(B.REC_CHECKSUM,'') <> COALESCE(A.column_hash,'');\r\n",
							"\"\"\"\r\n",
							"#d=spark.sql(qry_insert)\r\n",
							"#d.show(10)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select count(*) from etlhubConfirmed.dht_opportunity "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#update the existing rows in target table with new values and record status as U and expiry the existing records with current indicator as N.\r\n",
							"#delete FROM etlhubconfirmed.dht_employee;\r\n",
							"\r\n",
							"updins=\"\"\"\r\n",
							"INSERT INTO etlhubConfirmed.dht_opportunity\r\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS OPPORTUNITY_KEY,1 as OPPORTUNITY_VERSION , \r\n",
							"A.OPPORTUNITY_NUM,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'U' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT,\r\n",
							"A.ISA_CODE,A.ARCHIVE_REASON_CODE,A.BUSINESS_TRANSACTION_TYPE,A.CLIENT_REPRESENTATIVE_NAME,A.COMPETITOR_LIST, \r\n",
							"A.IDENTIFIER_NAME,A.OPPORTUNITY_NAME,A.OPPORTUNITY_SOURCE_CODE,A.SBS_SOL_VALID_IND, \r\n",
							"A.OPPORTUNITY_IDENTIFIER,A.ROGUE_IND,A.REASON_TO_ACT,A.SOLUTION_CATG,A.BRAND_SPONSOR_LIST,A.GBS_GEOGRAPHY_NAME, \r\n",
							"A.GBS_BUSINESS_UNIT_GROUP_NAME,A.GBS_BUSINESS_UNIT_NAME,A.TAG_LIST,A.OPPORTUNITY_LEGACY_NO,'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND  \r\n",
							"from fullJoin A \r\n",
							"WHERE TRIM(OPPORTUNITY_NUM) = TRIM(existing_OPPORTUNITY_NUM) and LOWER(column_hash) <> LOWER(existing_REC_CHECKSUM)\r\n",
							"AND NOT EXISTS\r\n",
							"(SELECT 1 FROM etlhubConfirmed.dht_opportunity B\r\n",
							"WHERE TRIM(A.OPPORTUNITY_NUM)=TRIM(B.OPPORTUNITY_NUM)\r\n",
							"and b.CURRENT_IND='Y'\r\n",
							"AND coalesce(A.column_hash,'')=coalesce(B.REC_CHECKSUM,'')) ;\r\n",
							"\"\"\"\r\n",
							"d1=spark.sql(updins)\r\n",
							"#d1.show()\r\n",
							"\r\n",
							"expqry=\"\"\"\r\n",
							"MERGE INTO etlhubConfirmed.dht_opportunity A\r\n",
							"USING fullJoin B\r\n",
							"ON B.OPPORTUNITY_NUM = A.OPPORTUNITY_NUM\r\n",
							"AND TRIM(A.OPPORTUNITY_NUM) = TRIM(B.existing_OPPORTUNITY_NUM) and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\r\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\r\n",
							",REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds\r\n",
							",IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"d2=spark.sql(expqry)\r\n",
							"#d2.show()\r\n",
							"\r\n",
							"#Soft deletes or no longer active in source\r\n",
							"softdelqry=\"\"\"\r\n",
							"MERGE INTO etlhubConfirmed.dht_opportunity  A\r\n",
							"USING fullJoin B\r\n",
							"ON A.OPPORTUNITY_NUM = B.existing_OPPORTUNITY_NUM\r\n",
							"AND B.OPPORTUNITY_NUM is NULL\r\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N' ,IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"df3=spark.sql(softdelqry)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\r\n",
							"select 'Number of Active Rows' as Title,count(*) from etlhubconfirmed.dht_deal_dim where CURRENT_IND='Y'\r\n",
							";\r\n",
							"\r\n",
							"\r\n",
							"select 'Duplicate Rows based on surrogate key' as Title,DEAL_KEY,count(*) from etlhubconfirmed.dht_deal_dim where CURRENT_IND='Y'\r\n",
							"    group by DEAL_KEY\r\n",
							"    having count(*)>1\r\n",
							";\r\n",
							"\r\n",
							"select 'Duplicate Rows based on natural key' as Title,DEAL_ID,count(*) from etlhubconfirmed.dht_deal_dim where CURRENT_IND='Y'\r\n",
							"    group by DEAL_ID\r\n",
							"    having count(*)>1\r\n",
							";\r\n",
							"\r\n",
							"select 'New rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_deal_dim where CURRENT_IND='Y'\r\n",
							"    and date(extract_dt)='2022-06-28' and rec_status='I'\r\n",
							";\r\n",
							"\r\n",
							"select 'Changed rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_deal_dim where CURRENT_IND='Y'\r\n",
							"    and date(extract_dt)='2022-06-28' and rec_status='U'\r\n",
							";\r\n",
							"\r\n",
							"select 'Changed rows updated in this run' as Title,count(*) from etlhubconfirmed.dht_deal_dim where CURRENT_IND='N'\r\n",
							"    and date(REC_END_DT)='2022-06-28' --and rec_status='I'\r\n",
							";\r\n",
							"\r\n",
							"select 'Rows no longer active in source' as Title,count(*) from etlhubconfirmed.dht_deal_dim where CURRENT_IND='Y'\r\n",
							"    AND date(REC_END_DT)='2022-06-28' --and rec_status='I'\r\n",
							";\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							" from etlhubConfirmed.dht_deal_dim1 where ACTIVE_IN_SOURCE_IND='N' \r\n",
							"    --where DEAL_IDENTIFIER in (7840900,7840899)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 18
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_dht_opportunity_role_load')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "2f485584-92a8-4ced-b043-e01239742bfa"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Opportunity Role Dimension load**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubconfirmed.dht_opportunity_role by sourceing data from ESA database.\n",
							"Since ESA is not accessible from Azure, a datastage job src_ESA_opportunityRole_Azure is executed on datastage to push the ESA extract to adls.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws,trim\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"from pyspark.sql.functions import col\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"import os\n",
							"import sys\n",
							"from datetime import datetime"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/ESA/' # fill in your relative folder path \n",
							"relative_archive_path='archive/'\n",
							"file_name='DS_OPPORTUNITY_ROLE_DATA.csv' \n",
							"natural_key=\"ROLE_CD\"\n",
							"tablename=\"etlhubConfirmed.dht_opportunity_role\"\n",
							"#natural_key=\"ROLE_CD\"\n",
							"keycolumn=\"OPPORTUNITY_ROLE_KEY\"\n",
							"stagingtable=\"DHTS_OPPORTUNITY_ROLE\"\n",
							"date = datetime.now().strftime(\"%Y_%m_%d-%I:%M:%S_%p\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Copy source CSV file to archive folder including timestamp in file name"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/ESA/DS_OPPORTUNITY_ROLE_DATA.csv"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"adls_arch_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_archive_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"relative_archive_path\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"arch_fil_name=file_name.rsplit(\".\")\n",
							"csv_archive_path = adls_arch_path + arch_fil_name[0] + '_' + date + '.' + arch_fil_name[1]\n",
							"print (csv_archive_path)\n",
							"\n",
							"incrementalData_DF = spark.read.load(csv_path,format=\"csv\", sep=\"||\", header=\"true\")\n",
							"\n",
							"incrementalData_DF.write.csv(csv_archive_path, header = 'true')\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"#Add checks for duplicate data, Check for 1. dups in source data based on natural Key and\n",
							"# 2. Dups in target already\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Create a dataframe with target deltalake table data with necessary columns\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\n",
							"#existingDataDF=spark.sql(\"SELECT tgt.* from etlhubConfirmed.dht_opportunity_role tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,trim(incrementalData_DF2.ROLE_CD) == trim(existingDataDF1.existing_ROLE_CD), \"fullouter\") \n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"fullJoin2.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .option(\"overwriteSchema\", \"true\") \\\n",
							"  .option(\"path\", \"abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/staging/DHTS_OPPORTUNITY_ROLE\") \\\n",
							"  .saveAsTable(stagingtable)\n",
							"   #.option(\"overwriteSchema\", \"true\") \\  .save(\"\") \\"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from DHTS_OPPORTUNITY_ROLE"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Data Validation Checks :\n",
							"\n",
							"- Check for duplicates in Source data based on natural key\n",
							"- Check for duplicates in target delta lake table based on natural key\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM incrementalData_DF2 GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry4=\"\"\"\n",
							"SELECT COUNT(*) as CNT, {} FROM {} WHERE CURRENT_IND='Y' GROUP BY {} HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"df3=spark.sql(qry3.format(natural_key,natural_key))\n",
							"cnt1=df3.count()\n",
							"\n",
							"print (cnt1)\n",
							"if cnt1 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in source:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(1)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"df4=spark.sql(qry4.format(natural_key,tablename,natural_key))\n",
							"cnt2=df4.count()\n",
							"\n",
							"print (cnt2)\n",
							"if cnt2 == 0:\n",
							"    print(\"no duplicates in target delta lake table\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates in target delta lake table:\")\n",
							"    df4.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(2)\n",
							"    print(\"This will not be printed\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry_4_recon1=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from DHTS_OPPORTUNITY_ROLE A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.ROLE_CD=B.ROLE_CD\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(tablename))\n",
							"print('New records or Inserts are:') \n",
							"df_4_recon1.show()\n",
							"\n",
							"qry_4_recon2=\"\"\"\n",
							"\n",
							"SELECT COUNT(*) from DHTS_OPPORTUNITY_ROLE A \n",
							"WHERE LOWER(ROLE_CD) = LOWER(existing_ROLE_CD) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE TRIM(LOWER(A.ROLE_CD))=TRIM(LOWER(B.ROLE_CD))\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon2=spark.sql(qry_4_recon2.format(tablename))\n",
							"print('Changed records or Updates are:') \n",
							"df_4_recon2.show()\n",
							"\n",
							"qry_4_recon3=\"\"\"\n",
							"SELECT COUNT(*) from \n",
							"etlhubconfirmed.DHT_OPPORTUNITY_ROLE A\n",
							"join fullJoin B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"\n",
							"\"\"\"\n",
							"\n",
							"df_4_recon3=spark.sql(qry_4_recon3.format(natural_key,natural_key,natural_key))\n",
							"print('Soft Deletes are:') \n",
							"df_4_recon3.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"#delete FROM etlhubconfirmed.dht_opportunity_role;\n",
							"\n",
							"qry_4_recon1=\"\"\"\n",
							"SELECT COUNT(*) from DHTS_OPPORTUNITY_ROLE A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM {} B\n",
							"WHERE A.ROLE_CD=B.ROLE_CD\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"\"\"\"\n",
							"df_4_recon1=spark.sql(qry_4_recon1.format(tablename))\n",
							"\n",
							"qry_ins_new_rows=\"\"\"\n",
							"\n",
							"INSERT INTO etlhubconfirmed.dht_opportunity_role\n",
							"select \n",
							"COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS OPPORTUNITY_ROLE_KEY,1 as OPPORTUNITY_ROLE_VERSION \n",
							",ROLE_CD\n",
							",ROLE_DESC\n",
							",'Y' AS CURRENT_IND\n",
							",CURRENT_TIMESTAMP AS EXTRACT_DT\n",
							",CURRENT_TIMESTAMP AS REC_START_DT\n",
							",'9999-12-31 00:00:00.000' as REC_END_DT\n",
							",'ESA' AS SOURCE_SYSTEM\n",
							",column_hash as REC_CHECKSUM\n",
							",'I' as REC_STATUS\n",
							",current_timestamp as IMG_LST_UPD_DT\n",
							",CURRENT_TIMESTAMP AS IMG_CREATED_DT\n",
							",'ED' AS DATA_IND\n",
							",'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from DHTS_OPPORTUNITY_ROLE A \n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.dht_opportunity_role B\n",
							"WHERE A.ROLE_CD=B.ROLE_CD\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM)\n",
							"\n",
							"--limit 10\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"d=spark.sql(qry_ins_new_rows)\n",
							"\n",
							"#d.show(10)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Update or expire the rows with existing instance of the changed rows:\n",
							"qry_4_upd_changes_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING DHTS_OPPORTUNITY_ROLE B\n",
							"ON A.{} = B.{}\n",
							"AND LOWER(B.{}) = LOWER(B.existing_{}) \n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\n",
							"    ,REC_END_DT= REC_START_DT -  INTERVAL 1 seconds --current_timestamp --existing_REC_START_DT-1\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							";\n",
							"\"\"\"\n",
							"f=spark.sql(qry_4_upd_changes_rows.format(tablename,natural_key,natural_key,natural_key,natural_key))\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"#Insert for changed rows which are missing in target and present in source based on Natural Key.\n",
							"qry_ins_changed_rows=\"\"\"\n",
							"INSERT INTO etlhubconfirmed.dht_opportunity_role\n",
							"select A.existing_OPPORTUNITY_ROLE_KEY\n",
							",1 + existing_OPPORTUNITY_ROLE_VERSION as OPPORTUNITY_ROLE_VERSION\n",
							",ROLE_CD\n",
							",ROLE_DESC\n",
							",'Y' AS CURRENT_IND\n",
							",CURRENT_TIMESTAMP AS EXTRACT_DT\n",
							",CURRENT_TIMESTAMP AS REC_START_DT\n",
							",'9999-12-31 00:00:00.000' as REC_END_DT\n",
							",'ESA' AS SOURCE_SYSTEM\n",
							",column_hash as REC_CHECKSUM\n",
							",'U' as REC_STATUS\n",
							", CURRENT_TIMESTAMP as IMG_LST_UPD_DT\n",
							",A.existing_IMG_CREATED_DT AS IMG_CREATED_DT\n",
							",'LG' AS DATA_IND\n",
							",'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from DHTS_OPPORTUNITY_ROLE A \n",
							"WHERE TRIM(LOWER(ROLE_CD)) = TRIM(LOWER(existing_ROLE_CD)) \n",
							"and LOWER(column_hash) <> LOWER(existing_rec_checksum)\n",
							"AND NOT EXISTS\n",
							"(SELECT 1 FROM etlhubConfirmed.dht_opportunity_role B\n",
							"WHERE TRIM(LOWER(A.ROLE_CD))=TRIM(LOWER(B.ROLE_CD))\n",
							"and b.CURRENT_IND='Y'\n",
							"AND A.column_hash=B.REC_CHECKSUM\n",
							")\n",
							";\n",
							"\"\"\"\n",
							"e=spark.sql(qry_ins_changed_rows)\n",
							"#e.show(4)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Soft Deletes - Update Active_Ind for the rows no longer active in source:\n",
							"\n",
							"qry_4_upd_deleted_rows=\"\"\"\n",
							"\n",
							"MERGE INTO {} A\n",
							"USING DHTS_OPPORTUNITY_ROLE B\n",
							"ON A.{} = B.existing_{}\n",
							"AND B.{} is NULL\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' \n",
							"AND A.REC_START_DT=b.existing_REC_START_DT\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\n",
							"    ,IMG_LST_UPD_DT=current_timestamp\n",
							"\n",
							";\n",
							"\"\"\"\n",
							"g=spark.sql(qry_4_upd_deleted_rows.format(tablename,natural_key,natural_key,natural_key))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"select 'Number of Active Rows' as Title,count(*) from etlhubconfirmed.dht_opportunity_role where CURRENT_IND='Y'\n",
							";\n",
							"\n",
							"select 'Duplicate Rows based on surrogate key' as Title,OPPORTUNITY_ROLE_KEY,count(*) from etlhubconfirmed.dht_opportunity_role where CURRENT_IND='Y'\n",
							"    group by OPPORTUNITY_ROLE_KEY\n",
							"    having count(*)>1\n",
							";\n",
							"\n",
							"select 'Duplicate Rows based on natural key' as Title,ROLE_CD,count(*) from etlhubconfirmed.dht_opportunity_role where CURRENT_IND='Y'\n",
							"    group by ROLE_CD\n",
							"    having count(*)>1\n",
							";\n",
							"\n",
							"select 'New rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_opportunity_role where CURRENT_IND='Y'\n",
							"    and date(extract_dt)=CURRENT_DATE and rec_status='I'\n",
							";\n",
							"\n",
							"select 'Changed rows inserted in this run' as Title,count(*) from etlhubconfirmed.dht_opportunity_role where CURRENT_IND='Y'\n",
							"    and date(extract_dt)=current_date and rec_status='U'\n",
							";\n",
							"\n",
							"select 'Changed rows updated in this run' as Title,count(*) from etlhubconfirmed.dht_opportunity_role where CURRENT_IND='N'\n",
							"    and date(REC_END_DT)=CURRENT_DATE --and rec_status='I'\n",
							";\n",
							"\n",
							"select 'Rows no longer active in source' as Title,count(*) from etlhubconfirmed.dht_opportunity_role where CURRENT_IND='Y'\n",
							"    AND \n",
							"     date(REC_END_DT)=current_date --and rec_status='I'\n",
							";\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 35
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_dhtr_country')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "787f9ef6-050c-4679-a650-bbb951ae3066"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Country Reference Data**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubconfirmed.dhtr_country by sourceing data from GREIW database."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws,trim\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"from pyspark.sql.functions import col\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"import os\n",
							"import sys\n",
							"from datetime import datetime"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/reference/' # fill in your relative folder path \n",
							"relative_archive_path='archive/'\n",
							"file_name='Country(kyndryl).csv' \n",
							"tablename=\"etlhubconfirmed.dhtr_country\"\n",
							"stagingtable=\"dhtsr_country\"\n",
							"date = datetime.now().strftime(\"%Y_%m_%d-%I:%M:%S_%p\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Copy source CSV file to archive folder including timestamp in file name"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/reference/Country(kyndryl).csv"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"adls_arch_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_archive_path) \n",
							"print('Archive account path: ' + adls_arch_path) \n",
							"\n",
							"relative_archive_path\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"arch_fil_name=file_name.rsplit(\".\")\n",
							"csv_archive_path = adls_arch_path + arch_fil_name[0] + '_' + date + '.' + arch_fil_name[1]\n",
							"print (csv_archive_path)\n",
							"\n",
							"incrementalData_DF = spark.read.load(csv_path,format=\"csv\", sep=\",\", header=\"true\")\n",
							"\n",
							"incrementalData_DF.write.csv(csv_archive_path, header = 'true')\n",
							"#incrementalData_DF.show()\n",
							"\n",
							"incrementalData_DF.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .option(\"overwriteSchema\", \"true\") \\\n",
							"  .option(\"path\", \"abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/staging/{}\".format(stagingtable)) \\\n",
							"  .saveAsTable(stagingtable)\n",
							"   #.option(\"overwriteSchema\", \"true\") \\  .save(\"\") \\"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select count(*) from dhtsr_country"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry_4_delete=\"\"\"\n",
							"\n",
							"DELETE FROM {} \n",
							"\n",
							"\"\"\"\n",
							"df_4_delete=spark.sql(qry_4_delete.format(tablename))\n",
							"print('Existing records are deleted') \n",
							"\n",
							"qry_ins_new_rows=\"\"\"\n",
							"\n",
							"INSERT INTO {}\n",
							"select \n",
							"CODE,\n",
							"LONGDESCRIPTION,\n",
							"MEDIUMDESCRIPTION,\n",
							"SHORTDESCRIPTION,\n",
							"COMMENTS,\n",
							"RECORDSTATUS,\n",
							"ROWCREATETS,\n",
							"ROWUPDATETS,\n",
							"COUNTRY_ALPHA3_COMPLIANCE_CD,\n",
							"COUNTRY_NUM3_COMPLIANCE_CD\n",
							"from {}\n",
							";\n",
							"\"\"\"\n",
							"d=spark.sql(qry_ins_new_rows.format(tablename,stagingtable))\n",
							"\n",
							"#d.show(10)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select count(*) from etlhubconfirmed.dhtr_country"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_dhtr_sample')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts/GREIW"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "45df0186-4d83-4e27-8e18-b2c15bd4c873"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**SPACE CLASS Reference Data**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubconfirmed.dhtr_space_class by sourceing data from GREIW database."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws,trim\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"from pyspark.sql.functions import col\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"import os\n",
							"import sys\n",
							"from datetime import datetime\n",
							"sc = SparkSession.builder.master(\"local\").appName(\"Test\").getOrCreate()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/reference/' # fill in your relative folder path \n",
							"relative_archive_path='archive/'\n",
							"file_name='space_class.csv' \n",
							"tablename=\"etlhubconfirmed.dhtr_space_class\"\n",
							"stagingtable=\"dhtsr_space_class\"\n",
							"date = datetime.now().strftime(\"%Y_%m_%d-%I:%M:%S_%p\")\n",
							"sc = SparkSession.builder.master(\"local\").appName(\"Test\").getOrCreate()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Copy source CSV file to archive folder including timestamp in file name"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/reference/space_class.csv"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"adls_arch_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_archive_path) \n",
							"print('Archive account path: ' + adls_arch_path) \n",
							"\n",
							"relative_archive_path\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"arch_fil_name=file_name.rsplit(\".\")\n",
							"csv_archive_path = adls_arch_path + arch_fil_name[0] + '_' + date + '.' + arch_fil_name[1]\n",
							"print (csv_archive_path)\n",
							"\n",
							"incrementalData_DF = spark.read.load(csv_path,format=\"csv\", sep=\",\", header=\"true\")\n",
							"\n",
							"incrementalData_DF.write.csv(csv_archive_path, header = 'true')\n",
							"#incrementalData_DF.show()\n",
							"\n",
							"incrementalData_DF.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .option(\"overwriteSchema\", \"true\") \\\n",
							"  .option(\"path\", \"abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/staging/{}\".format(stagingtable)) \\\n",
							"  .saveAsTable(stagingtable)\n",
							"   #.option(\"overwriteSchema\", \"true\") \\  .save(\"\") \\"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from dhtsr_space_class"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry_4_delete=\"\"\"\n",
							"\n",
							"DELETE FROM {} \n",
							"\n",
							"\"\"\"\n",
							"df_4_delete=spark.sql(qry_4_delete.format(tablename))\n",
							"print('Existing records are deleted') \n",
							"\n",
							"qry_ins_new_rows=\"\"\"\n",
							"\n",
							"INSERT INTO {}\n",
							"select \n",
							"trim(GROUP) ,\n",
							"trim(OCCUPANCY_TYPE),\n",
							"trim(SPACE_CLASS_CODE1),\n",
							"trim(SPACE_CLASS_CODE2),\n",
							"trim(SPACE_CLASS),\n",
							"trim(SPACE_CLASS_DESCR)\n",
							"from {}\n",
							";\n",
							"\"\"\"\n",
							"d=spark.sql(qry_ins_new_rows.format(tablename,stagingtable))\n",
							"\n",
							"#d.show(10)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select count(*) from etlhubconfirmed.dhtr_space_class"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 21
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_dhtr_space_class')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts/GREIW"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "fbbfbaa8-87d5-496d-97fe-b4677cd2d250"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**SPACE CLASS Reference Data**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubconfirmed.dhtr_space_class by sourceing data from GREIW database."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws,trim\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"from pyspark.sql.functions import col\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"import os\n",
							"import sys\n",
							"from datetime import datetime"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/reference/' # fill in your relative folder path \n",
							"relative_archive_path='archive/'\n",
							"file_name='space_class.csv' \n",
							"tablename=\"etlhubconfirmed.dhtr_space_class\"\n",
							"stagingtable=\"dhtsr_space_class\"\n",
							"date = datetime.now().strftime(\"%Y_%m_%d-%I:%M:%S_%p\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Copy source CSV file to archive folder including timestamp in file name"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/reference/space_class.csv"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"adls_arch_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_archive_path) \n",
							"print('Archive account path: ' + adls_arch_path) \n",
							"\n",
							"relative_archive_path\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"arch_fil_name=file_name.rsplit(\".\")\n",
							"csv_archive_path = adls_arch_path + arch_fil_name[0] + '_' + date + '.' + arch_fil_name[1]\n",
							"print (csv_archive_path)\n",
							"\n",
							"incrementalData_DF = spark.read.load(csv_path,format=\"csv\", sep=\",\", header=\"true\")\n",
							"\n",
							"incrementalData_DF.write.csv(csv_archive_path, header = 'true')\n",
							"#incrementalData_DF.show()\n",
							"\n",
							"incrementalData_DF.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .option(\"overwriteSchema\", \"true\") \\\n",
							"  .option(\"path\", \"abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/staging/{}\".format(stagingtable)) \\\n",
							"  .saveAsTable(stagingtable)\n",
							"   #.option(\"overwriteSchema\", \"true\") \\  .save(\"\") \\"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from dhtsr_space_class"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry_4_delete=\"\"\"\n",
							"\n",
							"DELETE FROM {} \n",
							"\n",
							"\"\"\"\n",
							"df_4_delete=spark.sql(qry_4_delete.format(tablename))\n",
							"print('Existing records are deleted') \n",
							"\n",
							"qry_ins_new_rows=\"\"\"\n",
							"\n",
							"INSERT INTO {}\n",
							"select \n",
							"trim(GROUP) ,\n",
							"trim(OCCUPANCY_TYPE),\n",
							"trim(SPACE_CLASS_CODE1),\n",
							"trim(SPACE_CLASS_CODE2),\n",
							"trim(SPACE_CLASS),\n",
							"trim(SPACE_CLASS_DESCR)\n",
							"from {}\n",
							";\n",
							"\"\"\"\n",
							"d=spark.sql(qry_ins_new_rows.format(tablename,stagingtable))\n",
							"\n",
							"#d.show(10)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select count(*) from etlhubconfirmed.dhtr_space_class"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 21
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_dhtr_space_floor_level')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c5efae21-5157-43a5-9187-385a9ec2873f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Space Floor Level Reference Data**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubconfirmed.dhtr_space_floor_level by sourceing data from GREIW database."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws,trim\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"from pyspark.sql.functions import col\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"import os\n",
							"import sys\n",
							"from datetime import datetime"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/reference/' # fill in your relative folder path \n",
							"relative_archive_path='archive/'\n",
							"file_name='DS_space_floor_level.csv' \n",
							"tablename=\"etlhubconfirmed.dhtr_space_floor_level\"\n",
							"stagingtable=\"dhtsr_space_floor_level\"\n",
							"date = datetime.now().strftime(\"%Y_%m_%d-%I:%M:%S_%p\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Copy source CSV file to archive folder including timestamp in file name"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/reference/DS_space_floor_level.csv"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"adls_arch_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_archive_path) \n",
							"print('Archive account path: ' + adls_arch_path) \n",
							"\n",
							"relative_archive_path\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"arch_fil_name=file_name.rsplit(\".\")\n",
							"csv_archive_path = adls_arch_path + arch_fil_name[0] + '_' + date + '.' + arch_fil_name[1]\n",
							"print (csv_archive_path)\n",
							"\n",
							"incrementalData_DF = spark.read.load(csv_path,format=\"csv\", sep=\",\", header=\"true\")\n",
							"\n",
							"incrementalData_DF.write.csv(csv_archive_path, header = 'true')\n",
							"#incrementalData_DF.show()\n",
							"\n",
							"incrementalData_DF.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .option(\"overwriteSchema\", \"true\") \\\n",
							"  .option(\"path\", \"abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/staging/{}\".format(stagingtable)) \\\n",
							"  .saveAsTable(stagingtable)\n",
							"   #.option(\"overwriteSchema\", \"true\") \\  .save(\"\") \\"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select count(*) from dhtsr_space_floor_level"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry_4_delete=\"\"\"\n",
							"\n",
							"DELETE FROM {} \n",
							"\n",
							"\"\"\"\n",
							"df_4_delete=spark.sql(qry_4_delete.format(tablename))\n",
							"print('Existing records are deleted') \n",
							"\n",
							"qry_ins_new_rows=\"\"\"\n",
							"\n",
							"INSERT INTO {}\n",
							"select \n",
							"FLOOR_ID,\n",
							"FLOOR_DESCR,\n",
							"USAGE_NOTES\n",
							"from {}\n",
							";\n",
							"\"\"\"\n",
							"d=spark.sql(qry_ins_new_rows.format(tablename,stagingtable))\n",
							"\n",
							"#d.show(10)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from etlhubconfirmed.dhtr_space_floor_level"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_dhtr_stte_prov')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts/GREIW"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "fb64267b-8011-4725-902f-ac62178cc1dd"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**STTE PROV Reference Data**\n",
							"\n",
							"This notebook loads data to deltalake table etlhubconfirmed.dhtr_stte_prov by sourceing data from GREIW database."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws,trim\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"from pyspark.sql.functions import col\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"import os\n",
							"import sys\n",
							"from datetime import datetime"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/reference/' # fill in your relative folder path \n",
							"relative_archive_path='archive/'\n",
							"file_name='stte_prov.csv' \n",
							"tablename=\"etlhubconfirmed.dhtr_stte_prov\"\n",
							"stagingtable=\"dhtsr_stte_prov\"\n",
							"date = datetime.now().strftime(\"%Y_%m_%d-%I:%M:%S_%p\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Copy source CSV file to archive folder including timestamp in file name"
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"#Read data from adls csv file extracted from source at \n",
							"https://adls4fsoetlhubdevuseast.dfs.core.windows.net/etlhubfilestorage/extract/reference/space_class.csv"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"adls_arch_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_archive_path) \n",
							"print('Archive account path: ' + adls_arch_path) \n",
							"\n",
							"relative_archive_path\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"arch_fil_name=file_name.rsplit(\".\")\n",
							"csv_archive_path = adls_arch_path + arch_fil_name[0] + '_' + date + '.' + arch_fil_name[1]\n",
							"print (csv_archive_path)\n",
							"\n",
							"incrementalData_DF = spark.read.load(csv_path,format=\"csv\", sep=\",\", header=\"true\")\n",
							"\n",
							"incrementalData_DF.write.csv(csv_archive_path, header = 'true')\n",
							"#incrementalData_DF.show()\n",
							"\n",
							"incrementalData_DF.write \\\n",
							"  .format(\"delta\") \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .option(\"overwriteSchema\", \"true\") \\\n",
							"  .option(\"path\", \"abfss://etlhubfilestorage@adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/staging/{}\".format(stagingtable)) \\\n",
							"  .saveAsTable(stagingtable)\n",
							"   #.option(\"overwriteSchema\", \"true\") \\  .save(\"\") \\"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from dhtsr_space_class"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"qry_4_delete=\"\"\"\n",
							"\n",
							"DELETE FROM {} \n",
							"\n",
							"\"\"\"\n",
							"df_4_delete=spark.sql(qry_4_delete.format(tablename))\n",
							"print('Existing records are deleted') \n",
							"\n",
							"qry_ins_new_rows=\"\"\"\n",
							"\n",
							"INSERT INTO {}\n",
							"select \n",
							"trim(CNTRY_CD) ,\n",
							"trim(STTE_PROV_CD),\n",
							"trim(STTE_PROV_DESC),\n",
							"ACTIVE_DATE,\n",
							"trim(STATUS_CD)\n",
							"from {}\n",
							";\n",
							"\"\"\"\n",
							"d=spark.sql(qry_ins_new_rows.format(tablename,stagingtable))\n",
							"\n",
							"#d.show(10)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select count(*) from etlhubconfirmed.dhtr_stte_prov"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_get_file_sharepoint')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub/Dimensions&Facts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7f4e893c-b1bf-4311-8149-ac5b9cff5095"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Import all the necessary libraries"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws,trim\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"from pyspark.sql.functions import col\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"import os\n",
							"import sys\n",
							"from datetime import datetime\n",
							"# Import library\n",
							"pip install py-topping\n",
							"pip install Office365-REST-Python-Client\n",
							"from py_topping.data_connection.sharepoint import lazy_SP365\n",
							"\n",
							"#from office365.runtime.auth.authentication_context import AuthenticationContext\n",
							"#from office365.sharepoint.client_context import ClientContext\n",
							"#from office365.sharepoint.files.file import File"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Define Parameters"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create connection\r\n",
							"sp = lazy_SP365(site_url = 'https://kyndryl-my.sharepoint.com/personal/srilatha_palivela_kyndryl_com/_layouts/15/onedrive.aspx'\r\n",
							"                   , client_id = 'Srilatha.Palivela@kyndryl.com'\r\n",
							"                   , client_secret = 'Sai2Krupa202207')\r\n",
							"# Create download path from download URL\r\n",
							"download_path = sp.create_link('https://kyndryl-my.sharepoint.com/personal/srilatha_palivela_kyndryl_com/_layouts/15/onedrive.aspx/Country(kyndryl).csv')\r\n",
							"# Download file\r\n",
							"sp.download(sharepoint_location = download_path \r\n",
							"            , local_location = 'https://portal.azure.com/#view/Microsoft_Azure_Storage/ContainerMenuBlade/~/overview/storageAccountId/%2Fsubscriptions%2Feb01b8c2-e8d1-4c19-9788-5469f1f3fd10%2FresourceGroups%2Frg-kyn-001494-dev-eus-001%2Fproviders%2FMicrosoft.Storage%2FstorageAccounts%2Fadls4fsoetlhubdevuseast/path/etlhubfilestorage/etag/%220x8DA52B876475420%22/defaultEncryptionScope/%24account-encryption-key/denyEncryptionScopeOverride~/false/defaultId//publicAccessVal/None')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#import office365.runtime.auth.AuthenticationContext\r\n",
							"from office365.runtime.auth.authentication_context import AuthenticationContext"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ctx_auth = AuthenticationContext(https://kyndryl-my.sharepoint.com/personal/srilatha_palivela_kyndryl_com/_layouts/15/onedrive.aspx)\n",
							"ctx_auth.acquire_token_for_user(Srilatha.Palivela@kyndryl.com, Sai2Krupa202207)   \n",
							"ctx = ClientContext(url, ctx_auth)\n",
							"response = File.open_binary(ctx, \"Country(kyndryl).csv\")\n",
							"with open(\"./Country(kyndryl).csv\", \"wb\") as local_file:\n",
							"    local_file.write(response.content)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_onedrive_integration')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ETLHub"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "2dd781cc-dd85-4940-9c2b-31d72b328d65"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from msdrive import OneDrive\n",
							"\n",
							"drive = OneDrive(\"access_token_here\")\n",
							"\n",
							"# List files and folders in root directory:\n",
							"drive.list_items()\n",
							"\n",
							"# List files and folders in sub-directory:\n",
							"drive.list_items(folder_path=\"/Documents\")\n",
							"\n",
							"# Get file or folder metadata:\n",
							"drive.get_item_data(item_path=\"/Documents/my-data.csv\")\n",
							"drive.get_item_data(item_id=\"01...\") # if you know the item ID\n",
							"\n",
							"# Download an existing file\n",
							"drive.download_item(item_path=\"/Documents/my-data.csv\", file_path=\"my-data.csv\")\n",
							"drive.download_item(item_id=\"01...\", file_path=\"my-data.csv\") # if you know the item ID\n",
							"\n",
							"# Upload a new or existing file\n",
							"drive.upload_item(item_path=\"/Documents/new-or-existing-file.csv\", file_path=\"new-or-existing-file.csv\")\n",
							"drive.upload_item(item_id=\"01...\", file_path=\"existing-file.csv\") # if you know the item ID"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_send_email')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "1dd798be-fc2a-4bf5-81f0-8a2533994450"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"import os\n",
							"import sys\n",
							"from datetime import datetime\n",
							"import smtplib\n",
							"from pathlib import Path\n",
							"from email.mime.multipart import MIMEMultipart\n",
							"from email.mime.base import MIMEBase\n",
							"from email.mime.text import MIMEText\n",
							"from email.utils import COMMASPACE, formatdate\n",
							"from email import encoders"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'etlhubfilestorage' # fill in your container name \n",
							"relative_path = 'extract/BMSIW/' # fill in your relative folder path \n",
							"relative_archive_path='archive/'\n",
							"file_name='DS_EMPLOYEE_DATA.csv' \n",
							"natural_key=\"EMPLOYEE_BUSINESS_CD\"\n",
							"tablename=\"etlhubConfirmed.dht_employee\"\n",
							"#natural_key=\"BUSINESS_PARTNER_ID\"\n",
							"keycolumn=\"EMPLOYEE_KEY\"\n",
							"date = datetime.now().strftime(\"%Y_%m_%d-%I:%M:%S_%p\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"adls_arch_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_archive_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"relative_archive_path\n",
							"# Read a csv file \n",
							"csv_path = adls_path + file_name\n",
							"arch_fil_name=file_name.rsplit(\".\")\n",
							"csv_archive_path = adls_arch_path + arch_fil_name[0] + '_' + date + '.' + arch_fil_name[1]\n",
							"print (csv_archive_path)\n",
							"\n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\n",
							"\n",
							"incrementalData_DF.write.csv(csv_archive_path, header = 'true')\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"#Add checks for duplicate data, Check for 1. dups in source data based on natural Key and\n",
							"# 2. Dups in target already\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"def send_mail(send_from = \"siva.seemakurti@kyndryl.com\", send_to = \"siva.seemakurti@kyndryl.com\", subject = \"Mail from ASA using pyspark notebook\", message = \"Testing Mail body\", files=[\"/mnt/<Mounted Point Directory>/\"],\n",
							"              server=\"<SMTP Host>\", port=\"<SMTP Port>\", username='<SMTP Username>', password='<SMTP Password>',\n",
							"              use_tls=True):\n",
							"\n",
							"    msg = MIMEMultipart()\n",
							"    msg['From'] = send_from\n",
							"    msg['To'] = COMMASPACE.join(send_to)\n",
							"    msg['Date'] = formatdate(localtime=True)\n",
							"    msg['Subject'] = subject\n",
							"\n",
							"    msg.attach(MIMEText(message))\n",
							"\n",
							"    for path in files:\n",
							"        part = MIMEBase('application', \"octet-stream\")\n",
							"        with open(path, 'rb') as file:\n",
							"            part.set_payload(file.read())\n",
							"        encoders.encode_base64(part)\n",
							"        part.add_header('Content-Disposition',\n",
							"                        'attachment; filename=\"{}\"'.format(Path(path).name))\n",
							"        msg.attach(part)\n",
							"\n",
							"    smtp = smtplib.SMTP(server, port)\n",
							"    if use_tls:\n",
							"        smtp.starttls()\n",
							"    smtp.login(username, password)\n",
							"    smtp.sendmail(send_from, send_to, msg.as_string())\n",
							"    smtp.quit()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/proj_dataframe')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "24323e3d-22b0-46e0-91ba-c3af8a273514"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"jdbcDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"dbtable\", \"DBXDH.DHT_PROJECT_SIV\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"jdbcDF.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/python_update')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "da86b2f5-6733-4b59-8303-2b54d39642f0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import pyodbc \r\n",
							"import sqlalchemy\r\n",
							"from delta.tables import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"import pyspark\r\n",
							"import sqlparse\r\n",
							"\r\n",
							"conn = pyodbc.connect('Driver={SQL Server};'\r\n",
							"                      'Server=sqlserver-kyn-001494-dev-eus-001.database.windows.net;'\r\n",
							"                      'Database=sqldb-etlhub-confirmed;'\r\n",
							"                      'user = sqladminuser;'\r\n",
							"                      'password = try2find$5;'\r\n",
							"                      )\r\n",
							"\r\n",
							"cursor = conn.cursor()\r\n",
							"cursor.execute('SELECT SIEBEL_SALES_STAGE_CODE,SIEBEL_SALES_STAGE_NAME,SSM_STEP_NO,SSM_STEP_NAME FROM DBXDH.DHTS_SELL_CYCLE')\r\n",
							"\r\n",
							"for i in cursor:\r\n",
							"    print(i)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"import pandas as pd\r\n",
							"import sqlalchemy\r\n",
							"from delta.tables import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"\r\n",
							"sellcycletgtDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"query\", \"SELECT SIEBEL_SALES_STAGE_CODE,SIEBEL_SALES_STAGE_NAME,SSM_STEP_NO,SSM_STEP_NAME FROM DBXDH.DHTS_SELL_CYCLE as tgt\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()\r\n",
							"\r\n",
							"sas_url = \"https://adls4fsoetlhubdevuseast.blob.core.windows.net/project/sell_cycle_2022.csv?sp=r&st=2022-05-18T06:38:22Z&se=2022-05-18T14:38:22Z&spr=https&sv=2020-08-04&sr=b&sig=jctwRkZecjg%2Fz%2FlPtH1Z8FWGN0Ge4Rb84SLbNsBNCvs%3D\"\r\n",
							"\r\n",
							"blob_client = BlobClient.from_blob_url(sas_url)\r\n",
							"blob_data = blob_client.download_blob()\r\n",
							"df = pd.read_csv(StringIO(blob_data.content_as_text()),header = 0)\r\n",
							"#print(df)\r\n",
							"#print(df.columns)\r\n",
							"sparkDF=spark.createDataFrame(df) \r\n",
							"#sparkDF.columns\r\n",
							"sparkDF.createOrReplaceTempView('sell_cycle_df')\r\n",
							"#sparkDF.show()\r\n",
							"sellcycletgtDF.toDF\r\n",
							"#sellcycle.show()\r\n",
							"newAddressesToInsert = sparkDF \\\r\n",
							".alias(\"updates\") \\\r\n",
							".join(sellcycletgtDF.alias(\"sell_cycle\"), \"SIEBEL_SALES_STAGE_CODE\") \\\r\n",
							".selectExpr(\"updates.SIEBEL_SALES_STAGE_CODE\",\"updates.SIEBEL_SALES_STAGE_NAME\",\"updates.SSM_STEP_NO\",\"updates.SSM_STEP_NAME\") \\\r\n",
							".where(\"sell_cycle.SIEBEL_SALES_STAGE_CODE = updates.SIEBEL_SALES_STAGE_CODE AND updates.SIEBEL_SALES_STAGE_NAME <> sell_cycle.SIEBEL_SALES_STAGE_NAME\")\r\n",
							"\r\n",
							"newAddressesToInsert.show()\r\n",
							"# Stage the update by unioning two sets of rows\r\n",
							"# 1. Rows that will be inserted in the whenNotMatched clause\r\n",
							"# 2. Rows that will either update the current addresses of existing customers or insert the new addresses of new customers\r\n",
							"stagedUpdates = (\r\n",
							"     newAddressesToInsert\r\n",
							"    .selectExpr(\"NULL as mergeKey\", \"*\")   # Rows for 1\r\n",
							"    .union(sparkDF.selectExpr(\"SIEBEL_SALES_STAGE_CODE as mergeKey\", \"*\"))\r\n",
							"    )\r\n",
							"#stagedUpdates.show()\r\n",
							"\r\n",
							"\r\n",
							"# Apply SCD Type 2 operation using merge\r\n",
							"DBXDH.DHTS_SELL_CYCLE.alias(\"sell_cycle\").merge(\r\n",
							"  stagedUpdates.alias(\"staged_updates\"),\r\n",
							"  \"sell_cycle.SIEBEL_SALES_STAGE_CODE = mergeKey\") \\\r\n",
							".whenMatchedUpdate(\r\n",
							"  condition = \"sell_cycle.SIEBEL_SALES_STAGE_CODE = staged_updates.SIEBEL_SALES_STAGE_CODE AND sell_cycle.SIEBEL_SALES_STAGE_NAME <> staged_updates.SIEBEL_SALES_STAGE_NAME\",\r\n",
							"  set = {                                      # Set current to false and endDate to source's effective date.\r\n",
							"   \"SIEBEL_SALES_STAGE_NAME\" : \"false\",\r\n",
							"   \"SSM_STEP_NO\" : \"false\"\r\n",
							"  }\r\n",
							"#).whenNotMatchedInsert(\r\n",
							"  #values = {\r\n",
							"    #\"SIEBEL_SALES_STAGE_CODE\": \"staged_updates.SIEBEL_SALES_STAGE_CODE\",\r\n",
							"    #\"SIEBEL_SALES_STAGE_NAME\": \"staged_updates.SIEBEL_SALES_STAGE_NAME\",\r\n",
							"    #\"SSM_STEP_NO\": \"staged_updates.SSM_STEP_NO\",\r\n",
							"    #\"SSM_STEP_NAME\": \"staged_updates.SSM_STEP_NAME\"  # Set current to true along with the new address and its effective date.\r\n",
							"    # }\r\n",
							").execute()\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 159
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#sellcycletgtDF.show()\r\n",
							"#sparkDF.show()\r\n",
							"# Rows to INSERT new addresses of existing customers\r\n",
							"newAddressesToInsert = sparkDF \\\r\n",
							".alias(\"updates\") \\\r\n",
							".join(sellcycletgtDF.toDF().alias(\"sell_cycle\"), \"SIEBEL_SALES_STAGE_CODE\") \\\r\n",
							".where(\"sell_cycle.SIEBEL_SALES_STAGE_CODE = true AND updates.SIEBEL_SALES_STAGE_NAME <> sell_cycle.SIEBEL_SALES_STAGE_NAME\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 62
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas as pd\r\n",
							"import sqlalchemy\r\n",
							"\r\n",
							"with dbEngine.begin() as con:\r\n",
							"    # 1. Create temp table\r\n",
							"    con.execute(\"\"\"\r\n",
							"        CREATE TABLE #sellcycletemp11 (\r\n",
							"            [SIEBEL_SALES_STAGE_CODE] NVARCHAR(100)\r\n",
							"            , [SIEBEL_SALES_STAGE_NAME] NVARCHAR(100)\r\n",
							"            , [SSM_STEP_NO]  NVARCHAR(100)\r\n",
							"            , [SSM_STEP_NAME] NVARCHAR(100)\r\n",
							"            )\r\n",
							"    \"\"\")\r\n",
							"    # 2. Insert into temp table\r\n",
							"    sql_insert_products = f\"INSERT INTO #sellcycletemp11 VALUES (?,?,?,?)\"\r\n",
							"    con.execute(sql_insert_products, df.values.tolist())\r\n",
							"\r\n",
							"    # 3. Merge the #sellcycletemp with sellcycle\r\n",
							"    sql_merge = \"\"\"\r\n",
							"        MERGE DBXDH.DHTS_SELL_CYCLE  AS Target\r\n",
							"        USING #sellcycletemp11 AS Source\r\n",
							"            ON Source.SIEBEL_SALES_STAGE_CODE = Target.SIEBEL_SALES_STAGE_CODE\r\n",
							"        /* new records ('right match') */\r\n",
							"        WHEN NOT MATCHED BY Target THEN\r\n",
							"            INSERT ([SIEBEL_SALES_STAGE_CODE], [SIEBEL_SALES_STAGE_NAME], [SSM_STEP_NO], [SSM_STEP_NAME]) \r\n",
							"            VALUES (source.SIEBEL_SALES_STAGE_CODE, source.SIEBEL_SALES_STAGE_NAME, source.SSM_STEP_NO, source.SSM_STEP_NAME)\r\n",
							"        /* matching records ('inner match') */\r\n",
							"        WHEN MATCHED THEN \r\n",
							"            UPDATE SET\r\n",
							"            Target.SIEBEL_SALES_STAGE_NAME= Source.SIEBEL_SALES_STAGE_NAME\r\n",
							"            , Target.SSM_STEP_NO = Source.SSM_STEP_NO\r\n",
							"            , Target.SSM_STEP_NAME = Source.SSM_STEP_NAME\r\n",
							"        /* deprecated records ('left match') */\r\n",
							"        /* WHEN NOT MATCHED BY Source THEN\r\n",
							"    DELETE */\r\n",
							" \r\n",
							" OUTPUT \r\n",
							"\t$action\r\n",
							" , inserted.*\r\n",
							";\r\n",
							"    \"\"\"\r\n",
							"    con.execute(sql_merge)\r\n",
							"\r\n",
							"    # 4. Delete temporary table\r\n",
							"    con.execute(\"\"\"DROP TABLE IF EXISTS #sellcycletemp5;\"\"\")\r\n",
							"    con.execute(\"\"\"DROP TABLE IF EXISTS #sellcycletemp11;\"\"\")\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 58
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sell_cycle_dataframe')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7504924f-718e-4b87-942f-94b54fbbfd54"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"jdbcDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"dbtable\", \"DBXDH.DHTS_SELL_CYCLE\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"jdbcsqlDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"query\", \"select * from DBXDH.DHTS_SELL_CYCLE\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"jdbcsqlDF.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stgsellcycleDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"query\", \"select * from DBXDH.DHTS_SELL_CYCLE\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sellcycleDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"query\", \"select * from DBXDH.DHTS_SELL_CYCLE\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stgsellcycleDF.show()\r\n",
							"sellcycleDF.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stgsellcycleDF.join(sellcycleDF,stgsellcycleDF.SIEBEL_SALES_STAGE_CODE ==  sellcycleDF.SIEBEL_SALES_STAGE_CODE,\"inner\") \\\r\n",
							"     .show(truncate=True)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stgsellcycleDF.createOrReplaceTempView(\"stgsellcycle\")\r\n",
							"sellcycleDF.createOrReplaceTempView(\"sellcycle\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"joinDF = spark.sql(\"select * from stgsellcycle e, sellcycle d where e.SIEBEL_SALES_STAGE_CODE == d.SIEBEL_SALES_STAGE_CODE\") \\\r\n",
							"  .show(truncate=False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"joinDF2 = spark.sql(\"select stg.* from stgsellcycle stg INNER JOIN sellcycle tgt ON stg.SIEBEL_SALES_STAGE_CODE == tgt.SIEBEL_SALES_STAGE_CODE\") \\\r\n",
							"  .show(truncate=False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"joinDF2 = spark.sql(\"select stg.* from stgsellcycle stg INNER JOIN sellcycle tgt ON stg.SIEBEL_SALES_STAGE_CODE == tgt.SIEBEL_SALES_STAGE_CODE\") \\\r\n",
							"  .show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stgsellcycleDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"query\", \"select SIEBEL_SALES_STAGE_CODE,SIEBEL_SALES_STAGE_NAME,SSM_STEP_NO,SSM_STEP_NAME from DBXDH.DHTS_SELL_CYCLE\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sellcycleDF1 = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"query\", \"SELECT CONVERT(VARCHAR(32),HashBytes('MD5', SIEBEL_SALES_STAGE_CODE),2) as nk_hash,     CONVERT(VARCHAR(32),HashBytes('MD5', SIEBEL_SALES_STAGE_NAME+SSM_STEP_NO+SSM_STEP_NAME),2) as column_hash,tgt.* FROM DBXDH.DHTS_SELL_CYCLE as tgt\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sellcycleDF1.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sell_cycle_df20220511')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "80a533ff-1e38-4ce6-83a4-f4ec6d52e944"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"sellcycletgtDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"query\", \"SELECT CONVERT(VARCHAR(32),HashBytes('MD5', SIEBEL_SALES_STAGE_CODE),2) as existing_nk_hash,     CONVERT(VARCHAR(32),HashBytes('MD5', SIEBEL_SALES_STAGE_NAME+SSM_STEP_NO+SSM_STEP_NAME),2) as existing_column_hash,tgt.* FROM DBXDH.DHTS_SELL_CYCLE as tgt\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sellcycletgtDF.show()\r\n",
							"sellcycletgtDF.createOrReplaceTempView(\"sellcycletgt\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"from pyspark.sql.functions import md5, concat_ws\r\n",
							"from sqlite3 import connect\r\n",
							"conn = connect(':memory:')\r\n",
							"columns = [\"SIEBEL_SALES_STAGE_NAME\",\"SSM_STEP_NO\",\"SSM_STEP_NAME\"]\r\n",
							"sas_url = \"https://adls4fsoetlhubdevuseast.blob.core.windows.net/project/sell_cycle.csv?sp=r&st=2022-05-11T09:48:50Z&se=2022-05-11T17:48:50Z&spr=https&sv=2020-08-04&sr=c&sig=ScsDhYGM4HWdCD8kMSMmLfY7Pex8jvmc02LvGatfwPI%3D\"\r\n",
							"blob_client = BlobClient.from_blob_url(sas_url)\r\n",
							"blob_data = blob_client.download_blob()\r\n",
							"df = pd.read_csv(StringIO(blob_data.content_as_text()))\r\n",
							"#print(df)\r\n",
							"sell_cycleDF=spark.createDataFrame(df)\r\n",
							"col_list=[]\r\n",
							"for i in sell_cycleDF.columns:\r\n",
							"    col_list.append(i)\r\n",
							"    print (col_list)\r\n",
							"sell_cyclenkDF = sell_cycleDF.withColumn(\"nk_hash\",md5(\"SIEBEL_SALES_STAGE_CODE\"))\r\n",
							"sell_cyclecolhashDF = sell_cyclenkDF.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\r\n",
							"#sell_cyclecolhashDF.show()\r\n",
							"sell_cyclecolhashDF.createOrReplaceTempView(\"sellcyclesrc\")\r\n",
							"#insertsellcycle=pd.read_sql(\"select a.SIEBEL_SALES_STAGE_CODE,a.SIEBEL_SALES_STAGE_NAME,a.SSM_STEP_NO,a.SSM_STEP_NAME from sellcyclesrc a left join  sellcycletgt b on a.nk_hash = b.existing_nk_hash where b.existing_nk_hash is null\",conn)\r\n",
							"#insertsellcycledf=spark.createDataFrame(insertsellcycle)\r\n",
							"#insertsellcycledf.show()\r\n",
							"#%%sql\r\n",
							"#select * from sellcyclesrc"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import col, desc\r\n",
							"(sell_cyclecolhashDF.select(\"SIEBEL_SALES_STAGE_CODE\",\"SIEBEL_SALES_STAGE_NAME\",\"SSM_STEP_NO\",\"SSM_STEP_NAME\")\r\n",
							" ).show(10)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"sell_cycleinsert = spark.sql(\"select a.SIEBEL_SALES_STAGE_CODE,a.SIEBEL_SALES_STAGE_NAME, a.SSM_STEP_NO,a.SSM_STEP_NAME from sellcyclesrc a full outer join sellcycletgt b on a.nk_hash = b.existing_nk_hash where b.existing_nk_hash is null\") \\\r\n",
							"  .show(truncate=False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"sell_cycleinsert.createOrReplaceTempView\r\n",
							"(\"sell_cycle_final\") \r\n",
							"#sqlContext.sql(\"insert into table DBXDH.DHTS_SELL_CYCLE select * from sell_cycleinsert\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 82
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"insertsellcycle=spark.createDataFrame(sql(\"select a.SIEBEL_SALES_STAGE_CODE,a.SIEBEL_SALES_STAGE_NAME,a.SSM_STEP_NO,a.SSM_STEP_NAME from sellcyclesrc a left join  sellcycletgt b on a.nk_hash = b.existing_nk_hash where b.existing_nk_hash is null\"))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							" import sqlalchemy\r\n",
							" # 3.Insert table, merge\r\n",
							"with dbEngine.begin() as con:\r\n",
							"    # 1. Create temp table\r\n",
							"    con.execute(\"\"\"\r\n",
							"        CREATE TABLE #sellcycletemp (\r\n",
							"             [nk_hash] NVARCHAR(100) UNIQUE\r\n",
							"            , [SIEBEL_SALES_STAGE_CODE] NVARCHAR(100)\r\n",
							"            , [SIEBEL_SALES_STAGE_NAME] NVARCHAR(100)\r\n",
							"            , [SSM_STEP_NO] NVARCHAR(100)\r\n",
							"            , [SSM_STEP_NAME] NVARCHAR(100)\r\n",
							"        )\r\n",
							"    \"\"\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sell_cycle_upsert_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0a6dbc01-0e2d-486b-9a4e-f1c7e4535654"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"sellcycletgtDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"query\", \"SELECT CONVERT(VARCHAR(32),HashBytes('MD5', SIEBEL_SALES_STAGE_CODE),2) as existing_nk_hash,     CONVERT(VARCHAR(32),HashBytes('MD5', SIEBEL_SALES_STAGE_NAME+SSM_STEP_NO+SSM_STEP_NAME),2) as existing_column_hash,tgt.* FROM DBXDH.DHTS_SELL_CYCLE as tgt\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from sqlalchemy.engine import URL\r\n",
							"connection_string = \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=sqlserver-kyn-001494-dev-eus-001.database.windows.net;DATABASE=sqldb-etlhub-confirmed;UID=sqladminuser;PWD=try2find$5\"\r\n",
							"connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\r\n",
							"\r\n",
							"dbEngine = create_engine(connection_url)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas as pd\r\n",
							"import sqlalchemy\r\n",
							"\r\n",
							"with dbEngine.begin() as con:\r\n",
							"    # 1. Create temp table\r\n",
							"    con.execute(\"\"\"\r\n",
							"        CREATE TABLE #sellcycletemp11 (\r\n",
							"            [SIEBEL_SALES_STAGE_CODE] NVARCHAR(100)\r\n",
							"            , [SIEBEL_SALES_STAGE_NAME] NVARCHAR(100)\r\n",
							"            , [SSM_STEP_NO]  NVARCHAR(100)\r\n",
							"            , [SSM_STEP_NAME] NVARCHAR(100)\r\n",
							"            )\r\n",
							"    \"\"\")\r\n",
							"    # 2. Insert into temp table\r\n",
							"    sql_insert_products = f\"INSERT INTO #sellcycletemp11 VALUES (?,?,?,?)\"\r\n",
							"    con.execute(sql_insert_products, df.values.tolist())\r\n",
							"\r\n",
							"    # 3. Merge the #sellcycletemp with sellcycle\r\n",
							"    sql_merge = \"\"\"\r\n",
							"        MERGE DBXDH.DHTS_SELL_CYCLE  AS Target\r\n",
							"        USING #sellcycletemp11 AS Source\r\n",
							"            ON Source.SIEBEL_SALES_STAGE_CODE = Target.SIEBEL_SALES_STAGE_CODE\r\n",
							"        /* new records ('right match') */\r\n",
							"        WHEN NOT MATCHED BY Target THEN\r\n",
							"            INSERT ([SIEBEL_SALES_STAGE_CODE], [SIEBEL_SALES_STAGE_NAME], [SSM_STEP_NO], [SSM_STEP_NAME]) \r\n",
							"            VALUES (source.SIEBEL_SALES_STAGE_CODE, source.SIEBEL_SALES_STAGE_NAME, source.SSM_STEP_NO, source.SSM_STEP_NAME)\r\n",
							"        /* matching records ('inner match') */\r\n",
							"        WHEN MATCHED THEN \r\n",
							"            UPDATE SET\r\n",
							"            Target.SIEBEL_SALES_STAGE_NAME= Source.SIEBEL_SALES_STAGE_NAME\r\n",
							"            , Target.SSM_STEP_NO = Source.SSM_STEP_NO\r\n",
							"            , Target.SSM_STEP_NAME = Source.SSM_STEP_NAME\r\n",
							"        /* deprecated records ('left match') */\r\n",
							"        /* WHEN NOT MATCHED BY Source THEN\r\n",
							"    DELETE */\r\n",
							" \r\n",
							" OUTPUT \r\n",
							"\t$action\r\n",
							" , inserted.*\r\n",
							";\r\n",
							"    \"\"\"\r\n",
							"    con.execute(sql_merge)\r\n",
							"\r\n",
							"    # 4. Delete temporary table\r\n",
							"    con.execute(\"\"\"DROP TABLE IF EXISTS #sellcycletemp5;\"\"\")\r\n",
							"    con.execute(\"\"\"DROP TABLE IF EXISTS #sellcycletemp11;\"\"\")\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 58
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sell_cycle_upsert_python')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8ed30c57-4f8c-44fa-bee2-ce8e2cdea06e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"sellcycletgtDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"query\", \"SELECT CONVERT(VARCHAR(32),HashBytes('MD5', SIEBEL_SALES_STAGE_CODE),2) as existing_nk_hash,     CONVERT(VARCHAR(32),HashBytes('MD5', SIEBEL_SALES_STAGE_NAME+SSM_STEP_NO+SSM_STEP_NAME),2) as existing_column_hash,tgt.* FROM DBXDH.DHTS_SELL_CYCLE as tgt\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from sqlalchemy.engine import URL\r\n",
							"connection_string = \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=sqlserver-kyn-001494-dev-eus-001.database.windows.net;DATABASE=sqldb-etlhub-confirmed;UID=sqladminuser;PWD=try2find$5\"\r\n",
							"connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\r\n",
							"\r\n",
							"dbEngine = create_engine(connection_url)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"import csv\r\n",
							"\r\n",
							"import pandas as pd\r\n",
							"import sqlalchemy\r\n",
							"\r\n",
							"def main():\r\n",
							"\r\n",
							"    # 1. Load the new products\r\n",
							"    df = pd.read_csv('project/sell_cycle_2022.csv', dtype={\r\n",
							"        'SIEBEL_SALES_STAGE_CODE': str,\r\n",
							"        'SIEBEL_SALES_STAGE_NAME': str,\r\n",
							"        'SSM_STEP_NO': str,\r\n",
							"        'SSM_STEP_NAME': str,\r\n",
							"        })"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 54
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas as pd\r\n",
							"import sqlalchemy\r\n",
							"\r\n",
							"with dbEngine.begin() as con:\r\n",
							"    # 1. Create temp table\r\n",
							"    con.execute(\"\"\"\r\n",
							"        CREATE TABLE #sellcycletemp11 (\r\n",
							"            [SIEBEL_SALES_STAGE_CODE] NVARCHAR(100)\r\n",
							"            , [SIEBEL_SALES_STAGE_NAME] NVARCHAR(100)\r\n",
							"            , [SSM_STEP_NO]  NVARCHAR(100)\r\n",
							"            , [SSM_STEP_NAME] NVARCHAR(100)\r\n",
							"            )\r\n",
							"    \"\"\")\r\n",
							"    # 2. Insert into temp table\r\n",
							"    sql_insert_products = f\"INSERT INTO #sellcycletemp11 VALUES (?,?,?,?)\"\r\n",
							"    con.execute(sql_insert_products, df.values.tolist())\r\n",
							"\r\n",
							"    # 3. Merge the #sellcycletemp with sellcycle\r\n",
							"    sql_merge = \"\"\"\r\n",
							"        MERGE DBXDH.DHTS_SELL_CYCLE  AS Target\r\n",
							"        USING #sellcycletemp11 AS Source\r\n",
							"            ON Source.SIEBEL_SALES_STAGE_CODE = Target.SIEBEL_SALES_STAGE_CODE\r\n",
							"        /* new records ('right match') */\r\n",
							"        WHEN NOT MATCHED BY Target THEN\r\n",
							"            INSERT ([SIEBEL_SALES_STAGE_CODE], [SIEBEL_SALES_STAGE_NAME], [SSM_STEP_NO], [SSM_STEP_NAME]) \r\n",
							"            VALUES (source.SIEBEL_SALES_STAGE_CODE, source.SIEBEL_SALES_STAGE_NAME, source.SSM_STEP_NO, source.SSM_STEP_NAME)\r\n",
							"        /* matching records ('inner match') */\r\n",
							"        WHEN MATCHED THEN \r\n",
							"            UPDATE SET\r\n",
							"            Target.SIEBEL_SALES_STAGE_NAME= Source.SIEBEL_SALES_STAGE_NAME\r\n",
							"            , Target.SSM_STEP_NO = Source.SSM_STEP_NO\r\n",
							"            , Target.SSM_STEP_NAME = Source.SSM_STEP_NAME\r\n",
							"        /* deprecated records ('left match') */\r\n",
							"        /* WHEN NOT MATCHED BY Source THEN\r\n",
							"    DELETE */\r\n",
							" \r\n",
							" OUTPUT \r\n",
							"\t$action\r\n",
							" , inserted.*\r\n",
							";\r\n",
							"    \"\"\"\r\n",
							"    con.execute(sql_merge)\r\n",
							"\r\n",
							"    # 4. Delete temporary table\r\n",
							"    con.execute(\"\"\"DROP TABLE IF EXISTS #sellcycletemp5;\"\"\")\r\n",
							"    con.execute(\"\"\"DROP TABLE IF EXISTS #sellcycletemp11;\"\"\")\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 58
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sell_cycle_upsert_python_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "dc8156f2-b57c-4665-bf0e-cc8508092b2a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"sellcycletgtDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"query\", \"SELECT SIEBEL_SALES_STAGE_CODE,SIEBEL_SALES_STAGE_NAME,SSM_STEP_NO,SSM_STEP_NAME FROM DBXDH.DHTS_SELL_CYCLE as tgt\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 149
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"import pandas as pd\r\n",
							"import sqlalchemy\r\n",
							"from delta.tables import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"\r\n",
							"sellcycletgtDF = spark.read.format(\"jdbc\") \\\r\n",
							"    .option(\"url\", f\"jdbc:sqlserver://sqlserver-kyn-001494-dev-eus-001.database.windows.net:1433;databaseName=sqldb-etlhub-confirmed;\") \\\r\n",
							"    .option(\"query\", \"SELECT SIEBEL_SALES_STAGE_CODE,SIEBEL_SALES_STAGE_NAME,SSM_STEP_NO,SSM_STEP_NAME FROM DBXDH.DHTS_SELL_CYCLE as tgt\") \\\r\n",
							"    .option(\"user\", \"sqladminuser\") \\\r\n",
							"    .option(\"password\", \"try2find$5\") \\\r\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"    .load()\r\n",
							"\r\n",
							"sas_url = \"https://adls4fsoetlhubdevuseast.blob.core.windows.net/project/sell_cycle_2022.csv?sp=r&st=2022-05-18T06:38:22Z&se=2022-05-18T14:38:22Z&spr=https&sv=2020-08-04&sr=b&sig=jctwRkZecjg%2Fz%2FlPtH1Z8FWGN0Ge4Rb84SLbNsBNCvs%3D\"\r\n",
							"\r\n",
							"blob_client = BlobClient.from_blob_url(sas_url)\r\n",
							"blob_data = blob_client.download_blob()\r\n",
							"df = pd.read_csv(StringIO(blob_data.content_as_text()),header = 0)\r\n",
							"#print(df)\r\n",
							"#print(df.columns)\r\n",
							"sparkDF=spark.createDataFrame(df) \r\n",
							"#sparkDF.columns\r\n",
							"sparkDF.createOrReplaceTempView('sell_cycle_df')\r\n",
							"#sparkDF.show()\r\n",
							"sellcycletgtDF.toDF\r\n",
							"#sellcycle.show()\r\n",
							"newAddressesToInsert = sparkDF \\\r\n",
							".alias(\"updates\") \\\r\n",
							".join(sellcycletgtDF.alias(\"sell_cycle\"), \"SIEBEL_SALES_STAGE_CODE\") \\\r\n",
							".selectExpr(\"updates.SIEBEL_SALES_STAGE_CODE\",\"updates.SIEBEL_SALES_STAGE_NAME\",\"updates.SSM_STEP_NO\",\"updates.SSM_STEP_NAME\") \\\r\n",
							".where(\"sell_cycle.SIEBEL_SALES_STAGE_CODE = updates.SIEBEL_SALES_STAGE_CODE AND updates.SIEBEL_SALES_STAGE_NAME <> sell_cycle.SIEBEL_SALES_STAGE_NAME\")\r\n",
							"\r\n",
							"newAddressesToInsert.show()\r\n",
							"# Stage the update by unioning two sets of rows\r\n",
							"# 1. Rows that will be inserted in the whenNotMatched clause\r\n",
							"# 2. Rows that will either update the current addresses of existing customers or insert the new addresses of new customers\r\n",
							"stagedUpdates = (\r\n",
							"     newAddressesToInsert\r\n",
							"    .selectExpr(\"NULL as mergeKey\", \"*\")   # Rows for 1\r\n",
							"    .union(sparkDF.selectExpr(\"SIEBEL_SALES_STAGE_CODE as mergeKey\", \"*\"))\r\n",
							"    )\r\n",
							"#stagedUpdates.show()\r\n",
							"\r\n",
							"\r\n",
							"# Apply SCD Type 2 operation using merge\r\n",
							"DBXDH.DHTS_SELL_CYCLE.alias(\"sell_cycle\").merge(\r\n",
							"  stagedUpdates.alias(\"staged_updates\"),\r\n",
							"  \"sell_cycle.SIEBEL_SALES_STAGE_CODE = mergeKey\") \\\r\n",
							".whenMatchedUpdate(\r\n",
							"  condition = \"sell_cycle.SIEBEL_SALES_STAGE_CODE = staged_updates.SIEBEL_SALES_STAGE_CODE AND sell_cycle.SIEBEL_SALES_STAGE_NAME <> staged_updates.SIEBEL_SALES_STAGE_NAME\",\r\n",
							"  set = {                                      # Set current to false and endDate to source's effective date.\r\n",
							"   \"SIEBEL_SALES_STAGE_NAME\" : \"false\",\r\n",
							"   \"SSM_STEP_NO\" : \"false\"\r\n",
							"  }\r\n",
							"#).whenNotMatchedInsert(\r\n",
							"  #values = {\r\n",
							"    #\"SIEBEL_SALES_STAGE_CODE\": \"staged_updates.SIEBEL_SALES_STAGE_CODE\",\r\n",
							"    #\"SIEBEL_SALES_STAGE_NAME\": \"staged_updates.SIEBEL_SALES_STAGE_NAME\",\r\n",
							"    #\"SSM_STEP_NO\": \"staged_updates.SSM_STEP_NO\",\r\n",
							"    #\"SSM_STEP_NAME\": \"staged_updates.SSM_STEP_NAME\"  # Set current to true along with the new address and its effective date.\r\n",
							"    # }\r\n",
							").execute()\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 159
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#sellcycletgtDF.show()\r\n",
							"#sparkDF.show()\r\n",
							"# Rows to INSERT new addresses of existing customers\r\n",
							"newAddressesToInsert = sparkDF \\\r\n",
							".alias(\"updates\") \\\r\n",
							".join(sellcycletgtDF.toDF().alias(\"sell_cycle\"), \"SIEBEL_SALES_STAGE_CODE\") \\\r\n",
							".where(\"sell_cycle.SIEBEL_SALES_STAGE_CODE = true AND updates.SIEBEL_SALES_STAGE_NAME <> sell_cycle.SIEBEL_SALES_STAGE_NAME\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 62
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas as pd\r\n",
							"import sqlalchemy\r\n",
							"\r\n",
							"with dbEngine.begin() as con:\r\n",
							"    # 1. Create temp table\r\n",
							"    con.execute(\"\"\"\r\n",
							"        CREATE TABLE #sellcycletemp11 (\r\n",
							"            [SIEBEL_SALES_STAGE_CODE] NVARCHAR(100)\r\n",
							"            , [SIEBEL_SALES_STAGE_NAME] NVARCHAR(100)\r\n",
							"            , [SSM_STEP_NO]  NVARCHAR(100)\r\n",
							"            , [SSM_STEP_NAME] NVARCHAR(100)\r\n",
							"            )\r\n",
							"    \"\"\")\r\n",
							"    # 2. Insert into temp table\r\n",
							"    sql_insert_products = f\"INSERT INTO #sellcycletemp11 VALUES (?,?,?,?)\"\r\n",
							"    con.execute(sql_insert_products, df.values.tolist())\r\n",
							"\r\n",
							"    # 3. Merge the #sellcycletemp with sellcycle\r\n",
							"    sql_merge = \"\"\"\r\n",
							"        MERGE DBXDH.DHTS_SELL_CYCLE  AS Target\r\n",
							"        USING #sellcycletemp11 AS Source\r\n",
							"            ON Source.SIEBEL_SALES_STAGE_CODE = Target.SIEBEL_SALES_STAGE_CODE\r\n",
							"        /* new records ('right match') */\r\n",
							"        WHEN NOT MATCHED BY Target THEN\r\n",
							"            INSERT ([SIEBEL_SALES_STAGE_CODE], [SIEBEL_SALES_STAGE_NAME], [SSM_STEP_NO], [SSM_STEP_NAME]) \r\n",
							"            VALUES (source.SIEBEL_SALES_STAGE_CODE, source.SIEBEL_SALES_STAGE_NAME, source.SSM_STEP_NO, source.SSM_STEP_NAME)\r\n",
							"        /* matching records ('inner match') */\r\n",
							"        WHEN MATCHED THEN \r\n",
							"            UPDATE SET\r\n",
							"            Target.SIEBEL_SALES_STAGE_NAME= Source.SIEBEL_SALES_STAGE_NAME\r\n",
							"            , Target.SSM_STEP_NO = Source.SSM_STEP_NO\r\n",
							"            , Target.SSM_STEP_NAME = Source.SSM_STEP_NAME\r\n",
							"        /* deprecated records ('left match') */\r\n",
							"        /* WHEN NOT MATCHED BY Source THEN\r\n",
							"    DELETE */\r\n",
							" \r\n",
							" OUTPUT \r\n",
							"\t$action\r\n",
							" , inserted.*\r\n",
							";\r\n",
							"    \"\"\"\r\n",
							"    con.execute(sql_merge)\r\n",
							"\r\n",
							"    # 4. Delete temporary table\r\n",
							"    con.execute(\"\"\"DROP TABLE IF EXISTS #sellcycletemp5;\"\"\")\r\n",
							"    con.execute(\"\"\"DROP TABLE IF EXISTS #sellcycletemp11;\"\"\")\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 58
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/xfrm_BPFACT_deltalake_scd_type2_with_parameters')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c7b31756-beac-4457-baf7-0056603ce502"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"#import necessary python libraries\r\n",
							"\r\n",
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"from pyspark.sql.functions import md5, concat_ws\r\n",
							"from sqlite3 import connect\r\n",
							"from pyspark.sql import functions as F\r\n",
							"from pyspark.sql.functions import trim,ltrim,rtrim\r\n",
							"#conn = connect(':memory:')\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession \r\n",
							"from pyspark.sql.types import * \r\n",
							"from delta.tables import *\r\n",
							"#import os\r\n",
							"import sys\r\n",
							"\r\n",
							"#Read data from adls csv file extracted from source at https://adls4fsoetlhubdevuseast.blob.core.windows.net/customer/BUSINESSPARTNER_FACT.csv\r\n",
							"\r\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \r\n",
							"container_name = 'customer' # fill in your container name \r\n",
							"relative_path = '' # fill in your relative folder path \r\n",
							"\r\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \r\n",
							"print('Primary storage account path: ' + adls_path) \r\n",
							"\r\n",
							"# Read a csv file \r\n",
							"csv_path = adls_path + 'BUSINESSPARTNER_FACT.csv' \r\n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\r\n",
							"\r\n",
							"tablename=\"etlhubConfirmed.opportunity_business_partner_daily_ft\"\r\n",
							"natural_key=['BUSINESS_PARTNER_KEY','OPPORTUNITY_KEY','INFLUENCER_ROLE_CODE']\r\n",
							"keycolumn=\"BUSINESS_PARTNER_FACT_KEY\"\r\n",
							"\r\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\r\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\r\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\r\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\r\n",
							"#existingMaxKeyDF.show()\r\n",
							"#joining with dimension tables\r\n",
							"DIMDataDF=spark.sql(\"SELECT OPPORTUNITY_KEY,OPPORTUNITY_VERSION,OPPORTUNITY_NUM FROM etlhubConfirmed.opportunity WHERE CURRENT_IND='Y'\")\r\n",
							"DIMDataDF1=spark.sql(\"SELECT BUSINESS_PARTNER_KEY,BUSINESS_PARTNER_VERSION,BUSINESS_PARTNER_ID FROM etlhubConfirmed.business_partner WHERE CURRENT_IND='Y'\")\r\n",
							"DIMJoin=incrementalData_DF.join(DIMDataDF,(trim(incrementalData_DF.OPPORTUNITY_NUM) == trim(DIMDataDF.OPPORTUNITY_NUM), \"fullouter\") \r\n",
							"DIMJoin1=DIMJoin.join(DIMDataDF1,DIMJoin.BUSINESS_PARTNER_ID == DIMDataDF1.BUSINESS_PARTNER_ID, \"fullouter\") \r\n",
							"DIMJoin1.createOrReplaceTempView('fullJoin')\r\n",
							"DIMFinalJoinDF=spark.sql(\"select OPPORTUNITY_KEY,BUSINESS_PARTNER_KEY,INFLUENCER_ROLE_CODE from fulljoin\")\r\n",
							"#DIMFinalJoinDF.show()\r\n",
							"incrementalData_DF1 = DIMFinalJoinDF.withColumn(\"nk_hash\", md5(concat_ws(\"\", *natural_key)))\r\n",
							"col_list=[]\r\n",
							"for i in DIMFinalJoinDF.columns:\r\n",
							"    col_list.append(i)\r\n",
							"# Add a checsum column to help identify the changed rows    \r\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\r\n",
							"\r\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,((incrementalData_DF2.OPPORTUNITY_KEY == existingDataDF1.existing_OPPORTUNITY_KEY) & (incrementalData_DF2.BUSINESS_PARTNER_KEY == existingDataDF1.existing_BUSINESS_PARTNER_KEY) & (incrementalData_DF2.INFLUENCER_ROLE_CODE == existingDataDF1.existing_INFLUENCER_ROLE_CODE)), \"fullouter\") \r\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\r\n",
							"#fullJoin2.show()\r\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\r\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\r\n",
							"#fullJoin2.show()\r\n",
							"qry= \"\"\"\r\n",
							"INSERT INTO {}\r\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS BUSINESS_PARTNER_FACT_KEY,1 as BUSINESS_PARTNER_FACT_VERSION , \r\n",
							"A.OPPORTUNITY_KEY,A.BUSINESS_PARTNER_KEY,A.INFLUENCER_ROLE_CODE,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT,\r\n",
							"'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"LEFT JOIN etlhubConfirmed.opportunity_business_partner_daily_ft B\r\n",
							"ON A.OPPORTUNITY_KEY=B.OPPORTUNITY_KEY AND A.BUSINESS_PARTNER_KEY=B.BUSINESS_PARTNER_KEY AND A.INFLUENCER_ROLE_CODE=B.INFLUENCER_ROLE_CODE\r\n",
							"AND B.CURRENT_IND='Y'\r\n",
							"WHERE existing_REC_CHECKSUM is null\r\n",
							"AND COALESCE(B.REC_CHECKSUM,'') <> COALESCE(A.column_hash,'')\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry.format(tablename))\r\n",
							"\r\n",
							"qry1= \"\"\"\r\n",
							"INSERT INTO {} \r\n",
							"select existing_BUSINESS_PARTNER_FACT_KEY,1+BUSINESS_PARTNER_FACT_VERSION as BUSINESS_PARTNER_FACT_VERSION , \r\n",
							"A.OPPORTUNITY_KEY,A.BUSINESS_PARTNER_KEY,A.INFLUENCER_ROLE_CODE,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'U' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT ,\r\n",
							"'LG' AS DATA_IND, 'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"WHERE OPPORTUNITY_KEY = existing_OPPORTUNITY_KEY AND BUSINESS_PARTNER_KEY=existing_BUSINESS_PARTNER_KEY AND TRIM(INFLUENCER_ROLE_CODE)=TRIM(existing_INFLUENCER_ROLE_CODE) and LOWER(column_hash) <> LOWER(existing_REC_CHECKSUM)\r\n",
							"AND NOT EXISTS\r\n",
							"(SELECT 1 FROM etlhubConfirmed.opportunity_business_partner_daily_ft B\r\n",
							"WHERE A.OPPORTUNITY_KEY=B.OPPORTUNITY_KEY AND A.BUSINESS_PARTNER_KEY=B.BUSINESS_PARTNER_KEY AND A.INFLUENCER_ROLE_CODE=B.INFLUENCER_ROLE_CODE\r\n",
							"and b.CURRENT_IND='Y'\r\n",
							"AND COALESCE(A.column_hash,'')=COALESCE(B.REC_CHECKSUM,''))\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry1.format(tablename))\r\n",
							"\r\n",
							"qry2=\"\"\"\r\n",
							"MERGE INTO {} A\r\n",
							"USING fullJoin B\r\n",
							"ON --A.OPPORTUNITY_KEY=B.OPPORTUNITY_KEY AND A.BUSINESS_PARTNER_KEY=B.BUSINESS_PARTNER_KEY AND A.INFLUENCER_ROLE_CODE=B.INFLUENCER_ROLE_CODE\r\n",
							"and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\r\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\r\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\r\n",
							"    ,IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry2.format(tablename))\r\n",
							"#Soft deletes or no longer active in source\r\n",
							"qry3=\"\"\"\r\n",
							"MERGE INTO {} A\r\n",
							"USING fullJoin B\r\n",
							"ON OPPORTUNITY_KEY = existing_OPPORTUNITY_KEY AND BUSINESS_PARTNER_KEY=existing_BUSINESS_PARTNER_KEY AND TRIM(INFLUENCER_ROLE_CODE)=TRIM(existing_INFLUENCER_ROLE_CODE)\r\n",
							"AND B.BUSINESS_PARTNER_FACT_KEY is NULL\r\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\r\n",
							"    --,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\r\n",
							"    ,IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry3.format(tablename))\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from etlhubConfirmed.opportunity_business_partner_daily_ft"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pyspark.sql.functions as f\r\n",
							"natural_key=[BUSINESS_PARTNER_KEY,OPPORTUNITY_KEY,INFLUENCER_ROLE_CODE]\r\n",
							"logical_gate = f.lit()\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\r\n",
							"#incrementalData_DF2.show()\r\n",
							"\r\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\r\n",
							"\r\n",
							"#Create a deltalake table with necessary columns\r\n",
							"\r\n",
							"#incrementalData_DF2.show()\r\n",
							"\r\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\r\n",
							"#existingDF.createOrReplaceTempView('existingDF')\r\n",
							"#existingDataDF.show()\r\n",
							"\r\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\r\n",
							"#existingMaxKeyDF.show()\r\n",
							"\r\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\r\n",
							"\r\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\r\n",
							"#existingDataDF1.printSchema()\r\n",
							"\r\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\r\n",
							"existingDataDF1.show()\r\n",
							"\r\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.OPPORTUNITY_NUM == existingDataDF1.existing_OPPORTUNITY_NUM, \"fullouter\") \r\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\r\n",
							"\r\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\r\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\r\n",
							"#fullJoin2.show()\r\n",
							"qry= \"\"\"\r\n",
							"INSERT INTO {}\r\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS OPPORTUNITY_KEY,1 as OPPORTUNITY_VERSION , \r\n",
							"A.OPPORTUNITY_NUM,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT,\r\n",
							"A.ISA_CODE,A.ARCHIVE_REASON_CODE,A.BUSINESS_TRANSACTION_TYPE,A.CLIENT_REPRESENTATIVE_NAME,A.COMPETITOR_LIST, \r\n",
							"A.IDENTIFIER_NAME,A.OPPORTUNITY_NAME,A.OPPORTUNITY_SOURCE_CODE,A.SBS_SOL_VALID_IND, \r\n",
							"A.OPPORTUNITY_IDENTIFIER,A.ROGUE_IND,A.REASON_TO_ACT,A.SOLUTION_CATG,A.BRAND_SPONSOR_LIST,A.GBS_GEOGRAPHY_NAME, \r\n",
							"A.GBS_BUSINESS_UNIT_GROUP_NAME,A.GBS_BUSINESS_UNIT_NAME,A.TAG_LIST,A.OPPORTUNITY_LEGACY_NO,'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"LEFT JOIN etlhubConfirmed.opportunity B\r\n",
							"ON A.OPPORTUNITY_NUM=B.OPPORTUNITY_NUM\r\n",
							"AND CURRENT_IND='Y'\r\n",
							"WHERE existing_REC_CHECKSUM is null\r\n",
							"AND COALESCE(B.REC_CHECKSUM,'') <> COALESCE(A.column_hash,'')\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry.format(tablename))\r\n",
							"\r\n",
							"qry1= \"\"\"\r\n",
							"INSERT INTO {} \r\n",
							"select existing_OPPORTUNITY_KEY,1+existing_OPPORTUNITY_VERSION as OPPORTUNITY_VERSION , \r\n",
							"A.OPPORTUNITY_NUM,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'U' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT ,\r\n",
							"A.ISA_CODE,A.ARCHIVE_REASON_CODE,A.BUSINESS_TRANSACTION_TYPE,A.CLIENT_REPRESENTATIVE_NAME,A.COMPETITOR_LIST, \r\n",
							"A.IDENTIFIER_NAME,A.OPPORTUNITY_NAME,A.OPPORTUNITY_SOURCE_CODE,A.SBS_SOL_VALID_IND, \r\n",
							"A.OPPORTUNITY_IDENTIFIER,A.ROGUE_IND,A.REASON_TO_ACT,A.SOLUTION_CATG,A.BRAND_SPONSOR_LIST,A.GBS_GEOGRAPHY_NAME, \r\n",
							"A.GBS_BUSINESS_UNIT_GROUP_NAME,A.GBS_BUSINESS_UNIT_NAME,A.TAG_LIST,A.OPPORTUNITY_LEGACY_NO,\r\n",
							"'LG' AS DATA_IND, 'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"WHERE LOWER(OPPORTUNITY_NUM) = LOWER(existing_OPPORTUNITY_NUM) and LOWER(column_hash) <> LOWER(existing_REC_CHECKSUM)\r\n",
							"AND NOT EXISTS\r\n",
							"(SELECT 1 FROM etlhubConfirmed.opportunity B\r\n",
							"WHERE A.OPPORTUNITY_NUM=B.OPPORTUNITY_NUM\r\n",
							"and b.CURRENT_IND='Y'\r\n",
							"AND A.column_hash=B.REC_CHECKSUM)\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry1.format(tablename))\r\n",
							"\r\n",
							"qry2=\"\"\"\r\n",
							"MERGE INTO {} A\r\n",
							"USING fullJoin B\r\n",
							"ON A.OPPORTUNITY_NUM = B.OPPORTUNITY_NUM\r\n",
							"AND LOWER(B.OPPORTUNITY_NUM) = LOWER(B.existing_OPPORTUNITY_NUM) and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\r\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\r\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\r\n",
							"    ,IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry2.format(tablename))\r\n",
							"#Soft deletes or no longer active in source\r\n",
							"qry3=\"\"\"\r\n",
							"MERGE INTO {} A\r\n",
							"USING fullJoin B\r\n",
							"ON A.OPPORTUNITY_NUM = B.existing_OPPORTUNITY_NUM\r\n",
							"AND B.OPPORTUNITY_NUM is NULL\r\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\r\n",
							"    --,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\r\n",
							"    ,IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry3.format(tablename))\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from etlhubConfirmed.opportunity where OPPORTUNITY_NUM = '00-0JWAXDR'\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--INSERTS OR NEW ROWS\r\n",
							"select 'New Rows for Insert After Update' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS BUSINESS_PARTNER_KEY1, A.* from fullJoin A WHERE existing_BUSINESS_PARTNER_KEY is null;\r\n",
							"--UPDATE ROWS\r\n",
							"select 'Changed Rows for Update/Insert After Update' as Title, a.* from fullJoin a WHERE LOWER(BUSINESS_PARTNER_ID) = LOWER(existing_BUSINESS_PARTNER_ID) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/xfrm_BP_deltalake_scd_type2_with_parameters')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "56ac7253-7a46-47ec-b49f-f3cd1ec994f1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"#import necessary python libraries\r\n",
							"\r\n",
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"from pyspark.sql.functions import md5, concat_ws\r\n",
							"from sqlite3 import connect\r\n",
							"from pyspark.sql import functions as F\r\n",
							"#conn = connect(':memory:')\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession \r\n",
							"from pyspark.sql.types import * \r\n",
							"from delta.tables import *\r\n",
							"#import os\r\n",
							"import sys\r\n",
							"\r\n",
							"#Read data from adls csv file extracted from source at https://adls4fsoetlhubdevuseast.blob.core.windows.net/customer/BUSPARTNER_DIM.csv\r\n",
							"\r\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \r\n",
							"container_name = 'customer' # fill in your container name \r\n",
							"relative_path = '' # fill in your relative folder path \r\n",
							"\r\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \r\n",
							"print('Primary storage account path: ' + adls_path) \r\n",
							"\r\n",
							"# Read a csv file \r\n",
							"csv_path = adls_path + 'BUSPARTNER_DIM_UPD.csv' \r\n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\r\n",
							"\r\n",
							"tablename=\"etlhubConfirmed.business_partner\"\r\n",
							"natural_key=\"BUSINESS_PARTNER_ID\"\r\n",
							"keycolumn=\"BUSINESS_PARTNER_KEY\"\r\n",
							"#columns1 = [\"BUS_PARTNER_NM\"]\r\n",
							"#columnsDF=spark.createDataFrame(incrementalData_DF)\r\n",
							"col_list=[]\r\n",
							"for i in incrementalData_DF.columns:\r\n",
							"    col_list.append(i)\r\n",
							"#incrementalData_DF.show()\r\n",
							"#print (col_list)\r\n",
							"\r\n",
							"# Add a checsum column to help identify the changed rows\r\n",
							"\r\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\r\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\r\n",
							"#incrementalData_DF2.show()\r\n",
							"\r\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\r\n",
							"\r\n",
							"#Create a deltalake table with necessary columns\r\n",
							"\r\n",
							"#incrementalData_DF2.show()\r\n",
							"\r\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\r\n",
							"#existingDF.createOrReplaceTempView('existingDF')\r\n",
							"#existingDataDF.show()\r\n",
							"\r\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\r\n",
							"#existingMaxKeyDF.show()\r\n",
							"\r\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\r\n",
							"\r\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\r\n",
							"#existingDataDF1.printSchema()\r\n",
							"\r\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\r\n",
							"#existingDataDF1.show()\r\n",
							"\r\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.BUSINESS_PARTNER_ID == existingDataDF1.existing_BUSINESS_PARTNER_ID, \"fullouter\") \r\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\r\n",
							"\r\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\r\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\r\n",
							"#fullJoin2.show()\r\n",
							"qry= \"\"\"\r\n",
							"INSERT INTO {}\r\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS BUSINESS_PARTNER_KEY,1 as BUSINESS_PARTNER_VERSION , \r\n",
							"A.BUSINESS_PARTNER_ID,A.BUS_PARTNER_NM,'' as AGR_BUS_PARTNER_ID,'' as AGR_BUS_PARTNER_NM,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'BMSIW' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT,'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"LEFT JOIN etlhubConfirmed.BUSINESS_PARTNER B\r\n",
							"ON A.BUSINESS_PARTNER_ID=B.BUSINESS_PARTNER_ID\r\n",
							"AND CURRENT_IND='Y'\r\n",
							"WHERE existing_REC_CHECKSUM is null\r\n",
							"AND COALESCE(B.REC_CHECKSUM,'') <> COALESCE(A.column_hash,'')\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry.format(tablename))\r\n",
							"\r\n",
							"qry1= \"\"\"\r\n",
							"INSERT INTO {} \r\n",
							"select existing_BUSINESS_PARTNER_KEY,1+existing_BUSINESS_PARTNER_VERSION as BUSINESS_PARTNER_VERSION , \r\n",
							"A.BUSINESS_PARTNER_ID,A.BUS_PARTNER_NM,'' as AGR_BUS_PARTNER_ID,'' as AGR_BUS_PARTNER_NM,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'BMSIW' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'U' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT , 'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"WHERE LOWER(BUSINESS_PARTNER_ID) = LOWER(existing_BUSINESS_PARTNER_ID) and LOWER(column_hash) <> LOWER(existing_REC_CHECKSUM)\r\n",
							"AND NOT EXISTS\r\n",
							"(SELECT 1 FROM etlhubConfirmed.BUSINESS_PARTNER B\r\n",
							"WHERE A.BUSINESS_PARTNER_ID=B.BUSINESS_PARTNER_ID\r\n",
							"and b.CURRENT_IND='Y'\r\n",
							"AND A.column_hash=B.REC_CHECKSUM)\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry1.format(tablename))\r\n",
							"\r\n",
							"qry2=\"\"\"\r\n",
							"MERGE INTO {} A\r\n",
							"USING fullJoin B\r\n",
							"ON A.BUSINESS_PARTNER_ID = B.BUSINESS_PARTNER_ID\r\n",
							"AND LOWER(B.BUSINESS_PARTNER_ID) = LOWER(B.existing_BUSINESS_PARTNER_ID) and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\r\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\r\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\r\n",
							"    ,IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry2.format(tablename))\r\n",
							"#Soft deletes or no longer active in source\r\n",
							"qry3=\"\"\"\r\n",
							"MERGE INTO {} A\r\n",
							"USING fullJoin B\r\n",
							"ON A.BUSINESS_PARTNER_ID = B.existing_BUSINESS_PARTNER_ID\r\n",
							"AND B.BUSINESS_PARTNER_ID is NULL\r\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\r\n",
							"    --,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\r\n",
							"    ,IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry3.format(tablename))\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--INSERTS OR NEW ROWS\r\n",
							"select 'New Rows for Insert After Update' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS BUSINESS_PARTNER_KEY1, A.* from fullJoin A WHERE existing_BUSINESS_PARTNER_KEY is null;\r\n",
							"--UPDATE ROWS\r\n",
							"select 'Changed Rows for Update/Insert After Update' as Title, a.* from fullJoin a WHERE LOWER(BUSINESS_PARTNER_ID) = LOWER(existing_BUSINESS_PARTNER_ID) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql \r\n",
							"select * from etlhubConfirmed.BUSINESS_PARTNER\r\n",
							"where current_ind = 'N'    --BUSINESS_PARTNER_ID = 'RAUE000167'\r\n",
							"--update etlhubConfirmed.BUSINESS_PARTNER\r\n",
							"--set BUS_PARTNER_NM = 'Updated with Azure'\r\n",
							"--where BUSINESS_PARTNER_ID = 'SUSE087302'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 27
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/xfrm_OPPORTUNITY_deltalake_scd_type2_with_parameters')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "231a7467-f64c-45d1-9193-7483f607d81e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"#import necessary python libraries\r\n",
							"\r\n",
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"from pyspark.sql.functions import md5, concat_ws\r\n",
							"from sqlite3 import connect\r\n",
							"from pyspark.sql import functions as F\r\n",
							"#conn = connect(':memory:')\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession \r\n",
							"from pyspark.sql.types import * \r\n",
							"from delta.tables import *\r\n",
							"#import os\r\n",
							"import sys\r\n",
							"\r\n",
							"#Read data from adls csv file extracted from source at https://adls4fsoetlhubdevuseast.blob.core.windows.net/customer/OPPORTUNITY.csv\r\n",
							"\r\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \r\n",
							"container_name = 'customer' # fill in your container name \r\n",
							"relative_path = '' # fill in your relative folder path \r\n",
							"\r\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \r\n",
							"print('Primary storage account path: ' + adls_path) \r\n",
							"\r\n",
							"# Read a csv file \r\n",
							"csv_path = adls_path + 'OPPORTUNITY_UPD.csv' \r\n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\r\n",
							"\r\n",
							"tablename=\"etlhubConfirmed.opportunity\"\r\n",
							"natural_key=\"OPPORTUNITY_NUM\"\r\n",
							"keycolumn=\"OPPORTUNITY_KEY\"\r\n",
							"\r\n",
							"col_list=[]\r\n",
							"for i in incrementalData_DF.columns:\r\n",
							"    col_list.append(i)\r\n",
							"#incrementalData_DF.show()\r\n",
							"#print (col_list)\r\n",
							"\r\n",
							"# Add a checsum column to help identify the changed rows\r\n",
							"\r\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\r\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\r\n",
							"#incrementalData_DF2.show()\r\n",
							"\r\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\r\n",
							"\r\n",
							"#Create a deltalake table with necessary columns\r\n",
							"\r\n",
							"#incrementalData_DF2.show()\r\n",
							"\r\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\r\n",
							"#existingDF.createOrReplaceTempView('existingDF')\r\n",
							"#existingDataDF.show()\r\n",
							"\r\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\r\n",
							"#existingMaxKeyDF.show()\r\n",
							"\r\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\r\n",
							"\r\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\r\n",
							"#existingDataDF1.printSchema()\r\n",
							"\r\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\r\n",
							"#existingDataDF1.show()\r\n",
							"\r\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.OPPORTUNITY_NUM == existingDataDF1.existing_OPPORTUNITY_NUM, \"fullouter\") \r\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\r\n",
							"\r\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\r\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\r\n",
							"#fullJoin2.show()\r\n",
							"qry= \"\"\"\r\n",
							"INSERT INTO {}\r\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS OPPORTUNITY_KEY,1 as OPPORTUNITY_VERSION , \r\n",
							"A.OPPORTUNITY_NUM,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT,\r\n",
							"A.ISA_CODE,A.ARCHIVE_REASON_CODE,A.BUSINESS_TRANSACTION_TYPE,A.CLIENT_REPRESENTATIVE_NAME,A.COMPETITOR_LIST, \r\n",
							"A.IDENTIFIER_NAME,A.OPPORTUNITY_NAME,A.OPPORTUNITY_SOURCE_CODE,A.SBS_SOL_VALID_IND, \r\n",
							"A.OPPORTUNITY_IDENTIFIER,A.ROGUE_IND,A.REASON_TO_ACT,A.SOLUTION_CATG,A.BRAND_SPONSOR_LIST,A.GBS_GEOGRAPHY_NAME, \r\n",
							"A.GBS_BUSINESS_UNIT_GROUP_NAME,A.GBS_BUSINESS_UNIT_NAME,A.TAG_LIST,A.OPPORTUNITY_LEGACY_NO,'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"LEFT JOIN etlhubConfirmed.opportunity B\r\n",
							"ON A.OPPORTUNITY_NUM=B.OPPORTUNITY_NUM\r\n",
							"AND CURRENT_IND='Y'\r\n",
							"WHERE existing_REC_CHECKSUM is null\r\n",
							"AND COALESCE(B.REC_CHECKSUM,'') <> COALESCE(A.column_hash,'')\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry.format(tablename))\r\n",
							"\r\n",
							"qry1= \"\"\"\r\n",
							"INSERT INTO {} \r\n",
							"select existing_OPPORTUNITY_KEY,1+existing_OPPORTUNITY_VERSION as OPPORTUNITY_VERSION , \r\n",
							"A.OPPORTUNITY_NUM,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'U' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT ,\r\n",
							"A.ISA_CODE,A.ARCHIVE_REASON_CODE,A.BUSINESS_TRANSACTION_TYPE,A.CLIENT_REPRESENTATIVE_NAME,A.COMPETITOR_LIST, \r\n",
							"A.IDENTIFIER_NAME,A.OPPORTUNITY_NAME,A.OPPORTUNITY_SOURCE_CODE,A.SBS_SOL_VALID_IND, \r\n",
							"A.OPPORTUNITY_IDENTIFIER,A.ROGUE_IND,A.REASON_TO_ACT,A.SOLUTION_CATG,A.BRAND_SPONSOR_LIST,A.GBS_GEOGRAPHY_NAME, \r\n",
							"A.GBS_BUSINESS_UNIT_GROUP_NAME,A.GBS_BUSINESS_UNIT_NAME,A.TAG_LIST,A.OPPORTUNITY_LEGACY_NO,\r\n",
							"'LG' AS DATA_IND, 'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"WHERE LOWER(OPPORTUNITY_NUM) = LOWER(existing_OPPORTUNITY_NUM) and LOWER(column_hash) <> LOWER(existing_REC_CHECKSUM)\r\n",
							"AND NOT EXISTS\r\n",
							"(SELECT 1 FROM etlhubConfirmed.opportunity B\r\n",
							"WHERE A.OPPORTUNITY_NUM=B.OPPORTUNITY_NUM\r\n",
							"and b.CURRENT_IND='Y'\r\n",
							"AND A.column_hash=B.REC_CHECKSUM)\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry1.format(tablename))\r\n",
							"\r\n",
							"qry2=\"\"\"\r\n",
							"MERGE INTO {} A\r\n",
							"USING fullJoin B\r\n",
							"ON A.OPPORTUNITY_NUM = B.OPPORTUNITY_NUM\r\n",
							"AND LOWER(B.OPPORTUNITY_NUM) = LOWER(B.existing_OPPORTUNITY_NUM) and LOWER(B.column_hash) <> LOWER(B.existing_rec_checksum)\r\n",
							"AND A.CURRENT_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET CURRENT_IND='N'\r\n",
							"    ,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\r\n",
							"    ,IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry2.format(tablename))\r\n",
							"#Soft deletes or no longer active in source\r\n",
							"qry3=\"\"\"\r\n",
							"MERGE INTO {} A\r\n",
							"USING fullJoin B\r\n",
							"ON A.OPPORTUNITY_NUM = B.existing_OPPORTUNITY_NUM\r\n",
							"AND B.OPPORTUNITY_NUM is NULL\r\n",
							"AND A.CURRENT_IND='Y' AND A.ACTIVE_IN_SOURCE_IND='Y' AND A.REC_START_DT=b.existing_REC_START_DT\r\n",
							"WHEN MATCHED THEN UPDATE SET ACTIVE_IN_SOURCE_IND='N'\r\n",
							"    --,REC_END_DT= existing_REC_START_DT -  INTERVAL 5 seconds --current_timestamp --existing_REC_START_DT-1\r\n",
							"    ,IMG_LST_UPD_DT=current_timestamp;\r\n",
							"\"\"\"\r\n",
							"spark.sql(qry3.format(tablename))\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from etlhubConfirmed.opportunity where OPPORTUNITY_NUM = '00-0JWAXDR'\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--INSERTS OR NEW ROWS\r\n",
							"select 'New Rows for Insert After Update' as Title, COALESCE(existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS BUSINESS_PARTNER_KEY1, A.* from fullJoin A WHERE existing_BUSINESS_PARTNER_KEY is null;\r\n",
							"--UPDATE ROWS\r\n",
							"select 'Changed Rows for Update/Insert After Update' as Title, a.* from fullJoin a WHERE LOWER(BUSINESS_PARTNER_ID) = LOWER(existing_BUSINESS_PARTNER_ID) and LOWER(column_hash) <> LOWER(existing_rec_checksum);\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/xfrm_deltalake_Customer_Dimension')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Siva"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7b237b58-edc8-4cb6-a618-04e511f508bd"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"SCD Type2 using adls as source and delta lake as target"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#import necessary python libraries\n",
							"\n",
							"from azure.storage.blob import BlobClient\n",
							"import pandas as pd\n",
							"from io import StringIO\n",
							"from pyspark.sql.functions import md5, concat_ws\n",
							"from sqlite3 import connect\n",
							"from pyspark.sql import functions as F\n",
							"#conn = connect(':memory:')\n",
							"\n",
							"from pyspark.sql import SparkSession \n",
							"from pyspark.sql.types import * \n",
							"from delta.tables import *\n",
							"#import os\n",
							"import sys\n",
							"\n",
							"#Read data from adls csv file extracted from source at https://adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/data/customer/\n",
							"\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \n",
							"container_name = 'customer' # fill in your container name \n",
							"relative_path = '' # fill in your relative folder path \n",
							"\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \n",
							"print('Primary storage account path: ' + adls_path) \n",
							"\n",
							"# Read a csv file \n",
							"csv_path = adls_path + 'customer_data.csv' \n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\n",
							"\n",
							"natural_key=\"CUSTOMER_NO\"\n",
							"#columns1 = [\"FINANCIAL_COUNTRY_CD\",\"CUSTOMER_DESC\",\"GBG_ID\"]\n",
							"\n",
							"# Get column list for creating Rec_Checksum\n",
							"\n",
							"#Add checks for duplicate data, Check for 1. dups in source data based on natural Key and\n",
							"# 2. Dups in target already\n",
							"\n",
							"col_list=[]\n",
							"for i in incrementalData_DF.columns:\n",
							"    col_list.append(i)\n",
							"    #print (col_list)\n",
							"\n",
							"# Add a checsum column to help identify the changed rows\n",
							"\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\n",
							"\n",
							"#sell_cyclecolhashDF.show()\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\n",
							"\n",
							"#Create a deltalake table with necessary columns\n",
							"\n",
							"#incrementalData_DF2.show()\n",
							"\n",
							"existingDataDF=spark.sql(\"SELECT tgt.* from etlhubConfirmed.customer_dimension tgt WHERE CURRENT_IND='Y'\")\n",
							"#existingDF.createOrReplaceTempView('existingDF')\n",
							"\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX(CUSTOMER_KEY) existing_MAX_KEY from etlhubConfirmed.customer_dimension tgt WHERE CURRENT_IND='Y'\")\n",
							"\n",
							"\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\n",
							"\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\n",
							"\n",
							"#existingDataDF1.printSchema()\n",
							"\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\n",
							"\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_NO, \"fullouter\") \n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\n",
							"\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\n",
							"\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\n",
							"\n",
							"qry= \"\"\"\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"AND CURRENT_IND='Y'\n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND B.REC_CHECKSUM <> A.column_hash\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry1= \"\"\"\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \n",
							"from fullJoin A \n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\n",
							"AND CURRENT_IND='Y'\n",
							"WHERE existing_REC_CHECKSUM is null\n",
							"AND B.REC_CHECKSUM <> A.column_hash\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"qry3=\"\"\"\n",
							"SELECT COUNT(*) as CNT, CUSTOMER_NO FROM incrementalData_DF2 GROUP BY CUSTOMER_NO HAVING COUNT(*)>1\n",
							";\n",
							"\"\"\"\n",
							"\n",
							"df3=spark.sql(qry3)\n",
							"cnt1=df3.count()\n",
							"\n",
							"print (cnt1)\n",
							"if cnt1 == 0:\n",
							"    print(\"No Duplicates in source data\")\n",
							"    status = 'success'\n",
							"else:\n",
							"    print(\"Below are the duplicates:\")\n",
							"    df3.show()\n",
							"    status = 'fail'\n",
							"    #os.abort() this will take the spark cluster also down\n",
							"    sys.exit(1)\n",
							"    print(\"This will not be printed\")\n",
							"print (\"this will not be printed either\")\n",
							"\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\n",
							"\n",
							"#try:\n",
							"#  sqlContext.sql(\"create table {}.`{}` as select * from mytempTable\".format(hivedb,table))\n",
							"#except:\n",
							"#   status = 'fail'\n",
							"\n",
							"#assert status == 'success', 'status should be success'\n",
							"\n",
							"#a=spark.sql(qry)\n",
							"\n",
							"#print( df3. || ' Duplicate found ')\n",
							"\n",
							"#a.num_affected_rows\n",
							"#print(numOutputRows)\n",
							"\n",
							"\n",
							"#deltaTable1 = DeltaTable.forPath(spark, 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2')\n",
							"\n",
							"#deltaTable = DeltaTable.forName(spark, 'etlhubConfirmed.customer_dimension')\n",
							"\n",
							"\n",
							"#fullHistoryDF = deltaTable.history()    # get the full history of the table\n",
							"\n",
							"#lastOperationDF = deltaTable.history(1) # get the last operation\n",
							"\n",
							"#print(lastOperationDF.operationMetrics)\n",
							"\n",
							"#lastOperationDF.show()\n",
							"\n",
							"#fullHistoryDF.show()\n",
							"\n",
							"#print(num_affected_rows)\n",
							"\n",
							"#print(num_inserted_rows)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"\n",
							"SELECT COUNT(*) as CNT, CUSTOMER_NO FROM incrementalData_DF2 GROUP BY CUSTOMER_NO HAVING COUNT(*)>1;"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/xfrm_deltalake_scd_type2_with_parameters')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Srilatha"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sPoolKyn001494",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "741bc7ae-a6b3-4c2f-9c88-e8060c51b585"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/eb01b8c2-e8d1-4c19-9788-5469f1f3fd10/resourceGroups/rg-kyn-001494-dev-eus-001/providers/Microsoft.Synapse/workspaces/asa-kyn-001494-dev-eus-001/bigDataPools/sPoolKyn001494",
						"name": "sPoolKyn001494",
						"type": "Spark",
						"endpoint": "https://asa-kyn-001494-dev-eus-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sPoolKyn001494",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"#import necessary python libraries\r\n",
							"\r\n",
							"from azure.storage.blob import BlobClient\r\n",
							"import pandas as pd\r\n",
							"from io import StringIO\r\n",
							"from pyspark.sql.functions import md5, concat_ws\r\n",
							"from sqlite3 import connect\r\n",
							"from pyspark.sql import functions as F\r\n",
							"#conn = connect(':memory:')\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession \r\n",
							"from pyspark.sql.types import * \r\n",
							"from delta.tables import *\r\n",
							"#import os\r\n",
							"import sys\r\n",
							"\r\n",
							"#Read data from adls csv file extracted from source at https://adls4fsoetlhubdevuseast.dfs.core.windows.net/deltalake/data/customer/\r\n",
							"\r\n",
							"account_name = 'adls4fsoetlhubdevuseast' # fill in your primary account name \r\n",
							"container_name = 'customer' # fill in your container name \r\n",
							"relative_path = '' # fill in your relative folder path \r\n",
							"\r\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \r\n",
							"print('Primary storage account path: ' + adls_path) \r\n",
							"\r\n",
							"# Read a csv file \r\n",
							"csv_path = adls_path + 'customer_data.csv' \r\n",
							"incrementalData_DF = spark.read.csv(csv_path, header = 'true')\r\n",
							"\r\n",
							"tablename=\"etlhubConfirmed.customer_dimension\"\r\n",
							"natural_key=\"CUSTOMER_NO\"\r\n",
							"keycolumn=\"CUSTOMER_KEY\"\r\n",
							"#columns1 = [\"FINANCIAL_COUNTRY_CD\",\"CUSTOMER_DESC\",\"GBG_ID\"]\r\n",
							"\r\n",
							"# Get column list for creating Rec_Checksum\r\n",
							"\r\n",
							"#Add checks for duplicate data, Check for 1. dups in source data based on natural Key and\r\n",
							"# 2. Dups in target already\r\n",
							"\r\n",
							"col_list=[]\r\n",
							"for i in incrementalData_DF.columns:\r\n",
							"    col_list.append(i)\r\n",
							"    #print (col_list)\r\n",
							"\r\n",
							"# Add a checsum column to help identify the changed rows\r\n",
							"\r\n",
							"incrementalData_DF1 = incrementalData_DF.withColumn(\"nk_hash\",md5(natural_key))\r\n",
							"incrementalData_DF2 = incrementalData_DF1.withColumn(\"column_hash\", md5(concat_ws(\"\", *col_list)))\r\n",
							"\r\n",
							"#sell_cyclecolhashDF.show()\r\n",
							"incrementalData_DF2.createOrReplaceTempView(\"incrementalData_DF2\")\r\n",
							"\r\n",
							"#Create a deltalake table with necessary columns\r\n",
							"\r\n",
							"#incrementalData_DF2.show()\r\n",
							"\r\n",
							"#existingDataDF=spark.sql(\"SELECT * from $table_name tgt WHERE CURRENT_IND='Y'\")\r\n",
							"existingDataDF=spark.sql(\"SELECT * FROM {}  WHERE CURRENT_IND='Y'\".format(tablename))\r\n",
							"#existingDF.createOrReplaceTempView('existingDF')\r\n",
							"#existingDataDF.show()\r\n",
							"\r\n",
							"existingMaxKeyDF=spark.sql(\"SELECT MAX({}) existing_MAX_KEY from {} WHERE CURRENT_IND='Y'\".format(keycolumn,tablename))\r\n",
							"#existingMaxKeyDF.show()\r\n",
							"\r\n",
							"# prefix all columns from target table with 'existing_'. This will help to differentiate columns when incremental and existing DF's are joined\r\n",
							"\r\n",
							"existingDataDF1 = existingDataDF.select([F.col(c).alias('existing_'+c) for c in existingDataDF.columns])\r\n",
							"#existingDataDF1.printSchema()\r\n",
							"\r\n",
							"existingDataDF1.createOrReplaceTempView('existingDataDF1')\r\n",
							"\r\n",
							"fullJoin1=incrementalData_DF2.join(existingDataDF1,incrementalData_DF2.CUSTOMER_NO == existingDataDF1.existing_CUSTOMER_NO, \"fullouter\") \r\n",
							"fullJoin2=fullJoin1.join(existingMaxKeyDF,None,\"CROSS\")\r\n",
							"\r\n",
							"fullJoin2.createOrReplaceTempView('fullJoin')\r\n",
							"#Insert for New rows which are missing in target and present in source based on Natural Key.\r\n",
							"\r\n",
							"qry= \"\"\"\r\n",
							"INSERT INTO etlhubConfirmed.customer_dimension \r\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \r\n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\r\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\r\n",
							"AND CURRENT_IND='Y'\r\n",
							"WHERE existing_REC_CHECKSUM is null\r\n",
							"AND B.REC_CHECKSUM <> A.column_hash\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"\r\n",
							"qry1= \"\"\"\r\n",
							"select COALESCE(A.existing_MAX_KEY,0) + ROW_NUMBER () OVER (ORDER BY 1) AS CUSTOMER_KEY,1 as VERSION , \r\n",
							"A.CUSTOMER_NO,A.FINANCIAL_COUNTRY_CD,A.GBG_ID,A.CUSTOMER_NAME,'Y' AS CURRENT_IND, CURRENT_TIMESTAMP AS EXTRACT_DT, \r\n",
							"CURRENT_TIMESTAMP AS REC_START_DT, '9999-12-31 00:00:00.000' as REC_END_DT, 'ESA' AS SOURCE_SYSTEM, A.column_hash as REC_CHECKSUM, \r\n",
							"'I' as REC_STATUS,current_timestamp as IMG_LST_UPD_DT, CURRENT_TIMESTAMP AS IMG_CREATED_DT, 'LG' AS DATA_IND, \r\n",
							"'Y' AS ACTIVE_IN_SOURCE_IND \r\n",
							"from fullJoin A \r\n",
							"LEFT JOIN etlhubConfirmed.customer_dimension B\r\n",
							"ON A.CUSTOMER_NO=B.CUSTOMER_NO\r\n",
							"AND CURRENT_IND='Y'\r\n",
							"WHERE existing_REC_CHECKSUM is null\r\n",
							"AND B.REC_CHECKSUM <> A.column_hash\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"\r\n",
							"qry3=\"\"\"\r\n",
							"SELECT COUNT(*) as CNT, CUSTOMER_NO FROM incrementalData_DF2 GROUP BY CUSTOMER_NO HAVING COUNT(*)>1\r\n",
							";\r\n",
							"\"\"\"\r\n",
							"\r\n",
							"df3=spark.sql(qry3)\r\n",
							"cnt1=df3.count()\r\n",
							"\r\n",
							"print (cnt1)\r\n",
							"if cnt1 == 0:\r\n",
							"    print(\"No Duplicates in source data\")\r\n",
							"    status = 'success'\r\n",
							"else:\r\n",
							"    print(\"Below are the duplicates:\")\r\n",
							"    df3.show()\r\n",
							"    status = 'fail'\r\n",
							"    #os.abort() this will take the spark cluster also down\r\n",
							"    sys.exit(1)\r\n",
							"    print(\"This will not be printed\")\r\n",
							"print (\"this will not be printed either\")\r\n",
							"\r\n",
							"#Below code can be used to evaluate if the DMLs are successful or not Exception handling purpose\r\n",
							"#try:\r\n",
							"#  sqlContext.sql(\"create table {}.`{}` as select * from mytempTable\".format(hivedb,table))\r\n",
							"#except:\r\n",
							"#   status = 'fail'\r\n",
							"\r\n",
							"#assert status == 'success', 'status should be success'\r\n",
							"\r\n",
							"#a=spark.sql(qry)\r\n",
							"\r\n",
							"#print( df3. || ' Duplicate found ')\r\n",
							"\r\n",
							"#a.num_affected_rows\r\n",
							"#print(numOutputRows)\r\n",
							"\r\n",
							"\r\n",
							"#deltaTable1 = DeltaTable.forPath(spark, 'abfss://deltalake@adls4fsoetlhubdevuseast.dfs.core.windows.net/data/customer/customer_dimension2')\r\n",
							"\r\n",
							"#deltaTable = DeltaTable.forName(spark, 'etlhubConfirmed.customer_dimension')\r\n",
							"\r\n",
							"\r\n",
							"#fullHistoryDF = deltaTable.history()    # get the full history of the table\r\n",
							"\r\n",
							"#lastOperationDF = deltaTable.history(1) # get the last operation\r\n",
							"\r\n",
							"#print(lastOperationDF.operationMetrics)\r\n",
							"\r\n",
							"#lastOperationDF.show()\r\n",
							"\r\n",
							"#fullHistoryDF.show()\r\n",
							"\r\n",
							"#print(num_affected_rows)\r\n",
							"\r\n",
							"#print(num_inserted_rows)\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sPoolKyn001494')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 30
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sPool001494New')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 30
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 15,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "XXXLarge",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.2",
				"isComputeIsolationEnabled": true,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dsqlpoolKyn001494DevEtlHubEUS001')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		}
	]
}